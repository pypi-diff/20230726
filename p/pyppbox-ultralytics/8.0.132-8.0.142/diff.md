# Comparing `tmp/pyppbox_ultralytics-8.0.132-py3-none-any.whl.zip` & `tmp/pyppbox_ultralytics-8.0.142-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,168 +1,151 @@
-Zip file size: 623254 bytes, number of entries: 166
--rw-r--r--  2.0 unx      549 b- defN 23-Jul-11 14:35 ultralytics/__init__.py
--rw-r--r--  2.0 unx   137419 b- defN 23-Jul-11 14:35 ultralytics/assets/bus.jpg
--rw-r--r--  2.0 unx    50427 b- defN 23-Jul-11 14:35 ultralytics/assets/zidane.jpg
--rw-r--r--  2.0 unx     2751 b- defN 23-Jul-11 14:35 ultralytics/datasets/Argoverse.yaml
--rw-r--r--  2.0 unx     1986 b- defN 23-Jul-11 14:35 ultralytics/datasets/GlobalWheat2020.yaml
--rw-r--r--  2.0 unx    42439 b- defN 23-Jul-11 14:35 ultralytics/datasets/ImageNet.yaml
--rw-r--r--  2.0 unx     9270 b- defN 23-Jul-11 14:35 ultralytics/datasets/Objects365.yaml
--rw-r--r--  2.0 unx     2437 b- defN 23-Jul-11 14:35 ultralytics/datasets/SKU-110K.yaml
--rw-r--r--  2.0 unx     3512 b- defN 23-Jul-11 14:35 ultralytics/datasets/VOC.yaml
--rw-r--r--  2.0 unx     3012 b- defN 23-Jul-11 14:35 ultralytics/datasets/VisDrone.yaml
--rw-r--r--  2.0 unx     1547 b- defN 23-Jul-11 14:35 ultralytics/datasets/coco-pose.yaml
--rw-r--r--  2.0 unx     2526 b- defN 23-Jul-11 14:35 ultralytics/datasets/coco.yaml
--rw-r--r--  2.0 unx     1862 b- defN 23-Jul-11 14:35 ultralytics/datasets/coco128-seg.yaml
--rw-r--r--  2.0 unx     1846 b- defN 23-Jul-11 14:35 ultralytics/datasets/coco128.yaml
--rw-r--r--  2.0 unx      895 b- defN 23-Jul-11 14:35 ultralytics/datasets/coco8-pose.yaml
--rw-r--r--  2.0 unx     1797 b- defN 23-Jul-11 14:35 ultralytics/datasets/coco8-seg.yaml
--rw-r--r--  2.0 unx     1777 b- defN 23-Jul-11 14:35 ultralytics/datasets/coco8.yaml
--rw-r--r--  2.0 unx     5178 b- defN 23-Jul-11 14:35 ultralytics/datasets/xView.yaml
--rw-r--r--  2.0 unx     4408 b- defN 23-Jul-11 14:35 ultralytics/hub/__init__.py
--rw-r--r--  2.0 unx     5209 b- defN 23-Jul-11 14:35 ultralytics/hub/auth.py
--rw-r--r--  2.0 unx     8479 b- defN 23-Jul-11 14:35 ultralytics/hub/session.py
--rw-r--r--  2.0 unx     9314 b- defN 23-Jul-11 14:35 ultralytics/hub/utils.py
--rw-r--r--  2.0 unx     1970 b- defN 23-Jul-11 14:35 ultralytics/models/rt-detr/rtdetr-l.yaml
--rw-r--r--  2.0 unx     2177 b- defN 23-Jul-11 14:35 ultralytics/models/rt-detr/rtdetr-x.yaml
--rw-r--r--  2.0 unx     1550 b- defN 23-Jul-11 14:35 ultralytics/models/v3/yolov3-spp.yaml
--rw-r--r--  2.0 unx     1252 b- defN 23-Jul-11 14:35 ultralytics/models/v3/yolov3-tiny.yaml
--rw-r--r--  2.0 unx     1537 b- defN 23-Jul-11 14:35 ultralytics/models/v3/yolov3.yaml
--rw-r--r--  2.0 unx     1923 b- defN 23-Jul-11 14:35 ultralytics/models/v5/yolov5-p6.yaml
--rw-r--r--  2.0 unx     1550 b- defN 23-Jul-11 14:35 ultralytics/models/v5/yolov5.yaml
--rw-r--r--  2.0 unx     1735 b- defN 23-Jul-11 14:35 ultralytics/models/v6/yolov6.yaml
--rw-r--r--  2.0 unx      920 b- defN 23-Jul-11 14:35 ultralytics/models/v8/yolov8-cls.yaml
--rw-r--r--  2.0 unx     1751 b- defN 23-Jul-11 14:35 ultralytics/models/v8/yolov8-p2.yaml
--rw-r--r--  2.0 unx     1856 b- defN 23-Jul-11 14:35 ultralytics/models/v8/yolov8-p6.yaml
--rw-r--r--  2.0 unx     1946 b- defN 23-Jul-11 14:35 ultralytics/models/v8/yolov8-pose-p6.yaml
--rw-r--r--  2.0 unx     1580 b- defN 23-Jul-11 14:35 ultralytics/models/v8/yolov8-pose.yaml
--rw-r--r--  2.0 unx     1920 b- defN 23-Jul-11 14:35 ultralytics/models/v8/yolov8-rtdetr.yaml
--rw-r--r--  2.0 unx     1490 b- defN 23-Jul-11 14:35 ultralytics/models/v8/yolov8-seg.yaml
--rw-r--r--  2.0 unx     1913 b- defN 23-Jul-11 14:35 ultralytics/models/v8/yolov8.yaml
--rw-r--r--  2.0 unx      555 b- defN 23-Jul-11 14:35 ultralytics/nn/__init__.py
--rw-r--r--  2.0 unx    25495 b- defN 23-Jul-11 14:35 ultralytics/nn/autobackend.py
--rw-r--r--  2.0 unx    12515 b- defN 23-Jul-11 14:35 ultralytics/nn/autoshape.py
--rw-r--r--  2.0 unx    34213 b- defN 23-Jul-11 14:35 ultralytics/nn/tasks.py
--rw-r--r--  2.0 unx     1587 b- defN 23-Jul-11 14:35 ultralytics/nn/modules/__init__.py
--rw-r--r--  2.0 unx    11816 b- defN 23-Jul-11 14:35 ultralytics/nn/modules/block.py
--rw-r--r--  2.0 unx    11647 b- defN 23-Jul-11 14:35 ultralytics/nn/modules/conv.py
--rw-r--r--  2.0 unx    15476 b- defN 23-Jul-11 14:35 ultralytics/nn/modules/head.py
--rw-r--r--  2.0 unx    15934 b- defN 23-Jul-11 14:35 ultralytics/nn/modules/transformer.py
--rw-r--r--  2.0 unx     3244 b- defN 23-Jul-11 14:35 ultralytics/nn/modules/utils.py
--rw-r--r--  2.0 unx      202 b- defN 23-Jul-11 14:35 ultralytics/tracker/__init__.py
--rw-r--r--  2.0 unx     2309 b- defN 23-Jul-11 14:35 ultralytics/tracker/track.py
--rw-r--r--  2.0 unx      890 b- defN 23-Jul-11 14:35 ultralytics/tracker/cfg/botsort.yaml
--rw-r--r--  2.0 unx      694 b- defN 23-Jul-11 14:35 ultralytics/tracker/cfg/bytetrack.yaml
--rw-r--r--  2.0 unx      171 b- defN 23-Jul-11 14:35 ultralytics/tracker/trackers/__init__.py
--rw-r--r--  2.0 unx     1609 b- defN 23-Jul-11 14:35 ultralytics/tracker/trackers/basetrack.py
--rw-r--r--  2.0 unx     5684 b- defN 23-Jul-11 14:35 ultralytics/tracker/trackers/bot_sort.py
--rw-r--r--  2.0 unx    14448 b- defN 23-Jul-11 14:35 ultralytics/tracker/trackers/byte_tracker.py
--rw-r--r--  2.0 unx        0 b- defN 23-Jul-11 14:35 ultralytics/tracker/utils/__init__.py
--rw-r--r--  2.0 unx    12214 b- defN 23-Jul-11 14:35 ultralytics/tracker/utils/gmc.py
--rw-r--r--  2.0 unx    18420 b- defN 23-Jul-11 14:35 ultralytics/tracker/utils/kalman_filter.py
--rw-r--r--  2.0 unx     8754 b- defN 23-Jul-11 14:35 ultralytics/tracker/utils/matching.py
--rw-r--r--  2.0 unx      142 b- defN 23-Jul-11 14:35 ultralytics/vit/__init__.py
--rw-r--r--  2.0 unx      197 b- defN 23-Jul-11 14:35 ultralytics/vit/rtdetr/__init__.py
--rw-r--r--  2.0 unx     7414 b- defN 23-Jul-11 14:35 ultralytics/vit/rtdetr/model.py
--rw-r--r--  2.0 unx     1859 b- defN 23-Jul-11 14:35 ultralytics/vit/rtdetr/predict.py
--rw-r--r--  2.0 unx     2950 b- defN 23-Jul-11 14:35 ultralytics/vit/rtdetr/train.py
--rw-r--r--  2.0 unx     6572 b- defN 23-Jul-11 14:35 ultralytics/vit/rtdetr/val.py
--rw-r--r--  2.0 unx      173 b- defN 23-Jul-11 14:35 ultralytics/vit/sam/__init__.py
--rw-r--r--  2.0 unx    13296 b- defN 23-Jul-11 14:35 ultralytics/vit/sam/amg.py
--rw-r--r--  2.0 unx     3921 b- defN 23-Jul-11 14:35 ultralytics/vit/sam/autosize.py
--rw-r--r--  2.0 unx     3837 b- defN 23-Jul-11 14:35 ultralytics/vit/sam/build.py
--rw-r--r--  2.0 unx     2364 b- defN 23-Jul-11 14:35 ultralytics/vit/sam/model.py
--rw-r--r--  2.0 unx     2182 b- defN 23-Jul-11 14:35 ultralytics/vit/sam/predict.py
--rw-r--r--  2.0 unx       42 b- defN 23-Jul-11 14:35 ultralytics/vit/sam/modules/__init__.py
--rw-r--r--  2.0 unx     6372 b- defN 23-Jul-11 14:35 ultralytics/vit/sam/modules/decoders.py
--rw-r--r--  2.0 unx    22545 b- defN 23-Jul-11 14:35 ultralytics/vit/sam/modules/encoders.py
--rw-r--r--  2.0 unx    15293 b- defN 23-Jul-11 14:35 ultralytics/vit/sam/modules/mask_generator.py
--rw-r--r--  2.0 unx    11244 b- defN 23-Jul-11 14:35 ultralytics/vit/sam/modules/prompt_predictor.py
--rw-r--r--  2.0 unx     7309 b- defN 23-Jul-11 14:35 ultralytics/vit/sam/modules/sam.py
--rw-r--r--  2.0 unx     8532 b- defN 23-Jul-11 14:35 ultralytics/vit/sam/modules/transformer.py
--rw-r--r--  2.0 unx       42 b- defN 23-Jul-11 14:35 ultralytics/vit/utils/__init__.py
--rw-r--r--  2.0 unx    13187 b- defN 23-Jul-11 14:35 ultralytics/vit/utils/loss.py
--rw-r--r--  2.0 unx    13000 b- defN 23-Jul-11 14:35 ultralytics/vit/utils/ops.py
--rw-r--r--  2.0 unx       94 b- defN 23-Jul-11 14:35 ultralytics/yolo/__init__.py
--rw-r--r--  2.0 unx    18414 b- defN 23-Jul-11 14:35 ultralytics/yolo/cfg/__init__.py
--rw-r--r--  2.0 unx     7363 b- defN 23-Jul-11 14:35 ultralytics/yolo/cfg/default.yaml
--rw-r--r--  2.0 unx      458 b- defN 23-Jul-11 14:35 ultralytics/yolo/data/__init__.py
--rw-r--r--  2.0 unx     2341 b- defN 23-Jul-11 14:35 ultralytics/yolo/data/annotator.py
--rw-r--r--  2.0 unx    37201 b- defN 23-Jul-11 14:35 ultralytics/yolo/data/augment.py
--rw-r--r--  2.0 unx    12649 b- defN 23-Jul-11 14:35 ultralytics/yolo/data/base.py
--rw-r--r--  2.0 unx     6574 b- defN 23-Jul-11 14:35 ultralytics/yolo/data/build.py
--rw-r--r--  2.0 unx     9190 b- defN 23-Jul-11 14:35 ultralytics/yolo/data/converter.py
--rw-r--r--  2.0 unx    13367 b- defN 23-Jul-11 14:35 ultralytics/yolo/data/dataset.py
--rw-r--r--  2.0 unx     1776 b- defN 23-Jul-11 14:35 ultralytics/yolo/data/dataset_wrappers.py
--rw-r--r--  2.0 unx    24052 b- defN 23-Jul-11 14:35 ultralytics/yolo/data/utils.py
--rw-r--r--  2.0 unx        0 b- defN 23-Jul-11 14:35 ultralytics/yolo/data/dataloaders/__init__.py
--rw-r--r--  2.0 unx    16303 b- defN 23-Jul-11 14:35 ultralytics/yolo/data/dataloaders/stream_loaders.py
--rw-r--r--  2.0 unx    17646 b- defN 23-Jul-11 14:35 ultralytics/yolo/data/dataloaders/v5augmentations.py
--rw-r--r--  2.0 unx    51260 b- defN 23-Jul-11 14:35 ultralytics/yolo/data/dataloaders/v5loader.py
--rw-r--r--  2.0 unx        0 b- defN 23-Jul-11 14:35 ultralytics/yolo/engine/__init__.py
--rw-r--r--  2.0 unx    43862 b- defN 23-Jul-11 14:35 ultralytics/yolo/engine/exporter.py
--rw-r--r--  2.0 unx    19808 b- defN 23-Jul-11 14:35 ultralytics/yolo/engine/model.py
--rw-r--r--  2.0 unx    16433 b- defN 23-Jul-11 14:35 ultralytics/yolo/engine/predictor.py
--rw-r--r--  2.0 unx    24545 b- defN 23-Jul-11 14:35 ultralytics/yolo/engine/results.py
--rw-r--r--  2.0 unx    31263 b- defN 23-Jul-11 14:35 ultralytics/yolo/engine/trainer.py
--rw-r--r--  2.0 unx    11715 b- defN 23-Jul-11 14:35 ultralytics/yolo/engine/validator.py
--rw-r--r--  2.0 unx      254 b- defN 23-Jul-11 14:35 ultralytics/yolo/fastsam/__init__.py
--rw-r--r--  2.0 unx     4583 b- defN 23-Jul-11 14:35 ultralytics/yolo/fastsam/model.py
--rw-r--r--  2.0 unx     2801 b- defN 23-Jul-11 14:35 ultralytics/yolo/fastsam/predict.py
--rw-r--r--  2.0 unx    16757 b- defN 23-Jul-11 14:35 ultralytics/yolo/fastsam/prompt.py
--rw-r--r--  2.0 unx     1897 b- defN 23-Jul-11 14:35 ultralytics/yolo/fastsam/utils.py
--rw-r--r--  2.0 unx    12430 b- defN 23-Jul-11 14:35 ultralytics/yolo/fastsam/val.py
--rw-r--r--  2.0 unx      179 b- defN 23-Jul-11 14:35 ultralytics/yolo/nas/__init__.py
--rw-r--r--  2.0 unx     5227 b- defN 23-Jul-11 14:35 ultralytics/yolo/nas/model.py
--rw-r--r--  2.0 unx     1465 b- defN 23-Jul-11 14:35 ultralytics/yolo/nas/predict.py
--rw-r--r--  2.0 unx      953 b- defN 23-Jul-11 14:35 ultralytics/yolo/nas/val.py
--rw-r--r--  2.0 unx    28182 b- defN 23-Jul-11 14:35 ultralytics/yolo/utils/__init__.py
--rw-r--r--  2.0 unx     3874 b- defN 23-Jul-11 14:35 ultralytics/yolo/utils/autobatch.py
--rw-r--r--  2.0 unx    15865 b- defN 23-Jul-11 14:35 ultralytics/yolo/utils/benchmarks.py
--rw-r--r--  2.0 unx    18123 b- defN 23-Jul-11 14:35 ultralytics/yolo/utils/checks.py
--rw-r--r--  2.0 unx     2596 b- defN 23-Jul-11 14:35 ultralytics/yolo/utils/dist.py
--rw-r--r--  2.0 unx    12788 b- defN 23-Jul-11 14:35 ultralytics/yolo/utils/downloads.py
--rw-r--r--  2.0 unx      317 b- defN 23-Jul-11 14:35 ultralytics/yolo/utils/errors.py
--rw-r--r--  2.0 unx     3588 b- defN 23-Jul-11 14:35 ultralytics/yolo/utils/files.py
--rw-r--r--  2.0 unx    14622 b- defN 23-Jul-11 14:35 ultralytics/yolo/utils/instance.py
--rw-r--r--  2.0 unx    19159 b- defN 23-Jul-11 14:35 ultralytics/yolo/utils/loss.py
--rw-r--r--  2.0 unx    42292 b- defN 23-Jul-11 14:35 ultralytics/yolo/utils/metrics.py
--rw-r--r--  2.0 unx    28304 b- defN 23-Jul-11 14:35 ultralytics/yolo/utils/ops.py
--rw-r--r--  2.0 unx     1241 b- defN 23-Jul-11 14:35 ultralytics/yolo/utils/patches.py
--rw-r--r--  2.0 unx    24551 b- defN 23-Jul-11 14:35 ultralytics/yolo/utils/plotting.py
--rw-r--r--  2.0 unx    13645 b- defN 23-Jul-11 14:35 ultralytics/yolo/utils/tal.py
--rw-r--r--  2.0 unx    22348 b- defN 23-Jul-11 14:35 ultralytics/yolo/utils/torch_utils.py
--rw-r--r--  2.0 unx     5413 b- defN 23-Jul-11 14:35 ultralytics/yolo/utils/tuner.py
--rw-r--r--  2.0 unx      214 b- defN 23-Jul-11 14:35 ultralytics/yolo/utils/callbacks/__init__.py
--rw-r--r--  2.0 unx     5593 b- defN 23-Jul-11 14:35 ultralytics/yolo/utils/callbacks/base.py
--rw-r--r--  2.0 unx     5902 b- defN 23-Jul-11 14:35 ultralytics/yolo/utils/callbacks/clearml.py
--rw-r--r--  2.0 unx    13045 b- defN 23-Jul-11 14:35 ultralytics/yolo/utils/callbacks/comet.py
--rw-r--r--  2.0 unx     4320 b- defN 23-Jul-11 14:35 ultralytics/yolo/utils/callbacks/dvc.py
--rw-r--r--  2.0 unx     3310 b- defN 23-Jul-11 14:35 ultralytics/yolo/utils/callbacks/hub.py
--rw-r--r--  2.0 unx     2620 b- defN 23-Jul-11 14:35 ultralytics/yolo/utils/callbacks/mlflow.py
--rw-r--r--  2.0 unx     3679 b- defN 23-Jul-11 14:35 ultralytics/yolo/utils/callbacks/neptune.py
--rw-r--r--  2.0 unx      495 b- defN 23-Jul-11 14:35 ultralytics/yolo/utils/callbacks/raytune.py
--rw-r--r--  2.0 unx     1525 b- defN 23-Jul-11 14:35 ultralytics/yolo/utils/callbacks/tensorboard.py
--rw-r--r--  2.0 unx     2188 b- defN 23-Jul-11 14:35 ultralytics/yolo/utils/callbacks/wb.py
--rw-r--r--  2.0 unx      158 b- defN 23-Jul-11 14:35 ultralytics/yolo/v8/__init__.py
--rw-r--r--  2.0 unx      391 b- defN 23-Jul-11 14:35 ultralytics/yolo/v8/classify/__init__.py
--rw-r--r--  2.0 unx     1929 b- defN 23-Jul-11 14:35 ultralytics/yolo/v8/classify/predict.py
--rw-r--r--  2.0 unx     6875 b- defN 23-Jul-11 14:35 ultralytics/yolo/v8/classify/train.py
--rw-r--r--  2.0 unx     4676 b- defN 23-Jul-11 14:35 ultralytics/yolo/v8/classify/val.py
--rw-r--r--  2.0 unx      277 b- defN 23-Jul-11 14:35 ultralytics/yolo/v8/detect/__init__.py
--rw-r--r--  2.0 unx     1854 b- defN 23-Jul-11 14:35 ultralytics/yolo/v8/detect/predict.py
--rw-r--r--  2.0 unx     7199 b- defN 23-Jul-11 14:35 ultralytics/yolo/v8/detect/train.py
--rw-r--r--  2.0 unx    14730 b- defN 23-Jul-11 14:35 ultralytics/yolo/v8/detect/val.py
--rw-r--r--  2.0 unx      247 b- defN 23-Jul-11 14:35 ultralytics/yolo/v8/pose/__init__.py
--rw-r--r--  2.0 unx     2375 b- defN 23-Jul-11 14:35 ultralytics/yolo/v8/pose/predict.py
--rw-r--r--  2.0 unx     2790 b- defN 23-Jul-11 14:35 ultralytics/yolo/v8/pose/train.py
--rw-r--r--  2.0 unx    10927 b- defN 23-Jul-11 14:35 ultralytics/yolo/v8/pose/val.py
--rw-r--r--  2.0 unx      295 b- defN 23-Jul-11 14:35 ultralytics/yolo/v8/segment/__init__.py
--rw-r--r--  2.0 unx     2839 b- defN 23-Jul-11 14:35 ultralytics/yolo/v8/segment/predict.py
--rw-r--r--  2.0 unx     2499 b- defN 23-Jul-11 14:35 ultralytics/yolo/v8/segment/train.py
--rw-r--r--  2.0 unx    12906 b- defN 23-Jul-11 14:35 ultralytics/yolo/v8/segment/val.py
--rw-r--r--  2.0 unx    34523 b- defN 23-Jul-11 14:36 pyppbox_ultralytics-8.0.132.dist-info/LICENSE
--rw-r--r--  2.0 unx     3968 b- defN 23-Jul-11 14:36 pyppbox_ultralytics-8.0.132.dist-info/METADATA
--rw-r--r--  2.0 unx       92 b- defN 23-Jul-11 14:36 pyppbox_ultralytics-8.0.132.dist-info/WHEEL
--rw-r--r--  2.0 unx      103 b- defN 23-Jul-11 14:36 pyppbox_ultralytics-8.0.132.dist-info/entry_points.txt
--rw-r--r--  2.0 unx       12 b- defN 23-Jul-11 14:36 pyppbox_ultralytics-8.0.132.dist-info/top_level.txt
--rw-rw-r--  2.0 unx    15220 b- defN 23-Jul-11 14:36 pyppbox_ultralytics-8.0.132.dist-info/RECORD
-166 files, 1505604 bytes uncompressed, 599010 bytes compressed:  60.2%
+Zip file size: 564776 bytes, number of entries: 149
+-rw-r--r--  2.0 unx      566 b- defN 23-Jul-26 20:58 ultralytics/__init__.py
+-rw-r--r--  2.0 unx   137419 b- defN 23-Jul-26 20:58 ultralytics/assets/bus.jpg
+-rw-r--r--  2.0 unx    50427 b- defN 23-Jul-26 20:58 ultralytics/assets/zidane.jpg
+-rw-r--r--  2.0 unx    18735 b- defN 23-Jul-26 20:58 ultralytics/cfg/__init__.py
+-rw-r--r--  2.0 unx     7173 b- defN 23-Jul-26 20:58 ultralytics/cfg/default.yaml
+-rw-r--r--  2.0 unx     1970 b- defN 23-Jul-26 20:58 ultralytics/cfg/models/rt-detr/rtdetr-l.yaml
+-rw-r--r--  2.0 unx     2177 b- defN 23-Jul-26 20:58 ultralytics/cfg/models/rt-detr/rtdetr-x.yaml
+-rw-r--r--  2.0 unx     1550 b- defN 23-Jul-26 20:58 ultralytics/cfg/models/v3/yolov3-spp.yaml
+-rw-r--r--  2.0 unx     1252 b- defN 23-Jul-26 20:58 ultralytics/cfg/models/v3/yolov3-tiny.yaml
+-rw-r--r--  2.0 unx     1537 b- defN 23-Jul-26 20:58 ultralytics/cfg/models/v3/yolov3.yaml
+-rw-r--r--  2.0 unx     1923 b- defN 23-Jul-26 20:58 ultralytics/cfg/models/v5/yolov5-p6.yaml
+-rw-r--r--  2.0 unx     1550 b- defN 23-Jul-26 20:58 ultralytics/cfg/models/v5/yolov5.yaml
+-rw-r--r--  2.0 unx     1735 b- defN 23-Jul-26 20:58 ultralytics/cfg/models/v6/yolov6.yaml
+-rw-r--r--  2.0 unx      920 b- defN 23-Jul-26 20:58 ultralytics/cfg/models/v8/yolov8-cls.yaml
+-rw-r--r--  2.0 unx     1751 b- defN 23-Jul-26 20:58 ultralytics/cfg/models/v8/yolov8-p2.yaml
+-rw-r--r--  2.0 unx     1856 b- defN 23-Jul-26 20:58 ultralytics/cfg/models/v8/yolov8-p6.yaml
+-rw-r--r--  2.0 unx     1946 b- defN 23-Jul-26 20:58 ultralytics/cfg/models/v8/yolov8-pose-p6.yaml
+-rw-r--r--  2.0 unx     1580 b- defN 23-Jul-26 20:58 ultralytics/cfg/models/v8/yolov8-pose.yaml
+-rw-r--r--  2.0 unx     1920 b- defN 23-Jul-26 20:58 ultralytics/cfg/models/v8/yolov8-rtdetr.yaml
+-rw-r--r--  2.0 unx     1490 b- defN 23-Jul-26 20:58 ultralytics/cfg/models/v8/yolov8-seg.yaml
+-rw-r--r--  2.0 unx     1913 b- defN 23-Jul-26 20:58 ultralytics/cfg/models/v8/yolov8.yaml
+-rw-r--r--  2.0 unx      890 b- defN 23-Jul-26 20:58 ultralytics/cfg/trackers/botsort.yaml
+-rw-r--r--  2.0 unx      694 b- defN 23-Jul-26 20:58 ultralytics/cfg/trackers/bytetrack.yaml
+-rw-r--r--  2.0 unx      389 b- defN 23-Jul-26 20:58 ultralytics/data/__init__.py
+-rw-r--r--  2.0 unx     1830 b- defN 23-Jul-26 20:58 ultralytics/data/annotator.py
+-rw-r--r--  2.0 unx    37439 b- defN 23-Jul-26 20:58 ultralytics/data/augment.py
+-rw-r--r--  2.0 unx    12660 b- defN 23-Jul-26 20:58 ultralytics/data/base.py
+-rw-r--r--  2.0 unx     6531 b- defN 23-Jul-26 20:58 ultralytics/data/build.py
+-rw-r--r--  2.0 unx     9180 b- defN 23-Jul-26 20:58 ultralytics/data/converter.py
+-rw-r--r--  2.0 unx    13378 b- defN 23-Jul-26 20:58 ultralytics/data/dataset.py
+-rw-r--r--  2.0 unx    16523 b- defN 23-Jul-26 20:58 ultralytics/data/loaders.py
+-rw-r--r--  2.0 unx    25868 b- defN 23-Jul-26 20:58 ultralytics/data/utils.py
+-rw-r--r--  2.0 unx        0 b- defN 23-Jul-26 20:58 ultralytics/data/dataloaders/__init__.py
+-rw-r--r--  2.0 unx        0 b- defN 23-Jul-26 20:58 ultralytics/engine/__init__.py
+-rw-r--r--  2.0 unx    46161 b- defN 23-Jul-26 20:58 ultralytics/engine/exporter.py
+-rw-r--r--  2.0 unx    19554 b- defN 23-Jul-26 20:58 ultralytics/engine/model.py
+-rw-r--r--  2.0 unx    16516 b- defN 23-Jul-26 20:58 ultralytics/engine/predictor.py
+-rw-r--r--  2.0 unx    24732 b- defN 23-Jul-26 20:58 ultralytics/engine/results.py
+-rw-r--r--  2.0 unx    31186 b- defN 23-Jul-26 20:58 ultralytics/engine/trainer.py
+-rw-r--r--  2.0 unx    11804 b- defN 23-Jul-26 20:58 ultralytics/engine/validator.py
+-rw-r--r--  2.0 unx     4393 b- defN 23-Jul-26 20:58 ultralytics/hub/__init__.py
+-rw-r--r--  2.0 unx     5193 b- defN 23-Jul-26 20:58 ultralytics/hub/auth.py
+-rw-r--r--  2.0 unx     8469 b- defN 23-Jul-26 20:58 ultralytics/hub/session.py
+-rw-r--r--  2.0 unx     9425 b- defN 23-Jul-26 20:58 ultralytics/hub/utils.py
+-rw-r--r--  2.0 unx       99 b- defN 23-Jul-26 20:58 ultralytics/models/__init__.py
+-rw-r--r--  2.0 unx      254 b- defN 23-Jul-26 20:58 ultralytics/models/fastsam/__init__.py
+-rw-r--r--  2.0 unx     4557 b- defN 23-Jul-26 20:58 ultralytics/models/fastsam/model.py
+-rw-r--r--  2.0 unx     2797 b- defN 23-Jul-26 20:58 ultralytics/models/fastsam/predict.py
+-rw-r--r--  2.0 unx    16752 b- defN 23-Jul-26 20:58 ultralytics/models/fastsam/prompt.py
+-rw-r--r--  2.0 unx     1986 b- defN 23-Jul-26 20:58 ultralytics/models/fastsam/utils.py
+-rw-r--r--  2.0 unx    12414 b- defN 23-Jul-26 20:58 ultralytics/models/fastsam/val.py
+-rw-r--r--  2.0 unx      179 b- defN 23-Jul-26 20:58 ultralytics/models/nas/__init__.py
+-rw-r--r--  2.0 unx     5206 b- defN 23-Jul-26 20:58 ultralytics/models/nas/model.py
+-rw-r--r--  2.0 unx     1445 b- defN 23-Jul-26 20:58 ultralytics/models/nas/predict.py
+-rw-r--r--  2.0 unx      947 b- defN 23-Jul-26 20:58 ultralytics/models/nas/val.py
+-rw-r--r--  2.0 unx      197 b- defN 23-Jul-26 20:58 ultralytics/models/rtdetr/__init__.py
+-rw-r--r--  2.0 unx     7384 b- defN 23-Jul-26 20:58 ultralytics/models/rtdetr/model.py
+-rw-r--r--  2.0 unx     1839 b- defN 23-Jul-26 20:58 ultralytics/models/rtdetr/predict.py
+-rw-r--r--  2.0 unx     2949 b- defN 23-Jul-26 20:58 ultralytics/models/rtdetr/train.py
+-rw-r--r--  2.0 unx     6561 b- defN 23-Jul-26 20:58 ultralytics/models/rtdetr/val.py
+-rw-r--r--  2.0 unx      176 b- defN 23-Jul-26 20:58 ultralytics/models/sam/__init__.py
+-rw-r--r--  2.0 unx    13296 b- defN 23-Jul-26 20:58 ultralytics/models/sam/amg.py
+-rw-r--r--  2.0 unx     4822 b- defN 23-Jul-26 20:58 ultralytics/models/sam/build.py
+-rw-r--r--  2.0 unx     2522 b- defN 23-Jul-26 20:58 ultralytics/models/sam/model.py
+-rw-r--r--  2.0 unx    18948 b- defN 23-Jul-26 20:58 ultralytics/models/sam/predict.py
+-rw-r--r--  2.0 unx       42 b- defN 23-Jul-26 20:58 ultralytics/models/sam/modules/__init__.py
+-rw-r--r--  2.0 unx     6372 b- defN 23-Jul-26 20:58 ultralytics/models/sam/modules/decoders.py
+-rw-r--r--  2.0 unx    22545 b- defN 23-Jul-26 20:58 ultralytics/models/sam/modules/encoders.py
+-rw-r--r--  2.0 unx     7309 b- defN 23-Jul-26 20:58 ultralytics/models/sam/modules/sam.py
+-rw-r--r--  2.0 unx    21760 b- defN 23-Jul-26 20:58 ultralytics/models/sam/modules/tiny_encoder.py
+-rw-r--r--  2.0 unx     8532 b- defN 23-Jul-26 20:58 ultralytics/models/sam/modules/transformer.py
+-rw-r--r--  2.0 unx       42 b- defN 23-Jul-26 20:58 ultralytics/models/utils/__init__.py
+-rw-r--r--  2.0 unx    13157 b- defN 23-Jul-26 20:58 ultralytics/models/utils/loss.py
+-rw-r--r--  2.0 unx    12990 b- defN 23-Jul-26 20:58 ultralytics/models/utils/ops.py
+-rw-r--r--  2.0 unx      162 b- defN 23-Jul-26 20:58 ultralytics/models/yolo/__init__.py
+-rw-r--r--  2.0 unx      403 b- defN 23-Jul-26 20:58 ultralytics/models/yolo/classify/__init__.py
+-rw-r--r--  2.0 unx     1914 b- defN 23-Jul-26 20:58 ultralytics/models/yolo/classify/predict.py
+-rw-r--r--  2.0 unx     6856 b- defN 23-Jul-26 20:58 ultralytics/models/yolo/classify/train.py
+-rw-r--r--  2.0 unx     4651 b- defN 23-Jul-26 20:58 ultralytics/models/yolo/classify/val.py
+-rw-r--r--  2.0 unx      277 b- defN 23-Jul-26 20:58 ultralytics/models/yolo/detect/__init__.py
+-rw-r--r--  2.0 unx     1839 b- defN 23-Jul-26 20:58 ultralytics/models/yolo/detect/predict.py
+-rw-r--r--  2.0 unx     5780 b- defN 23-Jul-26 20:58 ultralytics/models/yolo/detect/train.py
+-rw-r--r--  2.0 unx    13521 b- defN 23-Jul-26 20:58 ultralytics/models/yolo/detect/val.py
+-rw-r--r--  2.0 unx      247 b- defN 23-Jul-26 20:58 ultralytics/models/yolo/pose/__init__.py
+-rw-r--r--  2.0 unx     2369 b- defN 23-Jul-26 20:58 ultralytics/models/yolo/pose/predict.py
+-rw-r--r--  2.0 unx     2788 b- defN 23-Jul-26 20:58 ultralytics/models/yolo/pose/train.py
+-rw-r--r--  2.0 unx    10911 b- defN 23-Jul-26 20:58 ultralytics/models/yolo/pose/val.py
+-rw-r--r--  2.0 unx      295 b- defN 23-Jul-26 20:58 ultralytics/models/yolo/segment/__init__.py
+-rw-r--r--  2.0 unx     2833 b- defN 23-Jul-26 20:58 ultralytics/models/yolo/segment/predict.py
+-rw-r--r--  2.0 unx     2497 b- defN 23-Jul-26 20:58 ultralytics/models/yolo/segment/train.py
+-rw-r--r--  2.0 unx    12890 b- defN 23-Jul-26 20:58 ultralytics/models/yolo/segment/val.py
+-rw-r--r--  2.0 unx      555 b- defN 23-Jul-26 20:58 ultralytics/nn/__init__.py
+-rw-r--r--  2.0 unx    26091 b- defN 23-Jul-26 20:58 ultralytics/nn/autobackend.py
+-rw-r--r--  2.0 unx    36011 b- defN 23-Jul-26 20:58 ultralytics/nn/tasks.py
+-rw-r--r--  2.0 unx     1587 b- defN 23-Jul-26 20:58 ultralytics/nn/modules/__init__.py
+-rw-r--r--  2.0 unx    11841 b- defN 23-Jul-26 20:58 ultralytics/nn/modules/block.py
+-rw-r--r--  2.0 unx    11647 b- defN 23-Jul-26 20:58 ultralytics/nn/modules/conv.py
+-rw-r--r--  2.0 unx    16098 b- defN 23-Jul-26 20:58 ultralytics/nn/modules/head.py
+-rw-r--r--  2.0 unx    15934 b- defN 23-Jul-26 20:58 ultralytics/nn/modules/transformer.py
+-rw-r--r--  2.0 unx     3244 b- defN 23-Jul-26 20:58 ultralytics/nn/modules/utils.py
+-rw-r--r--  2.0 unx      227 b- defN 23-Jul-26 20:58 ultralytics/trackers/__init__.py
+-rw-r--r--  2.0 unx     1609 b- defN 23-Jul-26 20:58 ultralytics/trackers/basetrack.py
+-rw-r--r--  2.0 unx     5681 b- defN 23-Jul-26 20:58 ultralytics/trackers/bot_sort.py
+-rw-r--r--  2.0 unx    14446 b- defN 23-Jul-26 20:58 ultralytics/trackers/byte_tracker.py
+-rw-r--r--  2.0 unx     2324 b- defN 23-Jul-26 20:58 ultralytics/trackers/track.py
+-rw-r--r--  2.0 unx        0 b- defN 23-Jul-26 20:58 ultralytics/trackers/utils/__init__.py
+-rw-r--r--  2.0 unx    12209 b- defN 23-Jul-26 20:58 ultralytics/trackers/utils/gmc.py
+-rw-r--r--  2.0 unx    18420 b- defN 23-Jul-26 20:58 ultralytics/trackers/utils/kalman_filter.py
+-rw-r--r--  2.0 unx     8749 b- defN 23-Jul-26 20:58 ultralytics/trackers/utils/matching.py
+-rw-r--r--  2.0 unx    30202 b- defN 23-Jul-26 20:58 ultralytics/utils/__init__.py
+-rw-r--r--  2.0 unx     3864 b- defN 23-Jul-26 20:58 ultralytics/utils/autobatch.py
+-rw-r--r--  2.0 unx    16087 b- defN 23-Jul-26 20:58 ultralytics/utils/benchmarks.py
+-rw-r--r--  2.0 unx    19491 b- defN 23-Jul-26 20:58 ultralytics/utils/checks.py
+-rw-r--r--  2.0 unx     2591 b- defN 23-Jul-26 20:58 ultralytics/utils/dist.py
+-rw-r--r--  2.0 unx    12818 b- defN 23-Jul-26 20:58 ultralytics/utils/downloads.py
+-rw-r--r--  2.0 unx      312 b- defN 23-Jul-26 20:58 ultralytics/utils/errors.py
+-rw-r--r--  2.0 unx     5450 b- defN 23-Jul-26 20:58 ultralytics/utils/files.py
+-rw-r--r--  2.0 unx    14645 b- defN 23-Jul-26 20:58 ultralytics/utils/instance.py
+-rw-r--r--  2.0 unx    19144 b- defN 23-Jul-26 20:58 ultralytics/utils/loss.py
+-rw-r--r--  2.0 unx    42325 b- defN 23-Jul-26 20:58 ultralytics/utils/metrics.py
+-rw-r--r--  2.0 unx    29212 b- defN 23-Jul-26 20:58 ultralytics/utils/ops.py
+-rw-r--r--  2.0 unx     1246 b- defN 23-Jul-26 20:58 ultralytics/utils/patches.py
+-rw-r--r--  2.0 unx    24845 b- defN 23-Jul-26 20:58 ultralytics/utils/plotting.py
+-rw-r--r--  2.0 unx    13645 b- defN 23-Jul-26 20:58 ultralytics/utils/tal.py
+-rw-r--r--  2.0 unx    23082 b- defN 23-Jul-26 20:58 ultralytics/utils/torch_utils.py
+-rw-r--r--  2.0 unx     5403 b- defN 23-Jul-26 20:58 ultralytics/utils/tuner.py
+-rw-r--r--  2.0 unx      214 b- defN 23-Jul-26 20:58 ultralytics/utils/callbacks/__init__.py
+-rw-r--r--  2.0 unx     5593 b- defN 23-Jul-26 20:58 ultralytics/utils/callbacks/base.py
+-rw-r--r--  2.0 unx     5974 b- defN 23-Jul-26 20:58 ultralytics/utils/callbacks/clearml.py
+-rw-r--r--  2.0 unx    13115 b- defN 23-Jul-26 20:58 ultralytics/utils/callbacks/comet.py
+-rw-r--r--  2.0 unx     4386 b- defN 23-Jul-26 20:58 ultralytics/utils/callbacks/dvc.py
+-rw-r--r--  2.0 unx     3363 b- defN 23-Jul-26 20:58 ultralytics/utils/callbacks/hub.py
+-rw-r--r--  2.0 unx     2701 b- defN 23-Jul-26 20:58 ultralytics/utils/callbacks/mlflow.py
+-rw-r--r--  2.0 unx     3751 b- defN 23-Jul-26 20:58 ultralytics/utils/callbacks/neptune.py
+-rw-r--r--  2.0 unx      608 b- defN 23-Jul-26 20:58 ultralytics/utils/callbacks/raytune.py
+-rw-r--r--  2.0 unx     1716 b- defN 23-Jul-26 20:58 ultralytics/utils/callbacks/tensorboard.py
+-rw-r--r--  2.0 unx     2252 b- defN 23-Jul-26 20:58 ultralytics/utils/callbacks/wb.py
+-rw-r--r--  2.0 unx       94 b- defN 23-Jul-26 20:58 ultralytics/yolo/__init__.py
+-rw-r--r--  2.0 unx      373 b- defN 23-Jul-26 20:58 ultralytics/yolo/cfg/__init__.py
+-rw-r--r--  2.0 unx      823 b- defN 23-Jul-26 20:58 ultralytics/yolo/data/__init__.py
+-rw-r--r--  2.0 unx      385 b- defN 23-Jul-26 20:58 ultralytics/yolo/engine/__init__.py
+-rw-r--r--  2.0 unx      647 b- defN 23-Jul-26 20:58 ultralytics/yolo/utils/__init__.py
+-rw-r--r--  2.0 unx      387 b- defN 23-Jul-26 20:58 ultralytics/yolo/v8/__init__.py
+-rw-r--r--  2.0 unx    34523 b- defN 23-Jul-26 20:58 pyppbox_ultralytics-8.0.142.dist-info/LICENSE
+-rw-r--r--  2.0 unx     3813 b- defN 23-Jul-26 20:58 pyppbox_ultralytics-8.0.142.dist-info/METADATA
+-rw-r--r--  2.0 unx       92 b- defN 23-Jul-26 20:58 pyppbox_ultralytics-8.0.142.dist-info/WHEEL
+-rw-r--r--  2.0 unx      103 b- defN 23-Jul-26 20:58 pyppbox_ultralytics-8.0.142.dist-info/entry_points.txt
+-rw-r--r--  2.0 unx       12 b- defN 23-Jul-26 20:58 pyppbox_ultralytics-8.0.142.dist-info/top_level.txt
+-rw-rw-r--  2.0 unx    13554 b- defN 23-Jul-26 20:58 pyppbox_ultralytics-8.0.142.dist-info/RECORD
+149 files, 1362031 bytes uncompressed, 543202 bytes compressed:  60.1%
```

## zipnote {}

```diff
@@ -3,497 +3,446 @@
 
 Filename: ultralytics/assets/bus.jpg
 Comment: 
 
 Filename: ultralytics/assets/zidane.jpg
 Comment: 
 
-Filename: ultralytics/datasets/Argoverse.yaml
+Filename: ultralytics/cfg/__init__.py
 Comment: 
 
-Filename: ultralytics/datasets/GlobalWheat2020.yaml
+Filename: ultralytics/cfg/default.yaml
 Comment: 
 
-Filename: ultralytics/datasets/ImageNet.yaml
+Filename: ultralytics/cfg/models/rt-detr/rtdetr-l.yaml
 Comment: 
 
-Filename: ultralytics/datasets/Objects365.yaml
+Filename: ultralytics/cfg/models/rt-detr/rtdetr-x.yaml
 Comment: 
 
-Filename: ultralytics/datasets/SKU-110K.yaml
+Filename: ultralytics/cfg/models/v3/yolov3-spp.yaml
 Comment: 
 
-Filename: ultralytics/datasets/VOC.yaml
+Filename: ultralytics/cfg/models/v3/yolov3-tiny.yaml
 Comment: 
 
-Filename: ultralytics/datasets/VisDrone.yaml
+Filename: ultralytics/cfg/models/v3/yolov3.yaml
 Comment: 
 
-Filename: ultralytics/datasets/coco-pose.yaml
+Filename: ultralytics/cfg/models/v5/yolov5-p6.yaml
 Comment: 
 
-Filename: ultralytics/datasets/coco.yaml
+Filename: ultralytics/cfg/models/v5/yolov5.yaml
 Comment: 
 
-Filename: ultralytics/datasets/coco128-seg.yaml
+Filename: ultralytics/cfg/models/v6/yolov6.yaml
 Comment: 
 
-Filename: ultralytics/datasets/coco128.yaml
+Filename: ultralytics/cfg/models/v8/yolov8-cls.yaml
 Comment: 
 
-Filename: ultralytics/datasets/coco8-pose.yaml
+Filename: ultralytics/cfg/models/v8/yolov8-p2.yaml
 Comment: 
 
-Filename: ultralytics/datasets/coco8-seg.yaml
+Filename: ultralytics/cfg/models/v8/yolov8-p6.yaml
 Comment: 
 
-Filename: ultralytics/datasets/coco8.yaml
+Filename: ultralytics/cfg/models/v8/yolov8-pose-p6.yaml
 Comment: 
 
-Filename: ultralytics/datasets/xView.yaml
+Filename: ultralytics/cfg/models/v8/yolov8-pose.yaml
 Comment: 
 
-Filename: ultralytics/hub/__init__.py
+Filename: ultralytics/cfg/models/v8/yolov8-rtdetr.yaml
 Comment: 
 
-Filename: ultralytics/hub/auth.py
+Filename: ultralytics/cfg/models/v8/yolov8-seg.yaml
 Comment: 
 
-Filename: ultralytics/hub/session.py
+Filename: ultralytics/cfg/models/v8/yolov8.yaml
 Comment: 
 
-Filename: ultralytics/hub/utils.py
+Filename: ultralytics/cfg/trackers/botsort.yaml
 Comment: 
 
-Filename: ultralytics/models/rt-detr/rtdetr-l.yaml
+Filename: ultralytics/cfg/trackers/bytetrack.yaml
 Comment: 
 
-Filename: ultralytics/models/rt-detr/rtdetr-x.yaml
+Filename: ultralytics/data/__init__.py
 Comment: 
 
-Filename: ultralytics/models/v3/yolov3-spp.yaml
+Filename: ultralytics/data/annotator.py
 Comment: 
 
-Filename: ultralytics/models/v3/yolov3-tiny.yaml
+Filename: ultralytics/data/augment.py
 Comment: 
 
-Filename: ultralytics/models/v3/yolov3.yaml
+Filename: ultralytics/data/base.py
 Comment: 
 
-Filename: ultralytics/models/v5/yolov5-p6.yaml
+Filename: ultralytics/data/build.py
 Comment: 
 
-Filename: ultralytics/models/v5/yolov5.yaml
+Filename: ultralytics/data/converter.py
 Comment: 
 
-Filename: ultralytics/models/v6/yolov6.yaml
+Filename: ultralytics/data/dataset.py
 Comment: 
 
-Filename: ultralytics/models/v8/yolov8-cls.yaml
+Filename: ultralytics/data/loaders.py
 Comment: 
 
-Filename: ultralytics/models/v8/yolov8-p2.yaml
+Filename: ultralytics/data/utils.py
 Comment: 
 
-Filename: ultralytics/models/v8/yolov8-p6.yaml
+Filename: ultralytics/data/dataloaders/__init__.py
 Comment: 
 
-Filename: ultralytics/models/v8/yolov8-pose-p6.yaml
+Filename: ultralytics/engine/__init__.py
 Comment: 
 
-Filename: ultralytics/models/v8/yolov8-pose.yaml
+Filename: ultralytics/engine/exporter.py
 Comment: 
 
-Filename: ultralytics/models/v8/yolov8-rtdetr.yaml
+Filename: ultralytics/engine/model.py
 Comment: 
 
-Filename: ultralytics/models/v8/yolov8-seg.yaml
+Filename: ultralytics/engine/predictor.py
 Comment: 
 
-Filename: ultralytics/models/v8/yolov8.yaml
+Filename: ultralytics/engine/results.py
 Comment: 
 
-Filename: ultralytics/nn/__init__.py
+Filename: ultralytics/engine/trainer.py
 Comment: 
 
-Filename: ultralytics/nn/autobackend.py
+Filename: ultralytics/engine/validator.py
 Comment: 
 
-Filename: ultralytics/nn/autoshape.py
+Filename: ultralytics/hub/__init__.py
 Comment: 
 
-Filename: ultralytics/nn/tasks.py
+Filename: ultralytics/hub/auth.py
 Comment: 
 
-Filename: ultralytics/nn/modules/__init__.py
+Filename: ultralytics/hub/session.py
 Comment: 
 
-Filename: ultralytics/nn/modules/block.py
+Filename: ultralytics/hub/utils.py
 Comment: 
 
-Filename: ultralytics/nn/modules/conv.py
+Filename: ultralytics/models/__init__.py
 Comment: 
 
-Filename: ultralytics/nn/modules/head.py
+Filename: ultralytics/models/fastsam/__init__.py
 Comment: 
 
-Filename: ultralytics/nn/modules/transformer.py
+Filename: ultralytics/models/fastsam/model.py
 Comment: 
 
-Filename: ultralytics/nn/modules/utils.py
+Filename: ultralytics/models/fastsam/predict.py
 Comment: 
 
-Filename: ultralytics/tracker/__init__.py
+Filename: ultralytics/models/fastsam/prompt.py
 Comment: 
 
-Filename: ultralytics/tracker/track.py
+Filename: ultralytics/models/fastsam/utils.py
 Comment: 
 
-Filename: ultralytics/tracker/cfg/botsort.yaml
+Filename: ultralytics/models/fastsam/val.py
 Comment: 
 
-Filename: ultralytics/tracker/cfg/bytetrack.yaml
+Filename: ultralytics/models/nas/__init__.py
 Comment: 
 
-Filename: ultralytics/tracker/trackers/__init__.py
+Filename: ultralytics/models/nas/model.py
 Comment: 
 
-Filename: ultralytics/tracker/trackers/basetrack.py
+Filename: ultralytics/models/nas/predict.py
 Comment: 
 
-Filename: ultralytics/tracker/trackers/bot_sort.py
+Filename: ultralytics/models/nas/val.py
 Comment: 
 
-Filename: ultralytics/tracker/trackers/byte_tracker.py
+Filename: ultralytics/models/rtdetr/__init__.py
 Comment: 
 
-Filename: ultralytics/tracker/utils/__init__.py
+Filename: ultralytics/models/rtdetr/model.py
 Comment: 
 
-Filename: ultralytics/tracker/utils/gmc.py
+Filename: ultralytics/models/rtdetr/predict.py
 Comment: 
 
-Filename: ultralytics/tracker/utils/kalman_filter.py
+Filename: ultralytics/models/rtdetr/train.py
 Comment: 
 
-Filename: ultralytics/tracker/utils/matching.py
+Filename: ultralytics/models/rtdetr/val.py
 Comment: 
 
-Filename: ultralytics/vit/__init__.py
+Filename: ultralytics/models/sam/__init__.py
 Comment: 
 
-Filename: ultralytics/vit/rtdetr/__init__.py
+Filename: ultralytics/models/sam/amg.py
 Comment: 
 
-Filename: ultralytics/vit/rtdetr/model.py
+Filename: ultralytics/models/sam/build.py
 Comment: 
 
-Filename: ultralytics/vit/rtdetr/predict.py
+Filename: ultralytics/models/sam/model.py
 Comment: 
 
-Filename: ultralytics/vit/rtdetr/train.py
+Filename: ultralytics/models/sam/predict.py
 Comment: 
 
-Filename: ultralytics/vit/rtdetr/val.py
+Filename: ultralytics/models/sam/modules/__init__.py
 Comment: 
 
-Filename: ultralytics/vit/sam/__init__.py
+Filename: ultralytics/models/sam/modules/decoders.py
 Comment: 
 
-Filename: ultralytics/vit/sam/amg.py
+Filename: ultralytics/models/sam/modules/encoders.py
 Comment: 
 
-Filename: ultralytics/vit/sam/autosize.py
+Filename: ultralytics/models/sam/modules/sam.py
 Comment: 
 
-Filename: ultralytics/vit/sam/build.py
+Filename: ultralytics/models/sam/modules/tiny_encoder.py
 Comment: 
 
-Filename: ultralytics/vit/sam/model.py
+Filename: ultralytics/models/sam/modules/transformer.py
 Comment: 
 
-Filename: ultralytics/vit/sam/predict.py
+Filename: ultralytics/models/utils/__init__.py
 Comment: 
 
-Filename: ultralytics/vit/sam/modules/__init__.py
+Filename: ultralytics/models/utils/loss.py
 Comment: 
 
-Filename: ultralytics/vit/sam/modules/decoders.py
+Filename: ultralytics/models/utils/ops.py
 Comment: 
 
-Filename: ultralytics/vit/sam/modules/encoders.py
+Filename: ultralytics/models/yolo/__init__.py
 Comment: 
 
-Filename: ultralytics/vit/sam/modules/mask_generator.py
+Filename: ultralytics/models/yolo/classify/__init__.py
 Comment: 
 
-Filename: ultralytics/vit/sam/modules/prompt_predictor.py
+Filename: ultralytics/models/yolo/classify/predict.py
 Comment: 
 
-Filename: ultralytics/vit/sam/modules/sam.py
+Filename: ultralytics/models/yolo/classify/train.py
 Comment: 
 
-Filename: ultralytics/vit/sam/modules/transformer.py
+Filename: ultralytics/models/yolo/classify/val.py
 Comment: 
 
-Filename: ultralytics/vit/utils/__init__.py
+Filename: ultralytics/models/yolo/detect/__init__.py
 Comment: 
 
-Filename: ultralytics/vit/utils/loss.py
+Filename: ultralytics/models/yolo/detect/predict.py
 Comment: 
 
-Filename: ultralytics/vit/utils/ops.py
+Filename: ultralytics/models/yolo/detect/train.py
 Comment: 
 
-Filename: ultralytics/yolo/__init__.py
+Filename: ultralytics/models/yolo/detect/val.py
 Comment: 
 
-Filename: ultralytics/yolo/cfg/__init__.py
+Filename: ultralytics/models/yolo/pose/__init__.py
 Comment: 
 
-Filename: ultralytics/yolo/cfg/default.yaml
+Filename: ultralytics/models/yolo/pose/predict.py
 Comment: 
 
-Filename: ultralytics/yolo/data/__init__.py
+Filename: ultralytics/models/yolo/pose/train.py
 Comment: 
 
-Filename: ultralytics/yolo/data/annotator.py
+Filename: ultralytics/models/yolo/pose/val.py
 Comment: 
 
-Filename: ultralytics/yolo/data/augment.py
+Filename: ultralytics/models/yolo/segment/__init__.py
 Comment: 
 
-Filename: ultralytics/yolo/data/base.py
+Filename: ultralytics/models/yolo/segment/predict.py
 Comment: 
 
-Filename: ultralytics/yolo/data/build.py
+Filename: ultralytics/models/yolo/segment/train.py
 Comment: 
 
-Filename: ultralytics/yolo/data/converter.py
+Filename: ultralytics/models/yolo/segment/val.py
 Comment: 
 
-Filename: ultralytics/yolo/data/dataset.py
-Comment: 
-
-Filename: ultralytics/yolo/data/dataset_wrappers.py
+Filename: ultralytics/nn/__init__.py
 Comment: 
 
-Filename: ultralytics/yolo/data/utils.py
+Filename: ultralytics/nn/autobackend.py
 Comment: 
 
-Filename: ultralytics/yolo/data/dataloaders/__init__.py
+Filename: ultralytics/nn/tasks.py
 Comment: 
 
-Filename: ultralytics/yolo/data/dataloaders/stream_loaders.py
+Filename: ultralytics/nn/modules/__init__.py
 Comment: 
 
-Filename: ultralytics/yolo/data/dataloaders/v5augmentations.py
+Filename: ultralytics/nn/modules/block.py
 Comment: 
 
-Filename: ultralytics/yolo/data/dataloaders/v5loader.py
+Filename: ultralytics/nn/modules/conv.py
 Comment: 
 
-Filename: ultralytics/yolo/engine/__init__.py
+Filename: ultralytics/nn/modules/head.py
 Comment: 
 
-Filename: ultralytics/yolo/engine/exporter.py
+Filename: ultralytics/nn/modules/transformer.py
 Comment: 
 
-Filename: ultralytics/yolo/engine/model.py
+Filename: ultralytics/nn/modules/utils.py
 Comment: 
 
-Filename: ultralytics/yolo/engine/predictor.py
+Filename: ultralytics/trackers/__init__.py
 Comment: 
 
-Filename: ultralytics/yolo/engine/results.py
+Filename: ultralytics/trackers/basetrack.py
 Comment: 
 
-Filename: ultralytics/yolo/engine/trainer.py
+Filename: ultralytics/trackers/bot_sort.py
 Comment: 
 
-Filename: ultralytics/yolo/engine/validator.py
+Filename: ultralytics/trackers/byte_tracker.py
 Comment: 
 
-Filename: ultralytics/yolo/fastsam/__init__.py
+Filename: ultralytics/trackers/track.py
 Comment: 
 
-Filename: ultralytics/yolo/fastsam/model.py
+Filename: ultralytics/trackers/utils/__init__.py
 Comment: 
 
-Filename: ultralytics/yolo/fastsam/predict.py
+Filename: ultralytics/trackers/utils/gmc.py
 Comment: 
 
-Filename: ultralytics/yolo/fastsam/prompt.py
+Filename: ultralytics/trackers/utils/kalman_filter.py
 Comment: 
 
-Filename: ultralytics/yolo/fastsam/utils.py
+Filename: ultralytics/trackers/utils/matching.py
 Comment: 
 
-Filename: ultralytics/yolo/fastsam/val.py
+Filename: ultralytics/utils/__init__.py
 Comment: 
 
-Filename: ultralytics/yolo/nas/__init__.py
+Filename: ultralytics/utils/autobatch.py
 Comment: 
 
-Filename: ultralytics/yolo/nas/model.py
+Filename: ultralytics/utils/benchmarks.py
 Comment: 
 
-Filename: ultralytics/yolo/nas/predict.py
+Filename: ultralytics/utils/checks.py
 Comment: 
 
-Filename: ultralytics/yolo/nas/val.py
+Filename: ultralytics/utils/dist.py
 Comment: 
 
-Filename: ultralytics/yolo/utils/__init__.py
+Filename: ultralytics/utils/downloads.py
 Comment: 
 
-Filename: ultralytics/yolo/utils/autobatch.py
+Filename: ultralytics/utils/errors.py
 Comment: 
 
-Filename: ultralytics/yolo/utils/benchmarks.py
+Filename: ultralytics/utils/files.py
 Comment: 
 
-Filename: ultralytics/yolo/utils/checks.py
+Filename: ultralytics/utils/instance.py
 Comment: 
 
-Filename: ultralytics/yolo/utils/dist.py
+Filename: ultralytics/utils/loss.py
 Comment: 
 
-Filename: ultralytics/yolo/utils/downloads.py
+Filename: ultralytics/utils/metrics.py
 Comment: 
 
-Filename: ultralytics/yolo/utils/errors.py
+Filename: ultralytics/utils/ops.py
 Comment: 
 
-Filename: ultralytics/yolo/utils/files.py
+Filename: ultralytics/utils/patches.py
 Comment: 
 
-Filename: ultralytics/yolo/utils/instance.py
+Filename: ultralytics/utils/plotting.py
 Comment: 
 
-Filename: ultralytics/yolo/utils/loss.py
+Filename: ultralytics/utils/tal.py
 Comment: 
 
-Filename: ultralytics/yolo/utils/metrics.py
+Filename: ultralytics/utils/torch_utils.py
 Comment: 
 
-Filename: ultralytics/yolo/utils/ops.py
+Filename: ultralytics/utils/tuner.py
 Comment: 
 
-Filename: ultralytics/yolo/utils/patches.py
+Filename: ultralytics/utils/callbacks/__init__.py
 Comment: 
 
-Filename: ultralytics/yolo/utils/plotting.py
+Filename: ultralytics/utils/callbacks/base.py
 Comment: 
 
-Filename: ultralytics/yolo/utils/tal.py
+Filename: ultralytics/utils/callbacks/clearml.py
 Comment: 
 
-Filename: ultralytics/yolo/utils/torch_utils.py
+Filename: ultralytics/utils/callbacks/comet.py
 Comment: 
 
-Filename: ultralytics/yolo/utils/tuner.py
+Filename: ultralytics/utils/callbacks/dvc.py
 Comment: 
 
-Filename: ultralytics/yolo/utils/callbacks/__init__.py
+Filename: ultralytics/utils/callbacks/hub.py
 Comment: 
 
-Filename: ultralytics/yolo/utils/callbacks/base.py
+Filename: ultralytics/utils/callbacks/mlflow.py
 Comment: 
 
-Filename: ultralytics/yolo/utils/callbacks/clearml.py
+Filename: ultralytics/utils/callbacks/neptune.py
 Comment: 
 
-Filename: ultralytics/yolo/utils/callbacks/comet.py
+Filename: ultralytics/utils/callbacks/raytune.py
 Comment: 
 
-Filename: ultralytics/yolo/utils/callbacks/dvc.py
+Filename: ultralytics/utils/callbacks/tensorboard.py
 Comment: 
 
-Filename: ultralytics/yolo/utils/callbacks/hub.py
+Filename: ultralytics/utils/callbacks/wb.py
 Comment: 
 
-Filename: ultralytics/yolo/utils/callbacks/mlflow.py
+Filename: ultralytics/yolo/__init__.py
 Comment: 
 
-Filename: ultralytics/yolo/utils/callbacks/neptune.py
+Filename: ultralytics/yolo/cfg/__init__.py
 Comment: 
 
-Filename: ultralytics/yolo/utils/callbacks/raytune.py
+Filename: ultralytics/yolo/data/__init__.py
 Comment: 
 
-Filename: ultralytics/yolo/utils/callbacks/tensorboard.py
+Filename: ultralytics/yolo/engine/__init__.py
 Comment: 
 
-Filename: ultralytics/yolo/utils/callbacks/wb.py
+Filename: ultralytics/yolo/utils/__init__.py
 Comment: 
 
 Filename: ultralytics/yolo/v8/__init__.py
 Comment: 
 
-Filename: ultralytics/yolo/v8/classify/__init__.py
-Comment: 
-
-Filename: ultralytics/yolo/v8/classify/predict.py
-Comment: 
-
-Filename: ultralytics/yolo/v8/classify/train.py
-Comment: 
-
-Filename: ultralytics/yolo/v8/classify/val.py
-Comment: 
-
-Filename: ultralytics/yolo/v8/detect/__init__.py
-Comment: 
-
-Filename: ultralytics/yolo/v8/detect/predict.py
-Comment: 
-
-Filename: ultralytics/yolo/v8/detect/train.py
-Comment: 
-
-Filename: ultralytics/yolo/v8/detect/val.py
-Comment: 
-
-Filename: ultralytics/yolo/v8/pose/__init__.py
-Comment: 
-
-Filename: ultralytics/yolo/v8/pose/predict.py
-Comment: 
-
-Filename: ultralytics/yolo/v8/pose/train.py
-Comment: 
-
-Filename: ultralytics/yolo/v8/pose/val.py
-Comment: 
-
-Filename: ultralytics/yolo/v8/segment/__init__.py
-Comment: 
-
-Filename: ultralytics/yolo/v8/segment/predict.py
-Comment: 
-
-Filename: ultralytics/yolo/v8/segment/train.py
-Comment: 
-
-Filename: ultralytics/yolo/v8/segment/val.py
-Comment: 
-
-Filename: pyppbox_ultralytics-8.0.132.dist-info/LICENSE
+Filename: pyppbox_ultralytics-8.0.142.dist-info/LICENSE
 Comment: 
 
-Filename: pyppbox_ultralytics-8.0.132.dist-info/METADATA
+Filename: pyppbox_ultralytics-8.0.142.dist-info/METADATA
 Comment: 
 
-Filename: pyppbox_ultralytics-8.0.132.dist-info/WHEEL
+Filename: pyppbox_ultralytics-8.0.142.dist-info/WHEEL
 Comment: 
 
-Filename: pyppbox_ultralytics-8.0.132.dist-info/entry_points.txt
+Filename: pyppbox_ultralytics-8.0.142.dist-info/entry_points.txt
 Comment: 
 
-Filename: pyppbox_ultralytics-8.0.132.dist-info/top_level.txt
+Filename: pyppbox_ultralytics-8.0.142.dist-info/top_level.txt
 Comment: 
 
-Filename: pyppbox_ultralytics-8.0.132.dist-info/RECORD
+Filename: pyppbox_ultralytics-8.0.142.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## ultralytics/__init__.py

```diff
@@ -1,14 +1,14 @@
 # Ultralytics YOLO ðŸš€, AGPL-3.0 license
 
-__version__ = '8.0.132'
+__version__ = '8.0.142'
 
+from ultralytics.engine.model import YOLO
 from ultralytics.hub import start
-from ultralytics.vit.rtdetr import RTDETR
-from ultralytics.vit.sam import SAM
-from ultralytics.yolo.engine.model import YOLO
-from ultralytics.yolo.fastsam import FastSAM
-from ultralytics.yolo.nas import NAS
-from ultralytics.yolo.utils.checks import check_yolo as checks
-from ultralytics.yolo.utils.downloads import download
+from ultralytics.models import RTDETR, SAM
+from ultralytics.models.fastsam import FastSAM
+from ultralytics.models.nas import NAS
+from ultralytics.utils import SETTINGS as settings
+from ultralytics.utils.checks import check_yolo as checks
+from ultralytics.utils.downloads import download
 
-__all__ = '__version__', 'YOLO', 'NAS', 'SAM', 'FastSAM', 'RTDETR', 'checks', 'download', 'start'  # allow simpler import
+__all__ = '__version__', 'YOLO', 'NAS', 'SAM', 'FastSAM', 'RTDETR', 'checks', 'download', 'start', 'settings'  # allow simpler import
```

## ultralytics/hub/__init__.py

```diff
@@ -1,15 +1,15 @@
 # Ultralytics YOLO ðŸš€, AGPL-3.0 license
 
 import requests
 
+from ultralytics.data.utils import HUBDatasetStats
 from ultralytics.hub.auth import Auth
 from ultralytics.hub.utils import PREFIX
-from ultralytics.yolo.data.utils import HUBDatasetStats
-from ultralytics.yolo.utils import LOGGER, SETTINGS, USER_CONFIG_DIR, yaml_save
+from ultralytics.utils import LOGGER, SETTINGS, USER_CONFIG_DIR, yaml_save
 
 
 def login(api_key=''):
     """
     Log in to the Ultralytics HUB API using the provided API key.
 
     Args:
@@ -61,15 +61,15 @@
         LOGGER.info(f'{PREFIX}Model reset successfully')
         return
     LOGGER.warning(f'{PREFIX}Model reset failure {r.status_code} {r.reason}')
 
 
 def export_fmts_hub():
     """Returns a list of HUB-supported export formats."""
-    from ultralytics.yolo.engine.exporter import export_formats
+    from ultralytics.engine.exporter import export_formats
     return list(export_formats()['Argument'][1:]) + ['ultralytics_tflite', 'ultralytics_coreml']
 
 
 def export_model(model_id='', format='torchscript'):
     """Export a model to all formats."""
     assert format in export_fmts_hub(), f"Unsupported export format '{format}', valid formats are {export_fmts_hub()}"
     r = requests.post(f'https://api.ultralytics.com/v1/models/{model_id}/export',
```

## ultralytics/hub/auth.py

```diff
@@ -1,13 +1,13 @@
 # Ultralytics YOLO ðŸš€, AGPL-3.0 license
 
 import requests
 
 from ultralytics.hub.utils import HUB_API_ROOT, PREFIX, request_with_credentials
-from ultralytics.yolo.utils import LOGGER, SETTINGS, emojis, is_colab, set_settings
+from ultralytics.utils import LOGGER, SETTINGS, emojis, is_colab
 
 API_KEY_URL = 'https://hub.ultralytics.com/settings?tab=api+keys'
 
 
 class Auth:
     id_token = api_key = model_key = False
 
@@ -41,15 +41,15 @@
             success = self.auth_with_cookies()
         else:
             # Request an API key
             success = self.request_api_key()
 
         # Update SETTINGS with the new API key after successful authentication
         if success:
-            set_settings({'api_key': self.api_key})
+            SETTINGS.update({'api_key': self.api_key})
             # Log that the new login was successful
             if verbose:
                 LOGGER.info(f'{PREFIX}New authentication successful âœ…')
         elif verbose:
             LOGGER.info(f'{PREFIX}Retrieve API key from {API_KEY_URL}')
 
     def request_api_key(self, max_attempts=3):
```

## ultralytics/hub/session.py

```diff
@@ -3,16 +3,16 @@
 import sys
 from pathlib import Path
 from time import sleep
 
 import requests
 
 from ultralytics.hub.utils import HUB_API_ROOT, PREFIX, smart_request
-from ultralytics.yolo.utils import LOGGER, __version__, checks, emojis, is_colab, threaded
-from ultralytics.yolo.utils.errors import HUBModelError
+from ultralytics.utils import LOGGER, __version__, checks, emojis, is_colab, threaded
+from ultralytics.utils.errors import HUBModelError
 
 AGENT_NAME = f'python-{__version__}-colab' if is_colab() else f'python-{__version__}-local'
 
 
 class HUBTrainingSession:
     """
     HUB training session for Ultralytics HUB YOLO models. Handles model initialization, heartbeats, and checkpointing.
@@ -21,19 +21,19 @@
         url (str): Model identifier used to initialize the HUB training session.
 
     Attributes:
         agent_id (str): Identifier for the instance communicating with the server.
         model_id (str): Identifier for the YOLOv5 model being trained.
         model_url (str): URL for the model in Ultralytics HUB.
         api_url (str): API URL for the model in Ultralytics HUB.
-        auth_header (Dict): Authentication header for the Ultralytics HUB API requests.
-        rate_limits (Dict): Rate limits for different API calls (in seconds).
-        timers (Dict): Timers for rate limiting.
-        metrics_queue (Dict): Queue for the model's metrics.
-        model (Dict): Model data fetched from Ultralytics HUB.
+        auth_header (dict): Authentication header for the Ultralytics HUB API requests.
+        rate_limits (dict): Rate limits for different API calls (in seconds).
+        timers (dict): Timers for rate limiting.
+        metrics_queue (dict): Queue for the model's metrics.
+        model (dict): Model data fetched from Ultralytics HUB.
         alive (bool): Indicates if the heartbeat loop is active.
     """
 
     def __init__(self, url):
         """
         Initialize the HUBTrainingSession with the provided model identifier.
```

## ultralytics/hub/utils.py

```diff
@@ -7,17 +7,17 @@
 import threading
 import time
 from pathlib import Path
 
 import requests
 from tqdm import tqdm
 
-from ultralytics.yolo.utils import (ENVIRONMENT, LOGGER, ONLINE, RANK, SETTINGS, TESTS_RUNNING, TQDM_BAR_FORMAT,
-                                    TryExcept, __version__, colorstr, get_git_origin_url, is_colab, is_git_dir,
-                                    is_pip_package)
+from ultralytics.utils import (ENVIRONMENT, LOGGER, ONLINE, RANK, SETTINGS, TESTS_RUNNING, TQDM_BAR_FORMAT, TryExcept,
+                               __version__, colorstr, get_git_origin_url, is_colab, is_git_dir, is_pip_package)
+from ultralytics.utils.downloads import GITHUB_ASSET_NAMES
 
 PREFIX = colorstr('Ultralytics HUB: ')
 HELP_MSG = 'If this issue persists please visit https://github.com/ultralytics/hub/issues for assistance.'
 HUB_API_ROOT = os.environ.get('ULTRALYTICS_HUB_API', 'https://api.ultralytics.com')
 
 
 def request_with_credentials(url: str) -> any:
@@ -190,15 +190,17 @@
         """
         if not self.enabled:
             # Events disabled, do nothing
             return
 
         # Attempt to add to events
         if len(self.events) < 25:  # Events list limited to 25 events (drop any events past this)
-            params = {**self.metadata, **{'task': cfg.task}}
+            params = {
+                **self.metadata, 'task': cfg.task,
+                'model': cfg.model if cfg.model in GITHUB_ASSET_NAMES else 'custom'}
             if cfg.mode == 'export':
                 params['format'] = cfg.format
             self.events.append({'name': cfg.mode, 'params': params})
 
         # Check rate limit
         t = time.time()
         if (t - self.t) < self.rate_limit:
```

## ultralytics/nn/autobackend.py

```diff
@@ -1,44 +1,43 @@
 # Ultralytics YOLO ðŸš€, AGPL-3.0 license
 
 import ast
 import contextlib
 import json
-import os
 import platform
 import zipfile
 from collections import OrderedDict, namedtuple
 from pathlib import Path
 from urllib.parse import urlparse
 
 import cv2
 import numpy as np
 import torch
 import torch.nn as nn
 from PIL import Image
 
-from ultralytics.yolo.utils import ARM64, LINUX, LOGGER, ROOT, yaml_load
-from ultralytics.yolo.utils.checks import check_requirements, check_suffix, check_version, check_yaml
-from ultralytics.yolo.utils.downloads import attempt_download_asset, is_url
-from ultralytics.yolo.utils.ops import xywh2xyxy
+from ultralytics.utils import ARM64, LINUX, LOGGER, ROOT, yaml_load
+from ultralytics.utils.checks import check_requirements, check_suffix, check_version, check_yaml
+from ultralytics.utils.downloads import attempt_download_asset, is_url
+from ultralytics.utils.ops import xywh2xyxy
 
 
 def check_class_names(names):
     """Check class names. Map imagenet class codes to human-readable names if required. Convert lists to dicts."""
     if isinstance(names, list):  # names is a list
         names = dict(enumerate(names))  # convert to dict
     if isinstance(names, dict):
         # Convert 1) string keys to int, i.e. '0' to 0, and non-string values to strings, i.e. True to 'True'
         names = {int(k): str(v) for k, v in names.items()}
         n = len(names)
         if max(names.keys()) >= n:
             raise KeyError(f'{n}-class dataset requires class indices 0-{n - 1}, but you have invalid class indices '
                            f'{min(names.keys())}-{max(names.keys())} defined in your dataset YAML.')
         if isinstance(names[0], str) and names[0].startswith('n0'):  # imagenet class codes, i.e. 'n01440764'
-            map = yaml_load(ROOT / 'datasets/ImageNet.yaml')['map']  # human-readable names
+            map = yaml_load(ROOT / 'cfg/datasets/ImageNet.yaml')['map']  # human-readable names
             names = {k: map[v] for k, v in names.items()}
     return names
 
 
 class AutoBackend(nn.Module):
 
     def __init__(self,
@@ -79,24 +78,31 @@
             | ncnn                  | *_ncnn_model     |
         """
         super().__init__()
         w = str(weights[0] if isinstance(weights, list) else weights)
         nn_module = isinstance(weights, torch.nn.Module)
         pt, jit, onnx, xml, engine, coreml, saved_model, pb, tflite, edgetpu, tfjs, paddle, ncnn, triton = \
             self._model_type(w)
-        fp16 &= pt or jit or onnx or engine or nn_module or triton  # FP16
+        fp16 &= pt or jit or onnx or xml or engine or nn_module or triton  # FP16
         nhwc = coreml or saved_model or pb or tflite or edgetpu  # BHWC formats (vs torch BCWH)
         stride = 32  # default stride
         model, metadata = None, None
+
+        # Set device
         cuda = torch.cuda.is_available() and device.type != 'cpu'  # use CUDA
+        if cuda and not any([nn_module, pt, jit, engine]):  # GPU dataloader formats
+            device = torch.device('cpu')
+            cuda = False
+
+        # Download if not local
         if not (pt or triton or nn_module):
-            w = attempt_download_asset(w)  # download if not local
+            w = attempt_download_asset(w)
 
-        # NOTE: special case: in-memory pytorch model
-        if nn_module:
+        # Load model
+        if nn_module:  # in-memory PyTorch model
             model = weights.to(device)
             model = model.fuse(verbose=verbose) if fuse else model
             if hasattr(model, 'kpt_shape'):
                 kpt_shape = model.kpt_shape  # pose-only
             stride = max(int(model.stride.max()), 32)  # model stride
             names = model.module.names if hasattr(model, 'module') else model.names  # get class names
             model.half() if fp16 else model.float()
@@ -131,27 +137,27 @@
             import onnxruntime
             providers = ['CUDAExecutionProvider', 'CPUExecutionProvider'] if cuda else ['CPUExecutionProvider']
             session = onnxruntime.InferenceSession(w, providers=providers)
             output_names = [x.name for x in session.get_outputs()]
             metadata = session.get_modelmeta().custom_metadata_map  # metadata
         elif xml:  # OpenVINO
             LOGGER.info(f'Loading {w} for OpenVINO inference...')
-            check_requirements('openvino')  # requires openvino-dev: https://pypi.org/project/openvino-dev/
+            check_requirements('openvino>=2023.0')  # requires openvino-dev: https://pypi.org/project/openvino-dev/
             from openvino.runtime import Core, Layout, get_batch  # noqa
-            ie = Core()
+            core = Core()
             w = Path(w)
             if not w.is_file():  # if not *.xml
                 w = next(w.glob('*.xml'))  # get *.xml file from *_openvino_model dir
-            network = ie.read_model(model=str(w), weights=w.with_suffix('.bin'))
-            if network.get_parameters()[0].get_layout().empty:
-                network.get_parameters()[0].set_layout(Layout('NCHW'))
-            batch_dim = get_batch(network)
+            ov_model = core.read_model(model=str(w), weights=w.with_suffix('.bin'))
+            if ov_model.get_parameters()[0].get_layout().empty:
+                ov_model.get_parameters()[0].set_layout(Layout('NCHW'))
+            batch_dim = get_batch(ov_model)
             if batch_dim.is_static:
                 batch_size = batch_dim.get_length()
-            executable_network = ie.compile_model(network, device_name='CPU')  # device_name="MYRIAD" for NCS2
+            ov_compiled_model = core.compile_model(ov_model, device_name='AUTO')  # AUTO selects best available device
             metadata = w.parent / 'metadata.yaml'
         elif engine:  # TensorRT
             LOGGER.info(f'Loading {w} for TensorRT inference...')
             try:
                 import tensorrt as trt  # noqa https://developer.nvidia.com/nvidia-tensorrt-download
             except ImportError:
                 if LINUX:
@@ -199,15 +205,15 @@
             keras = False  # assume TF1 saved_model
             model = tf.keras.models.load_model(w) if keras else tf.saved_model.load(w)
             metadata = Path(w) / 'metadata.yaml'
         elif pb:  # GraphDef https://www.tensorflow.org/guide/migrate#a_graphpb_or_graphpbtxt
             LOGGER.info(f'Loading {w} for TensorFlow GraphDef inference...')
             import tensorflow as tf
 
-            from ultralytics.yolo.engine.exporter import gd_outputs
+            from ultralytics.engine.exporter import gd_outputs
 
             def wrap_frozen_graph(gd, inputs, outputs):
                 """Wrap frozen graphs for deployment."""
                 x = tf.compat.v1.wrap_function(lambda: tf.compat.v1.import_graph_def(gd, name=''), [])  # wrapped
                 ge = x.graph.as_graph_element
                 return x.prune(tf.nest.map_structure(ge, inputs), tf.nest.map_structure(ge, outputs))
 
@@ -253,36 +259,34 @@
                 config.enable_use_gpu(memory_pool_init_size_mb=2048, device_id=0)
             predictor = pdi.create_predictor(config)
             input_handle = predictor.get_input_handle(predictor.get_input_names()[0])
             output_names = predictor.get_output_names()
             metadata = w.parents[1] / 'metadata.yaml'
         elif ncnn:  # ncnn
             LOGGER.info(f'Loading {w} for ncnn inference...')
-            check_requirements('git+https://github.com/Tencent/ncnn.git' if ARM64 else 'ncnn')  # requires NCNN
+            check_requirements('git+https://github.com/Tencent/ncnn.git' if ARM64 else 'ncnn')  # requires ncnn
             import ncnn as pyncnn
             net = pyncnn.Net()
-            net.opt.num_threads = os.cpu_count()
             net.opt.use_vulkan_compute = cuda
             w = Path(w)
             if not w.is_file():  # if not *.param
                 w = next(w.glob('*.param'))  # get *.param file from *_ncnn_model dir
             net.load_param(str(w))
             net.load_model(str(w.with_suffix('.bin')))
             metadata = w.parent / 'metadata.yaml'
         elif triton:  # NVIDIA Triton Inference Server
-            LOGGER.info('Triton Inference Server not supported...')
-            '''
-            TODO:
+            """TODO
             check_requirements('tritonclient[all]')
             from utils.triton import TritonRemoteModel
             model = TritonRemoteModel(url=w)
             nhwc = model.runtime.startswith("tensorflow")
-            '''
+            """
+            raise NotImplementedError('Triton Inference Server is not currently supported.')
         else:
-            from ultralytics.yolo.engine.exporter import export_formats
+            from ultralytics.engine.exporter import export_formats
             raise TypeError(f"model='{w}' is not a supported model format. "
                             'See https://docs.ultralytics.com/modes/predict for help.'
                             f'\n\n{export_formats()}')
 
         # Load external metadata YAML
         if isinstance(metadata, (str, Path)) and Path(metadata).exists():
             metadata = yaml_load(metadata)
@@ -335,15 +339,15 @@
             self.net.setInput(im)
             y = self.net.forward()
         elif self.onnx:  # ONNX Runtime
             im = im.cpu().numpy()  # torch to numpy
             y = self.session.run(self.output_names, {self.session.get_inputs()[0].name: im})
         elif self.xml:  # OpenVINO
             im = im.cpu().numpy()  # FP32
-            y = list(self.executable_network([im]).values())
+            y = list(self.ov_compiled_model(im).values())
         elif self.engine:  # TensorRT
             if self.dynamic and im.shape != self.bindings['images'].shape:
                 i = self.model.get_binding_index('images')
                 self.context.set_binding_shape(i, im.shape)  # reshape if dynamic
                 self.bindings['images'] = self.bindings['images']._replace(shape=im.shape)
                 for name in self.output_names:
                     i = self.model.get_binding_index(name)
@@ -392,35 +396,42 @@
             elif self.pb:  # GraphDef
                 y = self.frozen_func(x=self.tf.constant(im))
                 if len(y) == 2 and len(self.names) == 999:  # segments and names not defined
                     ip, ib = (0, 1) if len(y[0].shape) == 4 else (1, 0)  # index of protos, boxes
                     nc = y[ib].shape[1] - y[ip].shape[3] - 4  # y = (1, 160, 160, 32), (1, 116, 8400)
                     self.names = {i: f'class{i}' for i in range(nc)}
             else:  # Lite or Edge TPU
-                input = self.input_details[0]
-                int8 = input['dtype'] == np.int8  # is TFLite quantized int8 model
-                if int8:
-                    scale, zero_point = input['quantization']
-                    im = (im / scale + zero_point).astype(np.int8)  # de-scale
-                self.interpreter.set_tensor(input['index'], im)
+                details = self.input_details[0]
+                integer = details['dtype'] in (np.int8, np.int16)  # is TFLite quantized int8 or int16 model
+                if integer:
+                    scale, zero_point = details['quantization']
+                    im = (im / scale + zero_point).astype(details['dtype'])  # de-scale
+                self.interpreter.set_tensor(details['index'], im)
                 self.interpreter.invoke()
                 y = []
                 for output in self.output_details:
                     x = self.interpreter.get_tensor(output['index'])
-                    if int8:
+                    if integer:
                         scale, zero_point = output['quantization']
                         x = (x.astype(np.float32) - zero_point) * scale  # re-scale
+                    if x.ndim > 2:  # if task is not classification
+                        # Denormalize xywh with input image size
+                        # xywh are normalized in TFLite/EdgeTPU to mitigate quantization error of integer models
+                        # See this PR for details: https://github.com/ultralytics/ultralytics/pull/1695
+                        x[:, 0] *= w
+                        x[:, 1] *= h
+                        x[:, 2] *= w
+                        x[:, 3] *= h
                     y.append(x)
             # TF segment fixes: export is reversed vs ONNX export and protos are transposed
             if len(y) == 2:  # segment with (det, proto) output order reversed
                 if len(y[1].shape) != 4:
                     y = list(reversed(y))  # should be y = (1, 116, 8400), (1, 160, 160, 32)
                 y[1] = np.transpose(y[1], (0, 3, 1, 2))  # should be y = (1, 116, 8400), (1, 32, 160, 160)
             y = [x if isinstance(x, np.ndarray) else x.numpy() for x in y]
-            # y[0][..., :4] *= [w, h, w, h]  # xywh normalized to pixels
 
         # for x in y:
         #     print(type(x), len(x)) if isinstance(x, (list, tuple)) else print(type(x), x.shape)  # debug shapes
         if isinstance(y, (list, tuple)):
             return self.from_numpy(y[0]) if len(y) == 1 else [self.from_numpy(x) for x in y]
         else:
             return self.from_numpy(y)
@@ -466,15 +477,15 @@
         This function takes a path to a model file and returns the model type
 
         Args:
             p: path to the model file. Defaults to path/to/model.pt
         """
         # Return model type from model path, i.e. path='path/to/model.onnx' -> type=onnx
         # types = [pt, jit, onnx, xml, engine, coreml, saved_model, pb, tflite, edgetpu, tfjs, paddle]
-        from ultralytics.yolo.engine.exporter import export_formats
+        from ultralytics.engine.exporter import export_formats
         sf = list(export_formats().Suffix)  # export suffixes
         if not is_url(p, check=False) and not isinstance(p, str):
             check_suffix(p, sf)  # checks
         url = urlparse(p)  # if url may be Triton inference server
         types = [s in Path(p).name for s in sf]
         types[8] &= not types[9]  # tflite &= not edgetpu
         triton = not any(types) and all([any(s in url.scheme for s in ['http', 'grpc']), url.netloc])
```

## ultralytics/nn/tasks.py

```diff
@@ -7,20 +7,20 @@
 import torch
 import torch.nn as nn
 
 from ultralytics.nn.modules import (AIFI, C1, C2, C3, C3TR, SPP, SPPF, Bottleneck, BottleneckCSP, C2f, C3Ghost, C3x,
                                     Classify, Concat, Conv, Conv2, ConvTranspose, Detect, DWConv, DWConvTranspose2d,
                                     Focus, GhostBottleneck, GhostConv, HGBlock, HGStem, Pose, RepC3, RepConv,
                                     RTDETRDecoder, Segment)
-from ultralytics.yolo.utils import DEFAULT_CFG_DICT, DEFAULT_CFG_KEYS, LOGGER, colorstr, emojis, yaml_load
-from ultralytics.yolo.utils.checks import check_requirements, check_suffix, check_yaml
-from ultralytics.yolo.utils.loss import v8ClassificationLoss, v8DetectionLoss, v8PoseLoss, v8SegmentationLoss
-from ultralytics.yolo.utils.plotting import feature_visualization
-from ultralytics.yolo.utils.torch_utils import (fuse_conv_and_bn, fuse_deconv_and_bn, initialize_weights,
-                                                intersect_dicts, make_divisible, model_info, scale_img, time_sync)
+from ultralytics.utils import DEFAULT_CFG_DICT, DEFAULT_CFG_KEYS, LOGGER, colorstr, emojis, yaml_load
+from ultralytics.utils.checks import check_requirements, check_suffix, check_yaml
+from ultralytics.utils.loss import v8ClassificationLoss, v8DetectionLoss, v8PoseLoss, v8SegmentationLoss
+from ultralytics.utils.plotting import feature_visualization
+from ultralytics.utils.torch_utils import (fuse_conv_and_bn, fuse_deconv_and_bn, initialize_weights, intersect_dicts,
+                                           make_divisible, model_info, scale_img, time_sync)
 
 try:
     import thop
 except ImportError:
     thop = None
 
 
@@ -408,15 +408,15 @@
 class RTDETRDetectionModel(DetectionModel):
 
     def __init__(self, cfg='rtdetr-l.yaml', ch=3, nc=None, verbose=True):
         super().__init__(cfg=cfg, ch=ch, nc=nc, verbose=verbose)
 
     def init_criterion(self):
         """Compute the classification loss between predictions and true labels."""
-        from ultralytics.vit.utils.loss import RTDETRDetectionLoss
+        from ultralytics.models.utils.loss import RTDETRDetectionLoss
 
         return RTDETRDetectionLoss(nc=self.nc, use_vfl=True)
 
     def loss(self, batch, preds=None):
         if not hasattr(self, 'criterion'):
             self.criterion = self.init_criterion()
 
@@ -494,32 +494,76 @@
         y = torch.cat(y, 2)  # nms ensemble, y shape(B, HW, C)
         return y, None  # inference, train output
 
 
 # Functions ------------------------------------------------------------------------------------------------------------
 
 
+@contextlib.contextmanager
+def temporary_modules(modules=None):
+    """
+    Context manager for temporarily adding or modifying modules in Python's module cache (`sys.modules`).
+
+    This function can be used to change the module paths during runtime. It's useful when refactoring code,
+    where you've moved a module from one location to another, but you still want to support the old import
+    paths for backwards compatibility.
+
+    Args:
+        modules (dict, optional): A dictionary mapping old module paths to new module paths.
+
+    Example:
+        with temporary_modules({'old.module.path': 'new.module.path'}):
+            import old.module.path  # this will now import new.module.path
+
+    Note:
+        The changes are only in effect inside the context manager and are undone once the context manager exits.
+        Be aware that directly manipulating `sys.modules` can lead to unpredictable results, especially in larger
+        applications or libraries. Use this function with caution.
+    """
+    if not modules:
+        modules = {}
+
+    import importlib
+    import sys
+    try:
+        # Set modules in sys.modules under their old name
+        for old, new in modules.items():
+            sys.modules[old] = importlib.import_module(new)
+
+        yield
+    finally:
+        # Remove the temporary module paths
+        for old in modules:
+            if old in sys.modules:
+                del sys.modules[old]
+
+
 def torch_safe_load(weight):
     """
     This function attempts to load a PyTorch model with the torch.load() function. If a ModuleNotFoundError is raised,
     it catches the error, logs a warning message, and attempts to install the missing module via the
     check_requirements() function. After installation, the function again attempts to load the model using torch.load().
 
     Args:
         weight (str): The file path of the PyTorch model.
 
     Returns:
         (dict): The loaded PyTorch model.
     """
-    from ultralytics.yolo.utils.downloads import attempt_download_asset
+    from ultralytics.utils.downloads import attempt_download_asset
 
     check_suffix(file=weight, suffix='.pt')
     file = attempt_download_asset(weight)  # search online if missing locally
     try:
-        return torch.load(file, map_location='cpu'), file  # load
+        with temporary_modules({
+                'ultralytics.yolo.utils': 'ultralytics.utils',
+                'ultralytics.yolo.v8': 'ultralytics.models.yolo',
+                'ultralytics.yolo.data': 'ultralytics.data'}):  # for legacy 8.0 Classify and Pose models
+            return torch.load(file, map_location='cpu'), file  # load
+
     except ModuleNotFoundError as e:  # e.name is missing module name
         if e.name == 'models':
             raise TypeError(
                 emojis(f'ERROR âŒï¸ {weight} appears to be an Ultralytics YOLOv5 model originally trained '
                        f'with https://github.com/ultralytics/yolov5.\nThis model is NOT forwards compatible with '
                        f'YOLOv8 at https://github.com/ultralytics/ultralytics.'
                        f"\nRecommend fixes are to train a new model using the latest 'ultralytics' package or to "
@@ -597,15 +641,15 @@
             m.recompute_scale_factor = None  # torch 1.11.0 compatibility
 
     # Return model and ckpt
     return model, ckpt
 
 
 def parse_model(d, ch, verbose=True):  # model_dict, input_channels(3)
-    # Parse a YOLO model.yaml dictionary into a PyTorch model
+    """Parse a YOLO model.yaml dictionary into a PyTorch model."""
     import ast
 
     # Args
     max_channels = float('inf')
     nc, act, scales = (d.get(x) for x in ('nc', 'activation', 'scales'))
     depth, width, kpt_shape = (d.get(x, 1.0) for x in ('depth_multiple', 'width_multiple', 'kpt_shape'))
     if scales:
@@ -651,18 +695,20 @@
                 args.insert(4, n)  # number of repeats
                 n = 1
 
         elif m is nn.BatchNorm2d:
             args = [ch[f]]
         elif m is Concat:
             c2 = sum(ch[x] for x in f)
-        elif m in (Detect, Segment, Pose, RTDETRDecoder):
+        elif m in (Detect, Segment, Pose):
             args.append([ch[x] for x in f])
             if m is Segment:
                 args[2] = make_divisible(min(args[2], max_channels) * width, 8)
+        elif m is RTDETRDecoder:  # special case, channels arg must be passed in index 1
+            args.insert(1, [ch[x] for x in f])
         else:
             c2 = ch[f]
 
         m_ = nn.Sequential(*(m(*args) for _ in range(n))) if n > 1 else m(*args)  # module
         t = str(m)[8:-2].replace('__main__.', '')  # module type
         m.np = sum(x.numel() for x in m_.parameters())  # number params
         m_.i, m_.f, m_.type = i, f, t  # attach index, 'from' index, type
```

## ultralytics/nn/modules/block.py

```diff
@@ -162,15 +162,15 @@
     def forward(self, x):
         """Forward pass through the CSP bottleneck with 2 convolutions."""
         a, b = self.cv1(x).chunk(2, 1)
         return self.cv2(torch.cat((self.m(a), b), 1))
 
 
 class C2f(nn.Module):
-    """CSP Bottleneck with 2 convolutions."""
+    """Faster Implementation of CSP Bottleneck with 2 convolutions."""
 
     def __init__(self, c1, c2, n=1, shortcut=False, g=1, e=0.5):  # ch_in, ch_out, number, shortcut, groups, expansion
         super().__init__()
         self.c = int(c2 * e)  # hidden channels
         self.cv1 = Conv(c1, 2 * self.c, 1, 1)
         self.cv2 = Conv((2 + n) * self.c, c2, 1)  # optional act=FReLU(c2)
         self.m = nn.ModuleList(Bottleneck(self.c, self.c, shortcut, g, k=((3, 3), (3, 3)), e=1.0) for _ in range(n))
```

## ultralytics/nn/modules/head.py

```diff
@@ -5,15 +5,15 @@
 
 import math
 
 import torch
 import torch.nn as nn
 from torch.nn.init import constant_, xavier_uniform_
 
-from ultralytics.yolo.utils.tal import dist2bbox, make_anchors
+from ultralytics.utils.tal import dist2bbox, make_anchors
 
 from .block import DFL, Proto
 from .conv import Conv
 from .transformer import MLP, DeformableTransformerDecoder, DeformableTransformerDecoderLayer
 from .utils import bias_init_with_prob, linear_init_
 
 __all__ = 'Detect', 'Segment', 'Pose', 'Classify', 'RTDETRDecoder'
@@ -54,14 +54,24 @@
         x_cat = torch.cat([xi.view(shape[0], self.no, -1) for xi in x], 2)
         if self.export and self.format in ('saved_model', 'pb', 'tflite', 'edgetpu', 'tfjs'):  # avoid TF FlexSplitV ops
             box = x_cat[:, :self.reg_max * 4]
             cls = x_cat[:, self.reg_max * 4:]
         else:
             box, cls = x_cat.split((self.reg_max * 4, self.nc), 1)
         dbox = dist2bbox(self.dfl(box), self.anchors.unsqueeze(0), xywh=True, dim=1) * self.strides
+
+        if self.export and self.format in ('tflite', 'edgetpu'):
+            # Normalize xywh with image size to mitigate quantization error of TFLite integer models as done in YOLOv5:
+            # https://github.com/ultralytics/yolov5/blob/0c8de3fca4a702f8ff5c435e67f378d1fce70243/models/tf.py#L307-L309
+            # See this PR for details: https://github.com/ultralytics/ultralytics/pull/1695
+            img_h = shape[2] * self.stride[0]
+            img_w = shape[3] * self.stride[0]
+            img_size = torch.tensor([img_w, img_h, img_w, img_h], device=dbox.device).reshape(1, 4, 1)
+            dbox /= img_size
+
         y = torch.cat((dbox, cls.sigmoid()), 1)
         return y if self.export else (y, x)
 
     def bias_init(self):
         """Initialize Detect() biases, WARNING: requires stride availability."""
         m = self  # self.model[-1]  # Detect() module
         # cf = torch.bincount(torch.tensor(np.concatenate(dataset.labels, 0)[:, 0]).long(), minlength=nc) + 1
@@ -215,15 +225,15 @@
         # decoder head
         self.dec_score_head = nn.ModuleList([nn.Linear(hd, nc) for _ in range(ndl)])
         self.dec_bbox_head = nn.ModuleList([MLP(hd, hd, 4, num_layers=3) for _ in range(ndl)])
 
         self._reset_parameters()
 
     def forward(self, x, batch=None):
-        from ultralytics.vit.utils.ops import get_cdn_group
+        from ultralytics.models.utils.ops import get_cdn_group
 
         # input projection and embedding
         feats, shapes = self._get_encoder_input(x)
 
         # prepare denoising training
         dn_embed, dn_bbox, attn_mask, dn_meta = \
             get_cdn_group(batch,
```

## ultralytics/yolo/cfg/__init__.py

```diff
@@ -1,421 +1,10 @@
-# Ultralytics YOLO ðŸš€, AGPL-3.0 license
-
-import contextlib
-import re
-import shutil
+import importlib
 import sys
-from difflib import get_close_matches
-from pathlib import Path
-from types import SimpleNamespace
-from typing import Dict, List, Union
-
-from ultralytics.yolo.utils import (DEFAULT_CFG, DEFAULT_CFG_DICT, DEFAULT_CFG_PATH, LOGGER, ROOT, USER_CONFIG_DIR,
-                                    IterableSimpleNamespace, __version__, checks, colorstr, deprecation_warn,
-                                    get_settings, yaml_load, yaml_print)
-
-# Define valid tasks and modes
-MODES = 'train', 'val', 'predict', 'export', 'track', 'benchmark'
-TASKS = 'detect', 'segment', 'classify', 'pose'
-TASK2DATA = {'detect': 'coco8.yaml', 'segment': 'coco8-seg.yaml', 'classify': 'imagenet100', 'pose': 'coco8-pose.yaml'}
-TASK2MODEL = {
-    'detect': 'yolov8n.pt',
-    'segment': 'yolov8n-seg.pt',
-    'classify': 'yolov8n-cls.pt',
-    'pose': 'yolov8n-pose.pt'}
-TASK2METRIC = {
-    'detect': 'metrics/mAP50-95(B)',
-    'segment': 'metrics/mAP50-95(M)',
-    'classify': 'metrics/accuracy_top1',
-    'pose': 'metrics/mAP50-95(P)'}
-
-
-CLI_HELP_MSG = \
-    f"""
-    Arguments received: {str(['yolo'] + sys.argv[1:])}. Ultralytics 'yolo' commands use the following syntax:
-
-        yolo TASK MODE ARGS
-
-        Where   TASK (optional) is one of {TASKS}
-                MODE (required) is one of {MODES}
-                ARGS (optional) are any number of custom 'arg=value' pairs like 'imgsz=320' that override defaults.
-                    See all ARGS at https://docs.ultralytics.com/usage/cfg or with 'yolo cfg'
-
-    1. Train a detection model for 10 epochs with an initial learning_rate of 0.01
-        yolo train data=coco128.yaml model=yolov8n.pt epochs=10 lr0=0.01
-
-    2. Predict a YouTube video using a pretrained segmentation model at image size 320:
-        yolo predict model=yolov8n-seg.pt source='https://youtu.be/Zgi9g1ksQHc' imgsz=320
-
-    3. Val a pretrained detection model at batch-size 1 and image size 640:
-        yolo val model=yolov8n.pt data=coco128.yaml batch=1 imgsz=640
-
-    4. Export a YOLOv8n classification model to ONNX format at image size 224 by 128 (no TASK required)
-        yolo export model=yolov8n-cls.pt format=onnx imgsz=224,128
-
-    5. Run special commands:
-        yolo help
-        yolo checks
-        yolo version
-        yolo settings
-        yolo copy-cfg
-        yolo cfg
-
-    Docs: https://docs.ultralytics.com
-    Community: https://community.ultralytics.com
-    GitHub: https://github.com/ultralytics/ultralytics
-    """
-
-# Define keys for arg type checks
-CFG_FLOAT_KEYS = 'warmup_epochs', 'box', 'cls', 'dfl', 'degrees', 'shear'
-CFG_FRACTION_KEYS = ('dropout', 'iou', 'lr0', 'lrf', 'momentum', 'weight_decay', 'warmup_momentum', 'warmup_bias_lr',
-                     'label_smoothing', 'hsv_h', 'hsv_s', 'hsv_v', 'translate', 'scale', 'perspective', 'flipud',
-                     'fliplr', 'mosaic', 'mixup', 'copy_paste', 'conf', 'iou', 'fraction')  # fraction floats 0.0 - 1.0
-CFG_INT_KEYS = ('epochs', 'patience', 'batch', 'workers', 'seed', 'close_mosaic', 'mask_ratio', 'max_det', 'vid_stride',
-                'line_width', 'workspace', 'nbs', 'save_period')
-CFG_BOOL_KEYS = ('save', 'exist_ok', 'verbose', 'deterministic', 'single_cls', 'rect', 'cos_lr', 'overlap_mask', 'val',
-                 'save_json', 'save_hybrid', 'half', 'dnn', 'plots', 'show', 'save_txt', 'save_conf', 'save_crop',
-                 'show_labels', 'show_conf', 'visualize', 'augment', 'agnostic_nms', 'retina_masks', 'boxes', 'keras',
-                 'optimize', 'int8', 'dynamic', 'simplify', 'nms', 'v5loader', 'profile')
-
-
-def cfg2dict(cfg):
-    """
-    Convert a configuration object to a dictionary, whether it is a file path, a string, or a SimpleNamespace object.
-
-    Args:
-        cfg (str | Path | SimpleNamespace): Configuration object to be converted to a dictionary.
-
-    Returns:
-        cfg (dict): Configuration object in dictionary format.
-    """
-    if isinstance(cfg, (str, Path)):
-        cfg = yaml_load(cfg)  # load dict
-    elif isinstance(cfg, SimpleNamespace):
-        cfg = vars(cfg)  # convert to dict
-    return cfg
-
-
-def get_cfg(cfg: Union[str, Path, Dict, SimpleNamespace] = DEFAULT_CFG_DICT, overrides: Dict = None):
-    """
-    Load and merge configuration data from a file or dictionary.
-
-    Args:
-        cfg (str | Path | Dict | SimpleNamespace): Configuration data.
-        overrides (str | Dict | optional): Overrides in the form of a file name or a dictionary. Default is None.
-
-    Returns:
-        (SimpleNamespace): Training arguments namespace.
-    """
-    cfg = cfg2dict(cfg)
-
-    # Merge overrides
-    if overrides:
-        overrides = cfg2dict(overrides)
-        check_cfg_mismatch(cfg, overrides)
-        cfg = {**cfg, **overrides}  # merge cfg and overrides dicts (prefer overrides)
-
-    # Special handling for numeric project/name
-    for k in 'project', 'name':
-        if k in cfg and isinstance(cfg[k], (int, float)):
-            cfg[k] = str(cfg[k])
-    if cfg.get('name') == 'model':  # assign model to 'name' arg
-        cfg['name'] = cfg.get('model', '').split('.')[0]
-        LOGGER.warning(f"WARNING âš ï¸ 'name=model' automatically updated to 'name={cfg['name']}'.")
-
-    # Type and Value checks
-    for k, v in cfg.items():
-        if v is not None:  # None values may be from optional args
-            if k in CFG_FLOAT_KEYS and not isinstance(v, (int, float)):
-                raise TypeError(f"'{k}={v}' is of invalid type {type(v).__name__}. "
-                                f"Valid '{k}' types are int (i.e. '{k}=0') or float (i.e. '{k}=0.5')")
-            elif k in CFG_FRACTION_KEYS:
-                if not isinstance(v, (int, float)):
-                    raise TypeError(f"'{k}={v}' is of invalid type {type(v).__name__}. "
-                                    f"Valid '{k}' types are int (i.e. '{k}=0') or float (i.e. '{k}=0.5')")
-                if not (0.0 <= v <= 1.0):
-                    raise ValueError(f"'{k}={v}' is an invalid value. "
-                                     f"Valid '{k}' values are between 0.0 and 1.0.")
-            elif k in CFG_INT_KEYS and not isinstance(v, int):
-                raise TypeError(f"'{k}={v}' is of invalid type {type(v).__name__}. "
-                                f"'{k}' must be an int (i.e. '{k}=8')")
-            elif k in CFG_BOOL_KEYS and not isinstance(v, bool):
-                raise TypeError(f"'{k}={v}' is of invalid type {type(v).__name__}. "
-                                f"'{k}' must be a bool (i.e. '{k}=True' or '{k}=False')")
-
-    # Return instance
-    return IterableSimpleNamespace(**cfg)
-
-
-def _handle_deprecation(custom):
-    """
-    Hardcoded function to handle deprecated config keys
-    """
-
-    for key in custom.copy().keys():
-        if key == 'hide_labels':
-            deprecation_warn(key, 'show_labels')
-            custom['show_labels'] = custom.pop('hide_labels') == 'False'
-        if key == 'hide_conf':
-            deprecation_warn(key, 'show_conf')
-            custom['show_conf'] = custom.pop('hide_conf') == 'False'
-        if key == 'line_thickness':
-            deprecation_warn(key, 'line_width')
-            custom['line_width'] = custom.pop('line_thickness')
-
-    return custom
-
-
-def check_cfg_mismatch(base: Dict, custom: Dict, e=None):
-    """
-    This function checks for any mismatched keys between a custom configuration list and a base configuration list.
-    If any mismatched keys are found, the function prints out similar keys from the base list and exits the program.
-
-    Args:
-        custom (Dict): a dictionary of custom configuration options
-        base (Dict): a dictionary of base configuration options
-    """
-    custom = _handle_deprecation(custom)
-    base, custom = (set(x.keys()) for x in (base, custom))
-    mismatched = [x for x in custom if x not in base]
-    if mismatched:
-        string = ''
-        for x in mismatched:
-            matches = get_close_matches(x, base)  # key list
-            matches = [f'{k}={DEFAULT_CFG_DICT[k]}' if DEFAULT_CFG_DICT.get(k) is not None else k for k in matches]
-            match_str = f'Similar arguments are i.e. {matches}.' if matches else ''
-            string += f"'{colorstr('red', 'bold', x)}' is not a valid YOLO argument. {match_str}\n"
-        raise SyntaxError(string + CLI_HELP_MSG) from e
-
-
-def merge_equals_args(args: List[str]) -> List[str]:
-    """
-    Merges arguments around isolated '=' args in a list of strings.
-    The function considers cases where the first argument ends with '=' or the second starts with '=',
-    as well as when the middle one is an equals sign.
-
-    Args:
-        args (List[str]): A list of strings where each element is an argument.
-
-    Returns:
-        List[str]: A list of strings where the arguments around isolated '=' are merged.
-    """
-    new_args = []
-    for i, arg in enumerate(args):
-        if arg == '=' and 0 < i < len(args) - 1:  # merge ['arg', '=', 'val']
-            new_args[-1] += f'={args[i + 1]}'
-            del args[i + 1]
-        elif arg.endswith('=') and i < len(args) - 1 and '=' not in args[i + 1]:  # merge ['arg=', 'val']
-            new_args.append(f'{arg}{args[i + 1]}')
-            del args[i + 1]
-        elif arg.startswith('=') and i > 0:  # merge ['arg', '=val']
-            new_args[-1] += arg
-        else:
-            new_args.append(arg)
-    return new_args
-
-
-def handle_yolo_hub(args: List[str]) -> None:
-    """
-    Handle Ultralytics HUB command-line interface (CLI) commands.
-
-    This function processes Ultralytics HUB CLI commands such as login and logout.
-    It should be called when executing a script with arguments related to HUB authentication.
-
-    Args:
-        args (List[str]): A list of command line arguments
-
-    Example:
-        python my_script.py hub login your_api_key
-    """
-    from ultralytics import hub
-
-    if args[0] == 'login':
-        key = args[1] if len(args) > 1 else ''
-        # Log in to Ultralytics HUB using the provided API key
-        hub.login(key)
-    elif args[0] == 'logout':
-        # Log out from Ultralytics HUB
-        hub.logout()
-
-
-def handle_yolo_settings(args: List[str]) -> None:
-    """
-    Handle YOLO settings command-line interface (CLI) commands.
-
-    This function processes YOLO settings CLI commands such as reset.
-    It should be called when executing a script with arguments related to YOLO settings management.
-
-    Args:
-        args (List[str]): A list of command line arguments for YOLO settings management.
-
-    Example:
-        python my_script.py yolo settings reset
-    """
-    path = USER_CONFIG_DIR / 'settings.yaml'  # get SETTINGS YAML file path
-    if any(args) and args[0] == 'reset':
-        path.unlink()  # delete the settings file
-        get_settings()  # create new settings
-        LOGGER.info('Settings reset successfully')  # inform the user that settings have been reset
-    yaml_print(path)  # print the current settings
-
-
-def entrypoint(debug=''):
-    """
-    This function is the ultralytics package entrypoint, it's responsible for parsing the command line arguments passed
-    to the package.
-
-    This function allows for:
-    - passing mandatory YOLO args as a list of strings
-    - specifying the task to be performed, either 'detect', 'segment' or 'classify'
-    - specifying the mode, either 'train', 'val', 'test', or 'predict'
-    - running special modes like 'checks'
-    - passing overrides to the package's configuration
-
-    It uses the package's default cfg and initializes it using the passed overrides.
-    Then it calls the CLI function with the composed cfg
-    """
-    args = (debug.split(' ') if debug else sys.argv)[1:]
-    if not args:  # no arguments passed
-        LOGGER.info(CLI_HELP_MSG)
-        return
-
-    special = {
-        'help': lambda: LOGGER.info(CLI_HELP_MSG),
-        'checks': checks.check_yolo,
-        'version': lambda: LOGGER.info(__version__),
-        'settings': lambda: handle_yolo_settings(args[1:]),
-        'cfg': lambda: yaml_print(DEFAULT_CFG_PATH),
-        'hub': lambda: handle_yolo_hub(args[1:]),
-        'login': lambda: handle_yolo_hub(args),
-        'copy-cfg': copy_default_cfg}
-    full_args_dict = {**DEFAULT_CFG_DICT, **{k: None for k in TASKS}, **{k: None for k in MODES}, **special}
-
-    # Define common mis-uses of special commands, i.e. -h, -help, --help
-    special.update({k[0]: v for k, v in special.items()})  # singular
-    special.update({k[:-1]: v for k, v in special.items() if len(k) > 1 and k.endswith('s')})  # singular
-    special = {**special, **{f'-{k}': v for k, v in special.items()}, **{f'--{k}': v for k, v in special.items()}}
-
-    overrides = {}  # basic overrides, i.e. imgsz=320
-    for a in merge_equals_args(args):  # merge spaces around '=' sign
-        if a.startswith('--'):
-            LOGGER.warning(f"WARNING âš ï¸ '{a}' does not require leading dashes '--', updating to '{a[2:]}'.")
-            a = a[2:]
-        if a.endswith(','):
-            LOGGER.warning(f"WARNING âš ï¸ '{a}' does not require trailing comma ',', updating to '{a[:-1]}'.")
-            a = a[:-1]
-        if '=' in a:
-            try:
-                re.sub(r' *= *', '=', a)  # remove spaces around equals sign
-                k, v = a.split('=', 1)  # split on first '=' sign
-                assert v, f"missing '{k}' value"
-                if k == 'cfg':  # custom.yaml passed
-                    LOGGER.info(f'Overriding {DEFAULT_CFG_PATH} with {v}')
-                    overrides = {k: val for k, val in yaml_load(checks.check_yaml(v)).items() if k != 'cfg'}
-                else:
-                    if v.lower() == 'none':
-                        v = None
-                    elif v.lower() == 'true':
-                        v = True
-                    elif v.lower() == 'false':
-                        v = False
-                    else:
-                        with contextlib.suppress(Exception):
-                            v = eval(v)
-                    overrides[k] = v
-            except (NameError, SyntaxError, ValueError, AssertionError) as e:
-                check_cfg_mismatch(full_args_dict, {a: ''}, e)
-
-        elif a in TASKS:
-            overrides['task'] = a
-        elif a in MODES:
-            overrides['mode'] = a
-        elif a.lower() in special:
-            special[a.lower()]()
-            return
-        elif a in DEFAULT_CFG_DICT and isinstance(DEFAULT_CFG_DICT[a], bool):
-            overrides[a] = True  # auto-True for default bool args, i.e. 'yolo show' sets show=True
-        elif a in DEFAULT_CFG_DICT:
-            raise SyntaxError(f"'{colorstr('red', 'bold', a)}' is a valid YOLO argument but is missing an '=' sign "
-                              f"to set its value, i.e. try '{a}={DEFAULT_CFG_DICT[a]}'\n{CLI_HELP_MSG}")
-        else:
-            check_cfg_mismatch(full_args_dict, {a: ''})
-
-    # Check keys
-    check_cfg_mismatch(full_args_dict, overrides)
-
-    # Mode
-    mode = overrides.get('mode', None)
-    if mode is None:
-        mode = DEFAULT_CFG.mode or 'predict'
-        LOGGER.warning(f"WARNING âš ï¸ 'mode' is missing. Valid modes are {MODES}. Using default 'mode={mode}'.")
-    elif mode not in MODES:
-        if mode not in ('checks', checks):
-            raise ValueError(f"Invalid 'mode={mode}'. Valid modes are {MODES}.\n{CLI_HELP_MSG}")
-        LOGGER.warning("WARNING âš ï¸ 'yolo mode=checks' is deprecated. Use 'yolo checks' instead.")
-        checks.check_yolo()
-        return
-
-    # Task
-    task = overrides.pop('task', None)
-    if task:
-        if task not in TASKS:
-            raise ValueError(f"Invalid 'task={task}'. Valid tasks are {TASKS}.\n{CLI_HELP_MSG}")
-        if 'model' not in overrides:
-            overrides['model'] = TASK2MODEL[task]
-
-    # Model
-    model = overrides.pop('model', DEFAULT_CFG.model)
-    if model is None:
-        model = 'yolov8n.pt'
-        LOGGER.warning(f"WARNING âš ï¸ 'model' is missing. Using default 'model={model}'.")
-    overrides['model'] = model
-    if 'rtdetr' in model.lower():  # guess architecture
-        from ultralytics import RTDETR
-        model = RTDETR(model)  # no task argument
-    elif 'sam' in model.lower():
-        from ultralytics import SAM
-        model = SAM(model)
-    else:
-        from ultralytics import YOLO
-        model = YOLO(model, task=task)
-    if isinstance(overrides.get('pretrained'), str):
-        model.load(overrides['pretrained'])
-
-    # Task Update
-    if task != model.task:
-        if task:
-            LOGGER.warning(f"WARNING âš ï¸ conflicting 'task={task}' passed with 'task={model.task}' model. "
-                           f"Ignoring 'task={task}' and updating to 'task={model.task}' to match model.")
-        task = model.task
-
-    # Mode
-    if mode in ('predict', 'track') and 'source' not in overrides:
-        overrides['source'] = DEFAULT_CFG.source or ROOT / 'assets' if (ROOT / 'assets').exists() \
-            else 'https://ultralytics.com/images/bus.jpg'
-        LOGGER.warning(f"WARNING âš ï¸ 'source' is missing. Using default 'source={overrides['source']}'.")
-    elif mode in ('train', 'val'):
-        if 'data' not in overrides:
-            overrides['data'] = TASK2DATA.get(task or DEFAULT_CFG.task, DEFAULT_CFG.data)
-            LOGGER.warning(f"WARNING âš ï¸ 'data' is missing. Using default 'data={overrides['data']}'.")
-    elif mode == 'export':
-        if 'format' not in overrides:
-            overrides['format'] = DEFAULT_CFG.format or 'torchscript'
-            LOGGER.warning(f"WARNING âš ï¸ 'format' is missing. Using default 'format={overrides['format']}'.")
-
-    # Run command in python
-    # getattr(model, mode)(**vars(get_cfg(overrides=overrides)))  # default args using default.yaml
-    getattr(model, mode)(**overrides)  # default args from model
-
 
-# Special modes --------------------------------------------------------------------------------------------------------
-def copy_default_cfg():
-    """Copy and create a new default configuration file with '_copy' appended to its name."""
-    new_file = Path.cwd() / DEFAULT_CFG_PATH.name.replace('.yaml', '_copy.yaml')
-    shutil.copy2(DEFAULT_CFG_PATH, new_file)
-    LOGGER.info(f'{DEFAULT_CFG_PATH} copied to {new_file}\n'
-                f"Example YOLO command with this new custom cfg:\n    yolo cfg='{new_file}' imgsz=320 batch=8")
+from ultralytics.utils import LOGGER
 
+# Set modules in sys.modules under their old name
+sys.modules['ultralytics.yolo.cfg'] = importlib.import_module('ultralytics.cfg')
 
-if __name__ == '__main__':
-    # Example Usage: entrypoint(debug='yolo predict model=yolov8n.pt')
-    entrypoint(debug='')
+LOGGER.warning("WARNING âš ï¸ 'ultralytics.yolo.cfg' is deprecated since '8.0.136' and will be removed in '8.1.0'. "
+               "Please use 'ultralytics.cfg' instead.")
```

## ultralytics/yolo/data/__init__.py

```diff
@@ -1,9 +1,17 @@
-# Ultralytics YOLO ðŸš€, AGPL-3.0 license
+import importlib
+import sys
 
-from .base import BaseDataset
-from .build import build_dataloader, build_yolo_dataset, load_inference_source
-from .dataset import ClassificationDataset, SemanticDataset, YOLODataset
-from .dataset_wrappers import MixAndRectDataset
+from ultralytics.utils import LOGGER
 
-__all__ = ('BaseDataset', 'ClassificationDataset', 'MixAndRectDataset', 'SemanticDataset', 'YOLODataset',
-           'build_yolo_dataset', 'build_dataloader', 'load_inference_source')
+# Set modules in sys.modules under their old name
+sys.modules['ultralytics.yolo.data'] = importlib.import_module('ultralytics.data')
+# This is for updating old cls models, or the way in following warning won't work.
+sys.modules['ultralytics.yolo.data.augment'] = importlib.import_module('ultralytics.data.augment')
+
+DATA_WARNING = """WARNING âš ï¸ 'ultralytics.yolo.data' is deprecated since '8.0.136' and will be removed in '8.1.0'. Please use 'ultralytics.data' instead.
+Note this warning may be related to loading older models. You can update your model to current structure with:
+    import torch
+    ckpt = torch.load("model.pt")  # applies to both official and custom models
+    torch.save(ckpt, "updated-model.pt")
+"""
+LOGGER.warning(DATA_WARNING)
```

## ultralytics/yolo/engine/__init__.py

```diff
@@ -0,0 +1,25 @@
+00000000: 696d 706f 7274 2069 6d70 6f72 746c 6962  import importlib
+00000010: 0a69 6d70 6f72 7420 7379 730a 0a66 726f  .import sys..fro
+00000020: 6d20 756c 7472 616c 7974 6963 732e 7574  m ultralytics.ut
+00000030: 696c 7320 696d 706f 7274 204c 4f47 4745  ils import LOGGE
+00000040: 520a 0a23 2053 6574 206d 6f64 756c 6573  R..# Set modules
+00000050: 2069 6e20 7379 732e 6d6f 6475 6c65 7320   in sys.modules 
+00000060: 756e 6465 7220 7468 6569 7220 6f6c 6420  under their old 
+00000070: 6e61 6d65 0a73 7973 2e6d 6f64 756c 6573  name.sys.modules
+00000080: 5b27 756c 7472 616c 7974 6963 732e 796f  ['ultralytics.yo
+00000090: 6c6f 2e65 6e67 696e 6527 5d20 3d20 696d  lo.engine'] = im
+000000a0: 706f 7274 6c69 622e 696d 706f 7274 5f6d  portlib.import_m
+000000b0: 6f64 756c 6528 2775 6c74 7261 6c79 7469  odule('ultralyti
+000000c0: 6373 2e65 6e67 696e 6527 290a 0a4c 4f47  cs.engine')..LOG
+000000d0: 4745 522e 7761 726e 696e 6728 2257 4152  GER.warning("WAR
+000000e0: 4e49 4e47 20e2 9aa0 efb8 8f20 2775 6c74  NING ...... 'ult
+000000f0: 7261 6c79 7469 6373 2e79 6f6c 6f2e 656e  ralytics.yolo.en
+00000100: 6769 6e65 2720 6973 2064 6570 7265 6361  gine' is depreca
+00000110: 7465 6420 7369 6e63 6520 2738 2e30 2e31  ted since '8.0.1
+00000120: 3336 2720 616e 6420 7769 6c6c 2062 6520  36' and will be 
+00000130: 7265 6d6f 7665 6420 696e 2027 382e 312e  removed in '8.1.
+00000140: 3027 2e20 220a 2020 2020 2020 2020 2020  0'. ".          
+00000150: 2020 2020 2022 506c 6561 7365 2075 7365       "Please use
+00000160: 2027 756c 7472 616c 7974 6963 732e 656e   'ultralytics.en
+00000170: 6769 6e65 2720 696e 7374 6561 642e 2229  gine' instead.")
+00000180: 0a                                       .
```

## ultralytics/yolo/utils/__init__.py

```diff
@@ -1,779 +1,15 @@
-# Ultralytics YOLO ðŸš€, AGPL-3.0 license
-
-import contextlib
-import inspect
-import logging.config
-import os
-import platform
-import re
-import subprocess
+import importlib
 import sys
-import threading
-import urllib
-import uuid
-from pathlib import Path
-from types import SimpleNamespace
-from typing import Union
-
-import cv2
-import matplotlib.pyplot as plt
-import numpy as np
-import torch
-import yaml
-
-from ultralytics import __version__
-
-# PyTorch Multi-GPU DDP Constants
-RANK = int(os.getenv('RANK', -1))
-LOCAL_RANK = int(os.getenv('LOCAL_RANK', -1))  # https://pytorch.org/docs/stable/elastic/run.html
-WORLD_SIZE = int(os.getenv('WORLD_SIZE', 1))
-
-# Other Constants
-FILE = Path(__file__).resolve()
-ROOT = FILE.parents[2]  # YOLO
-DEFAULT_CFG_PATH = ROOT / 'yolo/cfg/default.yaml'
-NUM_THREADS = min(8, max(1, os.cpu_count() - 1))  # number of YOLOv5 multiprocessing threads
-AUTOINSTALL = str(os.getenv('YOLO_AUTOINSTALL', True)).lower() == 'true'  # global auto-install mode
-VERBOSE = str(os.getenv('YOLO_VERBOSE', True)).lower() == 'true'  # global verbose mode
-TQDM_BAR_FORMAT = '{l_bar}{bar:10}{r_bar}'  # tqdm bar format
-LOGGING_NAME = 'ultralytics'
-MACOS, LINUX, WINDOWS = (platform.system() == x for x in ['Darwin', 'Linux', 'Windows'])  # environment booleans
-ARM64 = platform.machine() in ('arm64', 'aarch64')  # ARM64 booleans
-HELP_MSG = \
-    """
-    Usage examples for running YOLOv8:
-
-    1. Install the ultralytics package:
-
-        pip install ultralytics
-
-    2. Use the Python SDK:
-
-        from ultralytics import YOLO
-
-        # Load a model
-        model = YOLO('yolov8n.yaml')  # build a new model from scratch
-        model = YOLO("yolov8n.pt")  # load a pretrained model (recommended for training)
-
-        # Use the model
-        results = model.train(data="coco128.yaml", epochs=3)  # train the model
-        results = model.val()  # evaluate model performance on the validation set
-        results = model('https://ultralytics.com/images/bus.jpg')  # predict on an image
-        success = model.export(format='onnx')  # export the model to ONNX format
-
-    3. Use the command line interface (CLI):
-
-        YOLOv8 'yolo' CLI commands use the following syntax:
-
-            yolo TASK MODE ARGS
-
-            Where   TASK (optional) is one of [detect, segment, classify]
-                    MODE (required) is one of [train, val, predict, export]
-                    ARGS (optional) are any number of custom 'arg=value' pairs like 'imgsz=320' that override defaults.
-                        See all ARGS at https://docs.ultralytics.com/usage/cfg or with 'yolo cfg'
-
-        - Train a detection model for 10 epochs with an initial learning_rate of 0.01
-            yolo detect train data=coco128.yaml model=yolov8n.pt epochs=10 lr0=0.01
-
-        - Predict a YouTube video using a pretrained segmentation model at image size 320:
-            yolo segment predict model=yolov8n-seg.pt source='https://youtu.be/Zgi9g1ksQHc' imgsz=320
-
-        - Val a pretrained detection model at batch-size 1 and image size 640:
-            yolo detect val model=yolov8n.pt data=coco128.yaml batch=1 imgsz=640
-
-        - Export a YOLOv8n classification model to ONNX format at image size 224 by 128 (no TASK required)
-            yolo export model=yolov8n-cls.pt format=onnx imgsz=224,128
-
-        - Run special commands:
-            yolo help
-            yolo checks
-            yolo version
-            yolo settings
-            yolo copy-cfg
-            yolo cfg
-
-    Docs: https://docs.ultralytics.com
-    Community: https://community.ultralytics.com
-    GitHub: https://github.com/ultralytics/ultralytics
-    """
-
-# Settings
-torch.set_printoptions(linewidth=320, precision=4, profile='default')
-np.set_printoptions(linewidth=320, formatter={'float_kind': '{:11.5g}'.format})  # format short g, %precision=5
-# cv2.setNumThreads(0)  # prevent OpenCV from multithreading (incompatible with PyTorch DataLoader)
-os.environ['NUMEXPR_MAX_THREADS'] = str(NUM_THREADS)  # NumExpr max threads
-os.environ['CUBLAS_WORKSPACE_CONFIG'] = ':4096:8'  # for deterministic training
-os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'  # suppress verbose TF compiler warnings in Colab
-
-
-class SimpleClass:
-    """
-    Ultralytics SimpleClass is a base class providing helpful string representation, error reporting, and attribute
-    access methods for easier debugging and usage.
-    """
-
-    def __str__(self):
-        """Return a human-readable string representation of the object."""
-        attr = []
-        for a in dir(self):
-            v = getattr(self, a)
-            if not callable(v) and not a.startswith('_'):
-                if isinstance(v, SimpleClass):
-                    # Display only the module and class name for subclasses
-                    s = f'{a}: {v.__module__}.{v.__class__.__name__} object'
-                else:
-                    s = f'{a}: {repr(v)}'
-                attr.append(s)
-        return f'{self.__module__}.{self.__class__.__name__} object with attributes:\n\n' + '\n'.join(attr)
-
-    def __repr__(self):
-        """Return a machine-readable string representation of the object."""
-        return self.__str__()
-
-    def __getattr__(self, attr):
-        """Custom attribute access error message with helpful information."""
-        name = self.__class__.__name__
-        raise AttributeError(f"'{name}' object has no attribute '{attr}'. See valid attributes below.\n{self.__doc__}")
-
-
-class IterableSimpleNamespace(SimpleNamespace):
-    """
-    Ultralytics IterableSimpleNamespace is an extension class of SimpleNamespace that adds iterable functionality and
-    enables usage with dict() and for loops.
-    """
-
-    def __iter__(self):
-        """Return an iterator of key-value pairs from the namespace's attributes."""
-        return iter(vars(self).items())
-
-    def __str__(self):
-        """Return a human-readable string representation of the object."""
-        return '\n'.join(f'{k}={v}' for k, v in vars(self).items())
-
-    def __getattr__(self, attr):
-        """Custom attribute access error message with helpful information."""
-        name = self.__class__.__name__
-        raise AttributeError(f"""
-            '{name}' object has no attribute '{attr}'. This may be caused by a modified or out of date ultralytics
-            'default.yaml' file.\nPlease update your code with 'pip install -U ultralytics' and if necessary replace
-            {DEFAULT_CFG_PATH} with the latest version from
-            https://github.com/ultralytics/ultralytics/blob/main/ultralytics/yolo/cfg/default.yaml
-            """)
-
-    def get(self, key, default=None):
-        """Return the value of the specified key if it exists; otherwise, return the default value."""
-        return getattr(self, key, default)
-
-
-def plt_settings(rcparams=None, backend='Agg'):
-    """
-    Decorator to temporarily set rc parameters and the backend for a plotting function.
-
-    Usage:
-        decorator: @plt_settings({"font.size": 12})
-        context manager: with plt_settings({"font.size": 12}):
-
-    Args:
-        rcparams (dict): Dictionary of rc parameters to set.
-        backend (str, optional): Name of the backend to use. Defaults to 'Agg'.
-
-    Returns:
-        (Callable): Decorated function with temporarily set rc parameters and backend. This decorator can be
-            applied to any function that needs to have specific matplotlib rc parameters and backend for its execution.
-    """
-
-    if rcparams is None:
-        rcparams = {'font.size': 11}
-
-    def decorator(func):
-        """Decorator to apply temporary rc parameters and backend to a function."""
-
-        def wrapper(*args, **kwargs):
-            """Sets rc parameters and backend, calls the original function, and restores the settings."""
-            original_backend = plt.get_backend()
-            plt.switch_backend(backend)
-
-            with plt.rc_context(rcparams):
-                result = func(*args, **kwargs)
-
-            plt.switch_backend(original_backend)
-            return result
-
-        return wrapper
-
-    return decorator
-
-
-def set_logging(name=LOGGING_NAME, verbose=True):
-    """Sets up logging for the given name."""
-    rank = int(os.getenv('RANK', -1))  # rank in world for Multi-GPU trainings
-    level = logging.INFO if verbose and rank in {-1, 0} else logging.ERROR
-    logging.config.dictConfig({
-        'version': 1,
-        'disable_existing_loggers': False,
-        'formatters': {
-            name: {
-                'format': '%(message)s'}},
-        'handlers': {
-            name: {
-                'class': 'logging.StreamHandler',
-                'formatter': name,
-                'level': level}},
-        'loggers': {
-            name: {
-                'level': level,
-                'handlers': [name],
-                'propagate': False}}})
-
-
-def emojis(string=''):
-    """Return platform-dependent emoji-safe version of string."""
-    return string.encode().decode('ascii', 'ignore') if WINDOWS else string
-
-
-class EmojiFilter(logging.Filter):
-    """
-    A custom logging filter class for removing emojis in log messages.
-
-    This filter is particularly useful for ensuring compatibility with Windows terminals
-    that may not support the display of emojis in log messages.
-    """
-
-    def filter(self, record):
-        """Filter logs by emoji unicode characters on windows."""
-        record.msg = emojis(record.msg)
-        return super().filter(record)
-
-
-# Set logger
-set_logging(LOGGING_NAME, verbose=VERBOSE)  # run before defining LOGGER
-LOGGER = logging.getLogger(LOGGING_NAME)  # define globally (used in train.py, val.py, detect.py, etc.)
-if WINDOWS:  # emoji-safe logging
-    LOGGER.addFilter(EmojiFilter())
-
-
-def yaml_save(file='data.yaml', data=None):
-    """
-    Save YAML data to a file.
-
-    Args:
-        file (str, optional): File name. Default is 'data.yaml'.
-        data (dict): Data to save in YAML format.
-
-    Returns:
-        (None): Data is saved to the specified file.
-    """
-    if data is None:
-        data = {}
-    file = Path(file)
-    if not file.parent.exists():
-        # Create parent directories if they don't exist
-        file.parent.mkdir(parents=True, exist_ok=True)
-
-    # Convert Path objects to strings
-    for k, v in data.items():
-        if isinstance(v, Path):
-            data[k] = str(v)
-
-    # Dump data to file in YAML format
-    with open(file, 'w') as f:
-        yaml.safe_dump(data, f, sort_keys=False, allow_unicode=True)
-
-
-def yaml_load(file='data.yaml', append_filename=False):
-    """
-    Load YAML data from a file.
-
-    Args:
-        file (str, optional): File name. Default is 'data.yaml'.
-        append_filename (bool): Add the YAML filename to the YAML dictionary. Default is False.
-
-    Returns:
-        (dict): YAML data and file name.
-    """
-    with open(file, errors='ignore', encoding='utf-8') as f:
-        s = f.read()  # string
-
-        # Remove special characters
-        if not s.isprintable():
-            s = re.sub(r'[^\x09\x0A\x0D\x20-\x7E\x85\xA0-\uD7FF\uE000-\uFFFD\U00010000-\U0010ffff]+', '', s)
-
-        # Add YAML filename to dict and return
-        return {**yaml.safe_load(s), 'yaml_file': str(file)} if append_filename else yaml.safe_load(s)
-
-
-def yaml_print(yaml_file: Union[str, Path, dict]) -> None:
-    """
-    Pretty prints a yaml file or a yaml-formatted dictionary.
-
-    Args:
-        yaml_file: The file path of the yaml file or a yaml-formatted dictionary.
-
-    Returns:
-        None
-    """
-    yaml_dict = yaml_load(yaml_file) if isinstance(yaml_file, (str, Path)) else yaml_file
-    dump = yaml.dump(yaml_dict, sort_keys=False, allow_unicode=True)
-    LOGGER.info(f"Printing '{colorstr('bold', 'black', yaml_file)}'\n\n{dump}")
-
-
-# Default configuration
-DEFAULT_CFG_DICT = yaml_load(DEFAULT_CFG_PATH)
-for k, v in DEFAULT_CFG_DICT.items():
-    if isinstance(v, str) and v.lower() == 'none':
-        DEFAULT_CFG_DICT[k] = None
-DEFAULT_CFG_KEYS = DEFAULT_CFG_DICT.keys()
-DEFAULT_CFG = IterableSimpleNamespace(**DEFAULT_CFG_DICT)
-
-
-def is_colab():
-    """
-    Check if the current script is running inside a Google Colab notebook.
-
-    Returns:
-        (bool): True if running inside a Colab notebook, False otherwise.
-    """
-    return 'COLAB_RELEASE_TAG' in os.environ or 'COLAB_BACKEND_VERSION' in os.environ
-
-
-def is_kaggle():
-    """
-    Check if the current script is running inside a Kaggle kernel.
-
-    Returns:
-        (bool): True if running inside a Kaggle kernel, False otherwise.
-    """
-    return os.environ.get('PWD') == '/kaggle/working' and os.environ.get('KAGGLE_URL_BASE') == 'https://www.kaggle.com'
-
-
-def is_jupyter():
-    """
-    Check if the current script is running inside a Jupyter Notebook.
-    Verified on Colab, Jupyterlab, Kaggle, Paperspace.
-
-    Returns:
-        (bool): True if running inside a Jupyter Notebook, False otherwise.
-    """
-    with contextlib.suppress(Exception):
-        from IPython import get_ipython
-        return get_ipython() is not None
-    return False
-
-
-def is_docker() -> bool:
-    """
-    Determine if the script is running inside a Docker container.
-
-    Returns:
-        (bool): True if the script is running inside a Docker container, False otherwise.
-    """
-    file = Path('/proc/self/cgroup')
-    if file.exists():
-        with open(file) as f:
-            return 'docker' in f.read()
-    else:
-        return False
-
-
-def is_online() -> bool:
-    """
-    Check internet connectivity by attempting to connect to a known online host.
-
-    Returns:
-        (bool): True if connection is successful, False otherwise.
-    """
-    import socket
-
-    for host in '1.1.1.1', '8.8.8.8', '223.5.5.5':  # Cloudflare, Google, AliDNS:
-        try:
-            test_connection = socket.create_connection(address=(host, 53), timeout=2)
-        except (socket.timeout, socket.gaierror, OSError):
-            continue
-        else:
-            # If the connection was successful, close it to avoid a ResourceWarning
-            test_connection.close()
-            return True
-    return False
-
-
-ONLINE = is_online()
-
-
-def is_pip_package(filepath: str = __name__) -> bool:
-    """
-    Determines if the file at the given filepath is part of a pip package.
-
-    Args:
-        filepath (str): The filepath to check.
-
-    Returns:
-        (bool): True if the file is part of a pip package, False otherwise.
-    """
-    import importlib.util
-
-    # Get the spec for the module
-    spec = importlib.util.find_spec(filepath)
-
-    # Return whether the spec is not None and the origin is not None (indicating it is a package)
-    return spec is not None and spec.origin is not None
-
-
-def is_dir_writeable(dir_path: Union[str, Path]) -> bool:
-    """
-    Check if a directory is writeable.
-
-    Args:
-        dir_path (str | Path): The path to the directory.
-
-    Returns:
-        (bool): True if the directory is writeable, False otherwise.
-    """
-    return os.access(str(dir_path), os.W_OK)
-
-
-def is_pytest_running():
-    """
-    Determines whether pytest is currently running or not.
-
-    Returns:
-        (bool): True if pytest is running, False otherwise.
-    """
-    return ('PYTEST_CURRENT_TEST' in os.environ) or ('pytest' in sys.modules) or ('pytest' in Path(sys.argv[0]).stem)
-
-
-def is_github_actions_ci() -> bool:
-    """
-    Determine if the current environment is a GitHub Actions CI Python runner.
-
-    Returns:
-        (bool): True if the current environment is a GitHub Actions CI Python runner, False otherwise.
-    """
-    return 'GITHUB_ACTIONS' in os.environ and 'RUNNER_OS' in os.environ and 'RUNNER_TOOL_CACHE' in os.environ
-
-
-def is_git_dir():
-    """
-    Determines whether the current file is part of a git repository.
-    If the current file is not part of a git repository, returns None.
-
-    Returns:
-        (bool): True if current file is part of a git repository.
-    """
-    return get_git_dir() is not None
-
-
-def get_git_dir():
-    """
-    Determines whether the current file is part of a git repository and if so, returns the repository root directory.
-    If the current file is not part of a git repository, returns None.
-
-    Returns:
-        (Path | None): Git root directory if found or None if not found.
-    """
-    for d in Path(__file__).parents:
-        if (d / '.git').is_dir():
-            return d
-    return None  # no .git dir found
-
-
-def get_git_origin_url():
-    """
-    Retrieves the origin URL of a git repository.
-
-    Returns:
-        (str | None): The origin URL of the git repository.
-    """
-    if is_git_dir():
-        with contextlib.suppress(subprocess.CalledProcessError):
-            origin = subprocess.check_output(['git', 'config', '--get', 'remote.origin.url'])
-            return origin.decode().strip()
-    return None  # if not git dir or on error
-
-
-def get_git_branch():
-    """
-    Returns the current git branch name. If not in a git repository, returns None.
-
-    Returns:
-        (str | None): The current git branch name.
-    """
-    if is_git_dir():
-        with contextlib.suppress(subprocess.CalledProcessError):
-            origin = subprocess.check_output(['git', 'rev-parse', '--abbrev-ref', 'HEAD'])
-            return origin.decode().strip()
-    return None  # if not git dir or on error
-
-
-def get_default_args(func):
-    """Returns a dictionary of default arguments for a function.
-
-    Args:
-        func (callable): The function to inspect.
-
-    Returns:
-        (dict): A dictionary where each key is a parameter name, and each value is the default value of that parameter.
-    """
-    signature = inspect.signature(func)
-    return {k: v.default for k, v in signature.parameters.items() if v.default is not inspect.Parameter.empty}
-
-
-def get_user_config_dir(sub_dir='Ultralytics'):
-    """
-    Get the user config directory.
-
-    Args:
-        sub_dir (str): The name of the subdirectory to create.
-
-    Returns:
-        (Path): The path to the user config directory.
-    """
-    # Return the appropriate config directory for each operating system
-    if WINDOWS:
-        path = Path.home() / 'AppData' / 'Roaming' / sub_dir
-    elif MACOS:  # macOS
-        path = Path.home() / 'Library' / 'Application Support' / sub_dir
-    elif LINUX:
-        path = Path.home() / '.config' / sub_dir
-    else:
-        raise ValueError(f'Unsupported operating system: {platform.system()}')
-
-    # GCP and AWS lambda fix, only /tmp is writeable
-    if not is_dir_writeable(str(path.parent)):
-        path = Path('/tmp') / sub_dir
-        LOGGER.warning(f"WARNING âš ï¸ user config directory is not writeable, defaulting to '{path}'.")
-
-    # Create the subdirectory if it does not exist
-    path.mkdir(parents=True, exist_ok=True)
-
-    return path
-
-
-USER_CONFIG_DIR = Path(os.getenv('YOLO_CONFIG_DIR', get_user_config_dir()))  # Ultralytics settings dir
-SETTINGS_YAML = USER_CONFIG_DIR / 'settings.yaml'
-
-
-def colorstr(*input):
-    """Colors a string https://en.wikipedia.org/wiki/ANSI_escape_code, i.e.  colorstr('blue', 'hello world')."""
-    *args, string = input if len(input) > 1 else ('blue', 'bold', input[0])  # color arguments, string
-    colors = {
-        'black': '\033[30m',  # basic colors
-        'red': '\033[31m',
-        'green': '\033[32m',
-        'yellow': '\033[33m',
-        'blue': '\033[34m',
-        'magenta': '\033[35m',
-        'cyan': '\033[36m',
-        'white': '\033[37m',
-        'bright_black': '\033[90m',  # bright colors
-        'bright_red': '\033[91m',
-        'bright_green': '\033[92m',
-        'bright_yellow': '\033[93m',
-        'bright_blue': '\033[94m',
-        'bright_magenta': '\033[95m',
-        'bright_cyan': '\033[96m',
-        'bright_white': '\033[97m',
-        'end': '\033[0m',  # misc
-        'bold': '\033[1m',
-        'underline': '\033[4m'}
-    return ''.join(colors[x] for x in args) + f'{string}' + colors['end']
-
-
-class TryExcept(contextlib.ContextDecorator):
-    """YOLOv8 TryExcept class. Usage: @TryExcept() decorator or 'with TryExcept():' context manager."""
-
-    def __init__(self, msg='', verbose=True):
-        """Initialize TryExcept class with optional message and verbosity settings."""
-        self.msg = msg
-        self.verbose = verbose
-
-    def __enter__(self):
-        """Executes when entering TryExcept context, initializes instance."""
-        pass
-
-    def __exit__(self, exc_type, value, traceback):
-        """Defines behavior when exiting a 'with' block, prints error message if necessary."""
-        if self.verbose and value:
-            print(emojis(f"{self.msg}{': ' if self.msg else ''}{value}"))
-        return True
-
-
-def threaded(func):
-    """Multi-threads a target function and returns thread. Usage: @threaded decorator."""
-
-    def wrapper(*args, **kwargs):
-        """Multi-threads a given function and returns the thread."""
-        thread = threading.Thread(target=func, args=args, kwargs=kwargs, daemon=True)
-        thread.start()
-        return thread
-
-    return wrapper
-
-
-def set_sentry():
-    """
-    Initialize the Sentry SDK for error tracking and reporting. Only used if sentry_sdk package is installed and
-    sync=True in settings. Run 'yolo settings' to see and update settings YAML file.
-
-    Conditions required to send errors (ALL conditions must be met or no errors will be reported):
-        - sentry_sdk package is installed
-        - sync=True in YOLO settings
-        - pytest is not running
-        - running in a pip package installation
-        - running in a non-git directory
-        - running with rank -1 or 0
-        - online environment
-        - CLI used to run package (checked with 'yolo' as the name of the main CLI command)
-
-    The function also configures Sentry SDK to ignore KeyboardInterrupt and FileNotFoundError
-    exceptions and to exclude events with 'out of memory' in their exception message.
-
-    Additionally, the function sets custom tags and user information for Sentry events.
-    """
-
-    def before_send(event, hint):
-        """
-        Modify the event before sending it to Sentry based on specific exception types and messages.
-
-        Args:
-            event (dict): The event dictionary containing information about the error.
-            hint (dict): A dictionary containing additional information about the error.
-
-        Returns:
-            dict: The modified event or None if the event should not be sent to Sentry.
-        """
-        if 'exc_info' in hint:
-            exc_type, exc_value, tb = hint['exc_info']
-            if exc_type in (KeyboardInterrupt, FileNotFoundError) \
-                    or 'out of memory' in str(exc_value):
-                return None  # do not send event
-
-        event['tags'] = {
-            'sys_argv': sys.argv[0],
-            'sys_argv_name': Path(sys.argv[0]).name,
-            'install': 'git' if is_git_dir() else 'pip' if is_pip_package() else 'other',
-            'os': ENVIRONMENT}
-        return event
-
-    if SETTINGS['sync'] and \
-            RANK in (-1, 0) and \
-            Path(sys.argv[0]).name == 'yolo' and \
-            not TESTS_RUNNING and \
-            ONLINE and \
-            is_pip_package() and \
-            not is_git_dir():
-
-        # If sentry_sdk package is not installed then return and do not use Sentry
-        try:
-            import sentry_sdk  # noqa
-        except ImportError:
-            return
-
-        sentry_sdk.init(
-            dsn='https://5ff1556b71594bfea135ff0203a0d290@o4504521589325824.ingest.sentry.io/4504521592406016',
-            debug=False,
-            traces_sample_rate=1.0,
-            release=__version__,
-            environment='production',  # 'dev' or 'production'
-            before_send=before_send,
-            ignore_errors=[KeyboardInterrupt, FileNotFoundError])
-        sentry_sdk.set_user({'id': SETTINGS['uuid']})  # SHA-256 anonymized UUID hash
-
-        # Disable all sentry logging
-        for logger in 'sentry_sdk', 'sentry_sdk.errors':
-            logging.getLogger(logger).setLevel(logging.CRITICAL)
-
-
-def get_settings(file=SETTINGS_YAML, version='0.0.3'):
-    """
-    Loads a global Ultralytics settings YAML file or creates one with default values if it does not exist.
-
-    Args:
-        file (Path): Path to the Ultralytics settings YAML file. Defaults to 'settings.yaml' in the USER_CONFIG_DIR.
-        version (str): Settings version. If min settings version not met, new default settings will be saved.
-
-    Returns:
-        (dict): Dictionary of settings key-value pairs.
-    """
-    import hashlib
-
-    from ultralytics.yolo.utils.checks import check_version
-    from ultralytics.yolo.utils.torch_utils import torch_distributed_zero_first
-
-    git_dir = get_git_dir()
-    root = git_dir or Path()
-    datasets_root = (root.parent if git_dir and is_dir_writeable(root.parent) else root).resolve()
-    defaults = {
-        'datasets_dir': str(datasets_root / 'datasets'),  # default datasets directory.
-        'weights_dir': str(root / 'weights'),  # default weights directory.
-        'runs_dir': str(root / 'runs'),  # default runs directory.
-        'uuid': hashlib.sha256(str(uuid.getnode()).encode()).hexdigest(),  # SHA-256 anonymized UUID hash
-        'sync': True,  # sync analytics to help with YOLO development
-        'api_key': '',  # Ultralytics HUB API key (https://hub.ultralytics.com/)
-        'settings_version': version}  # Ultralytics settings version
-
-    with torch_distributed_zero_first(RANK):
-        if not file.exists():
-            yaml_save(file, defaults)
-        settings = yaml_load(file)
-
-        # Check that settings keys and types match defaults
-        correct = \
-            settings \
-            and settings.keys() == defaults.keys() \
-            and all(type(a) == type(b) for a, b in zip(settings.values(), defaults.values())) \
-            and check_version(settings['settings_version'], version)
-        if not correct:
-            LOGGER.warning('WARNING âš ï¸ Ultralytics settings reset to defaults. This is normal and may be due to a '
-                           'recent ultralytics package update, but may have overwritten previous settings. '
-                           f"\nView and update settings with 'yolo settings' or at '{file}'")
-            settings = defaults  # merge **defaults with **settings (prefer **settings)
-            yaml_save(file, settings)  # save updated defaults
-
-        return settings
-
-
-def set_settings(kwargs, file=SETTINGS_YAML):
-    """
-    Function that runs on a first-time ultralytics package installation to set up global settings and create necessary
-    directories.
-    """
-    SETTINGS.update(kwargs)
-    yaml_save(file, SETTINGS)
-
-
-def deprecation_warn(arg, new_arg, version=None):
-    """Issue a deprecation warning when a deprecated argument is used, suggesting an updated argument."""
-    if not version:
-        version = float(__version__[:3]) + 0.2  # deprecate after 2nd major release
-    LOGGER.warning(f"WARNING âš ï¸ '{arg}' is deprecated and will be removed in 'ultralytics {version}' in the future. "
-                   f"Please use '{new_arg}' instead.")
-
-
-def clean_url(url):
-    """Strip auth from URL, i.e. https://url.com/file.txt?auth -> https://url.com/file.txt."""
-    url = str(Path(url)).replace(':/', '://')  # Pathlib turns :// -> :/
-    return urllib.parse.unquote(url).split('?')[0]  # '%2F' to '/', split https://url.com/file.txt?auth
-
-
-def url2file(url):
-    """Convert URL to filename, i.e. https://url.com/file.txt?auth -> file.txt."""
-    return Path(clean_url(url)).name
-
-
-# Run below code on yolo/utils init ------------------------------------------------------------------------------------
 
-# Check first-install steps
-PREFIX = colorstr('Ultralytics: ')
-SETTINGS = get_settings()
-DATASETS_DIR = Path(SETTINGS['datasets_dir'])  # global datasets directory
-ENVIRONMENT = 'Colab' if is_colab() else 'Kaggle' if is_kaggle() else 'Jupyter' if is_jupyter() else \
-    'Docker' if is_docker() else platform.system()
-TESTS_RUNNING = is_pytest_running() or is_github_actions_ci()
-set_sentry()
+from ultralytics.utils import LOGGER
 
-# Apply monkey patches if the script is being run from within the parent directory of the script's location
-from .patches import imread, imshow, imwrite
+# Set modules in sys.modules under their old name
+sys.modules['ultralytics.yolo.utils'] = importlib.import_module('ultralytics.utils')
 
-# torch.save = torch_save
-if Path(inspect.stack()[0].filename).parent.parent.as_posix() in inspect.stack()[-1].filename:
-    cv2.imread, cv2.imwrite, cv2.imshow = imread, imwrite, imshow
+UTILS_WARNING = """WARNING âš ï¸ 'ultralytics.yolo.utils' is deprecated since '8.0.136' and will be removed in '8.1.0'. Please use 'ultralytics.utils' instead.
+Note this warning may be related to loading older models. You can update your model to current structure with:
+    import torch
+    ckpt = torch.load("model.pt")  # applies to both official and custom models
+    torch.save(ckpt, "updated-model.pt")
+"""
+LOGGER.warning(UTILS_WARNING)
```

## ultralytics/yolo/v8/__init__.py

```diff
@@ -1,5 +1,10 @@
-# Ultralytics YOLO ðŸš€, AGPL-3.0 license
+import importlib
+import sys
 
-from ultralytics.yolo.v8 import classify, detect, pose, segment
+from ultralytics.utils import LOGGER
 
-__all__ = 'classify', 'segment', 'detect', 'pose'
+# Set modules in sys.modules under their old name
+sys.modules['ultralytics.yolo.v8'] = importlib.import_module('ultralytics.models.yolo')
+
+LOGGER.warning("WARNING âš ï¸ 'ultralytics.yolo.v8' is deprecated since '8.0.136' and will be removed in '8.1.0'. "
+               "Please use 'ultralytics.models.yolo' instead.")
```

## Comparing `ultralytics/models/rt-detr/rtdetr-l.yaml` & `ultralytics/cfg/models/rt-detr/rtdetr-l.yaml`

 * *Files identical despite different names*

## Comparing `ultralytics/models/rt-detr/rtdetr-x.yaml` & `ultralytics/cfg/models/rt-detr/rtdetr-x.yaml`

 * *Files identical despite different names*

## Comparing `ultralytics/models/v3/yolov3-spp.yaml` & `ultralytics/cfg/models/v3/yolov3-spp.yaml`

 * *Files identical despite different names*

## Comparing `ultralytics/models/v3/yolov3-tiny.yaml` & `ultralytics/cfg/models/v3/yolov3-tiny.yaml`

 * *Files identical despite different names*

## Comparing `ultralytics/models/v3/yolov3.yaml` & `ultralytics/cfg/models/v3/yolov3.yaml`

 * *Files identical despite different names*

## Comparing `ultralytics/models/v5/yolov5-p6.yaml` & `ultralytics/cfg/models/v5/yolov5-p6.yaml`

 * *Files identical despite different names*

## Comparing `ultralytics/models/v5/yolov5.yaml` & `ultralytics/cfg/models/v5/yolov5.yaml`

 * *Files identical despite different names*

## Comparing `ultralytics/models/v6/yolov6.yaml` & `ultralytics/cfg/models/v6/yolov6.yaml`

 * *Files identical despite different names*

## Comparing `ultralytics/models/v8/yolov8-cls.yaml` & `ultralytics/cfg/models/v8/yolov8-cls.yaml`

 * *Files identical despite different names*

## Comparing `ultralytics/models/v8/yolov8-p2.yaml` & `ultralytics/cfg/models/v8/yolov8-p2.yaml`

 * *Files identical despite different names*

## Comparing `ultralytics/models/v8/yolov8-p6.yaml` & `ultralytics/cfg/models/v8/yolov8-p6.yaml`

 * *Files identical despite different names*

## Comparing `ultralytics/models/v8/yolov8-pose-p6.yaml` & `ultralytics/cfg/models/v8/yolov8-pose-p6.yaml`

 * *Files identical despite different names*

## Comparing `ultralytics/models/v8/yolov8-pose.yaml` & `ultralytics/cfg/models/v8/yolov8-pose.yaml`

 * *Files identical despite different names*

## Comparing `ultralytics/models/v8/yolov8-rtdetr.yaml` & `ultralytics/cfg/models/v8/yolov8-rtdetr.yaml`

 * *Files identical despite different names*

## Comparing `ultralytics/models/v8/yolov8-seg.yaml` & `ultralytics/cfg/models/v8/yolov8-seg.yaml`

 * *Files identical despite different names*

## Comparing `ultralytics/models/v8/yolov8.yaml` & `ultralytics/cfg/models/v8/yolov8.yaml`

 * *Files identical despite different names*

## Comparing `ultralytics/tracker/track.py` & `ultralytics/trackers/track.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,17 +1,18 @@
 # Ultralytics YOLO ðŸš€, AGPL-3.0 license
 
 from functools import partial
 
 import torch
 
-from ultralytics.yolo.utils import IterableSimpleNamespace, yaml_load
-from ultralytics.yolo.utils.checks import check_yaml
+from ultralytics.utils import IterableSimpleNamespace, yaml_load
+from ultralytics.utils.checks import check_yaml
 
-from .trackers import BOTSORT, BYTETracker
+from .bot_sort import BOTSORT
+from .byte_tracker import BYTETracker
 
 TRACKER_MAP = {'bytetrack': BYTETracker, 'botsort': BOTSORT}
 
 
 def on_predict_start(predictor, persist=False):
     """
     Initialize trackers for object tracking during prediction.
```

## Comparing `ultralytics/tracker/cfg/botsort.yaml` & `ultralytics/cfg/trackers/botsort.yaml`

 * *Files identical despite different names*

## Comparing `ultralytics/tracker/cfg/bytetrack.yaml` & `ultralytics/cfg/trackers/bytetrack.yaml`

 * *Files identical despite different names*

## Comparing `ultralytics/tracker/trackers/basetrack.py` & `ultralytics/trackers/basetrack.py`

 * *Files identical despite different names*

## Comparing `ultralytics/tracker/trackers/bot_sort.py` & `ultralytics/trackers/bot_sort.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,18 +1,18 @@
 # Ultralytics YOLO ðŸš€, AGPL-3.0 license
 
 from collections import deque
 
 import numpy as np
 
-from ..utils import matching
-from ..utils.gmc import GMC
-from ..utils.kalman_filter import KalmanFilterXYWH
 from .basetrack import TrackState
 from .byte_tracker import BYTETracker, STrack
+from .utils import matching
+from .utils.gmc import GMC
+from .utils.kalman_filter import KalmanFilterXYWH
 
 
 class BOTrack(STrack):
     shared_kalman = KalmanFilterXYWH()
 
     def __init__(self, tlwh, score, cls, feat=None, feat_history=50):
         """Initialize YOLOv8 object with temporal parameters, such as feature history, alpha and current features."""
```

## Comparing `ultralytics/tracker/trackers/byte_tracker.py` & `ultralytics/trackers/byte_tracker.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,14 +1,14 @@
 # Ultralytics YOLO ðŸš€, AGPL-3.0 license
 
 import numpy as np
 
-from ..utils import matching
-from ..utils.kalman_filter import KalmanFilterXYAH
 from .basetrack import BaseTrack, TrackState
+from .utils import matching
+from .utils.kalman_filter import KalmanFilterXYAH
 
 
 class STrack(BaseTrack):
     shared_kalman = KalmanFilterXYAH()
 
     def __init__(self, tlwh, score, cls):
         """wait activate."""
```

## Comparing `ultralytics/tracker/utils/gmc.py` & `ultralytics/trackers/utils/gmc.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,15 +1,15 @@
 # Ultralytics YOLO ðŸš€, AGPL-3.0 license
 
 import copy
 
 import cv2
 import numpy as np
 
-from ultralytics.yolo.utils import LOGGER
+from ultralytics.utils import LOGGER
 
 
 class GMC:
 
     def __init__(self, method='sparseOptFlow', downscale=2, verbose=None):
         """Initialize a video tracker with specified parameters."""
         super().__init__()
```

## Comparing `ultralytics/tracker/utils/kalman_filter.py` & `ultralytics/trackers/utils/kalman_filter.py`

 * *Files identical despite different names*

## Comparing `ultralytics/tracker/utils/matching.py` & `ultralytics/trackers/utils/matching.py`

 * *Files 0% similar despite different names*

```diff
@@ -7,15 +7,15 @@
 from .kalman_filter import chi2inv95
 
 try:
     import lap  # for linear_assignment
 
     assert lap.__version__  # verify package is not directory
 except (ImportError, AssertionError, AttributeError):
-    from ultralytics.yolo.utils.checks import check_requirements
+    from ultralytics.utils.checks import check_requirements
 
     check_requirements('lapx>=0.5.2')  # update to lap package from https://github.com/rathaROG/lapx
     import lap
 
 
 def merge_matches(m1, m2, shape):
     """Merge two sets of matches and return matched and unmatched indices."""
```

## Comparing `ultralytics/vit/rtdetr/model.py` & `ultralytics/models/rtdetr/model.py`

 * *Files 2% similar despite different names*

```diff
@@ -3,20 +3,20 @@
 RT-DETR model interface
 """
 
 from pathlib import Path
 
 import torch.nn as nn
 
+from ultralytics.cfg import get_cfg
+from ultralytics.engine.exporter import Exporter
 from ultralytics.nn.tasks import RTDETRDetectionModel, attempt_load_one_weight, yaml_model_load
-from ultralytics.yolo.cfg import get_cfg
-from ultralytics.yolo.engine.exporter import Exporter
-from ultralytics.yolo.utils import DEFAULT_CFG, DEFAULT_CFG_DICT, LOGGER, RANK, ROOT, is_git_dir
-from ultralytics.yolo.utils.checks import check_imgsz
-from ultralytics.yolo.utils.torch_utils import model_info, smart_inference_mode
+from ultralytics.utils import DEFAULT_CFG, DEFAULT_CFG_DICT, LOGGER, RANK, ROOT, is_git_dir
+from ultralytics.utils.checks import check_imgsz
+from ultralytics.utils.torch_utils import model_info, smart_inference_mode
 
 from .predict import RTDETRPredictor
 from .train import RTDETRTrainer
 from .val import RTDETRValidator
 
 
 class RTDETR:
@@ -68,15 +68,15 @@
             source (str | int | PIL | np.ndarray): The source of the image to make predictions on.
                           Accepts all source types accepted by the YOLO model.
             stream (bool): Whether to stream the predictions or not. Defaults to False.
             **kwargs : Additional keyword arguments passed to the predictor.
                        Check the 'configuration' section in the documentation for all available options.
 
         Returns:
-            (List[ultralytics.yolo.engine.results.Results]): The prediction results.
+            (List[ultralytics.engine.results.Results]): The prediction results.
         """
         if source is None:
             source = ROOT / 'assets' if is_git_dir() else 'https://ultralytics.com/images/bus.jpg'
             LOGGER.warning(f"WARNING âš ï¸ 'source' is missing. Using 'source={source}'.")
         overrides = dict(conf=0.25, task='detect', mode='predict')
         overrides.update(kwargs)  # prefer kwargs
         if not self.predictor:
```

## Comparing `ultralytics/vit/rtdetr/predict.py` & `ultralytics/models/rtdetr/predict.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,15 +1,15 @@
 # Ultralytics YOLO ðŸš€, AGPL-3.0 license
 
 import torch
 
-from ultralytics.yolo.data.augment import LetterBox
-from ultralytics.yolo.engine.predictor import BasePredictor
-from ultralytics.yolo.engine.results import Results
-from ultralytics.yolo.utils import ops
+from ultralytics.data.augment import LetterBox
+from ultralytics.engine.predictor import BasePredictor
+from ultralytics.engine.results import Results
+from ultralytics.utils import ops
 
 
 class RTDETRPredictor(BasePredictor):
 
     def postprocess(self, preds, img, orig_imgs):
         """Postprocess predictions and returns a list of Results objects."""
         nd = preds[0].shape[-1]
```

## Comparing `ultralytics/vit/rtdetr/train.py` & `ultralytics/models/rtdetr/train.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,16 +1,16 @@
 # Ultralytics YOLO ðŸš€, AGPL-3.0 license
 
 from copy import copy
 
 import torch
 
+from ultralytics.models.yolo.detect import DetectionTrainer
 from ultralytics.nn.tasks import RTDETRDetectionModel
-from ultralytics.yolo.utils import DEFAULT_CFG, RANK, colorstr
-from ultralytics.yolo.v8.detect import DetectionTrainer
+from ultralytics.utils import DEFAULT_CFG, RANK, colorstr
 
 from .val import RTDETRDataset, RTDETRValidator
 
 
 class RTDETRTrainer(DetectionTrainer):
 
     def get_model(self, cfg=None, weights=None, verbose=True):
```

## Comparing `ultralytics/vit/rtdetr/val.py` & `ultralytics/models/rtdetr/val.py`

 * *Files 6% similar despite different names*

```diff
@@ -2,18 +2,18 @@
 
 from pathlib import Path
 
 import cv2
 import numpy as np
 import torch
 
-from ultralytics.yolo.data import YOLODataset
-from ultralytics.yolo.data.augment import Compose, Format, v8_transforms
-from ultralytics.yolo.utils import colorstr, ops
-from ultralytics.yolo.v8.detect import DetectionValidator
+from ultralytics.data import YOLODataset
+from ultralytics.data.augment import Compose, Format, v8_transforms
+from ultralytics.models.yolo.detect import DetectionValidator
+from ultralytics.utils import colorstr, ops
 
 __all__ = 'RTDETRValidator',  # tuple or list
 
 
 # TODO: Temporarily, RT-DETR does not need padding.
 class RTDETRDataset(YOLODataset):
```

## Comparing `ultralytics/vit/sam/amg.py` & `ultralytics/models/sam/amg.py`

 * *Files identical despite different names*

## Comparing `ultralytics/vit/sam/build.py` & `ultralytics/models/sam/build.py`

 * *Files 14% similar despite different names*

```diff
@@ -6,18 +6,20 @@
 # This source code is licensed under the license found in the
 # LICENSE file in the root directory of this source tree.
 
 from functools import partial
 
 import torch
 
-from ...yolo.utils.downloads import attempt_download_asset
+from ultralytics.utils.downloads import attempt_download_asset
+
 from .modules.decoders import MaskDecoder
 from .modules.encoders import ImageEncoderViT, PromptEncoder
 from .modules.sam import Sam
+from .modules.tiny_encoder import TinyViT
 from .modules.transformer import TwoWayTransformer
 
 
 def build_sam_vit_h(checkpoint=None):
     """Build and return a Segment Anything Model (SAM) h-size model."""
     return _build_sam(
         encoder_embed_dim=1280,
@@ -46,41 +48,68 @@
         encoder_depth=12,
         encoder_num_heads=12,
         encoder_global_attn_indexes=[2, 5, 8, 11],
         checkpoint=checkpoint,
     )
 
 
-def _build_sam(
-    encoder_embed_dim,
-    encoder_depth,
-    encoder_num_heads,
-    encoder_global_attn_indexes,
-    checkpoint=None,
-):
+def build_mobile_sam(checkpoint=None):
+    """Build and return Mobile Segment Anything Model (Mobile-SAM)."""
+    return _build_sam(
+        encoder_embed_dim=[64, 128, 160, 320],
+        encoder_depth=[2, 2, 6, 2],
+        encoder_num_heads=[2, 4, 5, 10],
+        encoder_global_attn_indexes=None,
+        mobile_sam=True,
+        checkpoint=checkpoint,
+    )
+
+
+def _build_sam(encoder_embed_dim,
+               encoder_depth,
+               encoder_num_heads,
+               encoder_global_attn_indexes,
+               checkpoint=None,
+               mobile_sam=False):
     """Builds the selected SAM model architecture."""
     prompt_embed_dim = 256
     image_size = 1024
     vit_patch_size = 16
     image_embedding_size = image_size // vit_patch_size
+    image_encoder = (TinyViT(
+        img_size=1024,
+        in_chans=3,
+        num_classes=1000,
+        embed_dims=encoder_embed_dim,
+        depths=encoder_depth,
+        num_heads=encoder_num_heads,
+        window_sizes=[7, 7, 14, 7],
+        mlp_ratio=4.0,
+        drop_rate=0.0,
+        drop_path_rate=0.0,
+        use_checkpoint=False,
+        mbconv_expand_ratio=4.0,
+        local_conv_size=3,
+        layer_lr_decay=0.8,
+    ) if mobile_sam else ImageEncoderViT(
+        depth=encoder_depth,
+        embed_dim=encoder_embed_dim,
+        img_size=image_size,
+        mlp_ratio=4,
+        norm_layer=partial(torch.nn.LayerNorm, eps=1e-6),
+        num_heads=encoder_num_heads,
+        patch_size=vit_patch_size,
+        qkv_bias=True,
+        use_rel_pos=True,
+        global_attn_indexes=encoder_global_attn_indexes,
+        window_size=14,
+        out_chans=prompt_embed_dim,
+    ))
     sam = Sam(
-        image_encoder=ImageEncoderViT(
-            depth=encoder_depth,
-            embed_dim=encoder_embed_dim,
-            img_size=image_size,
-            mlp_ratio=4,
-            norm_layer=partial(torch.nn.LayerNorm, eps=1e-6),
-            num_heads=encoder_num_heads,
-            patch_size=vit_patch_size,
-            qkv_bias=True,
-            use_rel_pos=True,
-            global_attn_indexes=encoder_global_attn_indexes,
-            window_size=14,
-            out_chans=prompt_embed_dim,
-        ),
+        image_encoder=image_encoder,
         prompt_encoder=PromptEncoder(
             embed_dim=prompt_embed_dim,
             image_embedding_size=(image_embedding_size, image_embedding_size),
             input_image_size=(image_size, image_size),
             mask_in_chans=16,
         ),
         mask_decoder=MaskDecoder(
@@ -94,28 +123,30 @@
             transformer_dim=prompt_embed_dim,
             iou_head_depth=3,
             iou_head_hidden_dim=256,
         ),
         pixel_mean=[123.675, 116.28, 103.53],
         pixel_std=[58.395, 57.12, 57.375],
     )
-    sam.eval()
     if checkpoint is not None:
         checkpoint = attempt_download_asset(checkpoint)
         with open(checkpoint, 'rb') as f:
             state_dict = torch.load(f)
         sam.load_state_dict(state_dict)
+    sam.eval()
+    # sam.load_state_dict(torch.load(checkpoint), strict=True)
+    # sam.eval()
     return sam
 
 
 sam_model_map = {
-    # "default": build_sam_vit_h,
     'sam_h.pt': build_sam_vit_h,
     'sam_l.pt': build_sam_vit_l,
-    'sam_b.pt': build_sam_vit_b, }
+    'sam_b.pt': build_sam_vit_b,
+    'mobile_sam.pt': build_mobile_sam, }
 
 
 def build_sam(ckpt='sam_b.pt'):
     """Build a SAM model specified by ckpt."""
     model_builder = None
     for k in sam_model_map.keys():
         if ckpt.endswith(k):
```

## Comparing `ultralytics/vit/sam/model.py` & `ultralytics/models/sam/model.py`

 * *Files 16% similar despite different names*

```diff
@@ -1,51 +1,51 @@
 # Ultralytics YOLO ðŸš€, AGPL-3.0 license
 """
 SAM model interface
 """
 
-from ultralytics.yolo.cfg import get_cfg
+from ultralytics.cfg import get_cfg
+from ultralytics.utils.torch_utils import model_info
 
-from ...yolo.utils.torch_utils import model_info
 from .build import build_sam
 from .predict import Predictor
 
 
 class SAM:
 
     def __init__(self, model='sam_b.pt') -> None:
         if model and not model.endswith('.pt') and not model.endswith('.pth'):
             # Should raise AssertionError instead?
             raise NotImplementedError('Segment anything prediction requires pre-trained checkpoint')
         self.model = build_sam(model)
         self.task = 'segment'  # required
         self.predictor = None  # reuse predictor
 
-    def predict(self, source, stream=False, **kwargs):
+    def predict(self, source, stream=False, bboxes=None, points=None, labels=None, **kwargs):
         """Predicts and returns segmentation masks for given image or video source."""
-        overrides = dict(conf=0.25, task='segment', mode='predict')
+        overrides = dict(conf=0.25, task='segment', mode='predict', imgsz=1024)
         overrides.update(kwargs)  # prefer kwargs
         if not self.predictor:
             self.predictor = Predictor(overrides=overrides)
             self.predictor.setup_model(model=self.model)
         else:  # only update args if predictor is already setup
             self.predictor.args = get_cfg(self.predictor.args, overrides)
-        return self.predictor(source, stream=stream)
+        return self.predictor(source, stream=stream, bboxes=bboxes, points=points, labels=labels)
 
     def train(self, **kwargs):
         """Function trains models but raises an error as SAM models do not support training."""
         raise NotImplementedError("SAM models don't support training")
 
     def val(self, **kwargs):
         """Run validation given dataset."""
         raise NotImplementedError("SAM models don't support validation")
 
-    def __call__(self, source=None, stream=False, **kwargs):
+    def __call__(self, source=None, stream=False, bboxes=None, points=None, labels=None, **kwargs):
         """Calls the 'predict' function with given arguments to perform object detection."""
-        return self.predict(source, stream, **kwargs)
+        return self.predict(source, stream, bboxes, points, labels, **kwargs)
 
     def __getattr__(self, attr):
         """Raises error if object has no requested attribute."""
         name = self.__class__.__name__
         raise AttributeError(f"'{name}' object has no attribute '{attr}'. See valid attributes below.\n{self.__doc__}")
 
     def info(self, detailed=False, verbose=True):
```

## Comparing `ultralytics/vit/sam/predict.py` & `ultralytics/models/yolo/classify/predict.py`

 * *Files 22% similar despite different names*

```diff
@@ -1,54 +1,51 @@
 # Ultralytics YOLO ðŸš€, AGPL-3.0 license
 
-import numpy as np
 import torch
 
-from ultralytics.yolo.engine.predictor import BasePredictor
-from ultralytics.yolo.engine.results import Results
-from ultralytics.yolo.utils.torch_utils import select_device
-
-from .modules.mask_generator import SamAutomaticMaskGenerator
-
-
-class Predictor(BasePredictor):
-
-    def preprocess(self, im):
-        """Prepares input image for inference."""
-        # TODO: Only support bs=1 for now
-        # im = ResizeLongestSide(1024).apply_image(im[0])
-        # im = torch.as_tensor(im, device=self.device)
-        # im = im.permute(2, 0, 1).contiguous()[None, :, :, :]
-        return im[0]
-
-    def setup_model(self, model):
-        """Set up YOLO model with specified thresholds and device."""
-        device = select_device(self.args.device)
-        model.eval()
-        self.model = SamAutomaticMaskGenerator(model.to(device),
-                                               pred_iou_thresh=self.args.conf,
-                                               box_nms_thresh=self.args.iou)
-        self.device = device
-        # TODO: Temporary settings for compatibility
-        self.model.pt = False
-        self.model.triton = False
-        self.model.stride = 32
-        self.model.fp16 = False
-        self.done_warmup = True
-
-    def postprocess(self, preds, path, orig_imgs):
-        """Postprocesses inference output predictions to create detection masks for objects."""
-        names = dict(enumerate(list(range(len(preds)))))
+from ultralytics.engine.predictor import BasePredictor
+from ultralytics.engine.results import Results
+from ultralytics.utils import DEFAULT_CFG, ROOT
+
+
+class ClassificationPredictor(BasePredictor):
+
+    def __init__(self, cfg=DEFAULT_CFG, overrides=None, _callbacks=None):
+        super().__init__(cfg, overrides, _callbacks)
+        self.args.task = 'classify'
+
+    def preprocess(self, img):
+        """Converts input image to model-compatible data type."""
+        if not isinstance(img, torch.Tensor):
+            img = torch.stack([self.transforms(im) for im in img], dim=0)
+        img = (img if isinstance(img, torch.Tensor) else torch.from_numpy(img)).to(self.model.device)
+        return img.half() if self.model.fp16 else img.float()  # uint8 to fp16/32
+
+    def postprocess(self, preds, img, orig_imgs):
+        """Postprocesses predictions to return Results objects."""
         results = []
-        # TODO
-        for i, pred in enumerate([preds]):
-            masks = torch.from_numpy(np.stack([p['segmentation'] for p in pred], axis=0))
+        for i, pred in enumerate(preds):
             orig_img = orig_imgs[i] if isinstance(orig_imgs, list) else orig_imgs
             path = self.batch[0]
             img_path = path[i] if isinstance(path, list) else path
-            results.append(Results(orig_img=orig_img, path=img_path, names=names, masks=masks))
+            results.append(Results(orig_img=orig_img, path=img_path, names=self.model.names, probs=pred))
+
         return results
 
-    # def __call__(self, source=None, model=None, stream=False):
-    #     frame = cv2.imread(source)
-    #     preds = self.model.generate(frame)
-    #     return self.postprocess(preds, source, frame)
+
+def predict(cfg=DEFAULT_CFG, use_python=False):
+    """Run YOLO model predictions on input images/videos."""
+    model = cfg.model or 'yolov8n-cls.pt'  # or "resnet18"
+    source = cfg.source if cfg.source is not None else ROOT / 'assets' if (ROOT / 'assets').exists() \
+        else 'https://ultralytics.com/images/bus.jpg'
+
+    args = dict(model=model, source=source)
+    if use_python:
+        from ultralytics import YOLO
+        YOLO(model)(**args)
+    else:
+        predictor = ClassificationPredictor(overrides=args)
+        predictor.predict_cli()
+
+
+if __name__ == '__main__':
+    predict()
```

## Comparing `ultralytics/vit/sam/modules/decoders.py` & `ultralytics/models/sam/modules/decoders.py`

 * *Files identical despite different names*

## Comparing `ultralytics/vit/sam/modules/encoders.py` & `ultralytics/models/sam/modules/encoders.py`

 * *Files identical despite different names*

## Comparing `ultralytics/vit/sam/modules/sam.py` & `ultralytics/models/sam/modules/sam.py`

 * *Files identical despite different names*

## Comparing `ultralytics/vit/sam/modules/transformer.py` & `ultralytics/models/sam/modules/transformer.py`

 * *Files identical despite different names*

## Comparing `ultralytics/vit/utils/loss.py` & `ultralytics/models/utils/loss.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,16 +1,17 @@
 # Ultralytics YOLO ðŸš€, AGPL-3.0 license
 
 import torch
 import torch.nn as nn
 import torch.nn.functional as F
 
-from ultralytics.vit.utils.ops import HungarianMatcher
-from ultralytics.yolo.utils.loss import FocalLoss, VarifocalLoss
-from ultralytics.yolo.utils.metrics import bbox_iou
+from ultralytics.utils.loss import FocalLoss, VarifocalLoss
+from ultralytics.utils.metrics import bbox_iou
+
+from .ops import HungarianMatcher
 
 
 class DETRLoss(nn.Module):
 
     def __init__(self,
                  nc=80,
                  loss_gain=None,
```

## Comparing `ultralytics/vit/utils/ops.py` & `ultralytics/models/utils/ops.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,16 +1,16 @@
 # Ultralytics YOLO ðŸš€, AGPL-3.0 license
 
 import torch
 import torch.nn as nn
 import torch.nn.functional as F
 from scipy.optimize import linear_sum_assignment
 
-from ultralytics.yolo.utils.metrics import bbox_iou
-from ultralytics.yolo.utils.ops import xywh2xyxy, xyxy2xywh
+from ultralytics.utils.metrics import bbox_iou
+from ultralytics.utils.ops import xywh2xyxy, xyxy2xywh
 
 
 class HungarianMatcher(nn.Module):
     """
     A module implementing the HungarianMatcher, which is a differentiable module to solve the assignment problem in
     an end-to-end fashion.
```

## Comparing `ultralytics/yolo/cfg/default.yaml` & `ultralytics/cfg/default.yaml`

 * *Files 1% similar despite different names*

```diff
@@ -106,12 +106,9 @@
 mosaic: 1.0  # (float) image mosaic (probability)
 mixup: 0.0  # (float) image mixup (probability)
 copy_paste: 0.0  # (float) segment copy-paste (probability)
 
 # Custom config.yaml ---------------------------------------------------------------------------------------------------
 cfg:  # (str, optional) for overriding defaults.yaml
 
-# Debug, do not modify -------------------------------------------------------------------------------------------------
-v5loader: False  # (bool) use legacy YOLOv5 dataloader (deprecated)
-
 # Tracker settings ------------------------------------------------------------------------------------------------------
 tracker: botsort.yaml  # (str) tracker type, choices=[botsort.yaml, bytetrack.yaml]
```

## Comparing `ultralytics/yolo/data/annotator.py` & `ultralytics/data/annotator.py`

 * *Files 18% similar despite different names*

```diff
@@ -1,52 +1,38 @@
 from pathlib import Path
 
-from ultralytics import YOLO
-from ultralytics.vit.sam import PromptPredictor, build_sam
-from ultralytics.yolo.utils.torch_utils import select_device
+from ultralytics import SAM, YOLO
 
 
 def auto_annotate(data, det_model='yolov8x.pt', sam_model='sam_b.pt', device='', output_dir=None):
     """
     Automatically annotates images using a YOLO object detection model and a SAM segmentation model.
     Args:
         data (str): Path to a folder containing images to be annotated.
         det_model (str, optional): Pre-trained YOLO detection model. Defaults to 'yolov8x.pt'.
         sam_model (str, optional): Pre-trained SAM segmentation model. Defaults to 'sam_b.pt'.
         device (str, optional): Device to run the models on. Defaults to an empty string (CPU or GPU, if available).
         output_dir (str | None | optional): Directory to save the annotated results.
             Defaults to a 'labels' folder in the same directory as 'data'.
     """
-    device = select_device(device)
     det_model = YOLO(det_model)
-    sam_model = build_sam(sam_model)
-    det_model.to(device)
-    sam_model.to(device)
+    sam_model = SAM(sam_model)
 
     if not output_dir:
         output_dir = Path(str(data)).parent / 'labels'
     Path(output_dir).mkdir(exist_ok=True, parents=True)
 
-    prompt_predictor = PromptPredictor(sam_model)
-    det_results = det_model(data, stream=True)
+    det_results = det_model(data, stream=True, device=device)
 
     for result in det_results:
         boxes = result.boxes.xyxy  # Boxes object for bbox outputs
         class_ids = result.boxes.cls.int().tolist()  # noqa
         if len(class_ids):
-            prompt_predictor.set_image(result.orig_img)
-            masks, _, _ = prompt_predictor.predict_torch(
-                point_coords=None,
-                point_labels=None,
-                boxes=prompt_predictor.transform.apply_boxes_torch(boxes, result.orig_shape[:2]),
-                multimask_output=False,
-            )
-
-            result.update(masks=masks.squeeze(1))
-            segments = result.masks.xyn  # noqa
+            sam_results = sam_model(result.orig_img, bboxes=boxes, verbose=False, save=False, device=device)
+            segments = sam_results[0].masks.xyn  # noqa
 
             with open(str(Path(output_dir) / Path(result.path).stem) + '.txt', 'w') as f:
                 for i in range(len(segments)):
                     s = segments[i]
                     if len(s) == 0:
                         continue
                     segment = map(str, segments[i].reshape(-1).tolist())
```

## Comparing `ultralytics/yolo/data/augment.py` & `ultralytics/data/augment.py`

 * *Files 1% similar despite different names*

```diff
@@ -5,19 +5,20 @@
 from copy import deepcopy
 
 import cv2
 import numpy as np
 import torch
 import torchvision.transforms as T
 
-from ..utils import LOGGER, colorstr
-from ..utils.checks import check_version
-from ..utils.instance import Instances
-from ..utils.metrics import bbox_ioa
-from ..utils.ops import segment2box
+from ultralytics.utils import LOGGER, colorstr
+from ultralytics.utils.checks import check_version
+from ultralytics.utils.instance import Instances
+from ultralytics.utils.metrics import bbox_ioa
+from ultralytics.utils.ops import segment2box
+
 from .utils import polygons2masks, polygons2masks_overlap
 
 POSE_FLIPLR_INDEX = [0, 2, 1, 4, 3, 6, 5, 8, 7, 10, 9, 12, 11, 14, 13, 16, 15]
 
 
 # TODO: we might need a BaseTransform to make all these augments be compatible with both classification and semantic
 class BaseTransform:
@@ -534,21 +535,22 @@
         labels['instances'] = instances
         return labels
 
 
 class LetterBox:
     """Resize image and padding for detection, instance segmentation, pose."""
 
-    def __init__(self, new_shape=(640, 640), auto=False, scaleFill=False, scaleup=True, stride=32):
+    def __init__(self, new_shape=(640, 640), auto=False, scaleFill=False, scaleup=True, center=True, stride=32):
         """Initialize LetterBox object with specific parameters."""
         self.new_shape = new_shape
         self.auto = auto
         self.scaleFill = scaleFill
         self.scaleup = scaleup
         self.stride = stride
+        self.center = center  # Put the image in the middle or top-left
 
     def __call__(self, labels=None, image=None):
         """Return updated labels and image with added border."""
         if labels is None:
             labels = {}
         img = labels.get('img') if image is None else image
         shape = img.shape[:2]  # current shape [height, width]
@@ -568,23 +570,24 @@
         if self.auto:  # minimum rectangle
             dw, dh = np.mod(dw, self.stride), np.mod(dh, self.stride)  # wh padding
         elif self.scaleFill:  # stretch
             dw, dh = 0.0, 0.0
             new_unpad = (new_shape[1], new_shape[0])
             ratio = new_shape[1] / shape[1], new_shape[0] / shape[0]  # width, height ratios
 
-        dw /= 2  # divide padding into 2 sides
-        dh /= 2
+        if self.center:
+            dw /= 2  # divide padding into 2 sides
+            dh /= 2
         if labels.get('ratio_pad'):
             labels['ratio_pad'] = (labels['ratio_pad'], (dw, dh))  # for evaluation
 
         if shape[::-1] != new_unpad:  # resize
             img = cv2.resize(img, new_unpad, interpolation=cv2.INTER_LINEAR)
-        top, bottom = int(round(dh - 0.1)), int(round(dh + 0.1))
-        left, right = int(round(dw - 0.1)), int(round(dw + 0.1))
+        top, bottom = int(round(dh - 0.1)) if self.center else 0, int(round(dh + 0.1))
+        left, right = int(round(dw - 0.1)) if self.center else 0, int(round(dw + 0.1))
         img = cv2.copyMakeBorder(img, top, bottom, left, right, cv2.BORDER_CONSTANT,
                                  value=(114, 114, 114))  # add border
 
         if len(labels):
             labels = self._update_labels(labels, ratio, dw, dh)
             labels['img'] = img
             labels['resized_shape'] = new_shape
@@ -638,15 +641,16 @@
         labels['img'] = im
         labels['cls'] = cls
         labels['instances'] = instances
         return labels
 
 
 class Albumentations:
-    # YOLOv8 Albumentations class (optional, only used if package is installed)
+    """YOLOv8 Albumentations class (optional, only used if package is installed)"""
+
     def __init__(self, p=1.0):
         """Initialize the transform object for YOLO bbox formatted params."""
         self.p = p
         self.transform = None
         prefix = colorstr('albumentations: ')
         try:
             import albumentations as A
@@ -815,15 +819,15 @@
         hsv_h=0.015,  # image HSV-Hue augmentation (fraction)
         hsv_s=0.7,  # image HSV-Saturation augmentation (fraction)
         hsv_v=0.4,  # image HSV-Value augmentation (fraction)
         mean=(0.0, 0.0, 0.0),  # IMAGENET_MEAN
         std=(1.0, 1.0, 1.0),  # IMAGENET_STD
         auto_aug=False,
 ):
-    # YOLOv8 classification Albumentations (optional, only used if package is installed)
+    """YOLOv8 classification Albumentations (optional, only used if package is installed)."""
     prefix = colorstr('albumentations: ')
     try:
         import albumentations as A
         from albumentations.pytorch import ToTensorV2
 
         check_version(A.__version__, '1.0.3', hard=True)  # version requirement
         if augment:  # Resize and crop
@@ -847,15 +851,16 @@
     except ImportError:  # package not installed, skip
         pass
     except Exception as e:
         LOGGER.info(f'{prefix}{e}')
 
 
 class ClassifyLetterBox:
-    # YOLOv8 LetterBox class for image preprocessing, i.e. T.Compose([LetterBox(size), ToTensor()])
+    """YOLOv8 LetterBox class for image preprocessing, i.e. T.Compose([LetterBox(size), ToTensor()])"""
+
     def __init__(self, size=(640, 640), auto=False, stride=32):
         """Resizes image and crops it to center with max dimensions 'h' and 'w'."""
         super().__init__()
         self.h, self.w = (size, size) if isinstance(size, int) else size
         self.auto = auto  # pass max size integer, automatically solve for short side using stride
         self.stride = stride  # used with auto
 
@@ -867,29 +872,31 @@
         top, left = round((hs - h) / 2 - 0.1), round((ws - w) / 2 - 0.1)
         im_out = np.full((self.h, self.w, 3), 114, dtype=im.dtype)
         im_out[top:top + h, left:left + w] = cv2.resize(im, (w, h), interpolation=cv2.INTER_LINEAR)
         return im_out
 
 
 class CenterCrop:
-    # YOLOv8 CenterCrop class for image preprocessing, i.e. T.Compose([CenterCrop(size), ToTensor()])
+    """YOLOv8 CenterCrop class for image preprocessing, i.e. T.Compose([CenterCrop(size), ToTensor()])"""
+
     def __init__(self, size=640):
         """Converts an image from numpy array to PyTorch tensor."""
         super().__init__()
         self.h, self.w = (size, size) if isinstance(size, int) else size
 
     def __call__(self, im):  # im = np.array HWC
         imh, imw = im.shape[:2]
         m = min(imh, imw)  # min dimension
         top, left = (imh - m) // 2, (imw - m) // 2
         return cv2.resize(im[top:top + m, left:left + m], (self.w, self.h), interpolation=cv2.INTER_LINEAR)
 
 
 class ToTensor:
-    # YOLOv8 ToTensor class for image preprocessing, i.e. T.Compose([LetterBox(size), ToTensor()])
+    """YOLOv8 ToTensor class for image preprocessing, i.e. T.Compose([LetterBox(size), ToTensor()])."""
+
     def __init__(self, half=False):
         """Initialize YOLOv8 ToTensor object with optional half-precision support."""
         super().__init__()
         self.half = half
 
     def __call__(self, im):  # im = np.array HWC in BGR order
         im = np.ascontiguousarray(im.transpose((2, 0, 1))[::-1])  # HWC to CHW -> BGR to RGB -> contiguous
```

## Comparing `ultralytics/yolo/data/base.py` & `ultralytics/data/base.py`

 * *Files 0% similar despite different names*

```diff
@@ -11,15 +11,16 @@
 
 import cv2
 import numpy as np
 import psutil
 from torch.utils.data import Dataset
 from tqdm import tqdm
 
-from ..utils import DEFAULT_CFG, LOCAL_RANK, LOGGER, NUM_THREADS, TQDM_BAR_FORMAT
+from ultralytics.utils import DEFAULT_CFG, LOCAL_RANK, LOGGER, NUM_THREADS, TQDM_BAR_FORMAT
+
 from .utils import HELP_URL, IMG_FORMATS
 
 
 class BaseDataset(Dataset):
     """
     Base dataset class for loading and processing image data.
```

## Comparing `ultralytics/yolo/data/build.py` & `ultralytics/data/build.py`

 * *Files 6% similar despite different names*

```diff
@@ -5,20 +5,20 @@
 from pathlib import Path
 
 import numpy as np
 import torch
 from PIL import Image
 from torch.utils.data import dataloader, distributed
 
-from ultralytics.yolo.data.dataloaders.stream_loaders import (LOADERS, LoadImages, LoadPilAndNumpy, LoadScreenshots,
-                                                              LoadStreams, LoadTensor, SourceTypes, autocast_list)
-from ultralytics.yolo.data.utils import IMG_FORMATS, VID_FORMATS
-from ultralytics.yolo.utils.checks import check_file
+from ultralytics.data.loaders import (LOADERS, LoadImages, LoadPilAndNumpy, LoadScreenshots, LoadStreams, LoadTensor,
+                                      SourceTypes, autocast_list)
+from ultralytics.data.utils import IMG_FORMATS, VID_FORMATS
+from ultralytics.utils import RANK, colorstr
+from ultralytics.utils.checks import check_file
 
-from ..utils import RANK, colorstr
 from .dataset import YOLODataset
 from .utils import PIN_MEMORY
 
 
 class InfiniteDataLoader(dataloader.DataLoader):
     """Dataloader that reuses workers. Uses same syntax as vanilla DataLoader."""
 
@@ -59,15 +59,15 @@
     def __iter__(self):
         """Iterates over the 'sampler' and yields its contents."""
         while True:
             yield from iter(self.sampler)
 
 
 def seed_worker(worker_id):  # noqa
-    # Set dataloader worker seed https://pytorch.org/docs/stable/notes/randomness.html#dataloader
+    """Set dataloader worker seed https://pytorch.org/docs/stable/notes/randomness.html#dataloader."""
     worker_seed = torch.initial_seed() % 2 ** 32
     np.random.seed(worker_seed)
     random.seed(worker_seed)
 
 
 def build_yolo_dataset(cfg, img_path, batch, data, mode='train', rect=False, stride=32):
     """Build YOLO Dataset"""
```

## Comparing `ultralytics/yolo/data/converter.py` & `ultralytics/data/converter.py`

 * *Files 1% similar despite different names*

```diff
@@ -2,16 +2,16 @@
 from collections import defaultdict
 from pathlib import Path
 
 import cv2
 import numpy as np
 from tqdm import tqdm
 
-from ultralytics.yolo.utils.checks import check_requirements
-from ultralytics.yolo.utils.files import make_dirs
+from ultralytics.utils.checks import check_requirements
+from ultralytics.utils.files import make_dirs
 
 
 def coco91_to_coco80_class():
     """Converts 91-index COCO class IDs to 80-index COCO class IDs.
 
     Returns:
         (list): A list of 91 class IDs where the index represents the 80-index class ID and the value is the
```

## Comparing `ultralytics/yolo/data/dataset.py` & `ultralytics/data/dataset.py`

 * *Files 1% similar despite different names*

```diff
@@ -6,15 +6,16 @@
 
 import cv2
 import numpy as np
 import torch
 import torchvision
 from tqdm import tqdm
 
-from ..utils import LOCAL_RANK, NUM_THREADS, TQDM_BAR_FORMAT, is_dir_writeable
+from ultralytics.utils import LOCAL_RANK, NUM_THREADS, TQDM_BAR_FORMAT, is_dir_writeable
+
 from .augment import Compose, Format, Instances, LetterBox, classify_albumentations, classify_transforms, v8_transforms
 from .base import BaseDataset
 from .utils import HELP_URL, LOGGER, get_hash, img2label_paths, verify_image_label
 
 
 class YOLODataset(BaseDataset):
     """
```

## Comparing `ultralytics/yolo/data/utils.py` & `ultralytics/data/utils.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,31 +1,32 @@
 # Ultralytics YOLO ðŸš€, AGPL-3.0 license
 
 import contextlib
 import hashlib
 import json
 import os
+import random
 import subprocess
 import time
 import zipfile
 from multiprocessing.pool import ThreadPool
 from pathlib import Path
 from tarfile import is_tarfile
 
 import cv2
 import numpy as np
 from PIL import ExifTags, Image, ImageOps
 from tqdm import tqdm
 
 from ultralytics.nn.autobackend import check_class_names
-from ultralytics.yolo.utils import (DATASETS_DIR, LOGGER, NUM_THREADS, ROOT, SETTINGS_YAML, clean_url, colorstr, emojis,
-                                    yaml_load)
-from ultralytics.yolo.utils.checks import check_file, check_font, is_ascii
-from ultralytics.yolo.utils.downloads import download, safe_download, unzip_file
-from ultralytics.yolo.utils.ops import segments2boxes
+from ultralytics.utils import (DATASETS_DIR, LOGGER, NUM_THREADS, ROOT, SETTINGS_YAML, clean_url, colorstr, emojis,
+                               yaml_load)
+from ultralytics.utils.checks import check_file, check_font, is_ascii
+from ultralytics.utils.downloads import download, safe_download, unzip_file
+from ultralytics.utils.ops import segments2boxes
 
 HELP_URL = 'See https://docs.ultralytics.com/yolov5/tutorials/train_custom_data'
 IMG_FORMATS = 'bmp', 'dng', 'jpeg', 'jpg', 'mpo', 'png', 'tif', 'tiff', 'webp', 'pfm'  # image suffixes
 VID_FORMATS = 'asf', 'avi', 'gif', 'm4v', 'mkv', 'mov', 'mp4', 'mpeg', 'mpg', 'ts', 'wmv', 'webm'  # video suffixes
 PIN_MEMORY = str(os.getenv('PIN_MEMORY', True)).lower() == 'true'  # global pin_memory for dataloaders
 IMAGENET_MEAN = 0.485, 0.456, 0.406  # RGB mean
 IMAGENET_STD = 0.229, 0.224, 0.225  # RGB standard deviation
@@ -264,37 +265,42 @@
     check_font('Arial.ttf' if is_ascii(data['names']) else 'Arial.Unicode.ttf')  # download fonts
 
     return data  # dictionary
 
 
 def check_cls_dataset(dataset: str, split=''):
     """
-    Check a classification dataset such as Imagenet.
+    Checks a classification dataset such as Imagenet.
 
-    This function takes a `dataset` name as input and returns a dictionary containing information about the dataset.
-    If the dataset is not found, it attempts to download the dataset from the internet and save it locally.
+    This function accepts a `dataset` name and attempts to retrieve the corresponding dataset information.
+    If the dataset is not found locally, it attempts to download the dataset from the internet and save it locally.
 
     Args:
-        dataset (str): Name of the dataset.
-        split (str, optional): Dataset split, either 'val', 'test', or ''. Defaults to ''.
+        dataset (str): The name of the dataset.
+        split (str, optional): The split of the dataset. Either 'val', 'test', or ''. Defaults to ''.
 
     Returns:
-        data (dict): A dictionary containing the following keys and values:
-            'train': Path object for the directory containing the training set of the dataset
-            'val': Path object for the directory containing the validation set of the dataset
-            'test': Path object for the directory containing the test set of the dataset
-            'nc': Number of classes in the dataset
-            'names': List of class names in the dataset
+        (dict): A dictionary containing the following keys:
+            - 'train' (Path): The directory path containing the training set of the dataset.
+            - 'val' (Path): The directory path containing the validation set of the dataset.
+            - 'test' (Path): The directory path containing the test set of the dataset.
+            - 'nc' (int): The number of classes in the dataset.
+            - 'names' (dict): A dictionary of class names in the dataset.
+
+    Raises:
+        FileNotFoundError: If the specified dataset is not found and cannot be downloaded.
     """
-    data_dir = (DATASETS_DIR / dataset).resolve()
+
+    dataset = Path(dataset)
+    data_dir = (dataset if dataset.is_dir() else (DATASETS_DIR / dataset)).resolve()
     if not data_dir.is_dir():
         LOGGER.info(f'\nDataset not found âš ï¸, missing path {data_dir}, attempting download...')
         t = time.time()
-        if dataset == 'imagenet':
-            subprocess.run(f"bash {ROOT / 'yolo/data/scripts/get_imagenet.sh'}", shell=True, check=True)
+        if str(dataset) == 'imagenet':
+            subprocess.run(f"bash {ROOT / 'data/scripts/get_imagenet.sh'}", shell=True, check=True)
         else:
             url = f'https://github.com/ultralytics/yolov5/releases/download/v1.0/{dataset}.zip'
             download(url, dir=data_dir.parent)
         s = f"Dataset download success âœ… ({time.time() - t:.1f}s), saved to {colorstr('bold', data_dir)}\n"
         LOGGER.info(s)
     train_set = data_dir / 'train'
     val_set = data_dir / 'val' if (data_dir / 'val').exists() else None  # data/test or data/val
@@ -308,23 +314,23 @@
     names = [x.name for x in (data_dir / 'train').iterdir() if x.is_dir()]  # class names list
     names = dict(enumerate(sorted(names)))
     return {'train': train_set, 'val': val_set or test_set, 'test': test_set or val_set, 'nc': nc, 'names': names}
 
 
 class HUBDatasetStats():
     """
-    Class for generating HUB dataset JSON and `-hub` dataset directory
+    A class for generating HUB dataset JSON and `-hub` dataset directory.
 
-    Arguments
-        path:           Path to data.yaml or data.zip (with data.yaml inside data.zip)
-        task:           Dataset task. Options are 'detect', 'segment', 'pose', 'classify'.
-        autodownload:   Attempt to download dataset if not found locally
+    Args:
+        path (str): Path to data.yaml or data.zip (with data.yaml inside data.zip). Default is 'coco128.yaml'.
+        task (str): Dataset task. Options are 'detect', 'segment', 'pose', 'classify'. Default is 'detect'.
+        autodownload (bool): Attempt to download dataset if not found locally. Default is False.
 
     Usage
-        from ultralytics.yolo.data.utils import HUBDatasetStats
+        from ultralytics.data.utils import HUBDatasetStats
         stats = HUBDatasetStats('/Users/glennjocher/Downloads/coco8.zip', task='detect')  # detect dataset
         stats = HUBDatasetStats('/Users/glennjocher/Downloads/coco8-seg.zip', task='segment')  # segment dataset
         stats = HUBDatasetStats('/Users/glennjocher/Downloads/coco8-pose.zip', task='pose')  # pose dataset
         stats.get_json(save=False)
         stats.process_images()
     """
 
@@ -369,15 +375,15 @@
 
     def _hub_ops(self, f):
         """Saves a compressed image for HUB previews."""
         compress_one_image(f, self.im_dir / Path(f).name)  # save to dataset-hub
 
     def get_json(self, save=False, verbose=False):
         """Return dataset JSON for Ultralytics HUB."""
-        from ultralytics.yolo.data import YOLODataset  # ClassificationDataset
+        from ultralytics.data import YOLODataset  # ClassificationDataset
 
         def _round(labels):
             """Update labels to integer class and 4 decimal place floats."""
             if self.task == 'detect':
                 coordinates = labels['bboxes']
             elif self.task == 'segment':
                 coordinates = [x.flatten() for x in labels['segments']]
@@ -420,15 +426,15 @@
                 json.dump(self.stats, f)  # save stats.json
         if verbose:
             LOGGER.info(json.dumps(self.stats, indent=2, sort_keys=False))
         return self.stats
 
     def process_images(self):
         """Compress images for Ultralytics HUB."""
-        from ultralytics.yolo.data import YOLODataset  # ClassificationDataset
+        from ultralytics.data import YOLODataset  # ClassificationDataset
 
         for split in 'train', 'val', 'test':
             if self.data.get(split) is None:
                 continue
             dataset = YOLODataset(img_path=self.data[split], data=self.data)
             with ThreadPool(NUM_THREADS) as pool:
                 for _ in tqdm(pool.imap(self._hub_ops, dataset.im_files), total=len(dataset), desc=f'{split} images'):
@@ -447,15 +453,15 @@
         f (str): The path to the input image file.
         f_new (str, optional): The path to the output image file. If not specified, the input file will be overwritten.
         max_dim (int, optional): The maximum dimension (width or height) of the output image. Default is 1920 pixels.
         quality (int, optional): The image compression quality as a percentage. Default is 50%.
 
     Usage:
         from pathlib import Path
-        from ultralytics.yolo.data.utils import compress_one_image
+        from ultralytics.data.utils import compress_one_image
         for f in Path('/Users/glennjocher/Downloads/dataset').rglob('*.jpg'):
             compress_one_image(f)
     """
     try:  # use PIL
         im = Image.open(f)
         r = max_dim / max(im.height, im.width)  # ratio
         if r < 1.0:  # image too large
@@ -475,15 +481,15 @@
     """
     Deletes all ".DS_store" files under a specified directory.
 
     Args:
         path (str, optional): The directory path where the ".DS_store" files should be deleted.
 
     Usage:
-        from ultralytics.yolo.data.utils import delete_dsstore
+        from ultralytics.data.utils import delete_dsstore
         delete_dsstore('/Users/glennjocher/Downloads/dataset')
 
     Note:
         ".DS_store" files are created by the Apple operating system and contain metadata about folders and files. They
         are hidden system files and can cause issues when transferring files between different operating systems.
     """
     # Delete Apple .DS_store files
@@ -498,22 +504,54 @@
     Zips a directory and saves the archive to the specified output path.
 
     Args:
         dir (str): The path to the directory to be zipped.
         use_zipfile_library (bool): Whether to use zipfile library or shutil for zipping.
 
     Usage:
-        from ultralytics.yolo.data.utils import zip_directory
+        from ultralytics.data.utils import zip_directory
         zip_directory('/Users/glennjocher/Downloads/playground')
 
         zip -r coco8-pose.zip coco8-pose
     """
     delete_dsstore(dir)
     if use_zipfile_library:
         dir = Path(dir)
         with zipfile.ZipFile(dir.with_suffix('.zip'), 'w', zipfile.ZIP_DEFLATED) as zip_file:
             for file_path in dir.glob('**/*'):
                 if file_path.is_file():
                     zip_file.write(file_path, file_path.relative_to(dir))
     else:
         import shutil
         shutil.make_archive(dir, 'zip', dir)
+
+
+def autosplit(path=DATASETS_DIR / 'coco128/images', weights=(0.9, 0.1, 0.0), annotated_only=False):
+    """
+    Autosplit a dataset into train/val/test splits and save the resulting splits into autosplit_*.txt files.
+
+    Args:
+        path (Path, optional): Path to images directory. Defaults to DATASETS_DIR / 'coco128/images'.
+        weights (list | tuple, optional): Train, validation, and test split fractions. Defaults to (0.9, 0.1, 0.0).
+        annotated_only (bool, optional): If True, only images with an associated txt file are used. Defaults to False.
+
+    Usage:
+        from utils.dataloaders import autosplit
+        autosplit()
+    """
+
+    path = Path(path)  # images dir
+    files = sorted(x for x in path.rglob('*.*') if x.suffix[1:].lower() in IMG_FORMATS)  # image files only
+    n = len(files)  # number of files
+    random.seed(0)  # for reproducibility
+    indices = random.choices([0, 1, 2], weights=weights, k=n)  # assign each image to a split
+
+    txt = ['autosplit_train.txt', 'autosplit_val.txt', 'autosplit_test.txt']  # 3 txt files
+    for x in txt:
+        if (path.parent / x).exists():
+            (path.parent / x).unlink()  # remove existing
+
+    LOGGER.info(f'Autosplitting images from {path}' + ', using *.txt labeled images only' * annotated_only)
+    for i, img in tqdm(zip(indices, files), total=n):
+        if not annotated_only or Path(img2label_paths([str(img)])[0]).exists():  # check label
+            with open(path.parent / txt[i], 'a') as f:
+                f.write(f'./{img.relative_to(path.parent).as_posix()}' + '\n')  # add image to txt file
```

## Comparing `ultralytics/yolo/data/dataloaders/stream_loaders.py` & `ultralytics/data/loaders.py`

 * *Files 3% similar despite different names*

```diff
@@ -11,29 +11,30 @@
 
 import cv2
 import numpy as np
 import requests
 import torch
 from PIL import Image
 
-from ultralytics.yolo.data.utils import IMG_FORMATS, VID_FORMATS
-from ultralytics.yolo.utils import LOGGER, ROOT, is_colab, is_kaggle, ops
-from ultralytics.yolo.utils.checks import check_requirements
+from ultralytics.data.utils import IMG_FORMATS, VID_FORMATS
+from ultralytics.utils import LOGGER, ROOT, is_colab, is_kaggle, ops
+from ultralytics.utils.checks import check_requirements
 
 
 @dataclass
 class SourceTypes:
     webcam: bool = False
     screenshot: bool = False
     from_img: bool = False
     tensor: bool = False
 
 
 class LoadStreams:
-    # YOLOv8 streamloader, i.e. `yolo predict source='rtsp://example.com/media.mp4'  # RTSP, RTMP, HTTP streams`
+    """YOLOv8 streamloader, i.e. `yolo predict source='rtsp://example.com/media.mp4'  # RTSP, RTMP, HTTP streams`."""
+
     def __init__(self, sources='file.streams', imgsz=640, vid_stride=1):
         """Initialize instance variables and check for consistent input stream shapes."""
         torch.backends.cudnn.benchmark = True  # faster for fixed-size inference
         self.mode = 'stream'
         self.imgsz = imgsz
         self.vid_stride = vid_stride  # video frame-rate stride
         sources = Path(sources).read_text().rsplit() if os.path.isfile(sources) else [sources]
@@ -112,15 +113,16 @@
 
     def __len__(self):
         """Return the length of the sources object."""
         return len(self.sources)  # 1E12 frames = 32 streams at 30 FPS for 30 years
 
 
 class LoadScreenshots:
-    # YOLOv8 screenshot dataloader, i.e. `yolo predict source=screen`
+    """YOLOv8 screenshot dataloader, i.e. `yolo predict source=screen`."""
+
     def __init__(self, source, imgsz=640):
         """source = [screen_number left top width height] (pixels)."""
         check_requirements('mss')
         import mss  # noqa
 
         source, *params = source.split()
         self.screen, left, top, width, height = 0, None, None, None, None  # default to full screen 0
@@ -154,28 +156,33 @@
         s = f'screen {self.screen} (LTWH): {self.left},{self.top},{self.width},{self.height}: '
 
         self.frame += 1
         return str(self.screen), im0, None, s  # screen, img, original img, im0s, s
 
 
 class LoadImages:
-    # YOLOv8 image/video dataloader, i.e. `yolo predict source=image.jpg/vid.mp4`
+    """YOLOv8 image/video dataloader, i.e. `yolo predict source=image.jpg/vid.mp4`."""
+
     def __init__(self, path, imgsz=640, vid_stride=1):
         """Initialize the Dataloader and raise FileNotFoundError if file not found."""
+        parent = None
         if isinstance(path, str) and Path(path).suffix == '.txt':  # *.txt file with img/vid/dir on each line
+            parent = Path(path).parent
             path = Path(path).read_text().rsplit()
         files = []
         for p in sorted(path) if isinstance(path, (list, tuple)) else [path]:
-            p = str(Path(p).absolute())  # do not use .resolve() https://github.com/ultralytics/ultralytics/issues/2912
-            if '*' in p:
-                files.extend(sorted(glob.glob(p, recursive=True)))  # glob
-            elif os.path.isdir(p):
-                files.extend(sorted(glob.glob(os.path.join(p, '*.*'))))  # dir
-            elif os.path.isfile(p):
-                files.append(p)  # files
+            a = str(Path(p).absolute())  # do not use .resolve() https://github.com/ultralytics/ultralytics/issues/2912
+            if '*' in a:
+                files.extend(sorted(glob.glob(a, recursive=True)))  # glob
+            elif os.path.isdir(a):
+                files.extend(sorted(glob.glob(os.path.join(a, '*.*'))))  # dir
+            elif os.path.isfile(a):
+                files.append(a)  # files (absolute or relative to CWD)
+            elif parent and (parent / p).is_file():
+                files.append(str((parent / p).absolute()))  # files (relative to *.txt file parent)
             else:
                 raise FileNotFoundError(f'{p} does not exist')
 
         images = [x for x in files if x.split('.')[-1].lower() in IMG_FORMATS]
         videos = [x for x in files if x.split('.')[-1].lower() in VID_FORMATS]
         ni, nv = len(images), len(videos)
 
@@ -311,19 +318,18 @@
 
     @staticmethod
     def _single_check(im, stride=32):
         """Validate and format an image to torch.Tensor."""
         s = f'WARNING âš ï¸ torch.Tensor inputs should be BCHW i.e. shape(1, 3, 640, 640) ' \
             f'divisible by stride {stride}. Input shape{tuple(im.shape)} is incompatible.'
         if len(im.shape) != 4:
-            if len(im.shape) == 3:
-                LOGGER.warning(s)
-                im = im.unsqueeze(0)
-            else:
+            if len(im.shape) != 3:
                 raise ValueError(s)
+            LOGGER.warning(s)
+            im = im.unsqueeze(0)
         if im.shape[2] % stride or im.shape[3] % stride:
             raise ValueError(s)
         if im.max() > 1.0:
             LOGGER.warning(f'WARNING âš ï¸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is {im.max()}. '
                            f'Dividing input by 255.')
             im = im.float() / 255.0
```

## Comparing `ultralytics/yolo/data/dataloaders/v5augmentations.py` & `ultralytics/utils/checks.py`

 * *Files 26% similar despite different names*

```diff
@@ -1,407 +1,462 @@
 # Ultralytics YOLO ðŸš€, AGPL-3.0 license
-"""
-Image augmentation functions
-"""
-
+import contextlib
+import glob
+import inspect
 import math
-import random
+import os
+import platform
+import re
+import shutil
+import subprocess
+import time
+from pathlib import Path
+from typing import Optional
 
 import cv2
 import numpy as np
+import pkg_resources as pkg
+import psutil
+import requests
 import torch
-import torchvision.transforms as T
-import torchvision.transforms.functional as TF
+from matplotlib import font_manager
 
-from ultralytics.yolo.utils import LOGGER, colorstr
-from ultralytics.yolo.utils.checks import check_version
-from ultralytics.yolo.utils.metrics import bbox_ioa
-from ultralytics.yolo.utils.ops import resample_segments, segment2box, xywhn2xyxy
-
-IMAGENET_MEAN = 0.485, 0.456, 0.406  # RGB mean
-IMAGENET_STD = 0.229, 0.224, 0.225  # RGB standard deviation
-
-
-class Albumentations:
-    # YOLOv5 Albumentations class (optional, only used if package is installed)
-    def __init__(self, size=640):
-        """Instantiate object with image augmentations for YOLOv5."""
-        self.transform = None
-        prefix = colorstr('albumentations: ')
-        try:
-            import albumentations as A
-            check_version(A.__version__, '1.0.3', hard=True)  # version requirement
+from ultralytics.utils import (AUTOINSTALL, LOGGER, ONLINE, ROOT, USER_CONFIG_DIR, ThreadingLocked, TryExcept,
+                               clean_url, colorstr, downloads, emojis, is_colab, is_docker, is_jupyter, is_kaggle,
+                               is_online, is_pip_package, url2file)
+
+
+def is_ascii(s) -> bool:
+    """
+    Check if a string is composed of only ASCII characters.
+
+    Args:
+        s (str): String to be checked.
+
+    Returns:
+        bool: True if the string is composed only of ASCII characters, False otherwise.
+    """
+    # Convert list, tuple, None, etc. to string
+    s = str(s)
+
+    # Check if the string is composed of only ASCII characters
+    return all(ord(c) < 128 for c in s)
+
+
+def check_imgsz(imgsz, stride=32, min_dim=1, max_dim=2, floor=0):
+    """
+    Verify image size is a multiple of the given stride in each dimension. If the image size is not a multiple of the
+    stride, update it to the nearest multiple of the stride that is greater than or equal to the given floor value.
+
+    Args:
+        imgsz (int | cList[int]): Image size.
+        stride (int): Stride value.
+        min_dim (int): Minimum number of dimensions.
+        floor (int): Minimum allowed value for image size.
+
+    Returns:
+        (List[int]): Updated image size.
+    """
+    # Convert stride to integer if it is a tensor
+    stride = int(stride.max() if isinstance(stride, torch.Tensor) else stride)
+
+    # Convert image size to list if it is an integer
+    if isinstance(imgsz, int):
+        imgsz = [imgsz]
+    elif isinstance(imgsz, (list, tuple)):
+        imgsz = list(imgsz)
+    else:
+        raise TypeError(f"'imgsz={imgsz}' is of invalid type {type(imgsz).__name__}. "
+                        f"Valid imgsz types are int i.e. 'imgsz=640' or list i.e. 'imgsz=[640,640]'")
 
-            T = [
-                A.RandomResizedCrop(height=size, width=size, scale=(0.8, 1.0), ratio=(0.9, 1.11), p=0.0),
-                A.Blur(p=0.01),
-                A.MedianBlur(p=0.01),
-                A.ToGray(p=0.01),
-                A.CLAHE(p=0.01),
-                A.RandomBrightnessContrast(p=0.0),
-                A.RandomGamma(p=0.0),
-                A.ImageCompression(quality_lower=75, p=0.0)]  # transforms
-            self.transform = A.Compose(T, bbox_params=A.BboxParams(format='yolo', label_fields=['class_labels']))
-
-            LOGGER.info(prefix + ', '.join(f'{x}'.replace('always_apply=False, ', '') for x in T if x.p))
-        except ImportError:  # package not installed, skip
-            pass
-        except Exception as e:
-            LOGGER.info(f'{prefix}{e}')
-
-    def __call__(self, im, labels, p=1.0):
-        """Transforms input image and labels with probability 'p'."""
-        if self.transform and random.random() < p:
-            new = self.transform(image=im, bboxes=labels[:, 1:], class_labels=labels[:, 0])  # transformed
-            im, labels = new['image'], np.array([[c, *b] for c, b in zip(new['class_labels'], new['bboxes'])])
-        return im, labels
-
-
-def normalize(x, mean=IMAGENET_MEAN, std=IMAGENET_STD, inplace=False):
-    """Denormalize RGB images x per ImageNet stats in BCHW format, i.e. = (x - mean) / std."""
-    return TF.normalize(x, mean, std, inplace=inplace)
-
-
-def denormalize(x, mean=IMAGENET_MEAN, std=IMAGENET_STD):
-    """Denormalize RGB images x per ImageNet stats in BCHW format, i.e. = x * std + mean."""
-    for i in range(3):
-        x[:, i] = x[:, i] * std[i] + mean[i]
-    return x
-
-
-def augment_hsv(im, hgain=0.5, sgain=0.5, vgain=0.5):
-    """HSV color-space augmentation."""
-    if hgain or sgain or vgain:
-        r = np.random.uniform(-1, 1, 3) * [hgain, sgain, vgain] + 1  # random gains
-        hue, sat, val = cv2.split(cv2.cvtColor(im, cv2.COLOR_BGR2HSV))
-        dtype = im.dtype  # uint8
-
-        x = np.arange(0, 256, dtype=r.dtype)
-        lut_hue = ((x * r[0]) % 180).astype(dtype)
-        lut_sat = np.clip(x * r[1], 0, 255).astype(dtype)
-        lut_val = np.clip(x * r[2], 0, 255).astype(dtype)
-
-        im_hsv = cv2.merge((cv2.LUT(hue, lut_hue), cv2.LUT(sat, lut_sat), cv2.LUT(val, lut_val)))
-        cv2.cvtColor(im_hsv, cv2.COLOR_HSV2BGR, dst=im)  # no return needed
-
-
-def hist_equalize(im, clahe=True, bgr=False):
-    """Equalize histogram on BGR image 'im' with im.shape(n,m,3) and range 0-255."""
-    yuv = cv2.cvtColor(im, cv2.COLOR_BGR2YUV if bgr else cv2.COLOR_RGB2YUV)
-    if clahe:
-        c = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))
-        yuv[:, :, 0] = c.apply(yuv[:, :, 0])
+    # Apply max_dim
+    if len(imgsz) > max_dim:
+        msg = "'train' and 'val' imgsz must be an integer, while 'predict' and 'export' imgsz may be a [h, w] list " \
+              "or an integer, i.e. 'yolo export imgsz=640,480' or 'yolo export imgsz=640'"
+        if max_dim != 1:
+            raise ValueError(f'imgsz={imgsz} is not a valid image size. {msg}')
+        LOGGER.warning(f"WARNING âš ï¸ updating to 'imgsz={max(imgsz)}'. {msg}")
+        imgsz = [max(imgsz)]
+    # Make image size a multiple of the stride
+    sz = [max(math.ceil(x / stride) * stride, floor) for x in imgsz]
+
+    # Print warning message if image size was updated
+    if sz != imgsz:
+        LOGGER.warning(f'WARNING âš ï¸ imgsz={imgsz} must be multiple of max stride {stride}, updating to {sz}')
+
+    # Add missing dimensions if necessary
+    sz = [sz[0], sz[0]] if min_dim == 2 and len(sz) == 1 else sz[0] if min_dim == 1 and len(sz) == 1 else sz
+
+    return sz
+
+
+def check_version(current: str = '0.0.0',
+                  minimum: str = '0.0.0',
+                  name: str = 'version ',
+                  pinned: bool = False,
+                  hard: bool = False,
+                  verbose: bool = False) -> bool:
+    """
+    Check current version against the required minimum version.
+
+    Args:
+        current (str): Current version.
+        minimum (str): Required minimum version.
+        name (str): Name to be used in warning message.
+        pinned (bool): If True, versions must match exactly. If False, minimum version must be satisfied.
+        hard (bool): If True, raise an AssertionError if the minimum version is not met.
+        verbose (bool): If True, print warning message if minimum version is not met.
+
+    Returns:
+        (bool): True if minimum version is met, False otherwise.
+    """
+    current, minimum = (pkg.parse_version(x) for x in (current, minimum))
+    result = (current == minimum) if pinned else (current >= minimum)  # bool
+    warning_message = f'WARNING âš ï¸ {name}{minimum} is required by YOLOv8, but {name}{current} is currently installed'
+    if hard:
+        assert result, emojis(warning_message)  # assert min requirements met
+    if verbose and not result:
+        LOGGER.warning(warning_message)
+    return result
+
+
+def check_latest_pypi_version(package_name='pyppbox-ultralytics'):
+    """
+    Returns the latest version of a PyPI package without downloading or installing it.
+
+    Parameters:
+        package_name (str): The name of the package to find the latest version for.
+
+    Returns:
+        (str): The latest version of the package.
+    """
+    if package_name == 'pyppbox-ultralytics':
+        with contextlib.suppress(Exception):
+            requests.packages.urllib3.disable_warnings()  # Disable the InsecureRequestWarning
+            response = requests.get(f'https://pypi.org/pypi/{package_name}/json', timeout=3)
+            if response.status_code == 200:
+                return response.json()['info']['version']
     else:
-        yuv[:, :, 0] = cv2.equalizeHist(yuv[:, :, 0])  # equalize Y channel histogram
-    return cv2.cvtColor(yuv, cv2.COLOR_YUV2BGR if bgr else cv2.COLOR_YUV2RGB)  # convert YUV image to RGB
+        LOGGER.info(
+        f'This is custom pyppbox-ultralytics for pyppbox ðŸ˜ƒ '
+        f"ðŸŒ Check for the update here: https://github.com/rathaumons/ultralytics-for-pyppbox")
+    return None
+
+
+def check_pip_update_available():
+    """
+    Checks if a new version of the ultralytics package is available on PyPI.
+
+    Returns:
+        (bool): True if an update is available, False otherwise.
+    """
+    if ONLINE and is_pip_package():
+        with contextlib.suppress(Exception):
+            from ultralytics import __version__
+            latest = check_latest_pypi_version()
+            if pkg.parse_version(__version__) < pkg.parse_version(latest):  # update is available
+                LOGGER.info(f'New https://pypi.org/project/ultralytics/{latest} available ðŸ˜ƒ '
+                            f"Update with 'pip install -U ultralytics'")
+                return True
+    return False
+
+
+@ThreadingLocked()
+def check_font(font='Arial.ttf'):
+    """
+    Find font locally or download to user's configuration directory if it does not already exist.
+
+    Args:
+        font (str): Path or name of font.
+
+    Returns:
+        file (Path): Resolved font file path.
+    """
+    name = Path(font).name
+
+    # Check USER_CONFIG_DIR
+    file = USER_CONFIG_DIR / name
+    if file.exists():
+        return file
+
+    # Check system fonts
+    matches = [s for s in font_manager.findSystemFonts() if font in s]
+    if any(matches):
+        return matches[0]
+
+    # Download to USER_CONFIG_DIR if missing
+    url = f'https://ultralytics.com/assets/{name}'
+    if downloads.is_url(url):
+        downloads.safe_download(url=url, file=file)
+        return file
+
+
+def check_python(minimum: str = '3.7.0') -> bool:
+    """
+    Check current python version against the required minimum version.
+
+    Args:
+        minimum (str): Required minimum version of python.
+
+    Returns:
+        None
+    """
+    return check_version(platform.python_version(), minimum, name='Python ', hard=True)
+
+
+@TryExcept()
+def check_requirements(requirements=ROOT.parent / 'requirements.txt', exclude=(), install=True, cmds=''):
+    """
+    Check if installed dependencies meet YOLOv8 requirements and attempt to auto-update if needed.
+
+    Args:
+        requirements (Union[Path, str, List[str]]): Path to a requirements.txt file, a single package requirement as a
+            string, or a list of package requirements as strings.
+        exclude (Tuple[str]): Tuple of package names to exclude from checking.
+        install (bool): If True, attempt to auto-update packages that don't meet requirements.
+        cmds (str): Additional commands to pass to the pip install command when auto-updating.
+    """
+    prefix = colorstr('red', 'bold', 'requirements:')
+    check_python()  # check python version
+    check_torchvision()  # check torch-torchvision compatibility
+    if isinstance(requirements, Path):  # requirements.txt file
+        file = requirements.resolve()
+        assert file.exists(), f'{prefix} {file} not found, check failed.'
+        with file.open() as f:
+            requirements = [f'{x.name}{x.specifier}' for x in pkg.parse_requirements(f) if x.name not in exclude]
+    elif isinstance(requirements, str):
+        requirements = [requirements]
+
+    s = ''  # console string
+    pkgs = []
+    for r in requirements:
+        r_stripped = r.split('/')[-1].replace('.git', '')  # replace git+https://org/repo.git -> 'repo'
+        try:
+            pkg.require(r_stripped)
+        except (pkg.VersionConflict, pkg.DistributionNotFound):  # exception if requirements not met
+            try:  # attempt to import (slower but more accurate)
+                import importlib
+                importlib.import_module(next(pkg.parse_requirements(r_stripped)).name)
+            except ImportError:
+                s += f'"{r}" '
+                pkgs.append(r)
+
+    if s:
+        if install and AUTOINSTALL:  # check environment variable
+            n = len(pkgs)  # number of packages updates
+            LOGGER.info(f"{prefix} Ultralytics requirement{'s' * (n > 1)} {pkgs} not found, attempting AutoUpdate...")
+            try:
+                t = time.time()
+                assert is_online(), 'AutoUpdate skipped (offline)'
+                LOGGER.info(subprocess.check_output(f'pip install --no-cache {s} {cmds}', shell=True).decode())
+                dt = time.time() - t
+                LOGGER.info(
+                    f"{prefix} AutoUpdate success âœ… {dt:.1f}s, installed {n} package{'s' * (n > 1)}: {pkgs}\n"
+                    f"{prefix} âš ï¸ {colorstr('bold', 'Restart runtime or rerun command for updates to take effect')}\n")
+            except Exception as e:
+                LOGGER.warning(f'{prefix} âŒ {e}')
+                return False
+        else:
+            return False
+
+    return True
+
+
+def check_torchvision():
+    """
+    Checks the installed versions of PyTorch and Torchvision to ensure they're compatible.
+
+    This function checks the installed versions of PyTorch and Torchvision, and warns if they're incompatible according
+    to the provided compatibility table based on https://github.com/pytorch/vision#installation. The
+    compatibility table is a dictionary where the keys are PyTorch versions and the values are lists of compatible
+    Torchvision versions.
+    """
+
+    import torchvision
+
+    # Compatibility table
+    compatibility_table = {'2.0': ['0.15'], '1.13': ['0.14'], '1.12': ['0.13']}
+
+    # Extract only the major and minor versions
+    v_torch = '.'.join(torch.__version__.split('+')[0].split('.')[:2])
+    v_torchvision = '.'.join(torchvision.__version__.split('+')[0].split('.')[:2])
+
+    if v_torch in compatibility_table:
+        compatible_versions = compatibility_table[v_torch]
+        if all(pkg.parse_version(v_torchvision) != pkg.parse_version(v) for v in compatible_versions):
+            print(f'WARNING âš ï¸ torchvision=={v_torchvision} is incompatible with torch=={v_torch}.\n'
+                  f"Run 'pip install torchvision=={compatible_versions[0]}' to fix torchvision or "
+                  "'pip install -U torch torchvision' to update both.\n"
+                  'For a full compatibility table see https://github.com/pytorch/vision#installation')
+
+
+def check_suffix(file='yolov8n.pt', suffix='.pt', msg=''):
+    """Check file(s) for acceptable suffix."""
+    if file and suffix:
+        if isinstance(suffix, str):
+            suffix = (suffix, )
+        for f in file if isinstance(file, (list, tuple)) else [file]:
+            s = Path(f).suffix.lower().strip()  # file suffix
+            if len(s):
+                assert s in suffix, f'{msg}{f} acceptable suffix is {suffix}, not {s}'
+
+
+def check_yolov5u_filename(file: str, verbose: bool = True):
+    """Replace legacy YOLOv5 filenames with updated YOLOv5u filenames."""
+    if ('yolov3' in file or 'yolov5' in file) and 'u' not in file:
+        original_file = file
+        file = re.sub(r'(.*yolov5([nsmlx]))\.pt', '\\1u.pt', file)  # i.e. yolov5n.pt -> yolov5nu.pt
+        file = re.sub(r'(.*yolov5([nsmlx])6)\.pt', '\\1u.pt', file)  # i.e. yolov5n6.pt -> yolov5n6u.pt
+        file = re.sub(r'(.*yolov3(|-tiny|-spp))\.pt', '\\1u.pt', file)  # i.e. yolov3-spp.pt -> yolov3-sppu.pt
+        if file != original_file and verbose:
+            LOGGER.info(f"PRO TIP ðŸ’¡ Replace 'model={original_file}' with new 'model={file}'.\nYOLOv5 'u' models are "
+                        f'trained with https://github.com/ultralytics/ultralytics and feature improved performance vs '
+                        f'standard YOLOv5 models trained with https://github.com/ultralytics/yolov5.\n')
+    return file
+
+
+def check_file(file, suffix='', download=True, hard=True):
+    """Search/download file (if necessary) and return path."""
+    check_suffix(file, suffix)  # optional
+    file = str(file).strip()  # convert to string and strip spaces
+    file = check_yolov5u_filename(file)  # yolov5n -> yolov5nu
+    if not file or ('://' not in file and Path(file).exists()):  # exists ('://' check required in Windows Python<3.10)
+        return file
+    elif download and file.lower().startswith(('https://', 'http://', 'rtsp://', 'rtmp://')):  # download
+        url = file  # warning: Pathlib turns :// -> :/
+        file = url2file(file)  # '%2F' to '/', split https://url.com/file.txt?auth
+        if Path(file).exists():
+            LOGGER.info(f'Found {clean_url(url)} locally at {file}')  # file already exists
+        else:
+            downloads.safe_download(url=url, file=file, unzip=False)
+        return file
+    else:  # search
+        files = glob.glob(str(ROOT / 'cfg' / '**' / file), recursive=True)  # find file
+        if not files and hard:
+            raise FileNotFoundError(f"'{file}' does not exist")
+        elif len(files) > 1 and hard:
+            raise FileNotFoundError(f"Multiple files match '{file}', specify exact path: {files}")
+        return files[0] if len(files) else []  # return file
+
+
+def check_yaml(file, suffix=('.yaml', '.yml'), hard=True):
+    """Search/download YAML file (if necessary) and return path, checking suffix."""
+    return check_file(file, suffix, hard=hard)
 
 
-def replicate(im, labels):
-    """Replicate labels."""
-    h, w = im.shape[:2]
-    boxes = labels[:, 1:].astype(int)
-    x1, y1, x2, y2 = boxes.T
-    s = ((x2 - x1) + (y2 - y1)) / 2  # side length (pixels)
-    for i in s.argsort()[:round(s.size * 0.5)]:  # smallest indices
-        x1b, y1b, x2b, y2b = boxes[i]
-        bh, bw = y2b - y1b, x2b - x1b
-        yc, xc = int(random.uniform(0, h - bh)), int(random.uniform(0, w - bw))  # offset x, y
-        x1a, y1a, x2a, y2a = [xc, yc, xc + bw, yc + bh]
-        im[y1a:y2a, x1a:x2a] = im[y1b:y2b, x1b:x2b]  # im4[ymin:ymax, xmin:xmax]
-        labels = np.append(labels, [[labels[i, 0], x1a, y1a, x2a, y2a]], axis=0)
-
-    return im, labels
-
-
-def letterbox(im, new_shape=(640, 640), color=(114, 114, 114), auto=True, scaleFill=False, scaleup=True, stride=32):
-    """Resize and pad image while meeting stride-multiple constraints."""
-    shape = im.shape[:2]  # current shape [height, width]
-    if isinstance(new_shape, int):
-        new_shape = (new_shape, new_shape)
-
-    # Scale ratio (new / old)
-    r = min(new_shape[0] / shape[0], new_shape[1] / shape[1])
-    if not scaleup:  # only scale down, do not scale up (for better val mAP)
-        r = min(r, 1.0)
-
-    # Compute padding
-    ratio = r, r  # width, height ratios
-    new_unpad = int(round(shape[1] * r)), int(round(shape[0] * r))
-    dw, dh = new_shape[1] - new_unpad[0], new_shape[0] - new_unpad[1]  # wh padding
-    if auto:  # minimum rectangle
-        dw, dh = np.mod(dw, stride), np.mod(dh, stride)  # wh padding
-    elif scaleFill:  # stretch
-        dw, dh = 0.0, 0.0
-        new_unpad = (new_shape[1], new_shape[0])
-        ratio = new_shape[1] / shape[1], new_shape[0] / shape[0]  # width, height ratios
-
-    dw /= 2  # divide padding into 2 sides
-    dh /= 2
-
-    if shape[::-1] != new_unpad:  # resize
-        im = cv2.resize(im, new_unpad, interpolation=cv2.INTER_LINEAR)
-    top, bottom = int(round(dh - 0.1)), int(round(dh + 0.1))
-    left, right = int(round(dw - 0.1)), int(round(dw + 0.1))
-    im = cv2.copyMakeBorder(im, top, bottom, left, right, cv2.BORDER_CONSTANT, value=color)  # add border
-    return im, ratio, (dw, dh)
-
-
-def random_perspective(im,
-                       targets=(),
-                       segments=(),
-                       degrees=10,
-                       translate=.1,
-                       scale=.1,
-                       shear=10,
-                       perspective=0.0,
-                       border=(0, 0)):
-    # torchvision.transforms.RandomAffine(degrees=(-10, 10), translate=(0.1, 0.1), scale=(0.9, 1.1), shear=(-10, 10))
-    # targets = [cls, xyxy]
-
-    height = im.shape[0] + border[0] * 2  # shape(h,w,c)
-    width = im.shape[1] + border[1] * 2
-
-    # Center
-    C = np.eye(3)
-    C[0, 2] = -im.shape[1] / 2  # x translation (pixels)
-    C[1, 2] = -im.shape[0] / 2  # y translation (pixels)
-
-    # Perspective
-    P = np.eye(3)
-    P[2, 0] = random.uniform(-perspective, perspective)  # x perspective (about y)
-    P[2, 1] = random.uniform(-perspective, perspective)  # y perspective (about x)
-
-    # Rotation and Scale
-    R = np.eye(3)
-    a = random.uniform(-degrees, degrees)
-    # a += random.choice([-180, -90, 0, 90])  # add 90deg rotations to small rotations
-    s = random.uniform(1 - scale, 1 + scale)
-    # s = 2 ** random.uniform(-scale, scale)
-    R[:2] = cv2.getRotationMatrix2D(angle=a, center=(0, 0), scale=s)
-
-    # Shear
-    S = np.eye(3)
-    S[0, 1] = math.tan(random.uniform(-shear, shear) * math.pi / 180)  # x shear (deg)
-    S[1, 0] = math.tan(random.uniform(-shear, shear) * math.pi / 180)  # y shear (deg)
-
-    # Translation
-    T = np.eye(3)
-    T[0, 2] = random.uniform(0.5 - translate, 0.5 + translate) * width  # x translation (pixels)
-    T[1, 2] = random.uniform(0.5 - translate, 0.5 + translate) * height  # y translation (pixels)
-
-    # Combined rotation matrix
-    M = T @ S @ R @ P @ C  # order of operations (right to left) is IMPORTANT
-    if (border[0] != 0) or (border[1] != 0) or (M != np.eye(3)).any():  # image changed
-        if perspective:
-            im = cv2.warpPerspective(im, M, dsize=(width, height), borderValue=(114, 114, 114))
-        else:  # affine
-            im = cv2.warpAffine(im, M[:2], dsize=(width, height), borderValue=(114, 114, 114))
-
-    # Visualize
-    # import matplotlib.pyplot as plt
-    # ax = plt.subplots(1, 2, figsize=(12, 6))[1].ravel()
-    # ax[0].imshow(im[:, :, ::-1])  # base
-    # ax[1].imshow(im2[:, :, ::-1])  # warped
-
-    # Transform label coordinates
-    n = len(targets)
-    if n:
-        use_segments = any(x.any() for x in segments)
-        new = np.zeros((n, 4))
-        if use_segments:  # warp segments
-            segments = resample_segments(segments)  # upsample
-            for i, segment in enumerate(segments):
-                xy = np.ones((len(segment), 3))
-                xy[:, :2] = segment
-                xy = xy @ M.T  # transform
-                xy = xy[:, :2] / xy[:, 2:3] if perspective else xy[:, :2]  # perspective rescale or affine
-
-                # Clip
-                new[i] = segment2box(xy, width, height)
-
-        else:  # warp boxes
-            xy = np.ones((n * 4, 3))
-            xy[:, :2] = targets[:, [1, 2, 3, 4, 1, 4, 3, 2]].reshape(n * 4, 2)  # x1y1, x2y2, x1y2, x2y1
-            xy = xy @ M.T  # transform
-            xy = (xy[:, :2] / xy[:, 2:3] if perspective else xy[:, :2]).reshape(n, 8)  # perspective rescale or affine
-
-            # Create new boxes
-            x = xy[:, [0, 2, 4, 6]]
-            y = xy[:, [1, 3, 5, 7]]
-            new = np.concatenate((x.min(1), y.min(1), x.max(1), y.max(1))).reshape(4, n).T
-
-            # Clip
-            new[:, [0, 2]] = new[:, [0, 2]].clip(0, width)
-            new[:, [1, 3]] = new[:, [1, 3]].clip(0, height)
-
-        # Filter candidates
-        i = box_candidates(box1=targets[:, 1:5].T * s, box2=new.T, area_thr=0.01 if use_segments else 0.10)
-        targets = targets[i]
-        targets[:, 1:5] = new[i]
-
-    return im, targets
-
-
-def copy_paste(im, labels, segments, p=0.5):
-    """Implement Copy-Paste augmentation https://arxiv.org/abs/2012.07177, labels as nx5 np.array(cls, xyxy)."""
-    n = len(segments)
-    if p and n:
-        h, w, c = im.shape  # height, width, channels
-        im_new = np.zeros(im.shape, np.uint8)
-
-        # Calculate ioa first then select indexes randomly
-        boxes = np.stack([w - labels[:, 3], labels[:, 2], w - labels[:, 1], labels[:, 4]], axis=-1)  # (n, 4)
-        ioa = bbox_ioa(boxes, labels[:, 1:5])  # intersection over area
-        indexes = np.nonzero((ioa < 0.30).all(1))[0]  # (N, )
-        n = len(indexes)
-        for j in random.sample(list(indexes), k=round(p * n)):
-            l, box, s = labels[j], boxes[j], segments[j]
-            labels = np.concatenate((labels, [[l[0], *box]]), 0)
-            segments.append(np.concatenate((w - s[:, 0:1], s[:, 1:2]), 1))
-            cv2.drawContours(im_new, [segments[j].astype(np.int32)], -1, (1, 1, 1), cv2.FILLED)
-
-        result = cv2.flip(im, 1)  # augment segments (flip left-right)
-        i = cv2.flip(im_new, 1).astype(bool)
-        im[i] = result[i]  # cv2.imwrite('debug.jpg', im)  # debug
-
-    return im, labels, segments
-
-
-def cutout(im, labels, p=0.5):
-    """Applies image cutout augmentation https://arxiv.org/abs/1708.04552."""
-    if random.random() < p:
-        h, w = im.shape[:2]
-        scales = [0.5] * 1 + [0.25] * 2 + [0.125] * 4 + [0.0625] * 8 + [0.03125] * 16  # image size fraction
-        for s in scales:
-            mask_h = random.randint(1, int(h * s))  # create random masks
-            mask_w = random.randint(1, int(w * s))
-
-            # Box
-            xmin = max(0, random.randint(0, w) - mask_w // 2)
-            ymin = max(0, random.randint(0, h) - mask_h // 2)
-            xmax = min(w, xmin + mask_w)
-            ymax = min(h, ymin + mask_h)
-
-            # Apply random color mask
-            im[ymin:ymax, xmin:xmax] = [random.randint(64, 191) for _ in range(3)]
-
-            # Return unobscured labels
-            if len(labels) and s > 0.03:
-                box = np.array([[xmin, ymin, xmax, ymax]], dtype=np.float32)
-                ioa = bbox_ioa(box, xywhn2xyxy(labels[:, 1:5], w, h))[0]  # intersection over area
-                labels = labels[ioa < 0.60]  # remove >60% obscured labels
-
-    return labels
-
-
-def mixup(im, labels, im2, labels2):
-    """Applies MixUp augmentation https://arxiv.org/pdf/1710.09412.pdf."""
-    r = np.random.beta(32.0, 32.0)  # mixup ratio, alpha=beta=32.0
-    im = (im * r + im2 * (1 - r)).astype(np.uint8)
-    labels = np.concatenate((labels, labels2), 0)
-    return im, labels
-
-
-def box_candidates(box1, box2, wh_thr=2, ar_thr=100, area_thr=0.1, eps=1e-16):  # box1(4,n), box2(4,n)
-    # Compute candidate boxes: box1 before augment, box2 after augment, wh_thr (pixels), aspect_ratio_thr, area_ratio
-    w1, h1 = box1[2] - box1[0], box1[3] - box1[1]
-    w2, h2 = box2[2] - box2[0], box2[3] - box2[1]
-    ar = np.maximum(w2 / (h2 + eps), h2 / (w2 + eps))  # aspect ratio
-    return (w2 > wh_thr) & (h2 > wh_thr) & (w2 * h2 / (w1 * h1 + eps) > area_thr) & (ar < ar_thr)  # candidates
-
-
-def classify_albumentations(
-        augment=True,
-        size=224,
-        scale=(0.08, 1.0),
-        ratio=(0.75, 1.0 / 0.75),  # 0.75, 1.33
-        hflip=0.5,
-        vflip=0.0,
-        jitter=0.4,
-        mean=IMAGENET_MEAN,
-        std=IMAGENET_STD,
-        auto_aug=False):
-    # YOLOv5 classification Albumentations (optional, only used if package is installed)
-    prefix = colorstr('albumentations: ')
+def check_imshow(warn=False):
+    """Check if environment supports image displays."""
     try:
-        import albumentations as A
-        from albumentations.pytorch import ToTensorV2
-        check_version(A.__version__, '1.0.3', hard=True)  # version requirement
-        if augment:  # Resize and crop
-            T = [A.RandomResizedCrop(height=size, width=size, scale=scale, ratio=ratio)]
-            if auto_aug:
-                # TODO: implement AugMix, AutoAug & RandAug in albumentation
-                LOGGER.info(f'{prefix}auto augmentations are currently not supported')
-            else:
-                if hflip > 0:
-                    T += [A.HorizontalFlip(p=hflip)]
-                if vflip > 0:
-                    T += [A.VerticalFlip(p=vflip)]
-                if jitter > 0:
-                    jitter = float(jitter)
-                    T += [A.ColorJitter(jitter, jitter, jitter, 0)]  # brightness, contrast, satuaration, 0 hue
-        else:  # Use fixed crop for eval set (reproducibility)
-            T = [A.SmallestMaxSize(max_size=size), A.CenterCrop(height=size, width=size)]
-        T += [A.Normalize(mean=mean, std=std), ToTensorV2()]  # Normalize and convert to Tensor
-        LOGGER.info(prefix + ', '.join(f'{x}'.replace('always_apply=False, ', '') for x in T if x.p))
-        return A.Compose(T)
-
-    except ImportError:  # package not installed, skip
-        LOGGER.warning(f'{prefix}âš ï¸ not found, install with `pip install albumentations` (recommended)')
+        assert not any((is_colab(), is_kaggle(), is_docker()))
+        cv2.imshow('test', np.zeros((1, 1, 3)))
+        cv2.waitKey(1)
+        cv2.destroyAllWindows()
+        cv2.waitKey(1)
+        return True
     except Exception as e:
-        LOGGER.info(f'{prefix}{e}')
+        if warn:
+            LOGGER.warning(f'WARNING âš ï¸ Environment does not support cv2.imshow() or PIL Image.show()\n{e}')
+        return False
+
+
+def check_yolo(verbose=True, device=''):
+    """Return a human-readable YOLO software and hardware summary."""
+    from ultralytics.utils.torch_utils import select_device
+
+    if is_jupyter():
+        if check_requirements('wandb', install=False):
+            os.system('pip uninstall -y wandb')  # uninstall wandb: unwanted account creation prompt with infinite hang
+        if is_colab():
+            shutil.rmtree('sample_data', ignore_errors=True)  # remove colab /sample_data directory
+
+    if verbose:
+        # System info
+        gib = 1 << 30  # bytes per GiB
+        ram = psutil.virtual_memory().total
+        total, used, free = shutil.disk_usage('/')
+        s = f'({os.cpu_count()} CPUs, {ram / gib:.1f} GB RAM, {(total - free) / gib:.1f}/{total / gib:.1f} GB disk)'
+        with contextlib.suppress(Exception):  # clear display if ipython is installed
+            from IPython import display
+            display.clear_output()
+    else:
+        s = ''
 
+    select_device(device=device, newline=False)
+    LOGGER.info(f'Setup complete âœ… {s}')
 
-def classify_transforms(size=224):
-    """Transforms to apply if albumentations not installed."""
-    assert isinstance(size, int), f'ERROR: classify_transforms size {size} must be integer, not (list, tuple)'
-    # T.Compose([T.ToTensor(), T.Resize(size), T.CenterCrop(size), T.Normalize(IMAGENET_MEAN, IMAGENET_STD)])
-    return T.Compose([CenterCrop(size), ToTensor(), T.Normalize(IMAGENET_MEAN, IMAGENET_STD)])
-
-
-class LetterBox:
-    # YOLOv5 LetterBox class for image preprocessing, i.e. T.Compose([LetterBox(size), ToTensor()])
-    def __init__(self, size=(640, 640), auto=False, stride=32):
-        """Resizes and crops an image to a specified size for YOLOv5 preprocessing."""
-        super().__init__()
-        self.h, self.w = (size, size) if isinstance(size, int) else size
-        self.auto = auto  # pass max size integer, automatically solve for short side using stride
-        self.stride = stride  # used with auto
-
-    def __call__(self, im):  # im = np.array HWC
-        imh, imw = im.shape[:2]
-        r = min(self.h / imh, self.w / imw)  # ratio of new/old
-        h, w = round(imh * r), round(imw * r)  # resized image
-        hs, ws = (math.ceil(x / self.stride) * self.stride for x in (h, w)) if self.auto else self.h, self.w
-        top, left = round((hs - h) / 2 - 0.1), round((ws - w) / 2 - 0.1)
-        im_out = np.full((self.h, self.w, 3), 114, dtype=im.dtype)
-        im_out[top:top + h, left:left + w] = cv2.resize(im, (w, h), interpolation=cv2.INTER_LINEAR)
-        return im_out
-
-
-class CenterCrop:
-    # YOLOv5 CenterCrop class for image preprocessing, i.e. T.Compose([CenterCrop(size), ToTensor()])
-    def __init__(self, size=640):
-        """Converts input image into tensor for YOLOv5 processing."""
-        super().__init__()
-        self.h, self.w = (size, size) if isinstance(size, int) else size
-
-    def __call__(self, im):  # im = np.array HWC
-        imh, imw = im.shape[:2]
-        m = min(imh, imw)  # min dimension
-        top, left = (imh - m) // 2, (imw - m) // 2
-        return cv2.resize(im[top:top + m, left:left + m], (self.w, self.h), interpolation=cv2.INTER_LINEAR)
-
-
-class ToTensor:
-    # YOLOv5 ToTensor class for image preprocessing, i.e. T.Compose([LetterBox(size), ToTensor()])
-    def __init__(self, half=False):
-        """Initialize ToTensor class for YOLOv5 image preprocessing."""
-        super().__init__()
-        self.half = half
-
-    def __call__(self, im):  # im = np.array HWC in BGR order
-        im = np.ascontiguousarray(im.transpose((2, 0, 1))[::-1])  # HWC to CHW -> BGR to RGB -> contiguous
-        im = torch.from_numpy(im)  # to torch
-        im = im.half() if self.half else im.float()  # uint8 to fp16/32
-        im /= 255.0  # 0-255 to 0.0-1.0
-        return im
+
+def check_amp(model):
+    """
+    This function checks the PyTorch Automatic Mixed Precision (AMP) functionality of a YOLOv8 model.
+    If the checks fail, it means there are anomalies with AMP on the system that may cause NaN losses or zero-mAP
+    results, so AMP will be disabled during training.
+
+    Args:
+        model (nn.Module): A YOLOv8 model instance.
+
+    Returns:
+        (bool): Returns True if the AMP functionality works correctly with YOLOv8 model, else False.
+
+    Raises:
+        AssertionError: If the AMP checks fail, indicating anomalies with the AMP functionality on the system.
+    """
+    device = next(model.parameters()).device  # get model device
+    if device.type in ('cpu', 'mps'):
+        return False  # AMP only used on CUDA devices
+
+    def amp_allclose(m, im):
+        """All close FP32 vs AMP results."""
+        a = m(im, device=device, verbose=False)[0].boxes.data  # FP32 inference
+        with torch.cuda.amp.autocast(True):
+            b = m(im, device=device, verbose=False)[0].boxes.data  # AMP inference
+        del m
+        return a.shape == b.shape and torch.allclose(a, b.float(), atol=0.5)  # close to 0.5 absolute tolerance
+
+    f = ROOT / 'assets/bus.jpg'  # image to check
+    im = f if f.exists() else 'https://ultralytics.com/images/bus.jpg' if ONLINE else np.ones((640, 640, 3))
+    prefix = colorstr('AMP: ')
+    LOGGER.info(f'{prefix}running Automatic Mixed Precision (AMP) checks with YOLOv8n...')
+    warning_msg = "Setting 'amp=True'. If you experience zero-mAP or NaN losses you can disable AMP with amp=False."
+    try:
+        from ultralytics import YOLO
+        assert amp_allclose(YOLO('yolov8n.pt'), im)
+        LOGGER.info(f'{prefix}checks passed âœ…')
+    except ConnectionError:
+        LOGGER.warning(f'{prefix}checks skipped âš ï¸, offline and unable to download YOLOv8n. {warning_msg}')
+    except (AttributeError, ModuleNotFoundError):
+        LOGGER.warning(
+            f'{prefix}checks skipped âš ï¸. Unable to load YOLOv8n due to possible Ultralytics package modifications. {warning_msg}'
+        )
+    except AssertionError:
+        LOGGER.warning(f'{prefix}checks failed âŒ. Anomalies were detected with AMP on your system that may lead to '
+                       f'NaN losses or zero-mAP results, so AMP will be disabled during training.')
+        return False
+    return True
+
+
+def git_describe(path=ROOT):  # path must be a directory
+    """Return human-readable git description, i.e. v5.0-5-g3e25f1e https://git-scm.com/docs/git-describe."""
+    try:
+        assert (Path(path) / '.git').is_dir()
+        return subprocess.check_output(f'git -C {path} describe --tags --long --always', shell=True).decode()[:-1]
+    except AssertionError:
+        return ''
+
+
+def print_args(args: Optional[dict] = None, show_file=True, show_func=False):
+    """Print function arguments (optional args dict)."""
+
+    def strip_auth(v):
+        """Clean longer Ultralytics HUB URLs by stripping potential authentication information."""
+        return clean_url(v) if (isinstance(v, str) and v.startswith('http') and len(v) > 100) else v
+
+    x = inspect.currentframe().f_back  # previous frame
+    file, _, func, _, _ = inspect.getframeinfo(x)
+    if args is None:  # get args automatically
+        args, _, _, frm = inspect.getargvalues(x)
+        args = {k: v for k, v in frm.items() if k in args}
+    try:
+        file = Path(file).resolve().relative_to(ROOT).with_suffix('')
+    except ValueError:
+        file = Path(file).stem
+    s = (f'{file}: ' if show_file else '') + (f'{func}: ' if show_func else '')
+    LOGGER.info(colorstr(s) + ', '.join(f'{k}={strip_auth(v)}' for k, v in args.items()))
```

## Comparing `ultralytics/yolo/engine/exporter.py` & `ultralytics/engine/exporter.py`

 * *Files 2% similar despite different names*

```diff
@@ -12,15 +12,15 @@
 CoreML                  | `coreml`                  | yolov8n.mlmodel
 TensorFlow SavedModel   | `saved_model`             | yolov8n_saved_model/
 TensorFlow GraphDef     | `pb`                      | yolov8n.pb
 TensorFlow Lite         | `tflite`                  | yolov8n.tflite
 TensorFlow Edge TPU     | `edgetpu`                 | yolov8n_edgetpu.tflite
 TensorFlow.js           | `tfjs`                    | yolov8n_web_model/
 PaddlePaddle            | `paddle`                  | yolov8n_paddle_model/
-NCNN                    | `ncnn`                    | yolov8n_ncnn_model/
+ncnn                    | `ncnn`                    | yolov8n_ncnn_model/
 
 Requirements:
     $ pip install ultralytics[export]
 
 Python:
     from ultralytics import YOLO
     model = YOLO('yolov8n.pt')
@@ -51,29 +51,30 @@
 import json
 import os
 import shutil
 import subprocess
 import time
 import warnings
 from copy import deepcopy
+from datetime import datetime
 from pathlib import Path
 
 import torch
 
+from ultralytics.cfg import get_cfg
 from ultralytics.nn.autobackend import check_class_names
 from ultralytics.nn.modules import C2f, Detect, RTDETRDecoder
 from ultralytics.nn.tasks import DetectionModel, SegmentationModel
-from ultralytics.yolo.cfg import get_cfg
-from ultralytics.yolo.utils import (ARM64, DEFAULT_CFG, LINUX, LOGGER, MACOS, ROOT, __version__, callbacks, colorstr,
-                                    get_default_args, yaml_save)
-from ultralytics.yolo.utils.checks import check_imgsz, check_requirements, check_version
-from ultralytics.yolo.utils.downloads import attempt_download_asset, get_github_assets
-from ultralytics.yolo.utils.files import file_size
-from ultralytics.yolo.utils.ops import Profile
-from ultralytics.yolo.utils.torch_utils import get_latest_opset, select_device, smart_inference_mode
+from ultralytics.utils import (ARM64, DEFAULT_CFG, LINUX, LOGGER, MACOS, ROOT, WINDOWS, __version__, callbacks,
+                               colorstr, get_default_args, yaml_save)
+from ultralytics.utils.checks import check_imgsz, check_requirements, check_version
+from ultralytics.utils.downloads import attempt_download_asset, get_github_assets
+from ultralytics.utils.files import file_size, spaces_in_path
+from ultralytics.utils.ops import Profile
+from ultralytics.utils.torch_utils import get_latest_opset, select_device, smart_inference_mode
 
 
 def export_formats():
     """YOLOv8 export formats."""
     import pandas
     x = [
         ['PyTorch', '-', '.pt', True, True],
@@ -84,15 +85,15 @@
         ['CoreML', 'coreml', '.mlmodel', True, False],
         ['TensorFlow SavedModel', 'saved_model', '_saved_model', True, True],
         ['TensorFlow GraphDef', 'pb', '.pb', True, True],
         ['TensorFlow Lite', 'tflite', '.tflite', True, False],
         ['TensorFlow Edge TPU', 'edgetpu', '_edgetpu.tflite', True, False],
         ['TensorFlow.js', 'tfjs', '_web_model', True, False],
         ['PaddlePaddle', 'paddle', '_paddle_model', True, True],
-        ['NCNN', 'ncnn', '_ncnn_model', True, True], ]
+        ['ncnn', 'ncnn', '_ncnn_model', True, True], ]
     return pandas.DataFrame(x, columns=['Format', 'Argument', 'Suffix', 'CPU', 'GPU'])
 
 
 def gd_outputs(gd):
     """TensorFlow GraphDef model output node names."""
     name_list, input_list = [], []
     for node in gd.node:  # tensorflow.core.framework.node_def_pb2.NodeDef
@@ -107,19 +108,19 @@
 
     def outer_func(*args, **kwargs):
         """Export a model."""
         prefix = inner_args['prefix']
         try:
             with Profile() as dt:
                 f, model = inner_func(*args, **kwargs)
-            LOGGER.info(f'{prefix} export success âœ… {dt.t:.1f}s, saved as {f} ({file_size(f):.1f} MB)')
+            LOGGER.info(f"{prefix} export success âœ… {dt.t:.1f}s, saved as '{f}' ({file_size(f):.1f} MB)")
             return f, model
         except Exception as e:
             LOGGER.info(f'{prefix} export failure âŒ {dt.t:.1f}s: {e}')
-            return None, None
+            raise e
 
     return outer_func
 
 
 class Exporter:
     """
     A class for exporting a model.
@@ -215,24 +216,25 @@
         self.pretty_name = Path(self.model.yaml.get('yaml_file', self.file)).stem.replace('yolo', 'YOLO')
         trained_on = f'trained on {Path(self.args.data).name}' if self.args.data else '(untrained)'
         description = f'Ultralytics {self.pretty_name} model {trained_on}'
         self.metadata = {
             'description': description,
             'author': 'Ultralytics',
             'license': 'AGPL-3.0 https://ultralytics.com/license',
+            'date': datetime.now().isoformat(),
             'version': __version__,
             'stride': int(max(model.stride)),
             'task': model.task,
             'batch': self.args.batch,
             'imgsz': self.imgsz,
             'names': model.names}  # model metadata
         if model.task == 'pose':
             self.metadata['kpt_shape'] = model.model[-1].kpt_shape
 
-        LOGGER.info(f"\n{colorstr('PyTorch:')} starting from {file} with input shape {tuple(im.shape)} BCHW and "
+        LOGGER.info(f"\n{colorstr('PyTorch:')} starting from '{file}' with input shape {tuple(im.shape)} BCHW and "
                     f'output shape(s) {self.output_shape} ({file_size(file):.1f} MB)')
 
         # Exports
         f = [''] * len(fmts)  # exported filenames
         if jit or ncnn:  # TorchScript
             f[0], _ = self.export_torchscript()
         if engine:  # TensorRT required before ONNX
@@ -252,15 +254,15 @@
                 f[7], _ = self.export_tflite(s_model, nms=False, agnostic_nms=self.args.agnostic_nms)
             if edgetpu:
                 f[8], _ = self.export_edgetpu(tflite_model=Path(f[5]) / f'{self.file.stem}_full_integer_quant.tflite')
             if tfjs:
                 f[9], _ = self.export_tfjs()
         if paddle:  # PaddlePaddle
             f[10], _ = self.export_paddle()
-        if ncnn:  # NCNN
+        if ncnn:  # ncnn
             f[11], _ = self.export_ncnn()
 
         # Finish
         f = [str(x) for x in f if x]  # filter out '' and None
         if any(f):
             f = str(Path(f[-1]))
             square = self.imgsz[0] == self.imgsz[1]
@@ -308,18 +310,18 @@
         f = str(self.file.with_suffix('.onnx'))
 
         output_names = ['output0', 'output1'] if isinstance(self.model, SegmentationModel) else ['output0']
         dynamic = self.args.dynamic
         if dynamic:
             dynamic = {'images': {0: 'batch', 2: 'height', 3: 'width'}}  # shape(1,3,640,640)
             if isinstance(self.model, SegmentationModel):
-                dynamic['output0'] = {0: 'batch', 1: 'anchors'}  # shape(1,25200,85)
+                dynamic['output0'] = {0: 'batch', 2: 'anchors'}  # shape(1, 116, 8400)
                 dynamic['output1'] = {0: 'batch', 2: 'mask_height', 3: 'mask_width'}  # shape(1,32,160,160)
             elif isinstance(self.model, DetectionModel):
-                dynamic['output0'] = {0: 'batch', 1: 'anchors'}  # shape(1,25200,85)
+                dynamic['output0'] = {0: 'batch', 2: 'anchors'}  # shape(1, 84, 8400)
 
         torch.onnx.export(
             self.model.cpu() if dynamic else self.model,  # --dynamic only compatible with cpu
             self.im.cpu() if dynamic else self.im,
             f,
             verbose=False,
             opset_version=opset_version,
@@ -334,15 +336,15 @@
 
         # Simplify
         if self.args.simplify:
             try:
                 import onnxsim
 
                 LOGGER.info(f'{prefix} simplifying with onnxsim {onnxsim.__version__}...')
-                # subprocess.run(f'onnxsim {f} {f}', shell=True)
+                # subprocess.run(f'onnxsim "{f}" "{f}"', shell=True)
                 model_onnx, check = onnxsim.simplify(model_onnx)
                 assert check, 'Simplified ONNX model could not be validated'
             except Exception as e:
                 LOGGER.info(f'{prefix} simplifier failure: {e}')
 
         # Metadata
         for k, v in self.metadata.items():
@@ -351,15 +353,15 @@
 
         onnx.save(model_onnx, f)
         return f, model_onnx
 
     @try_export
     def export_openvino(self, prefix=colorstr('OpenVINO:')):
         """YOLOv8 OpenVINO export."""
-        check_requirements('openvino-dev>=2022.3')  # requires openvino-dev: https://pypi.org/project/openvino-dev/
+        check_requirements('openvino-dev>=2023.0')  # requires openvino-dev: https://pypi.org/project/openvino-dev/
         import openvino.runtime as ov  # noqa
         from openvino.tools import mo  # noqa
 
         LOGGER.info(f'\n{prefix} starting export with openvino {ov.__version__}...')
         f = str(self.file).replace(self.file.suffix, f'_openvino_model{os.sep}')
         f_onnx = self.file.with_suffix('.onnx')
         f_ov = str(Path(f) / self.file.with_suffix('.xml').name)
@@ -395,68 +397,79 @@
         f = str(self.file).replace(self.file.suffix, f'_paddle_model{os.sep}')
 
         pytorch2paddle(module=self.model, save_dir=f, jit_type='trace', input_examples=[self.im])  # export
         yaml_save(Path(f) / 'metadata.yaml', self.metadata)  # add metadata.yaml
         return f, None
 
     @try_export
-    def export_ncnn(self, prefix=colorstr('NCNN:')):
+    def export_ncnn(self, prefix=colorstr('ncnn:')):
         """
-        YOLOv8 NCNN export using PNNX https://github.com/pnnx/pnnx.
+        YOLOv8 ncnn export using PNNX https://github.com/pnnx/pnnx.
         """
-        check_requirements('git+https://github.com/Tencent/ncnn.git' if ARM64 else 'ncnn')  # requires NCNN
+        check_requirements('git+https://github.com/Tencent/ncnn.git' if ARM64 else 'ncnn')  # requires ncnn
         import ncnn  # noqa
 
-        LOGGER.info(f'\n{prefix} starting export with NCNN {ncnn.__version__}...')
+        LOGGER.info(f'\n{prefix} starting export with ncnn {ncnn.__version__}...')
         f = Path(str(self.file).replace(self.file.suffix, f'_ncnn_model{os.sep}'))
-        f_ts = str(self.file.with_suffix('.torchscript'))
+        f_ts = self.file.with_suffix('.torchscript')
 
-        if Path('./pnnx').is_file():
-            pnnx = './pnnx'
-        elif (ROOT / 'pnnx').is_file():
-            pnnx = ROOT / 'pnnx'
+        pnnx_filename = 'pnnx.exe' if WINDOWS else 'pnnx'
+        if Path(pnnx_filename).is_file():
+            pnnx = pnnx_filename
+        elif (ROOT / pnnx_filename).is_file():
+            pnnx = ROOT / pnnx_filename
         else:
             LOGGER.warning(
                 f'{prefix} WARNING âš ï¸ PNNX not found. Attempting to download binary file from '
                 'https://github.com/pnnx/pnnx/.\nNote PNNX Binary file must be placed in current working directory '
                 f'or in {ROOT}. See PNNX repo for full installation instructions.')
             _, assets = get_github_assets(repo='pnnx/pnnx')
             asset = [x for x in assets if ('macos' if MACOS else 'ubuntu' if LINUX else 'windows') in x][0]
             attempt_download_asset(asset, repo='pnnx/pnnx', release='latest')
             unzip_dir = Path(asset).with_suffix('')
-            pnnx = ROOT / 'pnnx'  # new location
-            (unzip_dir / 'pnnx').rename(pnnx)  # move binary to ROOT
+            pnnx = ROOT / pnnx_filename  # new location
+            (unzip_dir / pnnx_filename).rename(pnnx)  # move binary to ROOT
             shutil.rmtree(unzip_dir)  # delete unzip dir
             Path(asset).unlink()  # delete zip
             pnnx.chmod(0o777)  # set read, write, and execute permissions for everyone
 
-        cmd = [
-            str(pnnx),
-            f_ts,
+        use_ncnn = True
+        ncnn_args = [
+            f'ncnnparam={f / "model.ncnn.param"}',
+            f'ncnnbin={f / "model.ncnn.bin"}',
+            f'ncnnpy={f / "model_ncnn.py"}', ] if use_ncnn else []
+
+        use_pnnx = False
+        pnnx_args = [
             f'pnnxparam={f / "model.pnnx.param"}',
             f'pnnxbin={f / "model.pnnx.bin"}',
             f'pnnxpy={f / "model_pnnx.py"}',
-            f'pnnxonnx={f / "model.pnnx.onnx"}',
-            f'ncnnparam={f / "model.ncnn.param"}',
-            f'ncnnbin={f / "model.ncnn.bin"}',
-            f'ncnnpy={f / "model_ncnn.py"}',
+            f'pnnxonnx={f / "model.pnnx.onnx"}', ] if use_pnnx else []
+
+        cmd = [
+            str(pnnx),
+            str(f_ts),
+            *ncnn_args,
+            *pnnx_args,
             f'fp16={int(self.args.half)}',
             f'device={self.device.type}',
             f'inputshape="{[self.args.batch, 3, *self.imgsz]}"', ]
         f.mkdir(exist_ok=True)  # make ncnn_model directory
         LOGGER.info(f"{prefix} running '{' '.join(cmd)}'")
         subprocess.run(cmd, check=True)
+        for f_debug in 'debug.bin', 'debug.param', 'debug2.bin', 'debug2.param':  # remove debug files
+            Path(f_debug).unlink(missing_ok=True)
 
         yaml_save(f / 'metadata.yaml', self.metadata)  # add metadata.yaml
         return str(f), None
 
     @try_export
     def export_coreml(self, prefix=colorstr('CoreML:')):
         """YOLOv8 CoreML export."""
-        check_requirements('coremltools>=6.0')
+        check_requirements('coremltools>=6.0,<=6.2')
         import coremltools as ct  # noqa
 
         LOGGER.info(f'\n{prefix} starting export with coremltools {ct.__version__}...')
         f = self.file.with_suffix('.mlmodel')
 
         bias = [0.0, 0.0, 0.0]
         scale = 1 / 255
@@ -561,37 +574,66 @@
         """YOLOv8 TensorFlow SavedModel export."""
         try:
             import tensorflow as tf  # noqa
         except ImportError:
             cuda = torch.cuda.is_available()
             check_requirements(f"tensorflow{'-macos' if MACOS else '-aarch64' if ARM64 else '' if cuda else '-cpu'}")
             import tensorflow as tf  # noqa
-        check_requirements(('onnx', 'onnx2tf>=1.7.7', 'sng4onnx>=1.0.1', 'onnxsim>=0.4.17', 'onnx_graphsurgeon>=0.3.26',
+        check_requirements(('onnx', 'onnx2tf>=1.9.1', 'sng4onnx>=1.0.1', 'onnxsim>=0.4.17', 'onnx_graphsurgeon>=0.3.26',
                             'tflite_support', 'onnxruntime-gpu' if torch.cuda.is_available() else 'onnxruntime'),
                            cmds='--extra-index-url https://pypi.ngc.nvidia.com')
 
         LOGGER.info(f'\n{prefix} starting export with tensorflow {tf.__version__}...')
         f = Path(str(self.file).replace(self.file.suffix, '_saved_model'))
         if f.is_dir():
             import shutil
             shutil.rmtree(f)  # delete output folder
 
         # Export to ONNX
         self.args.simplify = True
         f_onnx, _ = self.export_onnx()
 
         # Export to TF
-        int8 = '-oiqt -qt per-tensor' if self.args.int8 else ''
-        cmd = f'onnx2tf -i {f_onnx} -o {f} -nuo --non_verbose {int8}'
-        LOGGER.info(f"\n{prefix} running '{cmd.strip()}'")
+        tmp_file = f / 'tmp_tflite_int8_calibration_images.npy'  # int8 calibration images file
+        if self.args.int8:
+            if self.args.data:
+                import numpy as np
+
+                from ultralytics.data.dataset import YOLODataset
+                from ultralytics.data.utils import check_det_dataset
+
+                # Generate calibration data for integer quantization
+                LOGGER.info(f"{prefix} collecting INT8 calibration images from 'data={self.args.data}'")
+                dataset = YOLODataset(check_det_dataset(self.args.data)['val'], imgsz=self.imgsz[0], augment=False)
+                images = []
+                n_images = 100  # maximum number of images
+                for n, batch in enumerate(dataset):
+                    if n >= n_images:
+                        break
+                    im = batch['img'].permute(1, 2, 0)[None]  # list to nparray, CHW to BHWC,
+                    images.append(im)
+                f.mkdir()
+                images = torch.cat(images, 0).float()
+                # mean = images.view(-1, 3).mean(0)  # imagenet mean [123.675, 116.28, 103.53]
+                # std = images.view(-1, 3).std(0)  # imagenet std [58.395, 57.12, 57.375]
+                np.save(str(tmp_file), images.numpy())  # BHWC
+                int8 = f'-oiqt -qt per-tensor -cind images "{tmp_file}" "[[[[0, 0, 0]]]]" "[[[[255, 255, 255]]]]"'
+            else:
+                int8 = '-oiqt -qt per-tensor'
+        else:
+            int8 = ''
+
+        cmd = f'onnx2tf -i "{f_onnx}" -o "{f}" -nuo --non_verbose {int8}'.strip()
+        LOGGER.info(f"{prefix} running '{cmd}'")
         subprocess.run(cmd, shell=True)
         yaml_save(f / 'metadata.yaml', self.metadata)  # add metadata.yaml
 
         # Remove/rename TFLite models
         if self.args.int8:
+            tmp_file.unlink(missing_ok=True)
             for file in f.rglob('*_dynamic_range_quant.tflite'):
                 file.rename(file.with_name(file.stem.replace('_dynamic_range_quant', '_int8') + file.suffix))
             for file in f.rglob('*_integer_quant_with_int16_act.tflite'):
                 file.unlink()  # delete extra fp16 activation TFLite files
 
         # Add TFLite metadata
         for file in f.rglob('*.tflite'):
@@ -650,39 +692,44 @@
                     'sudo apt-get update', 'sudo apt-get install edgetpu-compiler'):
                 subprocess.run(c if sudo else c.replace('sudo ', ''), shell=True, check=True)
         ver = subprocess.run(cmd, shell=True, capture_output=True, check=True).stdout.decode().split()[-1]
 
         LOGGER.info(f'\n{prefix} starting export with Edge TPU compiler {ver}...')
         f = str(tflite_model).replace('.tflite', '_edgetpu.tflite')  # Edge TPU model
 
-        cmd = f'edgetpu_compiler -s -d -k 10 --out_dir {Path(f).parent} {tflite_model}'
+        cmd = f'edgetpu_compiler -s -d -k 10 --out_dir "{Path(f).parent}" "{tflite_model}"'
         LOGGER.info(f"{prefix} running '{cmd}'")
-        subprocess.run(cmd.split(), check=True)
+        subprocess.run(cmd, shell=True)
         self._add_tflite_metadata(f)
         return f, None
 
     @try_export
     def export_tfjs(self, prefix=colorstr('TensorFlow.js:')):
         """YOLOv8 TensorFlow.js export."""
         check_requirements('tensorflowjs')
         import tensorflow as tf
         import tensorflowjs as tfjs  # noqa
 
         LOGGER.info(f'\n{prefix} starting export with tensorflowjs {tfjs.__version__}...')
         f = str(self.file).replace(self.file.suffix, '_web_model')  # js dir
-        f_pb = self.file.with_suffix('.pb')  # *.pb path
+        f_pb = str(self.file.with_suffix('.pb'))  # *.pb path
 
         gd = tf.Graph().as_graph_def()  # TF GraphDef
         with open(f_pb, 'rb') as file:
             gd.ParseFromString(file.read())
         outputs = ','.join(gd_outputs(gd))
         LOGGER.info(f'\n{prefix} output node names: {outputs}')
 
-        cmd = f'tensorflowjs_converter --input_format=tf_frozen_model --output_node_names={outputs} {f_pb} {f}'
-        subprocess.run(cmd.split(), check=True)
+        with spaces_in_path(f_pb) as fpb_, spaces_in_path(f) as f_:  # exporter can not handle spaces in path
+            cmd = f'tensorflowjs_converter --input_format=tf_frozen_model --output_node_names={outputs} "{fpb_}" "{f_}"'
+            LOGGER.info(f"{prefix} running '{cmd}'")
+            subprocess.run(cmd, shell=True)
+
+        if ' ' in str(f):
+            LOGGER.warning(f"{prefix} WARNING âš ï¸ your model may not work correctly with spaces in path '{f}'.")
 
         # f_json = Path(f) / 'model.json'  # *.json path
         # with open(f_json, 'w') as j:  # sort JSON Identity_* in ascending order
         #     subst = re.sub(
         #         r'{"outputs": {"Identity.?.?": {"name": "Identity.?.?"}, '
         #         r'"Identity.?.?": {"name": "Identity.?.?"}, '
         #         r'"Identity.?.?": {"name": "Identity.?.?"}, '
```

## Comparing `ultralytics/yolo/engine/model.py` & `ultralytics/engine/model.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,36 +1,35 @@
 # Ultralytics YOLO ðŸš€, AGPL-3.0 license
 
 import sys
 from pathlib import Path
 from typing import Union
 
-from ultralytics import yolo  # noqa
+from ultralytics.cfg import get_cfg
+from ultralytics.engine.exporter import Exporter
+from ultralytics.models import yolo  # noqa
 from ultralytics.nn.tasks import (ClassificationModel, DetectionModel, PoseModel, SegmentationModel,
                                   attempt_load_one_weight, guess_model_task, nn, yaml_model_load)
-from ultralytics.yolo.cfg import get_cfg
-from ultralytics.yolo.engine.exporter import Exporter
-from ultralytics.yolo.utils import (DEFAULT_CFG, DEFAULT_CFG_DICT, DEFAULT_CFG_KEYS, LOGGER, RANK, ROOT, callbacks,
-                                    is_git_dir, yaml_load)
-from ultralytics.yolo.utils.checks import check_file, check_imgsz, check_pip_update_available, check_yaml
-from ultralytics.yolo.utils.downloads import GITHUB_ASSET_STEMS
-from ultralytics.yolo.utils.torch_utils import smart_inference_mode
+from ultralytics.utils import (DEFAULT_CFG, DEFAULT_CFG_DICT, DEFAULT_CFG_KEYS, LOGGER, RANK, ROOT, callbacks,
+                               is_git_dir, yaml_load)
+from ultralytics.utils.checks import check_file, check_imgsz, check_pip_update_available, check_yaml
+from ultralytics.utils.downloads import GITHUB_ASSET_STEMS
+from ultralytics.utils.torch_utils import smart_inference_mode
 
 # Map head to model, trainer, validator, and predictor classes
 TASK_MAP = {
     'classify': [
-        ClassificationModel, yolo.v8.classify.ClassificationTrainer, yolo.v8.classify.ClassificationValidator,
-        yolo.v8.classify.ClassificationPredictor],
-    'detect': [
-        DetectionModel, yolo.v8.detect.DetectionTrainer, yolo.v8.detect.DetectionValidator,
-        yolo.v8.detect.DetectionPredictor],
+        ClassificationModel, yolo.classify.ClassificationTrainer, yolo.classify.ClassificationValidator,
+        yolo.classify.ClassificationPredictor],
+    'detect':
+    [DetectionModel, yolo.detect.DetectionTrainer, yolo.detect.DetectionValidator, yolo.detect.DetectionPredictor],
     'segment': [
-        SegmentationModel, yolo.v8.segment.SegmentationTrainer, yolo.v8.segment.SegmentationValidator,
-        yolo.v8.segment.SegmentationPredictor],
-    'pose': [PoseModel, yolo.v8.pose.PoseTrainer, yolo.v8.pose.PoseValidator, yolo.v8.pose.PosePredictor]}
+        SegmentationModel, yolo.segment.SegmentationTrainer, yolo.segment.SegmentationValidator,
+        yolo.segment.SegmentationPredictor],
+    'pose': [PoseModel, yolo.pose.PoseTrainer, yolo.pose.PoseValidator, yolo.pose.PosePredictor]}
 
 
 class YOLO:
     """
     YOLO (You Only Look Once) object detection model.
 
     Args:
@@ -59,19 +58,19 @@
             Raises TypeError if the model is not a PyTorch model.
         reset() -> None:
             Resets the model modules.
         info(verbose:bool=False) -> None:
             Logs the model info.
         fuse() -> None:
             Fuses the model for faster inference.
-        predict(source=None, stream=False, **kwargs) -> List[ultralytics.yolo.engine.results.Results]:
+        predict(source=None, stream=False, **kwargs) -> List[ultralytics.engine.results.Results]:
             Performs prediction using the YOLO model.
 
     Returns:
-        list(ultralytics.yolo.engine.results.Results): The prediction results.
+        list(ultralytics.engine.results.Results): The prediction results.
     """
 
     def __init__(self, model: Union[str, Path] = 'yolov8n.pt', task=None) -> None:
         """
         Initializes the YOLO model.
 
         Args:
@@ -226,15 +225,15 @@
             source (str | int | PIL | np.ndarray): The source of the image to make predictions on.
                           Accepts all source types accepted by the YOLO model.
             stream (bool): Whether to stream the predictions or not. Defaults to False.
             **kwargs : Additional keyword arguments passed to the predictor.
                        Check the 'configuration' section in the documentation for all available options.
 
         Returns:
-            (List[ultralytics.yolo.engine.results.Results]): The prediction results.
+            (List[ultralytics.engine.results.Results]): The prediction results.
         """
         if source is None:
             source = ROOT / 'assets' if is_git_dir() else 'https://ultralytics.com/images/bus.jpg'
             LOGGER.warning(f"WARNING âš ï¸ 'source' is missing. Using 'source={source}'.")
         is_cli = (sys.argv[0].endswith('yolo') or sys.argv[0].endswith('ultralytics')) and any(
             x in sys.argv for x in ('predict', 'track', 'mode=predict', 'mode=track'))
         overrides = self.overrides.copy()
@@ -261,19 +260,19 @@
         Args:
             source (str, optional): The input source for object tracking. Can be a file path or a video stream.
             stream (bool, optional): Whether the input source is a video stream. Defaults to False.
             persist (bool, optional): Whether to persist the trackers if they already exist. Defaults to False.
             **kwargs (optional): Additional keyword arguments for the tracking process.
 
         Returns:
-            (List[ultralytics.yolo.engine.results.Results]): The tracking results.
+            (List[ultralytics.engine.results.Results]): The tracking results.
 
         """
         if not hasattr(self.predictor, 'trackers'):
-            from ultralytics.tracker import register_tracker
+            from ultralytics.trackers import register_tracker
             register_tracker(self, persist)
         # ByteTrack-based method needs low confidence predictions as input
         conf = kwargs.get('conf') or 0.1
         kwargs['conf'] = conf
         kwargs['mode'] = 'track'
         return self.predict(source=source, stream=stream, **kwargs)
 
@@ -311,20 +310,27 @@
         """
         Benchmark a model on all export formats.
 
         Args:
             **kwargs : Any other args accepted by the validators. To see all args check 'configuration' section in docs
         """
         self._check_is_pytorch_model()
-        from ultralytics.yolo.utils.benchmarks import benchmark
+        from ultralytics.utils.benchmarks import benchmark
         overrides = self.model.args.copy()
         overrides.update(kwargs)
         overrides['mode'] = 'benchmark'
         overrides = {**DEFAULT_CFG_DICT, **overrides}  # fill in missing overrides keys with defaults
-        return benchmark(model=self, imgsz=overrides['imgsz'], half=overrides['half'], device=overrides['device'])
+        return benchmark(
+            model=self,
+            data=kwargs.get('data'),  # if no 'data' argument passed set data=None for default datasets
+            imgsz=overrides['imgsz'],
+            half=overrides['half'],
+            int8=overrides['int8'],
+            device=overrides['device'],
+            verbose=overrides['verbose'])
 
     def export(self, **kwargs):
         """
         Export model.
 
         Args:
             **kwargs : Any other args accepted by the predictors. To see all args check 'configuration' section in docs
@@ -333,14 +339,16 @@
         overrides = self.overrides.copy()
         overrides.update(kwargs)
         overrides['mode'] = 'export'
         if overrides.get('imgsz') is None:
             overrides['imgsz'] = self.model.args['imgsz']  # use trained imgsz unless custom value is passed
         if 'batch' not in kwargs:
             overrides['batch'] = 1  # default to 1 if not modified
+        if 'data' not in kwargs:
+            overrides['data'] = None  # default to None if not modified (avoid int8 calibration with coco.yaml)
         args = get_cfg(cfg=DEFAULT_CFG, overrides=overrides)
         args.task = self.task
         return Exporter(overrides=args, _callbacks=self.callbacks)(model=self.model)
 
     def train(self, **kwargs):
         """
         Trains the model on a given dataset.
@@ -385,32 +393,24 @@
             device (str): device
         """
         self._check_is_pytorch_model()
         self.model.to(device)
 
     def tune(self, *args, **kwargs):
         """
-        Runs hyperparameter tuning using Ray Tune.
-
-        Args:
-            data (str): The dataset to run the tuner on.
-            space (dict, optional): The hyperparameter search space. Defaults to None.
-            grace_period (int, optional): The grace period in epochs of the ASHA scheduler. Defaults to 10.
-            gpu_per_trial (int, optional): The number of GPUs to allocate per trial. Defaults to None.
-            max_samples (int, optional): The maximum number of trials to run. Defaults to 10.
-            train_args (dict, optional): Additional arguments to pass to the `train()` method. Defaults to {}.
+        Runs hyperparameter tuning using Ray Tune. See ultralytics.utils.tuner.run_ray_tune for Args.
 
         Returns:
             (dict): A dictionary containing the results of the hyperparameter search.
 
         Raises:
             ModuleNotFoundError: If Ray Tune is not installed.
         """
         self._check_is_pytorch_model()
-        from ultralytics.yolo.utils.tuner import run_ray_tune
+        from ultralytics.utils.tuner import run_ray_tune
         return run_ray_tune(self, *args, **kwargs)
 
     @property
     def names(self):
         """Returns class names of the loaded model."""
         return self.model.names if hasattr(self.model, 'names') else None
```

## Comparing `ultralytics/yolo/engine/predictor.py` & `ultralytics/engine/predictor.py`

 * *Files 2% similar despite different names*

```diff
@@ -30,22 +30,22 @@
 import platform
 from pathlib import Path
 
 import cv2
 import numpy as np
 import torch
 
+from ultralytics.cfg import get_cfg
+from ultralytics.data import load_inference_source
+from ultralytics.data.augment import LetterBox, classify_transforms
 from ultralytics.nn.autobackend import AutoBackend
-from ultralytics.yolo.cfg import get_cfg
-from ultralytics.yolo.data import load_inference_source
-from ultralytics.yolo.data.augment import LetterBox, classify_transforms
-from ultralytics.yolo.utils import DEFAULT_CFG, LOGGER, MACOS, SETTINGS, WINDOWS, callbacks, colorstr, ops
-from ultralytics.yolo.utils.checks import check_imgsz, check_imshow
-from ultralytics.yolo.utils.files import increment_path
-from ultralytics.yolo.utils.torch_utils import select_device, smart_inference_mode
+from ultralytics.utils import DEFAULT_CFG, LOGGER, MACOS, SETTINGS, WINDOWS, callbacks, colorstr, ops
+from ultralytics.utils.checks import check_imgsz, check_imshow
+from ultralytics.utils.files import increment_path
+from ultralytics.utils.torch_utils import select_device, smart_inference_mode
 
 STREAM_WARNING = """
     WARNING âš ï¸ stream/video/webcam/dir predict source will accumulate results in RAM unless `stream=True` is passed,
     causing potential out-of-memory errors for large sources or long-running streams/videos.
 
     Usage:
         results = model(source=..., stream=True)  # generator of Results objects
@@ -127,14 +127,19 @@
 
         img = im.to(self.device)
         img = img.half() if self.model.fp16 else img.float()  # uint8 to fp16/32
         if not_tensor:
             img /= 255  # 0 - 255 to 0.0 - 1.0
         return img
 
+    def inference(self, im, *args, **kwargs):
+        visualize = increment_path(self.save_dir / Path(self.batch[0][0]).stem,
+                                   mkdir=True) if self.args.visualize and (not self.source_type.tensor) else False
+        return self.model(im, augment=self.args.augment, visualize=visualize)
+
     def pre_transform(self, im):
         """Pre-transform input image before inference.
 
         Args:
             im (List(np.ndarray)): (N, 3, h, w) for tensor, [(h, w, 3) x N] for list.
 
         Return: A list of transformed imgs.
@@ -177,21 +182,21 @@
 
         return log_string
 
     def postprocess(self, preds, img, orig_imgs):
         """Post-processes predictions for an image and returns them."""
         return preds
 
-    def __call__(self, source=None, model=None, stream=False):
+    def __call__(self, source=None, model=None, stream=False, *args, **kwargs):
         """Performs inference on an image or stream."""
         self.stream = stream
         if stream:
-            return self.stream_inference(source, model)
+            return self.stream_inference(source, model, *args, **kwargs)
         else:
-            return list(self.stream_inference(source, model))  # merge list of Result into one
+            return list(self.stream_inference(source, model, *args, **kwargs))  # merge list of Result into one
 
     def predict_cli(self, source=None, model=None):
         """Method used for CLI prediction. It uses always generator as outputs as not required by CLI mode."""
         gen = self.stream_inference(source, model)
         for _ in gen:  # running CLI inference without accumulating any outputs (do not modify)
             pass
 
@@ -205,15 +210,15 @@
         if not getattr(self, 'stream', True) and (self.dataset.mode == 'stream' or  # streams
                                                   len(self.dataset) > 1000 or  # images
                                                   any(getattr(self.dataset, 'video_flag', [False]))):  # videos
             LOGGER.warning(STREAM_WARNING)
         self.vid_path, self.vid_writer = [None] * self.dataset.bs, [None] * self.dataset.bs
 
     @smart_inference_mode()
-    def stream_inference(self, source=None, model=None):
+    def stream_inference(self, source=None, model=None, *args, **kwargs):
         """Streams real-time inference on camera feed and saves results to file."""
         if self.args.verbose:
             LOGGER.info('')
 
         # Setup model
         if not self.model:
             self.setup_model(model)
@@ -232,24 +237,22 @@
 
         self.seen, self.windows, self.batch, profilers = 0, [], None, (ops.Profile(), ops.Profile(), ops.Profile())
         self.run_callbacks('on_predict_start')
         for batch in self.dataset:
             self.run_callbacks('on_predict_batch_start')
             self.batch = batch
             path, im0s, vid_cap, s = batch
-            visualize = increment_path(self.save_dir / Path(path[0]).stem,
-                                       mkdir=True) if self.args.visualize and (not self.source_type.tensor) else False
 
             # Preprocess
             with profilers[0]:
                 im = self.preprocess(im0s)
 
             # Inference
             with profilers[1]:
-                preds = self.model(im, augment=self.args.augment, visualize=visualize)
+                preds = self.inference(im, *args, **kwargs)
 
             # Postprocess
             with profilers[2]:
                 self.results = self.postprocess(preds, im, im0s)
             self.run_callbacks('on_predict_postprocess_end')
 
             # Visualize, save, write results
@@ -293,25 +296,24 @@
             s = f"\n{nl} label{'s' * (nl > 1)} saved to {self.save_dir / 'labels'}" if self.args.save_txt else ''
             LOGGER.info(f"Results saved to {colorstr('bold', self.save_dir)}{s}")
 
         self.run_callbacks('on_predict_end')
 
     def setup_model(self, model, verbose=True):
         """Initialize YOLO model with given parameters and set it to evaluation mode."""
-        device = select_device(self.args.device, verbose=verbose)
-        model = model or self.args.model
-        self.args.half &= device.type != 'cpu'  # half precision only supported on CUDA
-        self.model = AutoBackend(model,
-                                 device=device,
+        self.model = AutoBackend(model or self.args.model,
+                                 device=select_device(self.args.device, verbose=verbose),
                                  dnn=self.args.dnn,
                                  data=self.args.data,
                                  fp16=self.args.half,
                                  fuse=True,
                                  verbose=verbose)
-        self.device = device
+
+        self.device = self.model.device  # update device
+        self.args.half = self.model.fp16  # update half
         self.model.eval()
 
     def show(self, p):
         """Display an image in a window using OpenCV imshow()."""
         im0 = self.plotted_img
         if platform.system() == 'Linux' and p not in self.windows:
             self.windows.append(p)
```

## Comparing `ultralytics/yolo/engine/results.py` & `ultralytics/engine/results.py`

 * *Files 2% similar despite different names*

```diff
@@ -8,17 +8,17 @@
 from copy import deepcopy
 from functools import lru_cache
 from pathlib import Path
 
 import numpy as np
 import torch
 
-from ultralytics.yolo.data.augment import LetterBox
-from ultralytics.yolo.utils import LOGGER, SimpleClass, deprecation_warn, ops
-from ultralytics.yolo.utils.plotting import Annotator, colors, save_one_box
+from ultralytics.data.augment import LetterBox
+from ultralytics.utils import LOGGER, SimpleClass, deprecation_warn, ops
+from ultralytics.utils.plotting import Annotator, colors, save_one_box
 
 
 class BaseTensor(SimpleClass):
     """
     Base tensor class with additional methods for easy manipulation and device handling.
     """
 
@@ -110,14 +110,15 @@
         for k in self.keys:
             setattr(r, k, getattr(self, k)[idx])
         return r
 
     def update(self, boxes=None, masks=None, probs=None):
         """Update the boxes, masks, and probs attributes of the Results object."""
         if boxes is not None:
+            ops.clip_boxes(boxes, self.orig_shape)  # clip boxes
             self.boxes = Boxes(boxes, self.orig_shape)
         if masks is not None:
             self.masks = Masks(masks, self.orig_shape)
         if probs is not None:
             self.probs = probs
 
     def cpu(self):
@@ -166,15 +167,15 @@
             self,
             conf=True,
             line_width=None,
             font_size=None,
             font='Arial.ttf',
             pil=False,
             img=None,
-            img_gpu=None,
+            im_gpu=None,
             kpt_line=True,
             labels=True,
             boxes=True,
             masks=True,
             probs=True,
             **kwargs  # deprecated args TODO: remove support in 8.2
     ):
@@ -184,15 +185,15 @@
         Args:
             conf (bool): Whether to plot the detection confidence score.
             line_width (float, optional): The line width of the bounding boxes. If None, it is scaled to the image size.
             font_size (float, optional): The font size of the text. If None, it is scaled to the image size.
             font (str): The font to use for the text.
             pil (bool): Whether to return the image as a PIL Image.
             img (numpy.ndarray): Plot to another image. if not, plot to original image.
-            img_gpu (torch.Tensor): Normalized image in gpu with shape (1, 3, 640, 640), for faster mask plotting.
+            im_gpu (torch.Tensor): Normalized image in gpu with shape (1, 3, 640, 640), for faster mask plotting.
             kpt_line (bool): Whether to draw lines connecting keypoints.
             labels (bool): Whether to plot the label of bounding boxes.
             boxes (bool): Whether to plot the bounding boxes.
             masks (bool): Whether to plot the masks.
             probs (bool): Whether to plot classification probability
 
         Returns:
@@ -209,45 +210,51 @@
 
         if 'line_thickness' in kwargs:
             deprecation_warn('line_thickness', 'line_width')
             line_width = kwargs['line_thickness']
             assert type(line_width) == int, '`line_width` should be of int type, i.e, line_width=3'
 
         names = self.names
-        annotator = Annotator(deepcopy(self.orig_img if img is None else img),
-                              line_width,
-                              font_size,
-                              font,
-                              pil,
-                              example=names)
         pred_boxes, show_boxes = self.boxes, boxes
         pred_masks, show_masks = self.masks, masks
         pred_probs, show_probs = self.probs, probs
-        keypoints = self.keypoints
+        annotator = Annotator(
+            deepcopy(self.orig_img if img is None else img),
+            line_width,
+            font_size,
+            font,
+            pil or (pred_probs is not None and show_probs),  # Classify tasks default to pil=True
+            example=names)
+
+        # Plot Segment results
         if pred_masks and show_masks:
-            if img_gpu is None:
+            if im_gpu is None:
                 img = LetterBox(pred_masks.shape[1:])(image=annotator.result())
-                img_gpu = torch.as_tensor(img, dtype=torch.float16, device=pred_masks.data.device).permute(
+                im_gpu = torch.as_tensor(img, dtype=torch.float16, device=pred_masks.data.device).permute(
                     2, 0, 1).flip(0).contiguous() / 255
             idx = pred_boxes.cls if pred_boxes else range(len(pred_masks))
-            annotator.masks(pred_masks.data, colors=[colors(x, True) for x in idx], im_gpu=img_gpu)
+            annotator.masks(pred_masks.data, colors=[colors(x, True) for x in idx], im_gpu=im_gpu)
 
+        # Plot Detect results
         if pred_boxes and show_boxes:
             for d in reversed(pred_boxes):
                 c, conf, id = int(d.cls), float(d.conf) if conf else None, None if d.id is None else int(d.id.item())
                 name = ('' if id is None else f'id:{id} ') + names[c]
                 label = (f'{name} {conf:.2f}' if conf else name) if labels else None
                 annotator.box_label(d.xyxy.squeeze(), label, color=colors(c, True))
 
+        # Plot Classify results
         if pred_probs is not None and show_probs:
-            text = f"{', '.join(f'{names[j] if names else j} {pred_probs.data[j]:.2f}' for j in pred_probs.top5)}, "
-            annotator.text((32, 32), text, txt_color=(255, 255, 255))  # TODO: allow setting colors
-
-        if keypoints is not None:
-            for k in reversed(keypoints.data):
+            text = ',\n'.join(f'{names[j] if names else j} {pred_probs.data[j]:.2f}' for j in pred_probs.top5)
+            x = round(self.orig_shape[0] * 0.03)
+            annotator.text([x, x], text, txt_color=(255, 255, 255))  # TODO: allow setting colors
+
+        # Plot Pose results
+        if self.keypoints is not None:
+            for k in reversed(self.keypoints.data):
                 annotator.kpts(k, self.orig_shape, kpt_line=kpt_line)
 
         return annotator.result()
 
     def verbose(self):
         """
         Return log string for each task.
```

## Comparing `ultralytics/yolo/engine/trainer.py` & `ultralytics/engine/trainer.py`

 * *Files 1% similar despite different names*

```diff
@@ -17,25 +17,25 @@
 import torch
 from torch import distributed as dist
 from torch import nn, optim
 from torch.cuda import amp
 from torch.nn.parallel import DistributedDataParallel as DDP
 from tqdm import tqdm
 
+from ultralytics.cfg import get_cfg
+from ultralytics.data.utils import check_cls_dataset, check_det_dataset
 from ultralytics.nn.tasks import attempt_load_one_weight, attempt_load_weights
-from ultralytics.yolo.cfg import get_cfg
-from ultralytics.yolo.data.utils import check_cls_dataset, check_det_dataset
-from ultralytics.yolo.utils import (DEFAULT_CFG, LOGGER, RANK, SETTINGS, TQDM_BAR_FORMAT, __version__, callbacks,
-                                    clean_url, colorstr, emojis, yaml_save)
-from ultralytics.yolo.utils.autobatch import check_train_batch_size
-from ultralytics.yolo.utils.checks import check_amp, check_file, check_imgsz, print_args
-from ultralytics.yolo.utils.dist import ddp_cleanup, generate_ddp_command
-from ultralytics.yolo.utils.files import get_latest_run, increment_path
-from ultralytics.yolo.utils.torch_utils import (EarlyStopping, ModelEMA, de_parallel, init_seeds, one_cycle,
-                                                select_device, strip_optimizer)
+from ultralytics.utils import (DEFAULT_CFG, LOGGER, RANK, SETTINGS, TQDM_BAR_FORMAT, __version__, callbacks, clean_url,
+                               colorstr, emojis, yaml_save)
+from ultralytics.utils.autobatch import check_train_batch_size
+from ultralytics.utils.checks import check_amp, check_file, check_imgsz, print_args
+from ultralytics.utils.dist import ddp_cleanup, generate_ddp_command
+from ultralytics.utils.files import get_latest_run, increment_path
+from ultralytics.utils.torch_utils import (EarlyStopping, ModelEMA, de_parallel, init_seeds, one_cycle, select_device,
+                                           strip_optimizer)
 
 
 class BaseTrainer:
     """
     BaseTrainer
 
     A base class for creating trainers.
@@ -240,15 +240,15 @@
         self.train_loader = self.get_dataloader(self.trainset, batch_size=batch_size, rank=RANK, mode='train')
         if RANK in (-1, 0):
             self.test_loader = self.get_dataloader(self.testset, batch_size=batch_size * 2, rank=-1, mode='val')
             self.validator = self.get_validator()
             metric_keys = self.validator.metrics.keys + self.label_loss_items(prefix='val')
             self.metrics = dict(zip(metric_keys, [0] * len(metric_keys)))  # TODO: init metrics for plot_results()?
             self.ema = ModelEMA(self.model)
-            if self.args.plots and not self.args.v5loader:
+            if self.args.plots:
                 self.plot_training_labels()
 
         # Optimizer
         self.accumulate = max(round(self.args.nbs / self.batch_size), 1)  # accumulate loss before optimizing
         weight_decay = self.args.weight_decay * self.batch_size * self.accumulate / self.args.nbs  # scale weight_decay
         iterations = math.ceil(len(self.train_loader.dataset) / max(self.batch_size, self.args.nbs)) * self.epochs
         self.optimizer = self.build_optimizer(model=self.model,
```

## Comparing `ultralytics/yolo/engine/validator.py` & `ultralytics/engine/validator.py`

 * *Files 2% similar despite different names*

```diff
@@ -21,22 +21,22 @@
 import json
 import time
 from pathlib import Path
 
 import torch
 from tqdm import tqdm
 
+from ultralytics.cfg import get_cfg
+from ultralytics.data.utils import check_cls_dataset, check_det_dataset
 from ultralytics.nn.autobackend import AutoBackend
-from ultralytics.yolo.cfg import get_cfg
-from ultralytics.yolo.data.utils import check_cls_dataset, check_det_dataset
-from ultralytics.yolo.utils import DEFAULT_CFG, LOGGER, RANK, SETTINGS, TQDM_BAR_FORMAT, callbacks, colorstr, emojis
-from ultralytics.yolo.utils.checks import check_imgsz
-from ultralytics.yolo.utils.files import increment_path
-from ultralytics.yolo.utils.ops import Profile
-from ultralytics.yolo.utils.torch_utils import de_parallel, select_device, smart_inference_mode
+from ultralytics.utils import DEFAULT_CFG, LOGGER, RANK, SETTINGS, TQDM_BAR_FORMAT, callbacks, colorstr, emojis
+from ultralytics.utils.checks import check_imgsz
+from ultralytics.utils.files import increment_path
+from ultralytics.utils.ops import Profile
+from ultralytics.utils.torch_utils import de_parallel, select_device, smart_inference_mode
 
 
 class BaseValidator:
     """
     BaseValidator
 
     A base class for creating validators.
@@ -91,41 +91,44 @@
     @smart_inference_mode()
     def __call__(self, trainer=None, model=None):
         """
         Supports validation of a pre-trained model if passed or a model being trained
         if trainer is passed (trainer gets priority).
         """
         self.training = trainer is not None
+        augment = self.args.augment and (not self.training)
         if self.training:
             self.device = trainer.device
             self.data = trainer.data
             model = trainer.ema.ema or trainer.model
             self.args.half = self.device.type != 'cpu'  # force FP16 val during training
             model = model.half() if self.args.half else model.float()
             self.model = model
             self.loss = torch.zeros_like(trainer.loss_items, device=trainer.device)
             self.args.plots = trainer.stopper.possible_stop or (trainer.epoch == trainer.epochs - 1)
             model.eval()
         else:
             callbacks.add_integration_callbacks(self)
             self.run_callbacks('on_val_start')
             assert model is not None, 'Either trainer or model is needed for validation'
-            self.device = select_device(self.args.device, self.args.batch)
-            self.args.half &= self.device.type != 'cpu'
-            model = AutoBackend(model, device=self.device, dnn=self.args.dnn, data=self.args.data, fp16=self.args.half)
+            model = AutoBackend(model,
+                                device=select_device(self.args.device, self.args.batch),
+                                dnn=self.args.dnn,
+                                data=self.args.data,
+                                fp16=self.args.half)
             self.model = model
+            self.device = model.device  # update device
+            self.args.half = model.fp16  # update half
             stride, pt, jit, engine = model.stride, model.pt, model.jit, model.engine
             imgsz = check_imgsz(self.args.imgsz, stride=stride)
             if engine:
                 self.args.batch = model.batch_size
-            else:
-                self.device = model.device
-                if not pt and not jit:
-                    self.args.batch = 1  # export.py models default to batch-size 1
-                    LOGGER.info(f'Forcing batch=1 square inference (1,3,{imgsz},{imgsz}) for non-PyTorch models')
+            elif not pt and not jit:
+                self.args.batch = 1  # export.py models default to batch-size 1
+                LOGGER.info(f'Forcing batch=1 square inference (1,3,{imgsz},{imgsz}) for non-PyTorch models')
 
             if isinstance(self.args.data, str) and self.args.data.endswith('.yaml'):
                 self.data = check_det_dataset(self.args.data)
             elif self.args.task == 'classify':
                 self.data = check_cls_dataset(self.args.data, split=self.args.split)
             else:
                 raise FileNotFoundError(emojis(f"Dataset '{self.args.data}' for task={self.args.task} not found âŒ"))
@@ -153,15 +156,15 @@
             self.batch_i = batch_i
             # Preprocess
             with dt[0]:
                 batch = self.preprocess(batch)
 
             # Inference
             with dt[1]:
-                preds = model(batch['img'], augment=self.args.augment)
+                preds = model(batch['img'], augment=augment)
 
             # Loss
             with dt[2]:
                 if self.training:
                     self.loss += model.loss(batch, preds)[1]
 
             # Postprocess
```

## Comparing `ultralytics/yolo/fastsam/model.py` & `ultralytics/models/fastsam/model.py`

 * *Files 6% similar despite different names*

```diff
@@ -5,21 +5,21 @@
 Usage - Predict:
     from ultralytics import FastSAM
 
     model = FastSAM('last.pt')
     results = model.predict('ultralytics/assets/bus.jpg')
 """
 
-from ultralytics.yolo.cfg import get_cfg
-from ultralytics.yolo.engine.exporter import Exporter
-from ultralytics.yolo.engine.model import YOLO
-from ultralytics.yolo.utils import DEFAULT_CFG, LOGGER, ROOT, is_git_dir
-from ultralytics.yolo.utils.checks import check_imgsz
+from ultralytics.cfg import get_cfg
+from ultralytics.engine.exporter import Exporter
+from ultralytics.engine.model import YOLO
+from ultralytics.utils import DEFAULT_CFG, LOGGER, ROOT, is_git_dir
+from ultralytics.utils.checks import check_imgsz
+from ultralytics.utils.torch_utils import model_info, smart_inference_mode
 
-from ...yolo.utils.torch_utils import model_info, smart_inference_mode
 from .predict import FastSAMPredictor
 
 
 class FastSAM(YOLO):
 
     def __init__(self, model='FastSAM-x.pt'):
         """Call the __init__ method of the parent class (YOLO) with the updated default model"""
@@ -37,15 +37,15 @@
             source (str | int | PIL | np.ndarray): The source of the image to make predictions on.
                           Accepts all source types accepted by the YOLO model.
             stream (bool): Whether to stream the predictions or not. Defaults to False.
             **kwargs : Additional keyword arguments passed to the predictor.
                        Check the 'configuration' section in the documentation for all available options.
 
         Returns:
-            (List[ultralytics.yolo.engine.results.Results]): The prediction results.
+            (List[ultralytics.engine.results.Results]): The prediction results.
         """
         if source is None:
             source = ROOT / 'assets' if is_git_dir() else 'https://ultralytics.com/images/bus.jpg'
             LOGGER.warning(f"WARNING âš ï¸ 'source' is missing. Using 'source={source}'.")
         overrides = self.overrides.copy()
         overrides['conf'] = 0.25
         overrides.update(kwargs)  # prefer kwargs
```

## Comparing `ultralytics/yolo/fastsam/predict.py` & `ultralytics/models/fastsam/predict.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,15 +1,15 @@
 # Ultralytics YOLO ðŸš€, AGPL-3.0 license
 
 import torch
 
-from ultralytics.yolo.engine.results import Results
-from ultralytics.yolo.fastsam.utils import bbox_iou
-from ultralytics.yolo.utils import DEFAULT_CFG, ops
-from ultralytics.yolo.v8.detect.predict import DetectionPredictor
+from ultralytics.engine.results import Results
+from ultralytics.models.fastsam.utils import bbox_iou
+from ultralytics.models.yolo.detect.predict import DetectionPredictor
+from ultralytics.utils import DEFAULT_CFG, ops
 
 
 class FastSAMPredictor(DetectionPredictor):
 
     def __init__(self, cfg=DEFAULT_CFG, overrides=None, _callbacks=None):
         super().__init__(cfg, overrides, _callbacks)
         self.args.task = 'segment'
```

## Comparing `ultralytics/yolo/fastsam/prompt.py` & `ultralytics/models/fastsam/prompt.py`

 * *Files 0% similar despite different names*

```diff
@@ -18,15 +18,15 @@
         self.img_path = img_path
         self.ori_img = cv2.imread(img_path)
 
         # Import and assign clip
         try:
             import clip  # for linear_assignment
         except ImportError:
-            from ultralytics.yolo.utils.checks import check_requirements
+            from ultralytics.utils.checks import check_requirements
             check_requirements('git+https://github.com/openai/CLIP.git')  # required before installing lap from source
             import clip
         self.clip = clip
 
     @staticmethod
     def _segment_image(image, bbox):
         image_array = np.array(image)
```

## Comparing `ultralytics/yolo/fastsam/utils.py` & `ultralytics/models/fastsam/utils.py`

 * *Files 6% similar despite different names*

```diff
@@ -4,20 +4,20 @@
 
 
 def adjust_bboxes_to_image_border(boxes, image_shape, threshold=20):
     """
     Adjust bounding boxes to stick to image border if they are within a certain threshold.
 
     Args:
-        boxes: (n, 4)
-        image_shape: (height, width)
-        threshold: pixel threshold
+        boxes (torch.Tensor): (n, 4)
+        image_shape (tuple): (height, width)
+        threshold (int): pixel threshold
 
     Returns:
-        adjusted_boxes: adjusted bounding boxes
+        adjusted_boxes (torch.Tensor): adjusted bounding boxes
     """
 
     # Image dimensions
     h, w = image_shape
 
     # Adjust boxes
     boxes[boxes[:, 0] < threshold, 0] = 0  # x1
@@ -28,19 +28,19 @@
 
 
 def bbox_iou(box1, boxes, iou_thres=0.9, image_shape=(640, 640), raw_output=False):
     """
     Compute the Intersection-Over-Union of a bounding box with respect to an array of other bounding boxes.
 
     Args:
-        box1: (4, )
-        boxes: (n, 4)
+        box1 (torch.Tensor): (4, )
+        boxes (torch.Tensor): (n, 4)
 
     Returns:
-        high_iou_indices: Indices of boxes with IoU > thres
+        high_iou_indices (torch.Tensor): Indices of boxes with IoU > thres
     """
     boxes = adjust_bboxes_to_image_border(boxes, image_shape)
     # obtain coordinates for intersections
     x1 = torch.max(box1[0], boxes[:, 0])
     y1 = torch.max(box1[1], boxes[:, 1])
     x2 = torch.min(box1[2], boxes[:, 2])
     y2 = torch.min(box1[3], boxes[:, 3])
```

## Comparing `ultralytics/yolo/fastsam/val.py` & `ultralytics/models/fastsam/val.py`

 * *Files 2% similar despite different names*

```diff
@@ -3,19 +3,19 @@
 from multiprocessing.pool import ThreadPool
 from pathlib import Path
 
 import numpy as np
 import torch
 import torch.nn.functional as F
 
-from ultralytics.yolo.utils import LOGGER, NUM_THREADS, ops
-from ultralytics.yolo.utils.checks import check_requirements
-from ultralytics.yolo.utils.metrics import SegmentMetrics, box_iou, mask_iou
-from ultralytics.yolo.utils.plotting import output_to_target, plot_images
-from ultralytics.yolo.v8.detect import DetectionValidator
+from ultralytics.models.yolo.detect import DetectionValidator
+from ultralytics.utils import LOGGER, NUM_THREADS, ops
+from ultralytics.utils.checks import check_requirements
+from ultralytics.utils.metrics import SegmentMetrics, box_iou, mask_iou
+from ultralytics.utils.plotting import output_to_target, plot_images
 
 
 class FastSAMValidator(DetectionValidator):
 
     def __init__(self, dataloader=None, save_dir=None, pbar=None, args=None, _callbacks=None):
         """Initialize SegmentationValidator and set task to 'segment', metrics to SegmentMetrics."""
         super().__init__(dataloader, save_dir, pbar, args, _callbacks)
```

## Comparing `ultralytics/yolo/nas/model.py` & `ultralytics/models/nas/model.py`

 * *Files 2% similar despite different names*

```diff
@@ -9,20 +9,20 @@
     results = model.predict('ultralytics/assets/bus.jpg')
 """
 
 from pathlib import Path
 
 import torch
 
-from ultralytics.yolo.cfg import get_cfg
-from ultralytics.yolo.engine.exporter import Exporter
-from ultralytics.yolo.utils import DEFAULT_CFG, DEFAULT_CFG_DICT, LOGGER, ROOT, is_git_dir
-from ultralytics.yolo.utils.checks import check_imgsz
+from ultralytics.cfg import get_cfg
+from ultralytics.engine.exporter import Exporter
+from ultralytics.utils import DEFAULT_CFG, DEFAULT_CFG_DICT, LOGGER, ROOT, is_git_dir
+from ultralytics.utils.checks import check_imgsz
+from ultralytics.utils.torch_utils import model_info, smart_inference_mode
 
-from ...yolo.utils.torch_utils import model_info, smart_inference_mode
 from .predict import NASPredictor
 from .val import NASValidator
 
 
 class NAS:
 
     def __init__(self, model='yolo_nas_s.pt') -> None:
@@ -61,15 +61,15 @@
             source (str | int | PIL | np.ndarray): The source of the image to make predictions on.
                           Accepts all source types accepted by the YOLO model.
             stream (bool): Whether to stream the predictions or not. Defaults to False.
             **kwargs : Additional keyword arguments passed to the predictor.
                        Check the 'configuration' section in the documentation for all available options.
 
         Returns:
-            (List[ultralytics.yolo.engine.results.Results]): The prediction results.
+            (List[ultralytics.engine.results.Results]): The prediction results.
         """
         if source is None:
             source = ROOT / 'assets' if is_git_dir() else 'https://ultralytics.com/images/bus.jpg'
             LOGGER.warning(f"WARNING âš ï¸ 'source' is missing. Using 'source={source}'.")
         overrides = dict(conf=0.25, task='detect', mode='predict')
         overrides.update(kwargs)  # prefer kwargs
         if not self.predictor:
```

## Comparing `ultralytics/yolo/nas/predict.py` & `ultralytics/models/nas/predict.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,15 +1,15 @@
 # Ultralytics YOLO ðŸš€, AGPL-3.0 license
 
 import torch
 
-from ultralytics.yolo.engine.predictor import BasePredictor
-from ultralytics.yolo.engine.results import Results
-from ultralytics.yolo.utils import ops
-from ultralytics.yolo.utils.ops import xyxy2xywh
+from ultralytics.engine.predictor import BasePredictor
+from ultralytics.engine.results import Results
+from ultralytics.utils import ops
+from ultralytics.utils.ops import xyxy2xywh
 
 
 class NASPredictor(BasePredictor):
 
     def postprocess(self, preds_in, img, orig_imgs):
         """Postprocesses predictions and returns a list of Results objects."""
```

## Comparing `ultralytics/yolo/nas/val.py` & `ultralytics/models/nas/val.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,14 +1,14 @@
 # Ultralytics YOLO ðŸš€, AGPL-3.0 license
 
 import torch
 
-from ultralytics.yolo.utils import ops
-from ultralytics.yolo.utils.ops import xyxy2xywh
-from ultralytics.yolo.v8.detect import DetectionValidator
+from ultralytics.models.yolo.detect import DetectionValidator
+from ultralytics.utils import ops
+from ultralytics.utils.ops import xyxy2xywh
 
 __all__ = ['NASValidator']
 
 
 class NASValidator(DetectionValidator):
 
     def postprocess(self, preds_in):
```

## Comparing `ultralytics/yolo/utils/autobatch.py` & `ultralytics/utils/autobatch.py`

 * *Files 2% similar despite different names*

```diff
@@ -4,16 +4,16 @@
 """
 
 from copy import deepcopy
 
 import numpy as np
 import torch
 
-from ultralytics.yolo.utils import DEFAULT_CFG, LOGGER, colorstr
-from ultralytics.yolo.utils.torch_utils import profile
+from ultralytics.utils import DEFAULT_CFG, LOGGER, colorstr
+from ultralytics.utils.torch_utils import profile
 
 
 def check_train_batch_size(model, imgsz=640, amp=True):
     """
     Check YOLO training batch size using the autobatch() function.
 
     Args:
```

## Comparing `ultralytics/yolo/utils/benchmarks.py` & `ultralytics/utils/benchmarks.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,15 +1,15 @@
 # Ultralytics YOLO ðŸš€, AGPL-3.0 license
 """
 Benchmark a YOLO model formats for speed and accuracy
 
 Usage:
-    from ultralytics.yolo.utils.benchmarks import ProfileModels, benchmark
+    from ultralytics.utils.benchmarks import ProfileModels, benchmark
     ProfileModels(['yolov8n.yaml', 'yolov8s.yaml']).profile()
-    run_benchmarks(model='yolov8n.pt', imgsz=160)
+    benchmark(model='yolov8n.pt', imgsz=160)
 
 Format                  | `format=argument`         | Model
 ---                     | ---                       | ---
 PyTorch                 | -                         | yolov8n.pt
 TorchScript             | `torchscript`             | yolov8n.torchscript
 ONNX                    | `onnx`                    | yolov8n.onnx
 OpenVINO                | `openvino`                | yolov8n_openvino_model/
@@ -22,48 +22,51 @@
 TensorFlow.js           | `tfjs`                    | yolov8n_web_model/
 PaddlePaddle            | `paddle`                  | yolov8n_paddle_model/
 ncnn                    | `ncnn`                    | yolov8n_ncnn_model/
 """
 
 import glob
 import platform
+import sys
 import time
 from pathlib import Path
 
 import numpy as np
 import torch.cuda
 from tqdm import tqdm
 
 from ultralytics import YOLO
-from ultralytics.yolo.cfg import TASK2DATA, TASK2METRIC
-from ultralytics.yolo.engine.exporter import export_formats
-from ultralytics.yolo.utils import LINUX, LOGGER, MACOS, ROOT, SETTINGS
-from ultralytics.yolo.utils.checks import check_requirements, check_yolo
-from ultralytics.yolo.utils.downloads import download
-from ultralytics.yolo.utils.files import file_size
-from ultralytics.yolo.utils.torch_utils import select_device
+from ultralytics.cfg import TASK2DATA, TASK2METRIC
+from ultralytics.engine.exporter import export_formats
+from ultralytics.utils import LINUX, LOGGER, MACOS, ROOT, SETTINGS
+from ultralytics.utils.checks import check_requirements, check_yolo
+from ultralytics.utils.downloads import download
+from ultralytics.utils.files import file_size
+from ultralytics.utils.torch_utils import select_device
 
 
 def benchmark(model=Path(SETTINGS['weights_dir']) / 'yolov8n.pt',
+              data=None,
               imgsz=160,
               half=False,
               int8=False,
               device='cpu',
-              hard_fail=False):
+              verbose=False):
     """
     Benchmark a YOLO model across different formats for speed and accuracy.
 
     Args:
         model (str | Path | optional): Path to the model file or directory. Default is
             Path(SETTINGS['weights_dir']) / 'yolov8n.pt'.
+        data (str, optional): Dataset to evaluate on, inherited from TASK2DATA if not passed. Default is None.
         imgsz (int, optional): Image size for the benchmark. Default is 160.
         half (bool, optional): Use half-precision for the model if True. Default is False.
         int8 (bool, optional): Use int8-precision for the model if True. Default is False.
         device (str, optional): Device to run the benchmark on, either 'cpu' or 'cuda'. Default is 'cpu'.
-        hard_fail (bool | float | optional): If True or a float, assert benchmarks pass with given metric.
+        verbose (bool | float | optional): If True or a float, assert benchmarks pass with given metric.
             Default is False.
 
     Returns:
         df (pandas.DataFrame): A pandas DataFrame with benchmark results for each format, including file size,
             metric, and inference time.
     """
 
@@ -78,14 +81,16 @@
     t0 = time.time()
     for i, (name, format, suffix, cpu, gpu) in export_formats().iterrows():  # index, (name, format, suffix, CPU, GPU)
         emoji, filename = 'âŒ', None  # export defaults
         try:
             assert i != 9 or LINUX, 'Edge TPU export only supported on Linux'
             if i == 10:
                 assert MACOS or LINUX, 'TF.js export only supported on macOS and Linux'
+            elif i == 11:
+                assert sys.version_info < (3, 11), 'PaddlePaddle export only supported on Python<=3.10'
             if 'cpu' in device.type:
                 assert cpu, 'inference not supported on CPU'
             if 'cuda' in device.type:
                 assert gpu, 'inference not supported on GPU'
 
             # Export
             if format == '-':
@@ -102,46 +107,46 @@
             assert i not in (9, 10), 'inference not supported'  # Edge TPU and TF.js are unsupported
             assert i != 5 or platform.system() == 'Darwin', 'inference only supported on macOS>=10.13'  # CoreML
             if not (ROOT / 'assets/bus.jpg').exists():
                 download(url='https://ultralytics.com/images/bus.jpg', dir=ROOT / 'assets')
             export.predict(ROOT / 'assets/bus.jpg', imgsz=imgsz, device=device, half=half)
 
             # Validate
-            data = TASK2DATA[model.task]  # task to dataset, i.e. coco8.yaml for task=detect
+            data = data or TASK2DATA[model.task]  # task to dataset, i.e. coco8.yaml for task=detect
             key = TASK2METRIC[model.task]  # task to metric, i.e. metrics/mAP50-95(B) for task=detect
             results = export.val(data=data,
                                  batch=1,
                                  imgsz=imgsz,
                                  plots=False,
                                  device=device,
                                  half=half,
                                  int8=int8,
                                  verbose=False)
             metric, speed = results.results_dict[key], results.speed['inference']
             y.append([name, 'âœ…', round(file_size(filename), 1), round(metric, 4), round(speed, 2)])
         except Exception as e:
-            if hard_fail:
-                assert type(e) is AssertionError, f'Benchmark hard_fail for {name}: {e}'
+            if verbose:
+                assert type(e) is AssertionError, f'Benchmark failure for {name}: {e}'
             LOGGER.warning(f'ERROR âŒï¸ Benchmark failure for {name}: {e}')
             y.append([name, emoji, round(file_size(filename), 1), None, None])  # mAP, t_inference
 
     # Print results
     check_yolo(device=device)  # print system info
     df = pd.DataFrame(y, columns=['Format', 'Statusâ”', 'Size (MB)', key, 'Inference time (ms/im)'])
 
     name = Path(model.ckpt_path).name
     s = f'\nBenchmarks complete for {name} on {data} at imgsz={imgsz} ({time.time() - t0:.2f}s)\n{df}\n'
     LOGGER.info(s)
     with open('benchmarks.log', 'a', errors='ignore', encoding='utf-8') as f:
         f.write(s)
 
-    if hard_fail and isinstance(hard_fail, float):
+    if verbose and isinstance(verbose, float):
         metrics = df[key].array  # values to compare to floor
-        floor = hard_fail  # minimum metric floor to pass, i.e. = 0.29 mAP for YOLOv5n
-        assert all(x > floor for x in metrics if pd.notna(x)), f'HARD FAIL: one or more metric(s) < floor {floor}'
+        floor = verbose  # minimum metric floor to pass, i.e. = 0.29 mAP for YOLOv5n
+        assert all(x > floor for x in metrics if pd.notna(x)), f'Benchmark failure: metric(s) < floor {floor}'
 
     return df
 
 
 class ProfileModels:
     """
     ProfileModels class for profiling different models on ONNX and TensorRT.
```

## Comparing `ultralytics/yolo/utils/dist.py` & `ultralytics/utils/dist.py`

 * *Files 1% similar despite different names*

```diff
@@ -25,15 +25,15 @@
 
 def generate_ddp_file(trainer):
     """Generates a DDP file and returns its file name."""
     module, name = f'{trainer.__class__.__module__}.{trainer.__class__.__name__}'.rsplit('.', 1)
 
     content = f'''overrides = {vars(trainer.args)} \nif __name__ == "__main__":
     from {module} import {name}
-    from ultralytics.yolo.utils import DEFAULT_CFG_DICT
+    from ultralytics.utils import DEFAULT_CFG_DICT
 
     cfg = DEFAULT_CFG_DICT.copy()
     cfg.update(save_dir='')   # handle the extra key 'save_dir'
     trainer = {name}(cfg=cfg, overrides=overrides)
     trainer.train()'''
     (USER_CONFIG_DIR / 'DDP').mkdir(exist_ok=True)
     with tempfile.NamedTemporaryFile(prefix='_temp_',
```

## Comparing `ultralytics/yolo/utils/downloads.py` & `ultralytics/utils/downloads.py`

 * *Files 2% similar despite different names*

```diff
@@ -9,23 +9,24 @@
 from urllib import parse, request
 from zipfile import BadZipFile, ZipFile, is_zipfile
 
 import requests
 import torch
 from tqdm import tqdm
 
-from ultralytics.yolo.utils import LOGGER, checks, clean_url, emojis, is_online, url2file
+from ultralytics.utils import LOGGER, checks, clean_url, emojis, is_online, url2file
 
 GITHUB_ASSET_NAMES = [f'yolov8{k}{suffix}.pt' for k in 'nsmlx' for suffix in ('', '6', '-cls', '-seg', '-pose')] + \
                      [f'yolov5{k}u.pt' for k in 'nsmlx'] + \
                      [f'yolov3{k}u.pt' for k in ('', '-spp', '-tiny')] + \
                      [f'yolo_nas_{k}.pt' for k in 'sml'] + \
                      [f'sam_{k}.pt' for k in 'bl'] + \
                      [f'FastSAM-{k}.pt' for k in 'sx'] + \
-                     [f'rtdetr-{k}.pt' for k in 'lx']
+                     [f'rtdetr-{k}.pt' for k in 'lx'] + \
+                     ['mobile_sam.pt']
 GITHUB_ASSET_STEMS = [Path(k).stem for k in GITHUB_ASSET_NAMES]
 
 
 def is_url(url, check=True):
     """Check if string is URL and check if URL exists."""
     with contextlib.suppress(Exception):
         url = str(url)
@@ -144,30 +145,30 @@
     """
     f = dir / url2file(url) if dir else Path(file)  # URL converted to filename
     if '://' not in str(url) and Path(url).is_file():  # URL exists ('://' check required in Windows Python<3.10)
         f = Path(url)  # filename
     elif not f.is_file():  # URL and file do not exist
         assert dir or file, 'dir or file required for download'
         f = dir / url2file(url) if dir else Path(file)
-        desc = f'Downloading {clean_url(url)} to {f}'
+        desc = f"Downloading {clean_url(url)} to '{f}'"
         LOGGER.info(f'{desc}...')
         f.parent.mkdir(parents=True, exist_ok=True)  # make directory if missing
         check_disk_space(url)
         for i in range(retry + 1):
             try:
                 if curl or i > 0:  # curl download with retry, continue
                     s = 'sS' * (not progress)  # silent
                     r = subprocess.run(['curl', '-#', f'-{s}L', url, '-o', f, '--retry', '3', '-C', '-']).returncode
                     assert r == 0, f'Curl return value {r}'
                 else:  # urllib download
                     method = 'torch'
                     if method == 'torch':
                         torch.hub.download_url_to_file(url, f, progress=progress)
                     else:
-                        from ultralytics.yolo.utils import TQDM_BAR_FORMAT
+                        from ultralytics.utils import TQDM_BAR_FORMAT
                         with request.urlopen(url) as response, tqdm(total=int(response.getheader('Content-Length', 0)),
                                                                     desc=desc,
                                                                     disable=not progress,
                                                                     unit='B',
                                                                     unit_scale=True,
                                                                     unit_divisor=1024,
                                                                     bar_format=TQDM_BAR_FORMAT) as pbar:
@@ -207,15 +208,15 @@
         version = f'tags/{version}'  # i.e. tags/v6.2
     response = requests.get(f'https://api.github.com/repos/{repo}/releases/{version}').json()  # github api
     return response['tag_name'], [x['name'] for x in response['assets']]  # tag, assets
 
 
 def attempt_download_asset(file, repo='ultralytics/assets', release='v0.0.0'):
     """Attempt file download from GitHub release assets if not found locally. release = 'latest', 'v6.2', etc."""
-    from ultralytics.yolo.utils import SETTINGS  # scoped for circular import
+    from ultralytics.utils import SETTINGS  # scoped for circular import
 
     # YOLOv3/5u updates
     file = str(file)
     file = checks.check_yolov5u_filename(file)
     file = Path(file.strip().replace("'", ''))
     if file.exists():
         return str(file)
```

## Comparing `ultralytics/yolo/utils/instance.py` & `ultralytics/utils/instance.py`

 * *Files 0% similar despite different names*

```diff
@@ -16,14 +16,15 @@
     def parse(x):
         """Parse bounding boxes format between XYWH and LTWH."""
         return x if isinstance(x, abc.Iterable) else tuple(repeat(x, n))
 
     return parse
 
 
+to_2tuple = _ntuple(2)
 to_4tuple = _ntuple(4)
 
 # `xyxy` means left top and right bottom
 # `xywh` means center x, center y and width, height(yolo format)
 # `ltwh` means left top and width, height(coco format)
 _formats = ['xyxy', 'xywh', 'ltwh']
```

## Comparing `ultralytics/yolo/utils/loss.py` & `ultralytics/utils/loss.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,16 +1,16 @@
 # Ultralytics YOLO ðŸš€, AGPL-3.0 license
 
 import torch
 import torch.nn as nn
 import torch.nn.functional as F
 
-from ultralytics.yolo.utils.metrics import OKS_SIGMA
-from ultralytics.yolo.utils.ops import crop_mask, xywh2xyxy, xyxy2xywh
-from ultralytics.yolo.utils.tal import TaskAlignedAssigner, dist2bbox, make_anchors
+from ultralytics.utils.metrics import OKS_SIGMA
+from ultralytics.utils.ops import crop_mask, xywh2xyxy, xyxy2xywh
+from ultralytics.utils.tal import TaskAlignedAssigner, dist2bbox, make_anchors
 
 from .metrics import bbox_iou
 from .tal import bbox2dist
 
 
 class VarifocalLoss(nn.Module):
     """Varifocal loss by Zhang et al. https://arxiv.org/abs/2008.13367."""
```

## Comparing `ultralytics/yolo/utils/metrics.py` & `ultralytics/utils/metrics.py`

 * *Files 0% similar despite different names*

```diff
@@ -6,15 +6,15 @@
 import warnings
 from pathlib import Path
 
 import matplotlib.pyplot as plt
 import numpy as np
 import torch
 
-from ultralytics.yolo.utils import LOGGER, SimpleClass, TryExcept, plt_settings
+from ultralytics.utils import LOGGER, SimpleClass, TryExcept, plt_settings
 
 OKS_SIGMA = np.array([.26, .25, .25, .35, .35, .79, .79, .72, .72, .62, .62, 1.07, 1.07, .87, .87, .89, .89]) / 10.0
 
 
 # Boxes
 def box_area(box):
     """Return box area, where box shape is xyxy(4,n)."""
@@ -959,16 +959,16 @@
         pred, targets = torch.cat(pred), torch.cat(targets)
         correct = (targets[:, None] == pred).float()
         acc = torch.stack((correct[:, 0], correct.max(1).values), dim=1)  # (top1, top5) accuracy
         self.top1, self.top5 = acc.mean(0).tolist()
 
     @property
     def fitness(self):
-        """Returns top-5 accuracy as fitness score."""
-        return self.top5
+        """Returns mean of top-1 and top-5 accuracies as fitness score."""
+        return (self.top1 + self.top5) / 2
 
     @property
     def results_dict(self):
         """Returns a dictionary with model's performance metrics and fitness score."""
         return dict(zip(self.keys + ['fitness'], [self.top1, self.top5, self.fitness]))
 
     @property
```

## Comparing `ultralytics/yolo/utils/ops.py` & `ultralytics/utils/ops.py`

 * *Files 3% similar despite different names*

```diff
@@ -7,15 +7,15 @@
 
 import cv2
 import numpy as np
 import torch
 import torch.nn.functional as F
 import torchvision
 
-from ultralytics.yolo.utils import LOGGER
+from ultralytics.utils import LOGGER
 
 from .metrics import box_iou
 
 
 class Profile(contextlib.ContextDecorator):
     """
     YOLOv8 Profile class.
@@ -51,20 +51,25 @@
         Get current time.
         """
         if self.cuda:
             torch.cuda.synchronize()
         return time.time()
 
 
-def coco80_to_coco91_class():  # converts 80-index (val2014) to 91-index (paper)
-    # https://tech.amikelive.com/node-718/what-object-categories-labels-are-in-coco-dataset/
-    # a = np.loadtxt('data/coco.names', dtype='str', delimiter='\n')
-    # b = np.loadtxt('data/coco_paper.names', dtype='str', delimiter='\n')
-    # x1 = [list(a[i] == b).index(True) + 1 for i in range(80)]  # darknet to coco
-    # x2 = [list(b[i] == a).index(True) if any(b[i] == a) else None for i in range(91)]  # coco to darknet
+def coco80_to_coco91_class():  #
+    """
+    Converts 80-index (val2014) to 91-index (paper).
+    For details see https://tech.amikelive.com/node-718/what-object-categories-labels-are-in-coco-dataset/.
+
+    Example:
+        a = np.loadtxt('data/coco.names', dtype='str', delimiter='\n')
+        b = np.loadtxt('data/coco_paper.names', dtype='str', delimiter='\n')
+        x1 = [list(a[i] == b).index(True) + 1 for i in range(80)]  # darknet to coco
+        x2 = [list(b[i] == a).index(True) if any(b[i] == a) else None for i in range(91)]  # coco to darknet
+    """
     return [
         1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 27, 28, 31, 32, 33, 34,
         35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63,
         64, 65, 67, 70, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 84, 85, 86, 87, 88, 89, 90]
 
 
 def segment2box(segment, width=640, height=640):
@@ -83,39 +88,42 @@
     x, y = segment.T  # segment xy
     inside = (x >= 0) & (y >= 0) & (x <= width) & (y <= height)
     x, y, = x[inside], y[inside]
     return np.array([x.min(), y.min(), x.max(), y.max()], dtype=segment.dtype) if any(x) else np.zeros(
         4, dtype=segment.dtype)  # xyxy
 
 
-def scale_boxes(img1_shape, boxes, img0_shape, ratio_pad=None):
+def scale_boxes(img1_shape, boxes, img0_shape, ratio_pad=None, padding=True):
     """
     Rescales bounding boxes (in the format of xyxy) from the shape of the image they were originally specified in
     (img1_shape) to the shape of a different image (img0_shape).
 
     Args:
       img1_shape (tuple): The shape of the image that the bounding boxes are for, in the format of (height, width).
       boxes (torch.Tensor): the bounding boxes of the objects in the image, in the format of (x1, y1, x2, y2)
       img0_shape (tuple): the shape of the target image, in the format of (height, width).
       ratio_pad (tuple): a tuple of (ratio, pad) for scaling the boxes. If not provided, the ratio and pad will be
                          calculated based on the size difference between the two images.
+      padding (bool): If True, assuming the boxes is based on image augmented by yolo style. If False then do regular
+        rescaling.
 
     Returns:
       boxes (torch.Tensor): The scaled bounding boxes, in the format of (x1, y1, x2, y2)
     """
     if ratio_pad is None:  # calculate from img0_shape
         gain = min(img1_shape[0] / img0_shape[0], img1_shape[1] / img0_shape[1])  # gain  = old / new
         pad = round((img1_shape[1] - img0_shape[1] * gain) / 2 - 0.1), round(
             (img1_shape[0] - img0_shape[0] * gain) / 2 - 0.1)  # wh padding
     else:
         gain = ratio_pad[0][0]
         pad = ratio_pad[1]
 
-    boxes[..., [0, 2]] -= pad[0]  # x padding
-    boxes[..., [1, 3]] -= pad[1]  # y padding
+    if padding:
+        boxes[..., [0, 2]] -= pad[0]  # x padding
+        boxes[..., [1, 3]] -= pad[1]  # y padding
     boxes[..., :4] /= gain
     clip_boxes(boxes, img0_shape)
     return boxes
 
 
 def make_divisible(x, divisor):
     """
@@ -543,15 +551,15 @@
 
 
 def crop_mask(masks, boxes):
     """
     It takes a mask and a bounding box, and returns a mask that is cropped to the bounding box
 
     Args:
-      masks (torch.Tensor): [h, w, n] tensor of masks
+      masks (torch.Tensor): [n, h, w] tensor of masks
       boxes (torch.Tensor): [n, 4] tensor of bbox coordinates in relative point form
 
     Returns:
       (torch.Tensor): The masks are being cropped to the bounding box.
     """
     n, h, w = masks.shape
     x1, y1, x2, y2 = torch.chunk(boxes[:, :, None], 4, 1)  # x1 shape(n,1,1)
@@ -625,48 +633,69 @@
       shape (tuple): the size of the input image (h,w)
 
     Returns:
       masks (torch.Tensor): The returned masks with dimensions [h, w, n]
     """
     c, mh, mw = protos.shape  # CHW
     masks = (masks_in @ protos.float().view(c, -1)).sigmoid().view(-1, mh, mw)
-    gain = min(mh / shape[0], mw / shape[1])  # gain  = old / new
-    pad = (mw - shape[1] * gain) / 2, (mh - shape[0] * gain) / 2  # wh padding
-    top, left = int(pad[1]), int(pad[0])  # y, x
-    bottom, right = int(mh - pad[1]), int(mw - pad[0])
-    masks = masks[:, top:bottom, left:right]
-
-    masks = F.interpolate(masks[None], shape, mode='bilinear', align_corners=False)[0]  # CHW
+    masks = scale_masks(masks[None], shape)[0]  # CHW
     masks = crop_mask(masks, bboxes)  # CHW
     return masks.gt_(0.5)
 
 
-def scale_coords(img1_shape, coords, img0_shape, ratio_pad=None, normalize=False):
+def scale_masks(masks, shape, padding=True):
+    """
+    Rescale segment masks to shape.
+
+    Args:
+        masks (torch.Tensor): (N, C, H, W).
+        shape (tuple): Height and width.
+        padding (bool): If True, assuming the boxes is based on image augmented by yolo style. If False then do regular
+            rescaling.
+    """
+    mh, mw = masks.shape[2:]
+    gain = min(mh / shape[0], mw / shape[1])  # gain  = old / new
+    pad = [mw - shape[1] * gain, mh - shape[0] * gain]  # wh padding
+    if padding:
+        pad[0] /= 2
+        pad[1] /= 2
+    top, left = (int(pad[1]), int(pad[0])) if padding else (0, 0)  # y, x
+    bottom, right = (int(mh - pad[1]), int(mw - pad[0]))
+    masks = masks[..., top:bottom, left:right]
+
+    masks = F.interpolate(masks, shape, mode='bilinear', align_corners=False)  # NCHW
+    return masks
+
+
+def scale_coords(img1_shape, coords, img0_shape, ratio_pad=None, normalize=False, padding=True):
     """
     Rescale segment coordinates (xyxy) from img1_shape to img0_shape
 
     Args:
       img1_shape (tuple): The shape of the image that the coords are from.
       coords (torch.Tensor): the coords to be scaled
       img0_shape (tuple): the shape of the image that the segmentation is being applied to
       ratio_pad (tuple): the ratio of the image size to the padded image size.
       normalize (bool): If True, the coordinates will be normalized to the range [0, 1]. Defaults to False
+      padding (bool): If True, assuming the boxes is based on image augmented by yolo style. If False then do regular
+        rescaling.
 
     Returns:
       coords (torch.Tensor): the segmented image.
     """
     if ratio_pad is None:  # calculate from img0_shape
         gain = min(img1_shape[0] / img0_shape[0], img1_shape[1] / img0_shape[1])  # gain  = old / new
         pad = (img1_shape[1] - img0_shape[1] * gain) / 2, (img1_shape[0] - img0_shape[0] * gain) / 2  # wh padding
     else:
         gain = ratio_pad[0][0]
         pad = ratio_pad[1]
 
-    coords[..., 0] -= pad[0]  # x padding
-    coords[..., 1] -= pad[1]  # y padding
+    if padding:
+        coords[..., 0] -= pad[0]  # x padding
+        coords[..., 1] -= pad[1]  # y padding
     coords[..., 0] /= gain
     coords[..., 1] /= gain
     clip_coords(coords, img0_shape)
     if normalize:
         coords[..., 0] /= img0_shape[1]  # width
         coords[..., 1] /= img0_shape[0]  # height
     return coords
```

## Comparing `ultralytics/yolo/utils/patches.py` & `ultralytics/utils/patches.py`

 * *Files 2% similar despite different names*

```diff
@@ -30,15 +30,15 @@
 
 
 # PyTorch functions ----------------------------------------------------------------------------------------------------
 _torch_save = torch.save  # copy to avoid recursion errors
 
 
 def torch_save(*args, **kwargs):
-    # Use dill (if exists) to serialize the lambda functions where pickle does not do this
+    """Use dill (if exists) to serialize the lambda functions where pickle does not do this."""
     try:
         import dill as pickle
     except ImportError:
         import pickle
 
     if 'pickle_module' not in kwargs:
         kwargs['pickle_module'] = pickle
```

## Comparing `ultralytics/yolo/utils/plotting.py` & `ultralytics/utils/plotting.py`

 * *Files 2% similar despite different names*

```diff
@@ -9,23 +9,24 @@
 import matplotlib.pyplot as plt
 import numpy as np
 import torch
 from PIL import Image, ImageDraw, ImageFont
 from PIL import __version__ as pil_version
 from scipy.ndimage import gaussian_filter1d
 
-from ultralytics.yolo.utils import LOGGER, TryExcept, plt_settings, threaded
+from ultralytics.utils import LOGGER, TryExcept, plt_settings, threaded
 
 from .checks import check_font, check_version, is_ascii
 from .files import increment_path
 from .ops import clip_boxes, scale_image, xywh2xyxy, xyxy2xywh
 
 
 class Colors:
-    # Ultralytics color palette https://ultralytics.com/
+    """Ultralytics color palette https://ultralytics.com/."""
+
     def __init__(self):
         """Initialize colors as hex = matplotlib.colors.TABLEAU_COLORS.values()."""
         hexs = ('FF3838', 'FF9D97', 'FF701F', 'FFB21D', 'CFD231', '48F90A', '92CC17', '3DDB86', '1A9334', '00D4BB',
                 '2C99A8', '00C2FF', '344593', '6473FF', '0018EC', '8438FF', '520085', 'CB38FF', 'FF95C8', 'FF37C7')
         self.palette = [self.hex2rgb(f'#{c}') for c in hexs]
         self.n = len(self.palette)
         self.pose_palette = np.array([[255, 128, 0], [255, 153, 51], [255, 178, 102], [230, 230, 0], [255, 153, 255],
@@ -44,15 +45,16 @@
         return tuple(int(h[1 + i:1 + i + 2], 16) for i in (0, 2, 4))
 
 
 colors = Colors()  # create instance for 'from utils.plots import colors'
 
 
 class Annotator:
-    # YOLOv8 Annotator for train/val mosaics and jpgs and detect/hub inference annotations
+    """YOLOv8 Annotator for train/val mosaics and jpgs and detect/hub inference annotations."""
+
     def __init__(self, im, line_width=None, font_size=None, font='Arial.ttf', pil=False, example='abc'):
         """Initialize the Annotator class with image and line width along with color palette for keypoints and limbs."""
         assert im.data.contiguous, 'Image not contiguous. Apply np.ascontiguousarray(im) to Annotator() input images.'
         non_ascii = not is_ascii(example)  # non-latin labels, i.e. asian, arabic, cyrillic
         self.pil = pil or non_ascii
         if self.pil:  # use PIL
             self.im = im if isinstance(im, Image.Image) else Image.fromarray(im)
@@ -200,15 +202,22 @@
             xy[1] += 1 - h
         if self.pil:
             if box_style:
                 w, h = self.font.getsize(text)
                 self.draw.rectangle((xy[0], xy[1], xy[0] + w + 1, xy[1] + h + 1), fill=txt_color)
                 # Using `txt_color` for background and draw fg with white color
                 txt_color = (255, 255, 255)
-            self.draw.text(xy, text, fill=txt_color, font=self.font)
+            if '\n' in text:
+                lines = text.split('\n')
+                _, h = self.font.getsize(text)
+                for line in lines:
+                    self.draw.text(xy, line, fill=txt_color, font=self.font)
+                    xy[1] += h
+            else:
+                self.draw.text(xy, text, fill=txt_color, font=self.font)
         else:
             if box_style:
                 tf = max(self.lw - 1, 1)  # font thickness
                 w, h = cv2.getTextSize(text, 0, fontScale=self.lw / 3, thickness=tf)[0]  # text width, height
                 outside = xy[1] - h >= 3
                 p2 = xy[0] + w, xy[1] - h - 3 if outside else xy[1] + h + 3
                 cv2.rectangle(self.im, xy, p2, txt_color, -1, cv2.LINE_AA)  # filled
@@ -306,15 +315,15 @@
                 bboxes=np.zeros(0, dtype=np.float32),
                 masks=np.zeros(0, dtype=np.uint8),
                 kpts=np.zeros((0, 51), dtype=np.float32),
                 paths=None,
                 fname='images.jpg',
                 names=None,
                 on_plot=None):
-    # Plot image grid with labels
+    """Plot image grid with labels."""
     if isinstance(images, torch.Tensor):
         images = images.cpu().float().numpy()
     if isinstance(cls, torch.Tensor):
         cls = cls.cpu().numpy()
     if isinstance(bboxes, torch.Tensor):
         bboxes = bboxes.cpu().numpy()
     if isinstance(masks, torch.Tensor):
```

## Comparing `ultralytics/yolo/utils/tal.py` & `ultralytics/utils/tal.py`

 * *Files identical despite different names*

## Comparing `ultralytics/yolo/utils/torch_utils.py` & `ultralytics/utils/torch_utils.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,9 +1,8 @@
 # Ultralytics YOLO ðŸš€, AGPL-3.0 license
-
 import math
 import os
 import platform
 import random
 import time
 from contextlib import contextmanager
 from copy import deepcopy
@@ -13,27 +12,27 @@
 import numpy as np
 import torch
 import torch.distributed as dist
 import torch.nn as nn
 import torch.nn.functional as F
 import torchvision
 
-from ultralytics.yolo.utils import DEFAULT_CFG_DICT, DEFAULT_CFG_KEYS, LOGGER, RANK, __version__
-from ultralytics.yolo.utils.checks import check_version
+from ultralytics.utils import DEFAULT_CFG_DICT, DEFAULT_CFG_KEYS, LOGGER, RANK, __version__
+from ultralytics.utils.checks import check_version
 
 try:
     import thop
 except ImportError:
     thop = None
 
 TORCHVISION_0_10 = check_version(torchvision.__version__, '0.10.0')
 TORCH_1_9 = check_version(torch.__version__, '1.9.0')
 TORCH_1_11 = check_version(torch.__version__, '1.11.0')
 TORCH_1_12 = check_version(torch.__version__, '1.12.0')
-TORCH_2_0 = check_version(torch.__version__, minimum='2.0')
+TORCH_2_0 = check_version(torch.__version__, '2.0.0')
 
 
 @contextmanager
 def torch_distributed_zero_first(local_rank: int):
     """Decorator to make all processes in distributed training wait for each local_master to do something."""
     initialized = torch.distributed.is_available() and torch.distributed.is_initialized()
     if initialized and local_rank not in (-1, 0):
@@ -49,14 +48,24 @@
     def decorate(fn):
         """Applies appropriate torch decorator for inference mode based on torch version."""
         return (torch.inference_mode if TORCH_1_9 else torch.no_grad)()(fn)
 
     return decorate
 
 
+def get_cpu_info():
+    """Return a string with system CPU information, i.e. 'Apple M2'."""
+    import cpuinfo  # pip install py-cpuinfo
+
+    k = 'brand_raw', 'hardware_raw', 'arch_string_raw'  # info keys sorted by preference (not all keys always available)
+    info = cpuinfo.get_cpu_info()  # info dict
+    string = info.get(k[0] if k[0] in info else k[1] if k[1] in info else k[2], 'unknown')
+    return string.replace('(R)', '').replace('CPU ', '').replace('@ ', '')
+
+
 def select_device(device='', batch=0, newline=False, verbose=True):
     """Selects PyTorch Device. Options are device = None or 'cpu' or 0 or '0' or '0,1,2,3'."""
     s = f'Ultralytics YOLOv{__version__} ðŸš€ Python-{platform.python_version()} torch-{torch.__version__} '
     device = str(device).lower()
     for remove in 'cuda:', 'none', '(', ')', '[', ']', "'", ' ':
         device = device.replace(remove, '')  # to string, 'cuda:0' -> '0' and '(0, 1)' -> '0,1'
     cpu = device == 'cpu'
@@ -89,18 +98,18 @@
         space = ' ' * (len(s) + 1)
         for i, d in enumerate(devices):
             p = torch.cuda.get_device_properties(i)
             s += f"{'' if i == 0 else space}CUDA:{d} ({p.name}, {p.total_memory / (1 << 20):.0f}MiB)\n"  # bytes to MB
         arg = 'cuda:0'
     elif mps and getattr(torch, 'has_mps', False) and torch.backends.mps.is_available() and TORCH_2_0:
         # Prefer MPS if available
-        s += 'MPS\n'
+        s += f'MPS ({get_cpu_info()})\n'
         arg = 'mps'
     else:  # revert to CPU
-        s += 'CPU\n'
+        s += f'CPU ({get_cpu_info()})\n'
         arg = 'cpu'
 
     if verbose and RANK == -1:
         LOGGER.info(s if newline else s.rstrip())
     return torch.device(arg)
 
 
@@ -202,15 +211,15 @@
         {'model/parameters': 3151904,
          'model/GFLOPs': 8.746,
          'model/speed_ONNX(ms)': 41.244,
          'model/speed_TensorRT(ms)': 3.211,
          'model/speed_PyTorch(ms)': 18.755}
     """
     if trainer.args.profile:  # profile ONNX and TensorRT times
-        from ultralytics.yolo.utils.benchmarks import ProfileModels
+        from ultralytics.utils.benchmarks import ProfileModels
         results = ProfileModels([trainer.last], device=trainer.device).profile()[0]
         results.pop('model/name')
     else:  # only return PyTorch times from most recent validation
         results = {
             'model/parameters': get_num_params(trainer.model),
             'model/GFLOPs': round(get_flops(trainer.model), 3)}
     results['model/speed_PyTorch(ms)'] = round(trainer.validator.speed['inference'], 3)
@@ -228,15 +237,15 @@
         imgsz = imgsz if isinstance(imgsz, list) else [imgsz, imgsz]  # expand if int/float
         return flops * imgsz[0] / stride * imgsz[1] / stride  # 640x640 GFLOPs
     except Exception:
         return 0
 
 
 def get_flops_with_torch_profiler(model, imgsz=640):
-    # Compute model FLOPs (thop alternative)
+    """Compute model FLOPs (thop alternative)."""
     model = de_parallel(model)
     p = next(model.parameters())
     stride = (max(int(model.stride.max()), 32) if hasattr(model, 'stride') else 32) * 2  # max stride
     im = torch.zeros((1, p.shape[1], stride, stride), device=p.device)  # input image in BCHW format
     with torch.profiler.profile(with_flops=True) as prof:
         model(im)
     flops = sum(x.flops for x in prof.key_averages()) / 1E9
@@ -376,25 +385,31 @@
         s (str): file path to save the model with stripped optimizer to. If not provided, 'f' will be overwritten.
 
     Returns:
         None
 
     Usage:
         from pathlib import Path
-        from ultralytics.yolo.utils.torch_utils import strip_optimizer
+        from ultralytics.utils.torch_utils import strip_optimizer
         for f in Path('/Users/glennjocher/Downloads/weights').rglob('*.pt'):
             strip_optimizer(f)
     """
     # Use dill (if exists) to serialize the lambda functions where pickle does not do this
     try:
         import dill as pickle
     except ImportError:
         import pickle
 
     x = torch.load(f, map_location=torch.device('cpu'))
+    if 'model' not in x:
+        LOGGER.info(f'Skipping {f}, not a valid Ultralytics model.')
+        return
+
+    if hasattr(x['model'], 'args'):
+        x['model'].args = dict(x['model'].args)  # convert from IterableSimpleNamespace to dict
     args = {**DEFAULT_CFG_DICT, **x['train_args']} if 'train_args' in x else None  # combine args
     if x.get('ema'):
         x['model'] = x['ema']  # replace model with ema
     for k in 'optimizer', 'best_fitness', 'ema', 'updates':  # keys
         x[k] = None
     x['epoch'] = -1
     x['model'].half()  # to FP16
```

## Comparing `ultralytics/yolo/utils/tuner.py` & `ultralytics/utils/tuner.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # Ultralytics YOLO ðŸš€, AGPL-3.0 license
-from ultralytics.yolo.cfg import TASK2DATA, TASK2METRIC
-from ultralytics.yolo.utils import DEFAULT_CFG_DICT, LOGGER, NUM_THREADS
+from ultralytics.cfg import TASK2DATA, TASK2METRIC
+from ultralytics.utils import DEFAULT_CFG_DICT, LOGGER, NUM_THREADS
 
 
 def run_ray_tune(model,
                  space: dict = None,
                  grace_period: int = 10,
                  gpu_per_trial: int = None,
                  max_samples: int = 10,
```

## Comparing `ultralytics/yolo/utils/callbacks/base.py` & `ultralytics/utils/callbacks/base.py`

 * *Files identical despite different names*

## Comparing `ultralytics/yolo/utils/callbacks/clearml.py` & `ultralytics/utils/callbacks/clearml.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,25 +1,26 @@
 # Ultralytics YOLO ðŸš€, AGPL-3.0 license
 
 import re
 
 import matplotlib.image as mpimg
 import matplotlib.pyplot as plt
 
-from ultralytics.yolo.utils import LOGGER, TESTS_RUNNING
-from ultralytics.yolo.utils.torch_utils import model_info_for_loggers
+from ultralytics.utils import LOGGER, SETTINGS, TESTS_RUNNING
+from ultralytics.utils.torch_utils import model_info_for_loggers
 
 try:
     import clearml
     from clearml import Task
     from clearml.binding.frameworks.pytorch_bind import PatchPyTorchModelIO
     from clearml.binding.matplotlib_bind import PatchedMatplotlib
 
     assert hasattr(clearml, '__version__')  # verify package is not directory
     assert not TESTS_RUNNING  # do not log pytest
+    assert SETTINGS['clearml'] is True  # verify integration is enabled
 except (ImportError, AssertionError):
     clearml = None
 
 
 def _log_debug_samples(files, title='Debug Samples') -> None:
     """
     Log files (images) as debug samples in the ClearML task.
```

## Comparing `ultralytics/yolo/utils/callbacks/comet.py` & `ultralytics/utils/callbacks/comet.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,20 +1,21 @@
 # Ultralytics YOLO ðŸš€, AGPL-3.0 license
 
 import os
 from pathlib import Path
 
-from ultralytics.yolo.utils import LOGGER, RANK, TESTS_RUNNING, ops
-from ultralytics.yolo.utils.torch_utils import model_info_for_loggers
+from ultralytics.utils import LOGGER, RANK, SETTINGS, TESTS_RUNNING, ops
+from ultralytics.utils.torch_utils import model_info_for_loggers
 
 try:
     import comet_ml
 
     assert not TESTS_RUNNING  # do not log pytest
     assert hasattr(comet_ml, '__version__')  # verify package is not directory
+    assert SETTINGS['comet'] is True  # verify integration is enabled
 except (ImportError, AssertionError):
     comet_ml = None
 
 # Ensures certain logging functions only run for supported tasks
 COMET_SUPPORTED_TASKS = ['detect']
 
 # Names of plots created by YOLOv8 that are logged to Comet
```

## Comparing `ultralytics/yolo/utils/callbacks/dvc.py` & `ultralytics/utils/callbacks/dvc.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,21 +1,22 @@
 # Ultralytics YOLO ðŸš€, GPL-3.0 license
 import os
 
 import pkg_resources as pkg
 
-from ultralytics.yolo.utils import LOGGER, TESTS_RUNNING
-from ultralytics.yolo.utils.torch_utils import model_info_for_loggers
+from ultralytics.utils import LOGGER, SETTINGS, TESTS_RUNNING
+from ultralytics.utils.torch_utils import model_info_for_loggers
 
 try:
     from importlib.metadata import version
 
     import dvclive
 
     assert not TESTS_RUNNING  # do not log pytest
+    assert SETTINGS['dvc'] is True  # verify integration is enabled
 
     ver = version('dvclive')
     if pkg.parse_version(ver) < pkg.parse_version('2.11.0'):
         LOGGER.debug(f'DVCLive is detected but version {ver} is incompatible (>=2.11 required).')
         dvclive = None  # noqa: F811
 except (ImportError, AssertionError, TypeError):
     dvclive = None
@@ -113,16 +114,16 @@
 def on_train_end(trainer):
     if live:
         # At the end log the best metrics. It runs validator on the best model internally.
         all_metrics = {**trainer.label_loss_items(trainer.tloss, prefix='train'), **trainer.metrics, **trainer.lr}
         for metric, value in all_metrics.items():
             live.log_metric(metric, value, plot=False)
 
-        _log_plots(trainer.plots, 'eval')
-        _log_plots(trainer.validator.plots, 'eval')
+        _log_plots(trainer.plots, 'val')
+        _log_plots(trainer.validator.plots, 'val')
         _log_confusion_matrix(trainer.validator)
 
         if trainer.best.exists():
             live.log_artifact(trainer.best, copy=True)
 
         live.end()
```

## Comparing `ultralytics/yolo/utils/callbacks/hub.py` & `ultralytics/utils/callbacks/hub.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,15 +1,15 @@
 # Ultralytics YOLO ðŸš€, AGPL-3.0 license
 
 import json
 from time import time
 
 from ultralytics.hub.utils import PREFIX, events
-from ultralytics.yolo.utils import LOGGER
-from ultralytics.yolo.utils.torch_utils import model_info_for_loggers
+from ultralytics.utils import LOGGER, SETTINGS
+from ultralytics.utils.torch_utils import model_info_for_loggers
 
 
 def on_pretrain_routine_end(trainer):
     """Logs info before starting timer for upload rate limit."""
     session = getattr(trainer, 'hub_session', None)
     if session:
         # Start timer for upload rate limit
@@ -80,8 +80,8 @@
     'on_pretrain_routine_end': on_pretrain_routine_end,
     'on_fit_epoch_end': on_fit_epoch_end,
     'on_model_save': on_model_save,
     'on_train_end': on_train_end,
     'on_train_start': on_train_start,
     'on_val_start': on_val_start,
     'on_predict_start': on_predict_start,
-    'on_export_start': on_export_start}
+    'on_export_start': on_export_start} if SETTINGS['hub'] is True else {}  # verify enabled
```

## Comparing `ultralytics/yolo/utils/callbacks/mlflow.py` & `ultralytics/utils/callbacks/mlflow.py`

 * *Files 9% similar despite different names*

```diff
@@ -1,20 +1,21 @@
 # Ultralytics YOLO ðŸš€, AGPL-3.0 license
 
 import os
 import re
 from pathlib import Path
 
-from ultralytics.yolo.utils import LOGGER, TESTS_RUNNING, colorstr
+from ultralytics.utils import LOGGER, SETTINGS, TESTS_RUNNING, colorstr
 
 try:
     import mlflow
 
     assert not TESTS_RUNNING  # do not log pytest
     assert hasattr(mlflow, '__version__')  # verify package is not directory
+    assert SETTINGS['mlflow'] is True  # verify integration is enabled
 except (ImportError, AssertionError):
     mlflow = None
 
 
 def on_pretrain_routine_end(trainer):
     """Logs training parameters to MLflow."""
     global mlflow, run, run_id, experiment_name
@@ -22,15 +23,15 @@
     if os.environ.get('MLFLOW_TRACKING_URI') is None:
         mlflow = None
 
     if mlflow:
         mlflow_location = os.environ['MLFLOW_TRACKING_URI']  # "http://192.168.xxx.xxx:5000"
         mlflow.set_tracking_uri(mlflow_location)
 
-        experiment_name = os.environ.get('MLFLOW_EXPERIMENT') or trainer.args.project or '/Shared/YOLOv8'
+        experiment_name = os.environ.get('MLFLOW_EXPERIMENT_NAME') or trainer.args.project or '/Shared/YOLOv8'
         run_name = os.environ.get('MLFLOW_RUN') or trainer.args.name
         experiment = mlflow.get_experiment_by_name(experiment_name)
         if experiment is None:
             mlflow.create_experiment(experiment_name)
         mlflow.set_experiment(experiment_name)
 
         prefix = colorstr('MLFlow: ')
```

## Comparing `ultralytics/yolo/utils/callbacks/neptune.py` & `ultralytics/utils/callbacks/neptune.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,21 +1,22 @@
 # Ultralytics YOLO ðŸš€, AGPL-3.0 license
 
 import matplotlib.image as mpimg
 import matplotlib.pyplot as plt
 
-from ultralytics.yolo.utils import LOGGER, TESTS_RUNNING
-from ultralytics.yolo.utils.torch_utils import model_info_for_loggers
+from ultralytics.utils import LOGGER, SETTINGS, TESTS_RUNNING
+from ultralytics.utils.torch_utils import model_info_for_loggers
 
 try:
     import neptune
     from neptune.types import File
 
     assert not TESTS_RUNNING  # do not log pytest
     assert hasattr(neptune, '__version__')
+    assert SETTINGS['neptune'] is True  # verify integration is enabled
 except (ImportError, AssertionError):
     neptune = None
 
 run = None  # NeptuneAI experiment logger instance
 
 
 def _log_scalars(scalars, step=0):
```

## Comparing `ultralytics/yolo/utils/callbacks/tensorboard.py` & `ultralytics/utils/callbacks/tensorboard.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,16 +1,19 @@
 # Ultralytics YOLO ðŸš€, AGPL-3.0 license
 
-from ultralytics.yolo.utils import LOGGER, TESTS_RUNNING, colorstr
+from ultralytics.utils import LOGGER, SETTINGS, TESTS_RUNNING, colorstr
 
 try:
     from torch.utils.tensorboard import SummaryWriter
 
     assert not TESTS_RUNNING  # do not log pytest
-except (ImportError, AssertionError):
+    assert SETTINGS['tensorboard'] is True  # verify integration is enabled
+
+# TypeError for handling 'Descriptors cannot not be created directly.' protobuf errors in Windows
+except (ImportError, AssertionError, TypeError):
     SummaryWriter = None
 
 writer = None  # TensorBoard SummaryWriter instance
 
 
 def _log_scalars(scalars, step=0):
     """Logs scalar values to TensorBoard."""
```

## Comparing `ultralytics/yolo/utils/callbacks/wb.py` & `ultralytics/utils/callbacks/wb.py`

 * *Files 27% similar despite different names*

```diff
@@ -1,26 +1,27 @@
 # Ultralytics YOLO ðŸš€, AGPL-3.0 license
-from ultralytics.yolo.utils import TESTS_RUNNING
-from ultralytics.yolo.utils.torch_utils import model_info_for_loggers
+from ultralytics.utils import SETTINGS, TESTS_RUNNING
+from ultralytics.utils.torch_utils import model_info_for_loggers
 
 try:
     import wandb as wb
 
     assert hasattr(wb, '__version__')
     assert not TESTS_RUNNING  # do not log pytest
+    assert SETTINGS['wandb'] is True  # verify integration is enabled
 except (ImportError, AssertionError):
     wb = None
 
 _processed_plots = {}
 
 
 def _log_plots(plots, step):
     for name, params in plots.items():
         timestamp = params['timestamp']
-        if _processed_plots.get(name, None) != timestamp:
+        if _processed_plots.get(name) != timestamp:
             wb.run.log({name.stem: wb.Image(str(name))}, step=step)
             _processed_plots[name] = timestamp
 
 
 def on_pretrain_routine_start(trainer):
     """Initiate and start project if module is present."""
     wb.run or wb.init(project=trainer.args.project or 'YOLOv8', name=trainer.args.name, config=vars(trainer.args))
```

## Comparing `ultralytics/yolo/v8/classify/predict.py` & `ultralytics/models/yolo/detect/predict.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,51 +1,48 @@
 # Ultralytics YOLO ðŸš€, AGPL-3.0 license
 
 import torch
 
-from ultralytics.yolo.engine.predictor import BasePredictor
-from ultralytics.yolo.engine.results import Results
-from ultralytics.yolo.utils import DEFAULT_CFG, ROOT
-
-
-class ClassificationPredictor(BasePredictor):
-
-    def __init__(self, cfg=DEFAULT_CFG, overrides=None, _callbacks=None):
-        super().__init__(cfg, overrides, _callbacks)
-        self.args.task = 'classify'
-
-    def preprocess(self, img):
-        """Converts input image to model-compatible data type."""
-        if not isinstance(img, torch.Tensor):
-            img = torch.stack([self.transforms(im) for im in img], dim=0)
-        img = (img if isinstance(img, torch.Tensor) else torch.from_numpy(img)).to(self.model.device)
-        return img.half() if self.model.fp16 else img.float()  # uint8 to fp16/32
+from ultralytics.engine.predictor import BasePredictor
+from ultralytics.engine.results import Results
+from ultralytics.utils import DEFAULT_CFG, ROOT, ops
+
+
+class DetectionPredictor(BasePredictor):
 
     def postprocess(self, preds, img, orig_imgs):
-        """Postprocesses predictions to return Results objects."""
+        """Postprocesses predictions and returns a list of Results objects."""
+        preds = ops.non_max_suppression(preds,
+                                        self.args.conf,
+                                        self.args.iou,
+                                        agnostic=self.args.agnostic_nms,
+                                        max_det=self.args.max_det,
+                                        classes=self.args.classes)
+
         results = []
         for i, pred in enumerate(preds):
             orig_img = orig_imgs[i] if isinstance(orig_imgs, list) else orig_imgs
+            if not isinstance(orig_imgs, torch.Tensor):
+                pred[:, :4] = ops.scale_boxes(img.shape[2:], pred[:, :4], orig_img.shape)
             path = self.batch[0]
             img_path = path[i] if isinstance(path, list) else path
-            results.append(Results(orig_img=orig_img, path=img_path, names=self.model.names, probs=pred))
-
+            results.append(Results(orig_img=orig_img, path=img_path, names=self.model.names, boxes=pred))
         return results
 
 
 def predict(cfg=DEFAULT_CFG, use_python=False):
-    """Run YOLO model predictions on input images/videos."""
-    model = cfg.model or 'yolov8n-cls.pt'  # or "resnet18"
+    """Runs YOLO model inference on input image(s)."""
+    model = cfg.model or 'yolov8n.pt'
     source = cfg.source if cfg.source is not None else ROOT / 'assets' if (ROOT / 'assets').exists() \
         else 'https://ultralytics.com/images/bus.jpg'
 
     args = dict(model=model, source=source)
     if use_python:
         from ultralytics import YOLO
         YOLO(model)(**args)
     else:
-        predictor = ClassificationPredictor(overrides=args)
+        predictor = DetectionPredictor(overrides=args)
         predictor.predict_cli()
 
 
 if __name__ == '__main__':
     predict()
```

## Comparing `ultralytics/yolo/v8/classify/train.py` & `ultralytics/models/yolo/classify/train.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,19 +1,19 @@
 # Ultralytics YOLO ðŸš€, AGPL-3.0 license
 
 import torch
 import torchvision
 
+from ultralytics.data import ClassificationDataset, build_dataloader
+from ultralytics.engine.trainer import BaseTrainer
+from ultralytics.models import yolo
 from ultralytics.nn.tasks import ClassificationModel, attempt_load_one_weight
-from ultralytics.yolo import v8
-from ultralytics.yolo.data import ClassificationDataset, build_dataloader
-from ultralytics.yolo.engine.trainer import BaseTrainer
-from ultralytics.yolo.utils import DEFAULT_CFG, LOGGER, RANK, colorstr
-from ultralytics.yolo.utils.plotting import plot_images, plot_results
-from ultralytics.yolo.utils.torch_utils import is_parallel, strip_optimizer, torch_distributed_zero_first
+from ultralytics.utils import DEFAULT_CFG, LOGGER, RANK, colorstr
+from ultralytics.utils.plotting import plot_images, plot_results
+from ultralytics.utils.torch_utils import is_parallel, strip_optimizer, torch_distributed_zero_first
 
 
 class ClassificationTrainer(BaseTrainer):
 
     def __init__(self, cfg=DEFAULT_CFG, overrides=None, _callbacks=None):
         """Initialize a ClassificationTrainer object with optional configuration overrides and callbacks."""
         if overrides is None:
@@ -94,15 +94,15 @@
         """Returns a formatted string showing training progress."""
         return ('\n' + '%11s' * (4 + len(self.loss_names))) % \
             ('Epoch', 'GPU_mem', *self.loss_names, 'Instances', 'Size')
 
     def get_validator(self):
         """Returns an instance of ClassificationValidator for validation."""
         self.loss_names = ['loss']
-        return v8.classify.ClassificationValidator(self.test_loader, self.save_dir)
+        return yolo.classify.ClassificationValidator(self.test_loader, self.save_dir)
 
     def label_loss_items(self, loss_items=None, prefix='train'):
         """
         Returns a loss dict with labelled training loss items tensor
         """
         # Not needed for classification but necessary for segmentation & detection
         keys = [f'{prefix}/{x}' for x in self.loss_names]
```

## Comparing `ultralytics/yolo/v8/classify/val.py` & `ultralytics/models/yolo/classify/val.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,16 +1,16 @@
 # Ultralytics YOLO ðŸš€, AGPL-3.0 license
 
 import torch
 
-from ultralytics.yolo.data import ClassificationDataset, build_dataloader
-from ultralytics.yolo.engine.validator import BaseValidator
-from ultralytics.yolo.utils import DEFAULT_CFG, LOGGER
-from ultralytics.yolo.utils.metrics import ClassifyMetrics, ConfusionMatrix
-from ultralytics.yolo.utils.plotting import plot_images
+from ultralytics.data import ClassificationDataset, build_dataloader
+from ultralytics.engine.validator import BaseValidator
+from ultralytics.utils import DEFAULT_CFG, LOGGER
+from ultralytics.utils.metrics import ClassifyMetrics, ConfusionMatrix
+from ultralytics.utils.plotting import plot_images
 
 
 class ClassificationValidator(BaseValidator):
 
     def __init__(self, dataloader=None, save_dir=None, pbar=None, args=None, _callbacks=None):
         """Initializes ClassificationValidator instance with args, dataloader, save_dir, and progress bar."""
         super().__init__(dataloader, save_dir, pbar, args, _callbacks)
```

## Comparing `ultralytics/yolo/v8/detect/predict.py` & `ultralytics/models/yolo/pose/predict.py`

 * *Files 20% similar despite different names*

```diff
@@ -1,48 +1,58 @@
 # Ultralytics YOLO ðŸš€, AGPL-3.0 license
 
-import torch
+from ultralytics.engine.results import Results
+from ultralytics.models.yolo.detect.predict import DetectionPredictor
+from ultralytics.utils import DEFAULT_CFG, ROOT, ops
 
-from ultralytics.yolo.engine.predictor import BasePredictor
-from ultralytics.yolo.engine.results import Results
-from ultralytics.yolo.utils import DEFAULT_CFG, ROOT, ops
 
+class PosePredictor(DetectionPredictor):
 
-class DetectionPredictor(BasePredictor):
+    def __init__(self, cfg=DEFAULT_CFG, overrides=None, _callbacks=None):
+        super().__init__(cfg, overrides, _callbacks)
+        self.args.task = 'pose'
 
     def postprocess(self, preds, img, orig_imgs):
-        """Postprocesses predictions and returns a list of Results objects."""
+        """Return detection results for a given input image or list of images."""
         preds = ops.non_max_suppression(preds,
                                         self.args.conf,
                                         self.args.iou,
                                         agnostic=self.args.agnostic_nms,
                                         max_det=self.args.max_det,
-                                        classes=self.args.classes)
+                                        classes=self.args.classes,
+                                        nc=len(self.model.names))
 
         results = []
         for i, pred in enumerate(preds):
             orig_img = orig_imgs[i] if isinstance(orig_imgs, list) else orig_imgs
-            if not isinstance(orig_imgs, torch.Tensor):
-                pred[:, :4] = ops.scale_boxes(img.shape[2:], pred[:, :4], orig_img.shape)
+            shape = orig_img.shape
+            pred[:, :4] = ops.scale_boxes(img.shape[2:], pred[:, :4], shape).round()
+            pred_kpts = pred[:, 6:].view(len(pred), *self.model.kpt_shape) if len(pred) else pred[:, 6:]
+            pred_kpts = ops.scale_coords(img.shape[2:], pred_kpts, shape)
             path = self.batch[0]
             img_path = path[i] if isinstance(path, list) else path
-            results.append(Results(orig_img=orig_img, path=img_path, names=self.model.names, boxes=pred))
+            results.append(
+                Results(orig_img=orig_img,
+                        path=img_path,
+                        names=self.model.names,
+                        boxes=pred[:, :6],
+                        keypoints=pred_kpts))
         return results
 
 
 def predict(cfg=DEFAULT_CFG, use_python=False):
-    """Runs YOLO model inference on input image(s)."""
-    model = cfg.model or 'yolov8n.pt'
+    """Runs YOLO to predict objects in an image or video."""
+    model = cfg.model or 'yolov8n-pose.pt'
     source = cfg.source if cfg.source is not None else ROOT / 'assets' if (ROOT / 'assets').exists() \
         else 'https://ultralytics.com/images/bus.jpg'
 
     args = dict(model=model, source=source)
     if use_python:
         from ultralytics import YOLO
         YOLO(model)(**args)
     else:
-        predictor = DetectionPredictor(overrides=args)
+        predictor = PosePredictor(overrides=args)
         predictor.predict_cli()
 
 
 if __name__ == '__main__':
     predict()
```

## Comparing `ultralytics/yolo/v8/detect/train.py` & `ultralytics/models/yolo/detect/train.py`

 * *Files 13% similar despite different names*

```diff
@@ -1,58 +1,38 @@
 # Ultralytics YOLO ðŸš€, AGPL-3.0 license
 from copy import copy
 
 import numpy as np
 
+from ultralytics.data import build_dataloader, build_yolo_dataset
+from ultralytics.engine.trainer import BaseTrainer
+from ultralytics.models import yolo
 from ultralytics.nn.tasks import DetectionModel
-from ultralytics.yolo import v8
-from ultralytics.yolo.data import build_dataloader, build_yolo_dataset
-from ultralytics.yolo.data.dataloaders.v5loader import create_dataloader
-from ultralytics.yolo.engine.trainer import BaseTrainer
-from ultralytics.yolo.utils import DEFAULT_CFG, LOGGER, RANK, colorstr
-from ultralytics.yolo.utils.plotting import plot_images, plot_labels, plot_results
-from ultralytics.yolo.utils.torch_utils import de_parallel, torch_distributed_zero_first
+from ultralytics.utils import DEFAULT_CFG, LOGGER, RANK
+from ultralytics.utils.plotting import plot_images, plot_labels, plot_results
+from ultralytics.utils.torch_utils import de_parallel, torch_distributed_zero_first
 
 
 # BaseTrainer python usage
 class DetectionTrainer(BaseTrainer):
 
     def build_dataset(self, img_path, mode='train', batch=None):
-        """Build YOLO Dataset
+        """
+        Build YOLO Dataset.
 
         Args:
             img_path (str): Path to the folder containing images.
             mode (str): `train` mode or `val` mode, users are able to customize different augmentations for each mode.
             batch (int, optional): Size of batches, this is for `rect`. Defaults to None.
         """
         gs = max(int(de_parallel(self.model).stride.max() if self.model else 0), 32)
         return build_yolo_dataset(self.args, img_path, batch, self.data, mode=mode, rect=mode == 'val', stride=gs)
 
     def get_dataloader(self, dataset_path, batch_size=16, rank=0, mode='train'):
-        """TODO: manage splits differently."""
-        # Calculate stride - check if model is initialized
-        if self.args.v5loader:
-            LOGGER.warning("WARNING âš ï¸ 'v5loader' feature is deprecated and will be removed soon. You can train using "
-                           'the default YOLOv8 dataloader instead, no argument is needed.')
-            gs = max(int(de_parallel(self.model).stride.max() if self.model else 0), 32)
-            return create_dataloader(path=dataset_path,
-                                     imgsz=self.args.imgsz,
-                                     batch_size=batch_size,
-                                     stride=gs,
-                                     hyp=vars(self.args),
-                                     augment=mode == 'train',
-                                     cache=self.args.cache,
-                                     pad=0 if mode == 'train' else 0.5,
-                                     rect=self.args.rect or mode == 'val',
-                                     rank=rank,
-                                     workers=self.args.workers,
-                                     close_mosaic=self.args.close_mosaic != 0,
-                                     prefix=colorstr(f'{mode}: '),
-                                     shuffle=mode == 'train',
-                                     seed=self.args.seed)[0]
+        """Construct and return dataloader."""
         assert mode in ['train', 'val']
         with torch_distributed_zero_first(rank):  # init dataset *.cache only once if DDP
             dataset = self.build_dataset(dataset_path, mode, batch_size)
         shuffle = mode == 'train'
         if getattr(dataset, 'rect', False) and shuffle:
             LOGGER.warning("WARNING âš ï¸ 'rect=True' is incompatible with DataLoader shuffle, setting shuffle=False")
             shuffle = False
@@ -80,15 +60,15 @@
         if weights:
             model.load(weights)
         return model
 
     def get_validator(self):
         """Returns a DetectionValidator for YOLO model validation."""
         self.loss_names = 'box_loss', 'cls_loss', 'dfl_loss'
-        return v8.detect.DetectionValidator(self.test_loader, save_dir=self.save_dir, args=copy(self.args))
+        return yolo.detect.DetectionValidator(self.test_loader, save_dir=self.save_dir, args=copy(self.args))
 
     def label_loss_items(self, loss_items=None, prefix='train'):
         """
         Returns a loss dict with labelled training loss items tensor
         """
         # Not needed for classification but necessary for segmentation & detection
         keys = [f'{prefix}/{x}' for x in self.loss_names]
```

## Comparing `ultralytics/yolo/v8/detect/val.py` & `ultralytics/models/yolo/detect/val.py`

 * *Files 3% similar despite different names*

```diff
@@ -2,22 +2,21 @@
 
 import os
 from pathlib import Path
 
 import numpy as np
 import torch
 
-from ultralytics.yolo.data import build_dataloader, build_yolo_dataset
-from ultralytics.yolo.data.dataloaders.v5loader import create_dataloader
-from ultralytics.yolo.engine.validator import BaseValidator
-from ultralytics.yolo.utils import DEFAULT_CFG, LOGGER, colorstr, ops
-from ultralytics.yolo.utils.checks import check_requirements
-from ultralytics.yolo.utils.metrics import ConfusionMatrix, DetMetrics, box_iou
-from ultralytics.yolo.utils.plotting import output_to_target, plot_images
-from ultralytics.yolo.utils.torch_utils import de_parallel
+from ultralytics.data import build_dataloader, build_yolo_dataset
+from ultralytics.engine.validator import BaseValidator
+from ultralytics.utils import DEFAULT_CFG, LOGGER, ops
+from ultralytics.utils.checks import check_requirements
+from ultralytics.utils.metrics import ConfusionMatrix, DetMetrics, box_iou
+from ultralytics.utils.plotting import output_to_target, plot_images
+from ultralytics.utils.torch_utils import de_parallel
 
 
 class DetectionValidator(BaseValidator):
 
     def __init__(self, dataloader=None, save_dir=None, pbar=None, args=None, _callbacks=None):
         """Initialize detection model with necessary variables and settings."""
         super().__init__(dataloader, save_dir, pbar, args, _callbacks)
@@ -182,36 +181,17 @@
             mode (str): `train` mode or `val` mode, users are able to customize different augmentations for each mode.
             batch (int, optional): Size of batches, this is for `rect`. Defaults to None.
         """
         gs = max(int(de_parallel(self.model).stride if self.model else 0), 32)
         return build_yolo_dataset(self.args, img_path, batch, self.data, mode=mode, stride=gs)
 
     def get_dataloader(self, dataset_path, batch_size):
-        """TODO: manage splits differently."""
-        # Calculate stride - check if model is initialized
-        if self.args.v5loader:
-            LOGGER.warning("WARNING âš ï¸ 'v5loader' feature is deprecated and will be removed soon. You can train using "
-                           'the default YOLOv8 dataloader instead, no argument is needed.')
-            gs = max(int(de_parallel(self.model).stride if self.model else 0), 32)
-            return create_dataloader(path=dataset_path,
-                                     imgsz=self.args.imgsz,
-                                     batch_size=batch_size,
-                                     stride=gs,
-                                     hyp=vars(self.args),
-                                     cache=False,
-                                     pad=0.5,
-                                     rect=self.args.rect,
-                                     workers=self.args.workers,
-                                     prefix=colorstr(f'{self.args.mode}: '),
-                                     shuffle=False,
-                                     seed=self.args.seed)[0]
-
+        """Construct and return dataloader."""
         dataset = self.build_dataset(dataset_path, batch=batch_size, mode='val')
-        dataloader = build_dataloader(dataset, batch_size, self.args.workers, shuffle=False, rank=-1)
-        return dataloader
+        return build_dataloader(dataset, batch_size, self.args.workers, shuffle=False, rank=-1)  # return dataloader
 
     def plot_val_samples(self, batch, ni):
         """Plot validation image samples."""
         plot_images(batch['img'],
                     batch['batch_idx'],
                     batch['cls'].squeeze(-1),
                     batch['bboxes'],
```

## Comparing `ultralytics/yolo/v8/pose/predict.py` & `ultralytics/models/yolo/pose/train.py`

 * *Files 20% similar despite different names*

```diff
@@ -1,58 +1,77 @@
 # Ultralytics YOLO ðŸš€, AGPL-3.0 license
 
-from ultralytics.yolo.engine.results import Results
-from ultralytics.yolo.utils import DEFAULT_CFG, ROOT, ops
-from ultralytics.yolo.v8.detect.predict import DetectionPredictor
+from copy import copy
 
+from ultralytics.models import yolo
+from ultralytics.nn.tasks import PoseModel
+from ultralytics.utils import DEFAULT_CFG
+from ultralytics.utils.plotting import plot_images, plot_results
 
-class PosePredictor(DetectionPredictor):
+
+# BaseTrainer python usage
+class PoseTrainer(yolo.detect.DetectionTrainer):
 
     def __init__(self, cfg=DEFAULT_CFG, overrides=None, _callbacks=None):
+        """Initialize a PoseTrainer object with specified configurations and overrides."""
+        if overrides is None:
+            overrides = {}
+        overrides['task'] = 'pose'
         super().__init__(cfg, overrides, _callbacks)
-        self.args.task = 'pose'
 
-    def postprocess(self, preds, img, orig_imgs):
-        """Return detection results for a given input image or list of images."""
-        preds = ops.non_max_suppression(preds,
-                                        self.args.conf,
-                                        self.args.iou,
-                                        agnostic=self.args.agnostic_nms,
-                                        max_det=self.args.max_det,
-                                        classes=self.args.classes,
-                                        nc=len(self.model.names))
-
-        results = []
-        for i, pred in enumerate(preds):
-            orig_img = orig_imgs[i] if isinstance(orig_imgs, list) else orig_imgs
-            shape = orig_img.shape
-            pred[:, :4] = ops.scale_boxes(img.shape[2:], pred[:, :4], shape).round()
-            pred_kpts = pred[:, 6:].view(len(pred), *self.model.kpt_shape) if len(pred) else pred[:, 6:]
-            pred_kpts = ops.scale_coords(img.shape[2:], pred_kpts, shape)
-            path = self.batch[0]
-            img_path = path[i] if isinstance(path, list) else path
-            results.append(
-                Results(orig_img=orig_img,
-                        path=img_path,
-                        names=self.model.names,
-                        boxes=pred[:, :6],
-                        keypoints=pred_kpts))
-        return results
-
-
-def predict(cfg=DEFAULT_CFG, use_python=False):
-    """Runs YOLO to predict objects in an image or video."""
-    model = cfg.model or 'yolov8n-pose.pt'
-    source = cfg.source if cfg.source is not None else ROOT / 'assets' if (ROOT / 'assets').exists() \
-        else 'https://ultralytics.com/images/bus.jpg'
+    def get_model(self, cfg=None, weights=None, verbose=True):
+        """Get pose estimation model with specified configuration and weights."""
+        model = PoseModel(cfg, ch=3, nc=self.data['nc'], data_kpt_shape=self.data['kpt_shape'], verbose=verbose)
+        if weights:
+            model.load(weights)
+
+        return model
+
+    def set_model_attributes(self):
+        """Sets keypoints shape attribute of PoseModel."""
+        super().set_model_attributes()
+        self.model.kpt_shape = self.data['kpt_shape']
+
+    def get_validator(self):
+        """Returns an instance of the PoseValidator class for validation."""
+        self.loss_names = 'box_loss', 'pose_loss', 'kobj_loss', 'cls_loss', 'dfl_loss'
+        return yolo.pose.PoseValidator(self.test_loader, save_dir=self.save_dir, args=copy(self.args))
+
+    def plot_training_samples(self, batch, ni):
+        """Plot a batch of training samples with annotated class labels, bounding boxes, and keypoints."""
+        images = batch['img']
+        kpts = batch['keypoints']
+        cls = batch['cls'].squeeze(-1)
+        bboxes = batch['bboxes']
+        paths = batch['im_file']
+        batch_idx = batch['batch_idx']
+        plot_images(images,
+                    batch_idx,
+                    cls,
+                    bboxes,
+                    kpts=kpts,
+                    paths=paths,
+                    fname=self.save_dir / f'train_batch{ni}.jpg',
+                    on_plot=self.on_plot)
+
+    def plot_metrics(self):
+        """Plots training/val metrics."""
+        plot_results(file=self.csv, pose=True, on_plot=self.on_plot)  # save results.png
+
+
+def train(cfg=DEFAULT_CFG, use_python=False):
+    """Train the YOLO model on the given data and device."""
+    model = cfg.model or 'yolov8n-pose.yaml'
+    data = cfg.data or 'coco8-pose.yaml'
+    device = cfg.device if cfg.device is not None else ''
 
-    args = dict(model=model, source=source)
+    args = dict(model=model, data=data, device=device)
     if use_python:
         from ultralytics import YOLO
-        YOLO(model)(**args)
+        YOLO(model).train(**args)
     else:
-        predictor = PosePredictor(overrides=args)
-        predictor.predict_cli()
+        trainer = PoseTrainer(overrides=args)
+        trainer.train()
 
 
 if __name__ == '__main__':
-    predict()
+    train()
```

## Comparing `ultralytics/yolo/v8/pose/train.py` & `ultralytics/models/yolo/segment/train.py`

 * *Files 16% similar despite different names*

```diff
@@ -1,77 +1,65 @@
 # Ultralytics YOLO ðŸš€, AGPL-3.0 license
-
 from copy import copy
 
-from ultralytics.nn.tasks import PoseModel
-from ultralytics.yolo import v8
-from ultralytics.yolo.utils import DEFAULT_CFG
-from ultralytics.yolo.utils.plotting import plot_images, plot_results
+from ultralytics.models import yolo
+from ultralytics.nn.tasks import SegmentationModel
+from ultralytics.utils import DEFAULT_CFG, RANK
+from ultralytics.utils.plotting import plot_images, plot_results
 
 
 # BaseTrainer python usage
-class PoseTrainer(v8.detect.DetectionTrainer):
+class SegmentationTrainer(yolo.detect.DetectionTrainer):
 
     def __init__(self, cfg=DEFAULT_CFG, overrides=None, _callbacks=None):
-        """Initialize a PoseTrainer object with specified configurations and overrides."""
+        """Initialize a SegmentationTrainer object with given arguments."""
         if overrides is None:
             overrides = {}
-        overrides['task'] = 'pose'
+        overrides['task'] = 'segment'
         super().__init__(cfg, overrides, _callbacks)
 
     def get_model(self, cfg=None, weights=None, verbose=True):
-        """Get pose estimation model with specified configuration and weights."""
-        model = PoseModel(cfg, ch=3, nc=self.data['nc'], data_kpt_shape=self.data['kpt_shape'], verbose=verbose)
+        """Return SegmentationModel initialized with specified config and weights."""
+        model = SegmentationModel(cfg, ch=3, nc=self.data['nc'], verbose=verbose and RANK == -1)
         if weights:
             model.load(weights)
 
         return model
 
-    def set_model_attributes(self):
-        """Sets keypoints shape attribute of PoseModel."""
-        super().set_model_attributes()
-        self.model.kpt_shape = self.data['kpt_shape']
-
     def get_validator(self):
-        """Returns an instance of the PoseValidator class for validation."""
-        self.loss_names = 'box_loss', 'pose_loss', 'kobj_loss', 'cls_loss', 'dfl_loss'
-        return v8.pose.PoseValidator(self.test_loader, save_dir=self.save_dir, args=copy(self.args))
+        """Return an instance of SegmentationValidator for validation of YOLO model."""
+        self.loss_names = 'box_loss', 'seg_loss', 'cls_loss', 'dfl_loss'
+        return yolo.segment.SegmentationValidator(self.test_loader, save_dir=self.save_dir, args=copy(self.args))
 
     def plot_training_samples(self, batch, ni):
-        """Plot a batch of training samples with annotated class labels, bounding boxes, and keypoints."""
-        images = batch['img']
-        kpts = batch['keypoints']
-        cls = batch['cls'].squeeze(-1)
-        bboxes = batch['bboxes']
-        paths = batch['im_file']
-        batch_idx = batch['batch_idx']
-        plot_images(images,
-                    batch_idx,
-                    cls,
-                    bboxes,
-                    kpts=kpts,
-                    paths=paths,
+        """Creates a plot of training sample images with labels and box coordinates."""
+        plot_images(batch['img'],
+                    batch['batch_idx'],
+                    batch['cls'].squeeze(-1),
+                    batch['bboxes'],
+                    batch['masks'],
+                    paths=batch['im_file'],
                     fname=self.save_dir / f'train_batch{ni}.jpg',
                     on_plot=self.on_plot)
 
     def plot_metrics(self):
         """Plots training/val metrics."""
-        plot_results(file=self.csv, pose=True, on_plot=self.on_plot)  # save results.png
+        plot_results(file=self.csv, segment=True, on_plot=self.on_plot)  # save results.png
 
 
 def train(cfg=DEFAULT_CFG, use_python=False):
-    """Train the YOLO model on the given data and device."""
-    model = cfg.model or 'yolov8n-pose.yaml'
-    data = cfg.data or 'coco8-pose.yaml'
+    """Train a YOLO segmentation model based on passed arguments."""
+    model = cfg.model or 'yolov8n-seg.pt'
+    data = cfg.data or 'coco128-seg.yaml'  # or yolo.ClassificationDataset("mnist")
     device = cfg.device if cfg.device is not None else ''
 
     args = dict(model=model, data=data, device=device)
     if use_python:
         from ultralytics import YOLO
         YOLO(model).train(**args)
     else:
-        trainer = PoseTrainer(overrides=args)
+        trainer = SegmentationTrainer(overrides=args)
         trainer.train()
 
 
 if __name__ == '__main__':
     train()
```

## Comparing `ultralytics/yolo/v8/pose/val.py` & `ultralytics/models/yolo/pose/val.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,19 +1,19 @@
 # Ultralytics YOLO ðŸš€, AGPL-3.0 license
 
 from pathlib import Path
 
 import numpy as np
 import torch
 
-from ultralytics.yolo.utils import DEFAULT_CFG, LOGGER, ops
-from ultralytics.yolo.utils.checks import check_requirements
-from ultralytics.yolo.utils.metrics import OKS_SIGMA, PoseMetrics, box_iou, kpt_iou
-from ultralytics.yolo.utils.plotting import output_to_target, plot_images
-from ultralytics.yolo.v8.detect import DetectionValidator
+from ultralytics.models.yolo.detect import DetectionValidator
+from ultralytics.utils import DEFAULT_CFG, LOGGER, ops
+from ultralytics.utils.checks import check_requirements
+from ultralytics.utils.metrics import OKS_SIGMA, PoseMetrics, box_iou, kpt_iou
+from ultralytics.utils.plotting import output_to_target, plot_images
 
 
 class PoseValidator(DetectionValidator):
 
     def __init__(self, dataloader=None, save_dir=None, pbar=None, args=None, _callbacks=None):
         """Initialize a 'PoseValidator' object with custom parameters and assigned attributes."""
         super().__init__(dataloader, save_dir, pbar, args, _callbacks)
```

## Comparing `ultralytics/yolo/v8/segment/predict.py` & `ultralytics/models/yolo/segment/predict.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,14 +1,14 @@
 # Ultralytics YOLO ðŸš€, AGPL-3.0 license
 
 import torch
 
-from ultralytics.yolo.engine.results import Results
-from ultralytics.yolo.utils import DEFAULT_CFG, ROOT, ops
-from ultralytics.yolo.v8.detect.predict import DetectionPredictor
+from ultralytics.engine.results import Results
+from ultralytics.models.yolo.detect.predict import DetectionPredictor
+from ultralytics.utils import DEFAULT_CFG, ROOT, ops
 
 
 class SegmentationPredictor(DetectionPredictor):
 
     def __init__(self, cfg=DEFAULT_CFG, overrides=None, _callbacks=None):
         super().__init__(cfg, overrides, _callbacks)
         self.args.task = 'segment'
```

## Comparing `ultralytics/yolo/v8/segment/val.py` & `ultralytics/models/yolo/segment/val.py`

 * *Files 2% similar despite different names*

```diff
@@ -3,19 +3,19 @@
 from multiprocessing.pool import ThreadPool
 from pathlib import Path
 
 import numpy as np
 import torch
 import torch.nn.functional as F
 
-from ultralytics.yolo.utils import DEFAULT_CFG, LOGGER, NUM_THREADS, ops
-from ultralytics.yolo.utils.checks import check_requirements
-from ultralytics.yolo.utils.metrics import SegmentMetrics, box_iou, mask_iou
-from ultralytics.yolo.utils.plotting import output_to_target, plot_images
-from ultralytics.yolo.v8.detect import DetectionValidator
+from ultralytics.models.yolo.detect import DetectionValidator
+from ultralytics.utils import DEFAULT_CFG, LOGGER, NUM_THREADS, ops
+from ultralytics.utils.checks import check_requirements
+from ultralytics.utils.metrics import SegmentMetrics, box_iou, mask_iou
+from ultralytics.utils.plotting import output_to_target, plot_images
 
 
 class SegmentationValidator(DetectionValidator):
 
     def __init__(self, dataloader=None, save_dir=None, pbar=None, args=None, _callbacks=None):
         """Initialize SegmentationValidator and set task to 'segment', metrics to SegmentMetrics."""
         super().__init__(dataloader, save_dir, pbar, args, _callbacks)
```

## Comparing `pyppbox_ultralytics-8.0.132.dist-info/LICENSE` & `pyppbox_ultralytics-8.0.142.dist-info/LICENSE`

 * *Files identical despite different names*

## Comparing `pyppbox_ultralytics-8.0.132.dist-info/METADATA` & `pyppbox_ultralytics-8.0.142.dist-info/METADATA`

 * *Files 10% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: pyppbox-ultralytics
-Version: 8.0.132
+Version: 8.0.142
 Summary: Ultralytics YOLOv8 for SOTA object detection, multi-object tracking, instance segmentation, pose estimation and image classification.
 Home-page: https://github.com/rathaumons/ultralytics-for-pyppbox
 Author: rathaROG
 Author-email: hello@ultralytics.com
 License: AGPL-3.0
 Project-URL: Bug Reports, https://github.com/rathaumons/ultralytics-for-pyppbox/issues
 Project-URL: Funding, https://ultralytics.com
@@ -53,18 +53,18 @@
 Requires-Dist: openvino-dev (>=2022.3) ; extra == 'export'
 Requires-Dist: tensorflowjs ; extra == 'export'
 
 [![Auto Build Wheels](https://github.com/rathaumons/ultralytics-for-pyppbox/actions/workflows/autobuild.yaml/badge.svg)](https://github.com/rathaumons/ultralytics-for-pyppbox/actions/workflows/autobuild.yaml)
 
 # Customized Ultralytics for [`pyppbox`](https://github.com/rathaumons/pyppbox)
 
-* Updated: **July 11, 2023**
-* Synced with: v8.0.132 -> [[495edc2]](https://github.com/ultralytics/ultralytics/commit/495edc261f4769ed31b118eafd8cdc23da2935fd) + [[82920ef]](https://github.com/ultralytics/ultralytics/commit/82920ef7ec4c529439c68a4aecc5c7935ecdecc1) + [[48d7dbd]](https://github.com/ultralytics/ultralytics/commit/48d7dbdbf949da70162610b71af049c7926b4963)
+* Updated: **July 26, 2023**
+* Synced with: v8.0.142 -> [[9627323]](https://github.com/ultralytics/ultralytics/commit/9627323d7cb5e8107e7a471f87d471108b263451) + [[3c787eb]](3c787eb0806bab79dda88f0be9a476666118b52f)
 * All credit and info -> [[Original Ultralytics repo]](https://github.com/ultralytics/ultralytics)
-* What cutomized:
+* What customized:
     - Enable OpenCV multithreading
     - Remove restrictions on customized OpenCV
     - Disable dependency auto-install
     - Disable auto update
 
 ## Installation
```

## Comparing `pyppbox_ultralytics-8.0.132.dist-info/RECORD` & `pyppbox_ultralytics-8.0.142.dist-info/RECORD`

 * *Files 18% similar despite different names*

```diff
@@ -1,166 +1,149 @@
-ultralytics/__init__.py,sha256=NHUlkuqRXbYHjZYAFuTdVgERbVRLYadhyNUH0AGxqdM,549
+ultralytics/__init__.py,sha256=dio-Q90tJFenI6v-ZBvogig8AV-Pmen926lbbczS8aA,566
 ultralytics/assets/bus.jpg,sha256=wCAZxJecGR63Od3ZRERe9Aja1Weayrb9Ug751DS_vGM,137419
 ultralytics/assets/zidane.jpg,sha256=Ftc4aeMmen1O0A3o6GCDO9FlfBslLpTAw0gnetx7bts,50427
-ultralytics/datasets/Argoverse.yaml,sha256=Q6hKRtI52JOYt4qmjkeo192mmgSkuCdOnfiUTxtBy5A,2751
-ultralytics/datasets/GlobalWheat2020.yaml,sha256=pJTC0BI4SS7OHYzY4DSFjBrtYjwKjwGW6NVPAt093nU,1986
-ultralytics/datasets/ImageNet.yaml,sha256=Q9WD8krMa1DTgV0WipBuVoGLKsBYIqbjhAc0sE-lFNc,42439
-ultralytics/datasets/Objects365.yaml,sha256=V0ofWOg3JOWHpnyN1IRIs7FReQdF8vWTs52W1qoSfsk,9270
-ultralytics/datasets/SKU-110K.yaml,sha256=DR_tdd9G-UHmvi5EpJDjc750-nVpOZxFKtNPItlFeWQ,2437
-ultralytics/datasets/VOC.yaml,sha256=gND5C1SGD71W0XcUjxJoZkjpUMTAf1QL6DycURMzRy0,3512
-ultralytics/datasets/VisDrone.yaml,sha256=DIxBNzTofnoJ3MIQUKSfgjDlF5m9wSJADpT7fQHkbUU,3012
-ultralytics/datasets/coco-pose.yaml,sha256=XptD_5-UG_xDqkxZp4GH2r89qX2zHHcOwIC_EE4S18o,1547
-ultralytics/datasets/coco.yaml,sha256=uY0hMCLpwt52enrVqdJmZ6g2v5BpG7Qbz4Aqal4306s,2526
-ultralytics/datasets/coco128-seg.yaml,sha256=kgXB7FizkLnqfVADeyvpLtgyB2j2EfmdY3lb5foPaBU,1862
-ultralytics/datasets/coco128.yaml,sha256=MHv29nXWIRLCozWqvtcwluygfFmcDM24yvxWOhWLHWM,1846
-ultralytics/datasets/coco8-pose.yaml,sha256=tFrhKCjV4xLCMRJZquUBkotEPRxbdkGyIQWLXikgkUU,895
-ultralytics/datasets/coco8-seg.yaml,sha256=L5Lmq67z45Btp-BPL3jvjFkdoHq0vDojvXDFtmxCZL8,1797
-ultralytics/datasets/coco8.yaml,sha256=ZvPz2lG0U9NI0eUPjvqswNw39j4sBreTGiG7G-dgXKY,1777
-ultralytics/datasets/xView.yaml,sha256=w87Xnke6G9koCHAJB23Oao_lYPjLQ2GFvpKenl4SOEU,5178
-ultralytics/hub/__init__.py,sha256=Pxfo-2N_pZxX8rJrozERyIvRCA5IinXrVW11C74FrSc,4408
-ultralytics/hub/auth.py,sha256=Liig4N4DOzK-y16rm9w1m_L24sHghJHkVrKPz4lAn0c,5209
-ultralytics/hub/session.py,sha256=rIZI_w9yasHr4dg7tQB-6j-F81TQoDtr1DilHmVnyIQ,8479
-ultralytics/hub/utils.py,sha256=yO9ljHg9HTfxDcQlDPo_urJyXPftbd7-sjDlmgKtsp8,9314
-ultralytics/models/rt-detr/rtdetr-l.yaml,sha256=yDFWTOS4kLtglwkadf4848fJGcWiA5efnX3rP2zD0cU,1970
-ultralytics/models/rt-detr/rtdetr-x.yaml,sha256=hjcsEcoiyiX0qftk_DePztn15VoiZygXzDEe1jQFubQ,2177
-ultralytics/models/v3/yolov3-spp.yaml,sha256=SK-lrXpjXlOpod0VJIV_T3kQ_NGRONP6Ma1HFako3NA,1550
-ultralytics/models/v3/yolov3-tiny.yaml,sha256=RcnGwbby2WtgS9cZ3jBybQuAHBDMpgMADPb0x6_bDM0,1252
-ultralytics/models/v3/yolov3.yaml,sha256=wXUDtWndgxIFbuaoeuY5tFcYZv87ZqOlrdhLKEToiuw,1537
-ultralytics/models/v5/yolov5-p6.yaml,sha256=3nY2FYH2lz8pfb3J3kFcY09-075f2ErF_p_i9wlyXqE,1923
-ultralytics/models/v5/yolov5.yaml,sha256=rsBD3nnBQNMcmV7X-twn2BevDGIChErdIcB-3eR-10Y,1550
-ultralytics/models/v6/yolov6.yaml,sha256=rXb2qPqpm14WVLV4nh8ib-fEjhRa_nuMiCkGdelXbDo,1735
-ultralytics/models/v8/yolov8-cls.yaml,sha256=rO8Eq8OuTQUkYMQi2AtNgVvtsPHYNxEXr9xT29k-IG8,920
-ultralytics/models/v8/yolov8-p2.yaml,sha256=jgBsa0-uNwtDHGHJWLVEwuFwxq_twpl9ihRIc4b6bk8,1751
-ultralytics/models/v8/yolov8-p6.yaml,sha256=oGN0OQ9TtVnttzLd9frFVav8dPxe3OuUA8p4PFRrQmM,1856
-ultralytics/models/v8/yolov8-pose-p6.yaml,sha256=dtc_uUoXVE83E9Mhf-SSITavuho6A4iQOfQa6sFWzjc,1946
-ultralytics/models/v8/yolov8-pose.yaml,sha256=kfQnVeZK5sfdRGKuBUpNmIUxXO89qWchQUG-xQHkDDk,1580
-ultralytics/models/v8/yolov8-rtdetr.yaml,sha256=7jfhW8JwL2HojeNR4PYNvsB3rJkiGBMA7VrZ2xdHJRs,1920
-ultralytics/models/v8/yolov8-seg.yaml,sha256=QvXQOvQsyYj2GAB-ePB48g4N9jW5JdIkeH800Ev64uI,1490
-ultralytics/models/v8/yolov8.yaml,sha256=A01V3w3qsOBgAAT9e2AwxXhU_SALEI2IvdoqcPOTnxc,1913
+ultralytics/cfg/__init__.py,sha256=fhKmI6UCAru_EridNH8XN4ELdfxFjkE4ANJ8DRAb5sY,18735
+ultralytics/cfg/default.yaml,sha256=_nJYep5yHIiuwuYY3NKSs8nFefZ6kCU8LW7gCga3EYU,7173
+ultralytics/cfg/models/rt-detr/rtdetr-l.yaml,sha256=yDFWTOS4kLtglwkadf4848fJGcWiA5efnX3rP2zD0cU,1970
+ultralytics/cfg/models/rt-detr/rtdetr-x.yaml,sha256=hjcsEcoiyiX0qftk_DePztn15VoiZygXzDEe1jQFubQ,2177
+ultralytics/cfg/models/v3/yolov3-spp.yaml,sha256=SK-lrXpjXlOpod0VJIV_T3kQ_NGRONP6Ma1HFako3NA,1550
+ultralytics/cfg/models/v3/yolov3-tiny.yaml,sha256=RcnGwbby2WtgS9cZ3jBybQuAHBDMpgMADPb0x6_bDM0,1252
+ultralytics/cfg/models/v3/yolov3.yaml,sha256=wXUDtWndgxIFbuaoeuY5tFcYZv87ZqOlrdhLKEToiuw,1537
+ultralytics/cfg/models/v5/yolov5-p6.yaml,sha256=3nY2FYH2lz8pfb3J3kFcY09-075f2ErF_p_i9wlyXqE,1923
+ultralytics/cfg/models/v5/yolov5.yaml,sha256=rsBD3nnBQNMcmV7X-twn2BevDGIChErdIcB-3eR-10Y,1550
+ultralytics/cfg/models/v6/yolov6.yaml,sha256=rXb2qPqpm14WVLV4nh8ib-fEjhRa_nuMiCkGdelXbDo,1735
+ultralytics/cfg/models/v8/yolov8-cls.yaml,sha256=rO8Eq8OuTQUkYMQi2AtNgVvtsPHYNxEXr9xT29k-IG8,920
+ultralytics/cfg/models/v8/yolov8-p2.yaml,sha256=jgBsa0-uNwtDHGHJWLVEwuFwxq_twpl9ihRIc4b6bk8,1751
+ultralytics/cfg/models/v8/yolov8-p6.yaml,sha256=oGN0OQ9TtVnttzLd9frFVav8dPxe3OuUA8p4PFRrQmM,1856
+ultralytics/cfg/models/v8/yolov8-pose-p6.yaml,sha256=dtc_uUoXVE83E9Mhf-SSITavuho6A4iQOfQa6sFWzjc,1946
+ultralytics/cfg/models/v8/yolov8-pose.yaml,sha256=kfQnVeZK5sfdRGKuBUpNmIUxXO89qWchQUG-xQHkDDk,1580
+ultralytics/cfg/models/v8/yolov8-rtdetr.yaml,sha256=7jfhW8JwL2HojeNR4PYNvsB3rJkiGBMA7VrZ2xdHJRs,1920
+ultralytics/cfg/models/v8/yolov8-seg.yaml,sha256=QvXQOvQsyYj2GAB-ePB48g4N9jW5JdIkeH800Ev64uI,1490
+ultralytics/cfg/models/v8/yolov8.yaml,sha256=A01V3w3qsOBgAAT9e2AwxXhU_SALEI2IvdoqcPOTnxc,1913
+ultralytics/cfg/trackers/botsort.yaml,sha256=ffPTeDS3HUXCeEOXlzUgdxy1X3wYIDjT0_SlBWX29TY,890
+ultralytics/cfg/trackers/bytetrack.yaml,sha256=FFpmCj7E0xpOde7R-W3zUVcy6hKCHAij9qwp9SAp-pI,694
+ultralytics/data/__init__.py,sha256=TWN-3tE7pPBkGkvAFZoSexBkCw24Fp49swcKeIylHlE,389
+ultralytics/data/annotator.py,sha256=7Y1pinKuZRshlM0oMkwcI_rjRvzfWiHvv0N9PtstWCs,1830
+ultralytics/data/augment.py,sha256=se3Osfm4Y-PmjlWgCmJGhPpuK2RO19-miqDKDu_V388,37439
+ultralytics/data/base.py,sha256=_UyL9oT89BWnqAebRRtQmzy29PXrS6LFAW9QeqyXNQA,12660
+ultralytics/data/build.py,sha256=0eZkKrCbzgUyzWFYJ7pIoRT9jxj5x1AYT4e7zXPUhgc,6531
+ultralytics/data/converter.py,sha256=dmwVQ1xgMT8BJOuKkyMAB61ksLoakoo5Lmi8FSP9fy8,9180
+ultralytics/data/dataset.py,sha256=4c2CUflD2LXcDuK8jESDf_2egITzEial1dac_jwTGCA,13378
+ultralytics/data/loaders.py,sha256=slPkYjgGTz8gZndLyXZ2wWLrl5-R4SnMN_WJ652qDvU,16523
+ultralytics/data/utils.py,sha256=itqzAgkUvpEJpROdEtCGdFlBvs-vBBJXyAYfCrtRITc,25868
+ultralytics/data/dataloaders/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
+ultralytics/engine/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
+ultralytics/engine/exporter.py,sha256=VTU4G4IQQT6uagNokDDcIbce92jthQeZ8cH5UGoT0Hw,46161
+ultralytics/engine/model.py,sha256=chEtvevNBI1wPJRKzEpnPYSQMSYbkMWaRVbiTPVvDRg,19554
+ultralytics/engine/predictor.py,sha256=umAWoScvW_atJBTtSfSGt7sjhejfCxKrRvR7OXcWY_U,16516
+ultralytics/engine/results.py,sha256=FIdvChyT3PS6YrkUkd0eL0PZwapD0BAJJrTyT1DXDAI,24732
+ultralytics/engine/trainer.py,sha256=KjOsNyZqnoClRM9NSLQLnJXnYstUr-F2AJxsPJgUFxI,31186
+ultralytics/engine/validator.py,sha256=CG3RhlL0bA4kQOhmhTklEIx4tRRD2ar0DkVvu3kpO0s,11804
+ultralytics/hub/__init__.py,sha256=6Bqn7WzlD78tVMUpFKCtIpNljRld1qUIjkMjOJFa8xk,4393
+ultralytics/hub/auth.py,sha256=hZqPHQvtgndksDjV8a_BSVIpVkHyssibH8drIzhDdrc,5193
+ultralytics/hub/session.py,sha256=buKfWcZpU7B9mR8JUvYmjlbref6jSBk5jCcl36NucwM,8469
+ultralytics/hub/utils.py,sha256=wNcvkqi8mvccD40XqGAGiAB3ODpVgTn1RT4acFasEzw,9425
+ultralytics/models/__init__.py,sha256=UCT3Y8zazSjXuQrVw0LLrbkO8Itbp6SxaB6Zy5YApCU,99
+ultralytics/models/fastsam/__init__.py,sha256=c3N-XQAJ9Mkw1WGw5gev4xXCUNiOp-_IgxonvqeYSVM,254
+ultralytics/models/fastsam/model.py,sha256=7OioqJHhdZLkr5Wo7eGyPCRmhEs_xlUwETVnTYq4ynU,4557
+ultralytics/models/fastsam/predict.py,sha256=wwbcIHjRka9qNzdV5nRTQzbSSF7MPKoR0CMxjB8P_04,2797
+ultralytics/models/fastsam/prompt.py,sha256=Ib_IuKl13eTh2PQK2UYJsC4qlA75JRk-xBCE-NJ3iNw,16752
+ultralytics/models/fastsam/utils.py,sha256=ZUhphmZblA9cNdc5S-4XMykgtWAsUARsYX-5vtq2u8I,1986
+ultralytics/models/fastsam/val.py,sha256=JJF_bmFN2EC28jV12Xi_G-eSAPJCFn3UgCXaJcl2jkA,12414
+ultralytics/models/nas/__init__.py,sha256=O7qvgqJqoLB1NXwjTNHMJHJRhDwNHS2P_oyUV_b1qq8,179
+ultralytics/models/nas/model.py,sha256=1Eb9V8-rJHEIOsW5o12IDx-uzTSswT84Y2iRXuUzsv0,5206
+ultralytics/models/nas/predict.py,sha256=pYhSuUYM6GIhwVilBiZG_yOHdn9yU10Ixb15KssKTzo,1445
+ultralytics/models/nas/val.py,sha256=CNgkMlNyFZ_H7J5AegF6t2cbKnRUmrhIHAqr-FYNijI,947
+ultralytics/models/rtdetr/__init__.py,sha256=1Zpc6ZcizFO0EMhP8X4m3DG27vDBX4aM4RX0rMSeo6E,197
+ultralytics/models/rtdetr/model.py,sha256=18CdUjPuh5l5mNS9Idb8ex2hB0dibJqaedBmxDnKcXs,7384
+ultralytics/models/rtdetr/predict.py,sha256=vPSNeQW4K5CL6QgUJZIbfc8mSY8h4eWPzJ-pmJcrWio,1839
+ultralytics/models/rtdetr/train.py,sha256=c8mfVY3Mr-zp6cvpvqPOq74cPF8eAOlBCpYh9GUVG4o,2949
+ultralytics/models/rtdetr/val.py,sha256=Lu5H0722XuPvFLi8GrryghumEvtlzRU-pFUKzaosGt4,6561
+ultralytics/models/sam/__init__.py,sha256=CrrWvUKwxqzRp48rpXQo-1Fb9qDxAzNlh9hPdsGuDno,176
+ultralytics/models/sam/amg.py,sha256=Ynox9Hjoj62AHDKczIH_yawbo56P3b3-5bTZZiraXlA,13296
+ultralytics/models/sam/build.py,sha256=C1yj4l7Ec6guzK4Y0Vy43Vbg8CrG_MMYOpb73m6WCG4,4822
+ultralytics/models/sam/model.py,sha256=YEWmhdec0l5tPfxQI66nR8emSNCUdoj7rejLwTc3vAg,2522
+ultralytics/models/sam/predict.py,sha256=eZA3t2LS_KvKithLSCCPsWXhJGWpvQbXn3Nf672sfC8,18948
+ultralytics/models/sam/modules/__init__.py,sha256=mHtJuK4hwF8cuV-VHDc7tp6u6D1gHz2Z7JI8grmQDTs,42
+ultralytics/models/sam/modules/decoders.py,sha256=qgRip-OucTbpwbLm6Fi8cho-XNQ-5g-ZXo7JYt9WuT4,6372
+ultralytics/models/sam/modules/encoders.py,sha256=XxXkQVWNuOfAJPVE-aE8Eb4YjPjAdly9mamQSpGURJc,22545
+ultralytics/models/sam/modules/sam.py,sha256=nQbdy_KD_oOEyc1_T9qBGJMZt4iS3AGMAic-kzGh6qw,7309
+ultralytics/models/sam/modules/tiny_encoder.py,sha256=VLxsubppHyLYZr3lRov5yt1tNhdL_IpJ0k5CTrCDuD4,21760
+ultralytics/models/sam/modules/transformer.py,sha256=Y4LvEu2b36q6_n3dtTAZPtXe1Lgz_xWyE_v0QJG4B7Q,8532
+ultralytics/models/utils/__init__.py,sha256=mHtJuK4hwF8cuV-VHDc7tp6u6D1gHz2Z7JI8grmQDTs,42
+ultralytics/models/utils/loss.py,sha256=6uGn2V_Koe6XZE8fxe-BaZq30a4VfSwdf-7zRBZuRH4,13157
+ultralytics/models/utils/ops.py,sha256=lJTOueDD_TfPbdhLvp5feGrLX6hqYf7yocG5RlYTS68,12990
+ultralytics/models/yolo/__init__.py,sha256=ah-gl1j5rCh1kHsE0Ymv9SXYgRQULp9spQA4KOqdebM,162
+ultralytics/models/yolo/classify/__init__.py,sha256=Lp-uZAD_spvqBw7DQ95tOFvDJROtoiZ0dXHMjpbF6nA,403
+ultralytics/models/yolo/classify/predict.py,sha256=2_7mkXHwlgPiD3XaOGJ0f-qI8TOR2K4sexlj3dcjkeA,1914
+ultralytics/models/yolo/classify/train.py,sha256=aqPuEJRSAKOFq1Ekc5hzAUqRMN99v51hUQ-NdvOyngQ,6856
+ultralytics/models/yolo/classify/val.py,sha256=H8NQa9BdDR0gRQzwbXDSZrBE_A7OrP2CP1M2K-jmHIo,4651
+ultralytics/models/yolo/detect/__init__.py,sha256=1w44OZNPKQOlDIbxRqFDULl7RphH8KAT6cJawNY2uKU,277
+ultralytics/models/yolo/detect/predict.py,sha256=QYgsIA9ZpbeMMGotQWgKyWOPBa3a-6Yv-GHYs2OfW88,1839
+ultralytics/models/yolo/detect/train.py,sha256=OyjwopPwaUBeYnNjohr4tPFisKCL5BqCQ8v2FFbFy5A,5780
+ultralytics/models/yolo/detect/val.py,sha256=43eTCjzjFkTMnK79ZqxdwkEOYj3JcHEgJ4wlTB6aMmA,13521
+ultralytics/models/yolo/pose/__init__.py,sha256=V9epZgW3BaNv6zouhdeX_MMwWqmzPl6V8F8uBIfsfEE,247
+ultralytics/models/yolo/pose/predict.py,sha256=dzSD5_lmpXvAwUgz_thMwnkmIRetOD01ZFSKWBbL4Ww,2369
+ultralytics/models/yolo/pose/train.py,sha256=nNNY1UGLm-NXdv6KEvkdjMF_Awxw-NiGp6Pms0ZfPp4,2788
+ultralytics/models/yolo/pose/val.py,sha256=v_tN8AuaeubroS2pOwOmF1oB_z592rmqoLHZMSnc9Kw,10911
+ultralytics/models/yolo/segment/__init__.py,sha256=TOdf3ju-D5hSi-PYMpETFmv-wyhIRKGujdoeGDx7hbs,295
+ultralytics/models/yolo/segment/predict.py,sha256=Hb7WJGX80AGZl0kmsl0KY_4iId_KTGSZqk_KOPlpRgA,2833
+ultralytics/models/yolo/segment/train.py,sha256=KBCZHIGmoEbZVxk-f1ZT7Cvyu361ikjeEBl7bW_ipcg,2497
+ultralytics/models/yolo/segment/val.py,sha256=2__tkWxUAGAh4zu6TqMzkgXkXcfs_F-fcCEEOJL-M0g,12890
 ultralytics/nn/__init__.py,sha256=7T_GW3YsPg1kA-74UklF2UcabcRyttRZYrCOXiNnJqU,555
-ultralytics/nn/autobackend.py,sha256=GtMOcfQhFQSUgMAONbUaYu6RsRfq_omM4mzXp1Ytlvk,25495
-ultralytics/nn/autoshape.py,sha256=pINXO26MwTFbQxmKKeY6VR-4VXp4w_mUpNmzO0_PaK8,12515
-ultralytics/nn/tasks.py,sha256=pKUcklCb5cvupDqKRf-gToDs9P7rl8wmtl00UsI4ZuM,34213
+ultralytics/nn/autobackend.py,sha256=K-8a5k2SUQv3KdO6qhHiDLUFmoYE_x_ztDVKu6IOQd8,26091
+ultralytics/nn/tasks.py,sha256=ooiQ2KTnTt0F8geqf7yPjhk0MJhHtnDThz_SXaWVDR4,36011
 ultralytics/nn/modules/__init__.py,sha256=cOPs3TyJ14E_hTMrj6Jt2anRmz7h-kAn7J6o4wJH2dM,1587
-ultralytics/nn/modules/block.py,sha256=MEcu0HxDTQTbaFmPEk079OwW7uDM6rkCm9tdsrQQNiY,11816
+ultralytics/nn/modules/block.py,sha256=RR5EQzEdb2UKCYqRh-73GuKAmaIgTSMgMP6KbvOzxPI,11841
 ultralytics/nn/modules/conv.py,sha256=HRlTrzCeEbnuMBxhsCJN1H99EC7lbTMgGvkW7RHOtMM,11647
-ultralytics/nn/modules/head.py,sha256=7fjhP4XCubE-pKZXaah0yV2IFdaG0SushbCVR9VGwRY,15476
+ultralytics/nn/modules/head.py,sha256=S12zYT_6BRfyFG3eDKXKL5mTCojlTbbY_ejdumf7jvY,16098
 ultralytics/nn/modules/transformer.py,sha256=qxRJmBd_bZCkOQ0Et7DalHotlpeIuB0Mv_RkiN3c5Os,15934
 ultralytics/nn/modules/utils.py,sha256=PfcEhPt3O2NUHCHQEADD-WxdS1IDtLeNOBpgflLCDmM,3244
-ultralytics/tracker/__init__.py,sha256=-XzqO0nrPTyUqp7Hu1lAfHaFkny89ffqxDI2w4GL5mI,202
-ultralytics/tracker/track.py,sha256=wqHEOYv_NfzaBo_fzH1of6fTHlIrsmZvLRb-1jRCDhw,2309
-ultralytics/tracker/cfg/botsort.yaml,sha256=ffPTeDS3HUXCeEOXlzUgdxy1X3wYIDjT0_SlBWX29TY,890
-ultralytics/tracker/cfg/bytetrack.yaml,sha256=FFpmCj7E0xpOde7R-W3zUVcy6hKCHAij9qwp9SAp-pI,694
-ultralytics/tracker/trackers/__init__.py,sha256=Q6wmdCj9k5rNPejFjxwZIiBHVY2htG6s4UliNzr5uKA,171
-ultralytics/tracker/trackers/basetrack.py,sha256=Jh-1Q418_4CQfhgTjmGt3bQIVQdN5XJ2AiQx2dsPJuI,1609
-ultralytics/tracker/trackers/bot_sort.py,sha256=4MWTH16dX8_ign8f-SKfD7ivglexFdqJuV4xxgcflBM,5684
-ultralytics/tracker/trackers/byte_tracker.py,sha256=j7YaW8IFFGBYStImApsqWi0G6jGpo7Li0W9qbtXpGgE,14448
-ultralytics/tracker/utils/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-ultralytics/tracker/utils/gmc.py,sha256=mOFq4SF2IPSYzG4EwS3xR4u98ihsZmf4EoaUJzTT5UU,12214
-ultralytics/tracker/utils/kalman_filter.py,sha256=gMc4gDKo_dkMiL-Zkt-U7mTxjK_z4A_njJwk0rBBSOc,18420
-ultralytics/tracker/utils/matching.py,sha256=tWLttmZ_giA8aiIiBJu8zWUGRl26HsguN9PEDN_GBj0,8754
-ultralytics/vit/__init__.py,sha256=B5ApKsW5RjJneI7vy_IJeZIkx12q0rbtrEm3z8SVQKU,142
-ultralytics/vit/rtdetr/__init__.py,sha256=1Zpc6ZcizFO0EMhP8X4m3DG27vDBX4aM4RX0rMSeo6E,197
-ultralytics/vit/rtdetr/model.py,sha256=d9TLa1RlXMHKsPwPRG3EfGI1hd9cJ-sdyfU6QjwkpKs,7414
-ultralytics/vit/rtdetr/predict.py,sha256=wpru2rG4rynO4e7bFYg8qqvIFRpifSBVZ5TfCLVeZ6U,1859
-ultralytics/vit/rtdetr/train.py,sha256=NnBbmOTGVJKwKVSwKuL6_r_cqFGFHEBrYbyCvJDwBw0,2950
-ultralytics/vit/rtdetr/val.py,sha256=_HOsJwf9kjjB_B0nj7-EokXuEhstVCws6JPRNIUpBZY,6572
-ultralytics/vit/sam/__init__.py,sha256=lG2D7u4W-CZPY26Wvr3hvGjgi5AUvNppDnF49yLpb70,173
-ultralytics/vit/sam/amg.py,sha256=Ynox9Hjoj62AHDKczIH_yawbo56P3b3-5bTZZiraXlA,13296
-ultralytics/vit/sam/autosize.py,sha256=vRyMbxRxjKaThM4JYZlpXzTFeEhba8IAdwEsVTxPO_0,3921
-ultralytics/vit/sam/build.py,sha256=mb37cPSkhHC4-Cao9n0fdqqCc78ORudX4v_rUse2nfE,3837
-ultralytics/vit/sam/model.py,sha256=UI6tF1ZXKb6yuEGXbYGzIWM7DXBMNyWSh1tsXABnZIw,2364
-ultralytics/vit/sam/predict.py,sha256=V3XDsCth6LQ0d4iywsBigiEmgb9jVf4uty6OXdu7G0k,2182
-ultralytics/vit/sam/modules/__init__.py,sha256=mHtJuK4hwF8cuV-VHDc7tp6u6D1gHz2Z7JI8grmQDTs,42
-ultralytics/vit/sam/modules/decoders.py,sha256=qgRip-OucTbpwbLm6Fi8cho-XNQ-5g-ZXo7JYt9WuT4,6372
-ultralytics/vit/sam/modules/encoders.py,sha256=XxXkQVWNuOfAJPVE-aE8Eb4YjPjAdly9mamQSpGURJc,22545
-ultralytics/vit/sam/modules/mask_generator.py,sha256=uQB1n_JCQP-1Bf0mfTgKOJv5JuJke7gc5W3ZJ-RHdUI,15293
-ultralytics/vit/sam/modules/prompt_predictor.py,sha256=1y8tOlOB0JatfPHts1s_oxxJcr8leHPHYkQIn9VLwCk,11244
-ultralytics/vit/sam/modules/sam.py,sha256=nQbdy_KD_oOEyc1_T9qBGJMZt4iS3AGMAic-kzGh6qw,7309
-ultralytics/vit/sam/modules/transformer.py,sha256=Y4LvEu2b36q6_n3dtTAZPtXe1Lgz_xWyE_v0QJG4B7Q,8532
-ultralytics/vit/utils/__init__.py,sha256=mHtJuK4hwF8cuV-VHDc7tp6u6D1gHz2Z7JI8grmQDTs,42
-ultralytics/vit/utils/loss.py,sha256=QC_GKl-kPw1cWqNhKaHDhCcx_tlG2N7p-Wr-qq3niOQ,13187
-ultralytics/vit/utils/ops.py,sha256=g_kbdM50roj71KBkVlk93Hf3VjK-Spy7-IoCCNoVMpE,13000
+ultralytics/trackers/__init__.py,sha256=dR9unDaRBd6MgMnTKxqJZ0KsJ8BeFGg-LTYQvC7BnIY,227
+ultralytics/trackers/basetrack.py,sha256=Jh-1Q418_4CQfhgTjmGt3bQIVQdN5XJ2AiQx2dsPJuI,1609
+ultralytics/trackers/bot_sort.py,sha256=7b8RUBgiTDup4XlaKbl0c9-4fDkyar_VfyYeX__98r0,5681
+ultralytics/trackers/byte_tracker.py,sha256=_2elPbLtR_muGDqmN6hUOdj82m88PFCBAj7vs4J77Ts,14446
+ultralytics/trackers/track.py,sha256=NhoNZxEv8cN-myRo17PrKhoR2nFM8lWN3Ne8qrC1zJQ,2324
+ultralytics/trackers/utils/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
+ultralytics/trackers/utils/gmc.py,sha256=vzuRF6yBwN17ZlCqWXFc_SJuVhfbfCIsWOgPRI_SF1c,12209
+ultralytics/trackers/utils/kalman_filter.py,sha256=gMc4gDKo_dkMiL-Zkt-U7mTxjK_z4A_njJwk0rBBSOc,18420
+ultralytics/trackers/utils/matching.py,sha256=N-qZsjmhk6_JfNpwhjX9MTkBZ9Rm-_q1gZjpIVoy7WM,8749
+ultralytics/utils/__init__.py,sha256=DplmuQ2W2U5aR4MM2Xal7xXxrkQ7-YNryHJb2MMQDPY,30202
+ultralytics/utils/autobatch.py,sha256=gX1mFbPhKhGYzqLqmvMJVRHRRMIit9tbEkA5bcUY-PA,3864
+ultralytics/utils/benchmarks.py,sha256=zvj74bxrtCwwkZiyEPTjxkNHPDy2Ocf7AWGMNasGqns,16087
+ultralytics/utils/checks.py,sha256=-2sQYtNuzYvQ8t8mzykxtfiqClptm8AISQbUskRXGOc,19491
+ultralytics/utils/dist.py,sha256=C-C6eTOFuFV391juWMx-8ldFDty0dOS7W7eS-6vdOjU,2591
+ultralytics/utils/downloads.py,sha256=DrmB69W9BR8r565grjOFHtIyYYim1isetGqdws7m0cI,12818
+ultralytics/utils/errors.py,sha256=cfSux6tp3fVLtkv4bShfHyZHPncbo3jbQsRR-zUDcoU,312
+ultralytics/utils/files.py,sha256=PFHeKx0HzmyR5wZSYzhxq_X9HHGwwNuEV5PhvKouw_0,5450
+ultralytics/utils/instance.py,sha256=iGxITi9ydxom1GB-VjVuUbIY4jgCax3aiWwY_uXoRIQ,14645
+ultralytics/utils/loss.py,sha256=yfQ0cE35lY5GF7V-0bP4QPOqzGWIjJBBnoF-AweBQak,19144
+ultralytics/utils/metrics.py,sha256=3GOKuVUkAPo1fLHtOBawAeq4trVyqlQy8hV8isPdTY4,42325
+ultralytics/utils/ops.py,sha256=Iecy95EclptUHN2dvBVrxHh0mincbpNGP75HZj0PdfE,29212
+ultralytics/utils/patches.py,sha256=L0Bm8ma7BVyuO8hW-XDTBzL06GBJ9tl2ov4cAfGOh60,1246
+ultralytics/utils/plotting.py,sha256=AYdvMh8I1jYCeo1kGl9cUu-a5jVfVBhX-yVMgZ_a6_Q,24845
+ultralytics/utils/tal.py,sha256=BFovrLm0m5_Pv-hFzYMAD3oonS1MvDqdy51f_Cx6Aq8,13645
+ultralytics/utils/torch_utils.py,sha256=GzQLQSlnN_Cs8P0kjqpqy7HsTTqNavWQghCA-KXz7pI,23082
+ultralytics/utils/tuner.py,sha256=oHtPYVTtum8As2cwclcsjBd5H_x_dNFiaUzFyMG1XhI,5403
+ultralytics/utils/callbacks/__init__.py,sha256=nhrnMPpPDb5fgqw42w8e7fC5TjEPC-jp04dpQtaQtkU,214
+ultralytics/utils/callbacks/base.py,sha256=VpiMIW9qiyncMq9cLRmm5WGr38On0LVTK2XNDmliEbE,5593
+ultralytics/utils/callbacks/clearml.py,sha256=izRkp9wZvFUskG6ofYV_vcuPNSAYqiQSJc5USeEuurI,5974
+ultralytics/utils/callbacks/comet.py,sha256=jDfK-ntD9ShFJJNAXiB0Ra8SHKuKxSJWidftQrlojt4,13115
+ultralytics/utils/callbacks/dvc.py,sha256=8cRgEYyom8844U2tb_Te_YGfdme6C_I1XzkGbw9ysvc,4386
+ultralytics/utils/callbacks/hub.py,sha256=UZkEJzNQ05bOJXCJJzBveTA-SbYNnT2LT6zFtov9hh0,3363
+ultralytics/utils/callbacks/mlflow.py,sha256=o4xiScF0TJNFBuC3EhO3XRtOd3gd7GZi9ISLfNBPhfE,2701
+ultralytics/utils/callbacks/neptune.py,sha256=fr5vf9kXLTwcL41Ag9B_Paxf9SZsWTm4u6ninvKCQxY,3751
+ultralytics/utils/callbacks/raytune.py,sha256=ckLwRlgkmwoEieNDoL3TFCf-4by5hWNHj5AiJ5v9GGg,608
+ultralytics/utils/callbacks/tensorboard.py,sha256=z3ho0BLxEfc0K0YdiCYbYaMlo-jCTbixCm1cHHYYD_A,1716
+ultralytics/utils/callbacks/wb.py,sha256=CKnErGwfNTvfGc0RhaAy5Ax9hwy3Ijl97_SNzFM9jy0,2252
 ultralytics/yolo/__init__.py,sha256=iPyUTxLglm5hj3cN6AvcZDLESMExho7eV8jD7pMYEDE,94
-ultralytics/yolo/cfg/__init__.py,sha256=9EQwXH8tLYcU2eEeuqUL2FtOeTbeil183D18-oawwtc,18414
-ultralytics/yolo/cfg/default.yaml,sha256=R_LRUpdR0JwpEej-NpZUjLUL57f0I2xFIxfF4yfwoJc,7363
-ultralytics/yolo/data/__init__.py,sha256=HQ0eH1ISZOYtqHgUBMtyPq00RhfR3zwBUfqdWRq_lTw,458
-ultralytics/yolo/data/annotator.py,sha256=ecARKmikLYHxG2Hao6IHLK_1-vtZn06RpzMKKO4mAjU,2341
-ultralytics/yolo/data/augment.py,sha256=OGtMnPmeJjxtsPFXd_B3Lf5kbJUXCzFGXPFase70Ea8,37201
-ultralytics/yolo/data/base.py,sha256=UW9Sc100h4BhoQ535OFFEHTw-P44vta8MFUrW8mkEDk,12649
-ultralytics/yolo/data/build.py,sha256=9eVmHXp79QpTLVB-sbaZSM6Qaet9YbiiiTW4TviAGEs,6574
-ultralytics/yolo/data/converter.py,sha256=itcD1BgwqtWcFXf669aQaa-o0sfvV9zGTEgv3_UsrWk,9190
-ultralytics/yolo/data/dataset.py,sha256=gfCKjo5iBvPFvdzh8rs1EOMJa4mFcvbOvisGJpqpjZg,13367
-ultralytics/yolo/data/dataset_wrappers.py,sha256=a5uwWLhWDRCG5VFto6I3zknxaElxnrcOHiuTAKmmmYg,1776
-ultralytics/yolo/data/utils.py,sha256=IhF_OkZ0407eNaCDI6Okjn-aTFfXxUUzmP2SdkYKfVk,24052
-ultralytics/yolo/data/dataloaders/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-ultralytics/yolo/data/dataloaders/stream_loaders.py,sha256=52HRTn6M_GACU0_BeM9UWbnQ1hvKTkuXvAaAnsID73w,16303
-ultralytics/yolo/data/dataloaders/v5augmentations.py,sha256=t39RSuHCDCX4MV9EkIoJWaPOjkYlNbbpi-30fg725rw,17646
-ultralytics/yolo/data/dataloaders/v5loader.py,sha256=RoBWZljfI7Zme_gqF6nq6-2WkMOF7MuL-YCYUpk0bAU,51260
-ultralytics/yolo/engine/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-ultralytics/yolo/engine/exporter.py,sha256=jXOX_qCBMKdnWKx2L9ZR3y1qasUFOW-H3MYFfVJo2wQ,43862
-ultralytics/yolo/engine/model.py,sha256=giR-uKjCj41wD6tD64lkf_Pero2R0xzsN2pIrtSwACI,19808
-ultralytics/yolo/engine/predictor.py,sha256=bCFBEK4MzWUXuZFjdFjwMHvNpWQ4ohIGLtGBLVNGjww,16433
-ultralytics/yolo/engine/results.py,sha256=0EctB4LPxUakB-aNVVk-YuxAAbR-AjFL0tnlcmeKYww,24545
-ultralytics/yolo/engine/trainer.py,sha256=MG7e0KzAuuyK34w0n2PAYFif37g3VC3LkulVCI35_rA,31263
-ultralytics/yolo/engine/validator.py,sha256=ZbOde8CClZQgTFXJ68v4DP_etR1prJYrwMayyKUcfTk,11715
-ultralytics/yolo/fastsam/__init__.py,sha256=c3N-XQAJ9Mkw1WGw5gev4xXCUNiOp-_IgxonvqeYSVM,254
-ultralytics/yolo/fastsam/model.py,sha256=OaBpPmpOzKWL-3MxHD8-6iUq9VnxMw8SGtrv1KazwXU,4583
-ultralytics/yolo/fastsam/predict.py,sha256=NoqX3tCbLMvqRExkmkKLMOvTXRFfL7jurx-oBBk8T0o,2801
-ultralytics/yolo/fastsam/prompt.py,sha256=mcGkw6VN2zGvCt6Jlel2MAi0NcoeSJSN32ra1YZgwmE,16757
-ultralytics/yolo/fastsam/utils.py,sha256=HaJXAnkvNvXN5Odln2NzwydYz24gwBx0tJUnswk_kAs,1897
-ultralytics/yolo/fastsam/val.py,sha256=lK7Ko18KeugCaNaUbYcAK815t252oD9_2TN40o-FXXE,12430
-ultralytics/yolo/nas/__init__.py,sha256=O7qvgqJqoLB1NXwjTNHMJHJRhDwNHS2P_oyUV_b1qq8,179
-ultralytics/yolo/nas/model.py,sha256=JlnKZbzBA0MEc7niLuavHDlt6BRXIjHX0HWMV5-XBGk,5227
-ultralytics/yolo/nas/predict.py,sha256=WbO9fxO2o_WA3nM7KkgHDgr2eJWdYsT725BcJ6_Qtrs,1465
-ultralytics/yolo/nas/val.py,sha256=D9jIFcafuD768X5l9Q56hrXIz-Jl45uEReIsPyrlNgs,953
-ultralytics/yolo/utils/__init__.py,sha256=XWWfKctQ5_gEVqlP8z13xctVGUqbW3bE4aS3AITuSC0,28182
-ultralytics/yolo/utils/autobatch.py,sha256=QQKiecejp-UV2YZ2KsvVE5IuJelAJX_lxdl9oop1Dhk,3874
-ultralytics/yolo/utils/benchmarks.py,sha256=YEGPKrZcFvKSp8FtrccGMg6rbjdZmtGbo1vyG48gqac,15865
-ultralytics/yolo/utils/checks.py,sha256=wgtub8uk9myRIzOxV2ggRjpDuMtYiGuO6hudKXFrBm8,18123
-ultralytics/yolo/utils/dist.py,sha256=qavAVcOeyLV0z31Spo1G9batXljnYzp1NN8YWFoBTxk,2596
-ultralytics/yolo/utils/downloads.py,sha256=wHxpD_4Lf6gJrUcLKsw0a0C8ZmheqBkgRLXbDNdiPTU,12788
-ultralytics/yolo/utils/errors.py,sha256=u8NUGZbWVrr4O3ez6UXnabNb_zRwJUPJqDOUSpmX3-k,317
-ultralytics/yolo/utils/files.py,sha256=7431UxyB9-D2vhkp8JqO_IXApYoWsWfSH3H5JNemKjU,3588
-ultralytics/yolo/utils/instance.py,sha256=c3HqPoXs0zmrWdGdp0gF5eLzTlKYE25zDmA-1s5UyK8,14622
-ultralytics/yolo/utils/loss.py,sha256=CDXPVoM6RkuS5HkzqT7wiT4v2lQBBjaNe-LohsWiN5k,19159
-ultralytics/yolo/utils/metrics.py,sha256=5_7IPCkQ3ALIT0x2RIx9tuqEEeq3L9xBozXCMxHL1zo,42292
-ultralytics/yolo/utils/ops.py,sha256=ehzb6MZjlqdfP-plS5vLjD7fHZmX4GRQ_TiDl29fVSA,28304
-ultralytics/yolo/utils/patches.py,sha256=lxG_qZlTxiCcKWrVFtvSVA8WULRa483fOJ4ffYyGQ50,1241
-ultralytics/yolo/utils/plotting.py,sha256=WE2DaQHvcpR6GtZ2XoHjk5Fa6Ygjo-fjbf-1qLlBYQc,24551
-ultralytics/yolo/utils/tal.py,sha256=BFovrLm0m5_Pv-hFzYMAD3oonS1MvDqdy51f_Cx6Aq8,13645
-ultralytics/yolo/utils/torch_utils.py,sha256=41mAnRv4f9O7NwFIEdYKvbXeVK2v2kpoDX9rEYzq3og,22348
-ultralytics/yolo/utils/tuner.py,sha256=j8zL0yrrkxf0CzuzJDK-uEJGx7boYbnyEQZCJWZoSus,5413
-ultralytics/yolo/utils/callbacks/__init__.py,sha256=nhrnMPpPDb5fgqw42w8e7fC5TjEPC-jp04dpQtaQtkU,214
-ultralytics/yolo/utils/callbacks/base.py,sha256=VpiMIW9qiyncMq9cLRmm5WGr38On0LVTK2XNDmliEbE,5593
-ultralytics/yolo/utils/callbacks/clearml.py,sha256=VYtuNTlB8M5rIFQupm53uhDEXe8qZrrOjXXHTI0bIUw,5902
-ultralytics/yolo/utils/callbacks/comet.py,sha256=Yev4FX-aOXLjCV273-EcJFibZZJ8d4TkgvTekDcO3fM,13045
-ultralytics/yolo/utils/callbacks/dvc.py,sha256=P55vfMvAS_4ZCnmr-3xJQ2SlU16OzbfGTbrBGb6x160,4320
-ultralytics/yolo/utils/callbacks/hub.py,sha256=Z-F48IcG2PtlJo3mnzp31YdXrS8nyMn6004-Ke6NLyM,3310
-ultralytics/yolo/utils/callbacks/mlflow.py,sha256=bzj44B7euQ7ZA9sJBUJeEBT75JJZnf1rJaaNqnqdALM,2620
-ultralytics/yolo/utils/callbacks/neptune.py,sha256=lBvXuXkgHfiDkdeRAM7TbK7xQBSAKxYQQAwHt7SMNi8,3679
-ultralytics/yolo/utils/callbacks/raytune.py,sha256=wHdhqCDtK1GbDQTixpxJPzS8NWtEkXpkdIlhQmEZdHg,495
-ultralytics/yolo/utils/callbacks/tensorboard.py,sha256=05UPBY4sBzh2FaQmWgav0sFgoLlhLqJg-5gfSz7AYpY,1525
-ultralytics/yolo/utils/callbacks/wb.py,sha256=VOCLNESyUGLnIwc38mZU0sapP8CroXq4bu4Ulm5d9F8,2188
-ultralytics/yolo/v8/__init__.py,sha256=RyCznTHg-tT2UbALd0KE_ymzlcgWclG1KABMhym4YyQ,158
-ultralytics/yolo/v8/classify/__init__.py,sha256=jIs4eZagpiogjkVRo44diEFxej_XJmggysQ968fv2HQ,391
-ultralytics/yolo/v8/classify/predict.py,sha256=FgZt5PC6d861OKP37wHqxr1AkP59q1bHDMi0-qP1oWU,1929
-ultralytics/yolo/v8/classify/train.py,sha256=0eaAPTt5lkXiHnlgqBgLZ7cc77lWAwYLHdwiWCWuF2k,6875
-ultralytics/yolo/v8/classify/val.py,sha256=b8VJuuAUDbOwTmg5IEcn_NqTFJ5BCubc9FGdY6lpq8c,4676
-ultralytics/yolo/v8/detect/__init__.py,sha256=1w44OZNPKQOlDIbxRqFDULl7RphH8KAT6cJawNY2uKU,277
-ultralytics/yolo/v8/detect/predict.py,sha256=BbsRQpswiBvF88uWuOfwNguRbyXIJO7HZ7WYBblcCvY,1854
-ultralytics/yolo/v8/detect/train.py,sha256=uorDhWqt1GMjUnc3m4xamnEDROHGWcty3cJFUANbpok,7199
-ultralytics/yolo/v8/detect/val.py,sha256=uZGSs7Vgq8UGEgz-oMsVSYarfKz_czW0AS1S1FJvdns,14730
-ultralytics/yolo/v8/pose/__init__.py,sha256=V9epZgW3BaNv6zouhdeX_MMwWqmzPl6V8F8uBIfsfEE,247
-ultralytics/yolo/v8/pose/predict.py,sha256=0XTJWgxYZMzNyKHgswQFMSnzHYcfJbbrIQ7eysyCPtI,2375
-ultralytics/yolo/v8/pose/train.py,sha256=lfim51E8qvUmcuxa-fGVw8nPOr6Z6Nisv5AXLm6qVz8,2790
-ultralytics/yolo/v8/pose/val.py,sha256=RSVwO4D63s94dJiYrZJEIz6sInpUMGmgqmpuY_Zrfqs,10927
-ultralytics/yolo/v8/segment/__init__.py,sha256=TOdf3ju-D5hSi-PYMpETFmv-wyhIRKGujdoeGDx7hbs,295
-ultralytics/yolo/v8/segment/predict.py,sha256=QSwXjC7sq7l7kT0W1UrwGZrUXNqkveRGRIsRnlQlWYM,2839
-ultralytics/yolo/v8/segment/train.py,sha256=Ag0uHlJp297dljtECqZ5VVfnWj7m86oD1Dy1_s4vIEs,2499
-ultralytics/yolo/v8/segment/val.py,sha256=R3D_itui1lU0iXC1aqmZvmwGX77oJt--bQ7MhTErsBY,12906
-pyppbox_ultralytics-8.0.132.dist-info/LICENSE,sha256=DZak_2itbUtvHzD3E7GNUYSRK6jdOJ-GqncQ2weavLA,34523
-pyppbox_ultralytics-8.0.132.dist-info/METADATA,sha256=eg1q8UAw7xBbx3utdZdJxy2BxnrKPQKHEas84Q8SILg,3968
-pyppbox_ultralytics-8.0.132.dist-info/WHEEL,sha256=pkctZYzUS4AYVn6dJ-7367OJZivF2e8RA9b_ZBjif18,92
-pyppbox_ultralytics-8.0.132.dist-info/entry_points.txt,sha256=Ck1F6qKNokeHozQD5pmaFgXHL6dKyC2qCdyXao2e6Yg,103
-pyppbox_ultralytics-8.0.132.dist-info/top_level.txt,sha256=XP49TwiMw4QGsvTLSYiJhz1xF_k7ev5mQ8jJXaXi45Q,12
-pyppbox_ultralytics-8.0.132.dist-info/RECORD,,
+ultralytics/yolo/cfg/__init__.py,sha256=i2_j4OZ1-IaQqAWkFJgf4uDH18cMs8t-J6T-fQGnP7Q,373
+ultralytics/yolo/data/__init__.py,sha256=yf8vCgHeoINeZBjKOT_SmCJozgAcoUEUHsKfVNHKCI0,823
+ultralytics/yolo/engine/__init__.py,sha256=TAHs06pR9q32XhqRfnS7bk_n7JCZMoJD9LlcySx-7M8,385
+ultralytics/yolo/utils/__init__.py,sha256=GK4bnt8wEbsJPJ5MTQGwR_6W4Nr8OdAaFHimCQ4YcSQ,647
+ultralytics/yolo/v8/__init__.py,sha256=cPnEgfUp79LakWJRJhwMMWSMba31TpiNXUDe2dugHCI,387
+pyppbox_ultralytics-8.0.142.dist-info/LICENSE,sha256=DZak_2itbUtvHzD3E7GNUYSRK6jdOJ-GqncQ2weavLA,34523
+pyppbox_ultralytics-8.0.142.dist-info/METADATA,sha256=BdXxafkoRZwfcFWgCRPqMfOnNYHXDagswuZn8Qkzuc8,3813
+pyppbox_ultralytics-8.0.142.dist-info/WHEEL,sha256=AtBG6SXL3KF_v0NxLf0ehyVOh0cold-JbJYXNGorC6Q,92
+pyppbox_ultralytics-8.0.142.dist-info/entry_points.txt,sha256=Ck1F6qKNokeHozQD5pmaFgXHL6dKyC2qCdyXao2e6Yg,103
+pyppbox_ultralytics-8.0.142.dist-info/top_level.txt,sha256=XP49TwiMw4QGsvTLSYiJhz1xF_k7ev5mQ8jJXaXi45Q,12
+pyppbox_ultralytics-8.0.142.dist-info/RECORD,,
```


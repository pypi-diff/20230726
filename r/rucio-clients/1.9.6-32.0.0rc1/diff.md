# Comparing `tmp/rucio-clients-1.9.6.tar.gz` & `tmp/rucio-clients-32.0.0rc1.tar.gz`

## filetype from file(1)

```diff
@@ -1 +1 @@
-gzip compressed data, was "dist/rucio-clients-1.9.6.tar", last modified: Tue Feb  7 09:03:44 2017, max compression
+gzip compressed data, was "rucio-clients-32.0.0rc1.tar", last modified: Wed Jul 26 15:14:47 2023, max compression
```

## Comparing `rucio-clients-1.9.6.tar` & `rucio-clients-32.0.0rc1.tar`

### file list

```diff
@@ -1,788 +1,202 @@
-drwxr-xr-x   0 barisits (51071) zp        (1307)        0 2017-02-07 09:03:44.000000 rucio-clients-1.9.6/
-drwxr-xr-x   0 barisits (51071) zp        (1307)        0 2017-02-07 09:03:44.000000 rucio-clients-1.9.6/bin/
--rwxr-xr-x   0 barisits (51071) zp        (1307)   121517 2017-01-31 11:31:28.000000 rucio-clients-1.9.6/bin/rucio
--rwxr-xr-x   0 barisits (51071) zp        (1307)    44207 2016-11-28 16:40:28.000000 rucio-clients-1.9.6/bin/rucio-admin
-drwxr-xr-x   0 barisits (51071) zp        (1307)        0 2017-02-07 09:03:44.000000 rucio-clients-1.9.6/doc/
-drwxr-xr-x   0 barisits (51071) zp        (1307)        0 2017-02-07 09:03:44.000000 rucio-clients-1.9.6/doc/atlasnote/
--rwxr-xr-x   0 barisits (51071) zp        (1307)   164832 2016-11-25 15:17:13.000000 rucio-clients-1.9.6/doc/atlasnote/AN_atlaslogo.pdf
--rwxr-xr-x   0 barisits (51071) zp        (1307)    88058 2016-11-25 15:17:13.000000 rucio-clients-1.9.6/doc/atlasnote/AN_cernlogo.pdf
--rwxr-xr-x   0 barisits (51071) zp        (1307)     2877 2016-11-25 15:17:13.000000 rucio-clients-1.9.6/doc/atlasnote/AtlasRucioNote.aux
--rw-r--r--   0 barisits (51071) zp        (1307)   440060 2016-11-25 15:17:13.000000 rucio-clients-1.9.6/doc/atlasnote/AtlasRucioNote.pdf
--rwxr-xr-x   0 barisits (51071) zp        (1307)    23795 2016-11-25 15:17:13.000000 rucio-clients-1.9.6/doc/atlasnote/AtlasRucioNote.tex
--rw-r--r--   0 barisits (51071) zp        (1307)    65762 2016-11-25 15:17:13.000000 rucio-clients-1.9.6/doc/atlasnote/accounts.graffle
--rw-r--r--   0 barisits (51071) zp        (1307)    31807 2016-11-25 15:17:13.000000 rucio-clients-1.9.6/doc/atlasnote/accounts.pdf
--rw-r--r--   0 barisits (51071) zp        (1307)    27065 2016-11-25 15:17:13.000000 rucio-clients-1.9.6/doc/atlasnote/accounts.png
--rw-r--r--   0 barisits (51071) zp        (1307)   125132 2016-11-25 15:17:13.000000 rucio-clients-1.9.6/doc/atlasnote/aggregation_hierarchy.graffle
--rw-r--r--   0 barisits (51071) zp        (1307)    44183 2016-11-25 15:17:13.000000 rucio-clients-1.9.6/doc/atlasnote/aggregation_hierarchy.pdf
--rwxr-xr-x   0 barisits (51071) zp        (1307)     6980 2016-11-25 15:17:13.000000 rucio-clients-1.9.6/doc/atlasnote/atlasnote.cls
--rwxr-xr-x   0 barisits (51071) zp        (1307)     6557 2016-11-25 15:17:13.000000 rucio-clients-1.9.6/doc/atlasnote/booktabs.sty
--rw-r--r--   0 barisits (51071) zp        (1307)   132902 2016-11-25 15:17:13.000000 rucio-clients-1.9.6/doc/atlasnote/dataset_hierarchy.graffle
--rw-r--r--   0 barisits (51071) zp        (1307)    44639 2016-11-25 15:17:13.000000 rucio-clients-1.9.6/doc/atlasnote/dataset_hierarchy.pdf
--rwxr-xr-x   0 barisits (51071) zp        (1307)    18775 2016-11-25 15:17:13.000000 rucio-clients-1.9.6/doc/atlasnote/fncychap.sty
--rwxr-xr-x   0 barisits (51071) zp        (1307)    14665 2016-11-25 15:17:13.000000 rucio-clients-1.9.6/doc/atlasnote/sphinx.sty
--rwxr-xr-x   0 barisits (51071) zp        (1307)     2073 2016-11-25 15:17:13.000000 rucio-clients-1.9.6/doc/atlasnote/sphinxhowto.cls
--rwxr-xr-x   0 barisits (51071) zp        (1307)     3421 2016-11-25 15:17:13.000000 rucio-clients-1.9.6/doc/atlasnote/sphinxmanual.cls
-drwxr-xr-x   0 barisits (51071) zp        (1307)        0 2017-02-07 09:03:44.000000 rucio-clients-1.9.6/doc/build/
-drwxr-xr-x   0 barisits (51071) zp        (1307)        0 2017-02-07 09:03:44.000000 rucio-clients-1.9.6/doc/build/doctrees/
-drwxr-xr-x   0 barisits (51071) zp        (1307)        0 2017-02-07 09:03:44.000000 rucio-clients-1.9.6/doc/build/doctrees/man/
--rw-r--r--   0 barisits (51071) zp        (1307)    13923 2016-11-25 15:03:17.000000 rucio-clients-1.9.6/doc/build/doctrees/man/rucio-admin.doctree
--rw-r--r--   0 barisits (51071) zp        (1307)    21710 2016-11-25 15:03:17.000000 rucio-clients-1.9.6/doc/build/doctrees/man/rucio.doctree
-drwxr-xr-x   0 barisits (51071) zp        (1307)        0 2017-02-07 09:03:44.000000 rucio-clients-1.9.6/doc/build/doctrees/rest/
--rw-r--r--   0 barisits (51071) zp        (1307)     6101 2016-11-25 15:03:17.000000 rucio-clients-1.9.6/doc/build/doctrees/rest/attach_dids_to_dids.doctree
-drwxr-xr-x   0 barisits (51071) zp        (1307)        0 2017-02-07 09:03:44.000000 rucio-clients-1.9.6/doc/build/doctrees/usecases/
--rw-r--r--   0 barisits (51071) zp        (1307)     4822 2016-11-25 15:03:17.000000 rucio-clients-1.9.6/doc/build/doctrees/usecases/add_account_identity.doctree
--rw-r--r--   0 barisits (51071) zp        (1307)     4818 2016-11-25 15:03:17.000000 rucio-clients-1.9.6/doc/build/doctrees/usecases/add_metadata_dataset.doctree
--rw-r--r--   0 barisits (51071) zp        (1307)     4785 2016-11-25 15:03:17.000000 rucio-clients-1.9.6/doc/build/doctrees/usecases/add_metadata_file.doctree
--rw-r--r--   0 barisits (51071) zp        (1307)     5249 2016-11-25 15:03:17.000000 rucio-clients-1.9.6/doc/build/doctrees/usecases/add_scope_to_account.doctree
--rw-r--r--   0 barisits (51071) zp        (1307)     8508 2016-11-25 15:03:17.000000 rucio-clients-1.9.6/doc/build/doctrees/usecases/add_subscription.doctree
--rw-r--r--   0 barisits (51071) zp        (1307)    12024 2016-11-25 15:03:17.000000 rucio-clients-1.9.6/doc/build/doctrees/usecases/authentication.doctree
--rw-r--r--   0 barisits (51071) zp        (1307)     4960 2016-11-25 15:03:17.000000 rucio-clients-1.9.6/doc/build/doctrees/usecases/consistency_file_between_storage_and_rucio.doctree
--rw-r--r--   0 barisits (51071) zp        (1307)     5038 2016-11-25 15:03:17.000000 rucio-clients-1.9.6/doc/build/doctrees/usecases/delete_file_replica_from_storage.doctree
--rw-r--r--   0 barisits (51071) zp        (1307)     4927 2016-11-25 15:03:17.000000 rucio-clients-1.9.6/doc/build/doctrees/usecases/detect_site_reach_watermark.doctree
--rw-r--r--   0 barisits (51071) zp        (1307)     5230 2016-11-25 15:03:17.000000 rucio-clients-1.9.6/doc/build/doctrees/usecases/download_all_files_from_a_given_list_of_file_replicas_from_rucio.doctree
--rw-r--r--   0 barisits (51071) zp        (1307)     4988 2016-11-25 15:03:17.000000 rucio-clients-1.9.6/doc/build/doctrees/usecases/download_all_files_from_a_given_list_of_files_from_rucio.doctree
--rw-r--r--   0 barisits (51071) zp        (1307)     5030 2016-11-25 15:03:17.000000 rucio-clients-1.9.6/doc/build/doctrees/usecases/download_all_files_in_a_dataset_from_rucio.doctree
--rw-r--r--   0 barisits (51071) zp        (1307)     4835 2016-11-25 15:03:17.000000 rucio-clients-1.9.6/doc/build/doctrees/usecases/download_files_from_rucio.doctree
--rw-r--r--   0 barisits (51071) zp        (1307)     4400 2016-11-25 15:03:17.000000 rucio-clients-1.9.6/doc/build/doctrees/usecases/obsolete_dataset.doctree
--rw-r--r--   0 barisits (51071) zp        (1307)     5231 2016-11-25 15:03:17.000000 rucio-clients-1.9.6/doc/build/doctrees/usecases/register_account.doctree
--rw-r--r--   0 barisits (51071) zp        (1307)     5081 2016-11-25 15:03:17.000000 rucio-clients-1.9.6/doc/build/doctrees/usecases/register_file_already_on_storage_system.doctree
--rw-r--r--   0 barisits (51071) zp        (1307)     4524 2016-11-25 15:03:17.000000 rucio-clients-1.9.6/doc/build/doctrees/usecases/register_transfer_request_file_fts.doctree
--rw-r--r--   0 barisits (51071) zp        (1307)     4380 2016-11-25 15:03:17.000000 rucio-clients-1.9.6/doc/build/doctrees/usecases/remove_replication_rules_from_file.doctree
--rw-r--r--   0 barisits (51071) zp        (1307)     5772 2016-11-25 15:03:17.000000 rucio-clients-1.9.6/doc/build/doctrees/usecases/reupload_after_failure.doctree
--rw-r--r--   0 barisits (51071) zp        (1307)     7617 2016-11-25 15:03:17.000000 rucio-clients-1.9.6/doc/build/doctrees/usecases/search.doctree
--rw-r--r--   0 barisits (51071) zp        (1307)     3221 2016-11-25 15:03:17.000000 rucio-clients-1.9.6/doc/build/doctrees/usecases/select_unwanted_files_for_deletion.doctree
--rw-r--r--   0 barisits (51071) zp        (1307)     4412 2016-11-25 15:03:17.000000 rucio-clients-1.9.6/doc/build/doctrees/usecases/set_replication_rule_to_file.doctree
--rw-r--r--   0 barisits (51071) zp        (1307)     5464 2016-11-25 15:03:17.000000 rucio-clients-1.9.6/doc/build/doctrees/usecases/upload_file_with_replication_rule.doctree
--rw-r--r--   0 barisits (51071) zp        (1307)     7457 2016-11-25 15:03:17.000000 rucio-clients-1.9.6/doc/build/doctrees/usecases/usecase_upload_file_into_rucio.doctree
--rw-r--r--   0 barisits (51071) zp        (1307)     5107 2016-11-25 15:03:17.000000 rucio-clients-1.9.6/doc/build/doctrees/usecases/where_are_the_replicas_for_a_file.doctree
--rw-r--r--   0 barisits (51071) zp        (1307)     7150 2016-11-25 15:03:17.000000 rucio-clients-1.9.6/doc/build/doctrees/Acronyms_and_Abbreviations.doctree
--rw-r--r--   0 barisits (51071) zp        (1307)     2393 2016-11-25 15:03:16.000000 rucio-clients-1.9.6/doc/build/doctrees/Architecture.doctree
--rw-r--r--   0 barisits (51071) zp        (1307)    21792 2016-11-25 15:03:17.000000 rucio-clients-1.9.6/doc/build/doctrees/Comparison_matrix.doctree
--rw-r--r--   0 barisits (51071) zp        (1307)    81561 2016-11-25 15:03:17.000000 rucio-clients-1.9.6/doc/build/doctrees/REST_Account.doctree
--rw-r--r--   0 barisits (51071) zp        (1307)     8685 2016-11-25 15:03:17.000000 rucio-clients-1.9.6/doc/build/doctrees/REST_Intro.doctree
--rw-r--r--   0 barisits (51071) zp        (1307)    28088 2016-11-25 15:03:17.000000 rucio-clients-1.9.6/doc/build/doctrees/REST_authentication.doctree
--rw-r--r--   0 barisits (51071) zp        (1307)    90687 2016-11-25 15:03:17.000000 rucio-clients-1.9.6/doc/build/doctrees/REST_did.doctree
--rw-r--r--   0 barisits (51071) zp        (1307)     8194 2016-11-25 15:03:17.000000 rucio-clients-1.9.6/doc/build/doctrees/REST_identity.doctree
--rw-r--r--   0 barisits (51071) zp        (1307)     9921 2016-11-25 15:03:17.000000 rucio-clients-1.9.6/doc/build/doctrees/REST_lock.doctree
--rw-r--r--   0 barisits (51071) zp        (1307)    19528 2016-11-25 15:03:17.000000 rucio-clients-1.9.6/doc/build/doctrees/REST_meta.doctree
--rw-r--r--   0 barisits (51071) zp        (1307)     7204 2016-11-25 15:03:17.000000 rucio-clients-1.9.6/doc/build/doctrees/REST_redirect.doctree
--rw-r--r--   0 barisits (51071) zp        (1307)    43167 2016-11-25 15:03:17.000000 rucio-clients-1.9.6/doc/build/doctrees/REST_replica.doctree
--rw-r--r--   0 barisits (51071) zp        (1307)    93418 2016-11-25 15:03:17.000000 rucio-clients-1.9.6/doc/build/doctrees/REST_rse.doctree
--rw-r--r--   0 barisits (51071) zp        (1307)    33047 2016-11-25 15:03:17.000000 rucio-clients-1.9.6/doc/build/doctrees/REST_rule.doctree
--rw-r--r--   0 barisits (51071) zp        (1307)    14549 2016-11-25 15:03:17.000000 rucio-clients-1.9.6/doc/build/doctrees/REST_scope.doctree
--rw-r--r--   0 barisits (51071) zp        (1307)    31270 2016-11-25 15:03:17.000000 rucio-clients-1.9.6/doc/build/doctrees/REST_subscription.doctree
--rw-r--r--   0 barisits (51071) zp        (1307)     8088 2016-11-25 15:03:16.000000 rucio-clients-1.9.6/doc/build/doctrees/Rucio_Project_Meeting.doctree
--rw-r--r--   0 barisits (51071) zp        (1307)    78224 2016-11-25 15:03:17.000000 rucio-clients-1.9.6/doc/build/doctrees/account.doctree
--rw-r--r--   0 barisits (51071) zp        (1307)    21746 2016-11-25 15:03:17.000000 rucio-clients-1.9.6/doc/build/doctrees/accountlimit.doctree
--rw-r--r--   0 barisits (51071) zp        (1307)   196168 2016-11-25 15:03:17.000000 rucio-clients-1.9.6/doc/build/doctrees/api_curl_examples.doctree
--rw-r--r--   0 barisits (51071) zp        (1307)    27111 2016-11-25 15:03:17.000000 rucio-clients-1.9.6/doc/build/doctrees/atlas_integration_testbed.doctree
--rw-r--r--   0 barisits (51071) zp        (1307)     2054 2016-11-25 15:03:17.000000 rucio-clients-1.9.6/doc/build/doctrees/authentication_and_identity.doctree
--rw-r--r--   0 barisits (51071) zp        (1307)    27974 2016-11-25 15:03:17.000000 rucio-clients-1.9.6/doc/build/doctrees/cli_admin_examples.doctree
--rw-r--r--   0 barisits (51071) zp        (1307)    21844 2016-11-25 15:03:16.000000 rucio-clients-1.9.6/doc/build/doctrees/cli_examples.doctree
--rw-r--r--   0 barisits (51071) zp        (1307)     8060 2016-11-25 15:03:17.000000 rucio-clients-1.9.6/doc/build/doctrees/client_examples.doctree
--rw-r--r--   0 barisits (51071) zp        (1307)   116795 2016-11-25 15:03:17.000000 rucio-clients-1.9.6/doc/build/doctrees/client_howto.doctree
--rw-r--r--   0 barisits (51071) zp        (1307)    39959 2016-11-25 15:03:16.000000 rucio-clients-1.9.6/doc/build/doctrees/client_tutorial.doctree
--rw-r--r--   0 barisits (51071) zp        (1307)   155605 2016-11-25 15:03:17.000000 rucio-clients-1.9.6/doc/build/doctrees/core_constants.doctree
--rw-r--r--   0 barisits (51071) zp        (1307)    21418 2016-11-25 15:03:17.000000 rucio-clients-1.9.6/doc/build/doctrees/developing.doctree
--rw-r--r--   0 barisits (51071) zp        (1307)    18604 2016-11-25 15:03:17.000000 rucio-clients-1.9.6/doc/build/doctrees/development_guidelines.doctree
--rw-r--r--   0 barisits (51071) zp        (1307)   153643 2016-11-25 15:03:16.000000 rucio-clients-1.9.6/doc/build/doctrees/did.doctree
--rw-r--r--   0 barisits (51071) zp        (1307)   452979 2016-11-25 15:03:17.000000 rucio-clients-1.9.6/doc/build/doctrees/environment.pickle
--rw-r--r--   0 barisits (51071) zp        (1307)   224246 2016-11-25 15:03:17.000000 rucio-clients-1.9.6/doc/build/doctrees/exception.doctree
--rw-r--r--   0 barisits (51071) zp        (1307)    37099 2016-11-25 15:03:17.000000 rucio-clients-1.9.6/doc/build/doctrees/identity.doctree
--rw-r--r--   0 barisits (51071) zp        (1307)    18065 2016-11-25 15:03:17.000000 rucio-clients-1.9.6/doc/build/doctrees/index.doctree
--rw-r--r--   0 barisits (51071) zp        (1307)    15084 2016-11-25 15:03:17.000000 rucio-clients-1.9.6/doc/build/doctrees/installing_atlas_clients.doctree
--rw-r--r--   0 barisits (51071) zp        (1307)    11551 2016-11-25 15:03:17.000000 rucio-clients-1.9.6/doc/build/doctrees/installing_clients.doctree
--rw-r--r--   0 barisits (51071) zp        (1307)    12786 2016-11-25 15:03:17.000000 rucio-clients-1.9.6/doc/build/doctrees/installing_server.doctree
--rw-r--r--   0 barisits (51071) zp        (1307)    19939 2016-11-25 15:03:17.000000 rucio-clients-1.9.6/doc/build/doctrees/lock.doctree
--rw-r--r--   0 barisits (51071) zp        (1307)    39479 2016-11-25 15:03:17.000000 rucio-clients-1.9.6/doc/build/doctrees/meta-data.doctree
--rw-r--r--   0 barisits (51071) zp        (1307)     7248 2016-11-25 15:03:17.000000 rucio-clients-1.9.6/doc/build/doctrees/naming_convention_db.doctree
--rw-r--r--   0 barisits (51071) zp        (1307)     3506 2016-11-25 15:03:17.000000 rucio-clients-1.9.6/doc/build/doctrees/overview_Accounting_and_quota.doctree
--rw-r--r--   0 barisits (51071) zp        (1307)     2773 2016-11-25 15:03:17.000000 rucio-clients-1.9.6/doc/build/doctrees/overview_Architecture.doctree
--rw-r--r--   0 barisits (51071) zp        (1307)     9766 2016-11-25 15:03:17.000000 rucio-clients-1.9.6/doc/build/doctrees/overview_Data_Deletion.doctree
--rw-r--r--   0 barisits (51071) zp        (1307)     3278 2016-11-25 15:03:17.000000 rucio-clients-1.9.6/doc/build/doctrees/overview_Database_Schema.doctree
--rw-r--r--   0 barisits (51071) zp        (1307)     3459 2016-11-25 15:03:17.000000 rucio-clients-1.9.6/doc/build/doctrees/overview_Deployment.doctree
--rw-r--r--   0 barisits (51071) zp        (1307)     6135 2016-11-25 15:03:17.000000 rucio-clients-1.9.6/doc/build/doctrees/overview_Exception_Handling.doctree
--rw-r--r--   0 barisits (51071) zp        (1307)    17155 2016-11-25 15:03:17.000000 rucio-clients-1.9.6/doc/build/doctrees/overview_File_Dataset_Container.doctree
--rw-r--r--   0 barisits (51071) zp        (1307)     6672 2016-11-25 15:03:17.000000 rucio-clients-1.9.6/doc/build/doctrees/overview_Meta-data_attributes.doctree
--rw-r--r--   0 barisits (51071) zp        (1307)     3043 2016-11-25 15:03:17.000000 rucio-clients-1.9.6/doc/build/doctrees/overview_Notifications.doctree
--rw-r--r--   0 barisits (51071) zp        (1307)     2956 2016-11-25 15:03:17.000000 rucio-clients-1.9.6/doc/build/doctrees/overview_Permission_model.doctree
--rw-r--r--   0 barisits (51071) zp        (1307)     9183 2016-11-25 15:03:17.000000 rucio-clients-1.9.6/doc/build/doctrees/overview_Replica_management.doctree
--rw-r--r--   0 barisits (51071) zp        (1307)     6869 2016-11-25 15:03:17.000000 rucio-clients-1.9.6/doc/build/doctrees/overview_Rucio_Storage_Element.doctree
--rw-r--r--   0 barisits (51071) zp        (1307)   117157 2016-11-25 15:03:17.000000 rucio-clients-1.9.6/doc/build/doctrees/overview_Rucio_Storage_Element_Manager.doctree
--rw-r--r--   0 barisits (51071) zp        (1307)     5437 2016-11-25 15:03:17.000000 rucio-clients-1.9.6/doc/build/doctrees/overview_Rucio_account.doctree
--rw-r--r--   0 barisits (51071) zp        (1307)    95260 2016-11-25 15:03:17.000000 rucio-clients-1.9.6/doc/build/doctrees/overview_Rules.doctree
--rw-r--r--   0 barisits (51071) zp        (1307)     9339 2016-11-25 15:03:17.000000 rucio-clients-1.9.6/doc/build/doctrees/overview_Scheduled_Transfers.doctree
--rw-r--r--   0 barisits (51071) zp        (1307)    35789 2016-11-25 15:03:17.000000 rucio-clients-1.9.6/doc/build/doctrees/overview_Subscriptions.doctree
--rw-r--r--   0 barisits (51071) zp        (1307)     3214 2016-11-25 15:03:17.000000 rucio-clients-1.9.6/doc/build/doctrees/overview_Traceability_and_logging.doctree
--rw-r--r--   0 barisits (51071) zp        (1307)     3326 2016-11-25 15:03:17.000000 rucio-clients-1.9.6/doc/build/doctrees/overview_flow.doctree
--rw-r--r--   0 barisits (51071) zp        (1307)     9984 2016-11-25 15:03:17.000000 rucio-clients-1.9.6/doc/build/doctrees/permission.doctree
--rw-r--r--   0 barisits (51071) zp        (1307)     3498 2016-11-25 15:03:17.000000 rucio-clients-1.9.6/doc/build/doctrees/replica.doctree
--rw-r--r--   0 barisits (51071) zp        (1307)    23662 2016-11-25 15:03:17.000000 rucio-clients-1.9.6/doc/build/doctrees/replication_rules_examples.doctree
--rw-r--r--   0 barisits (51071) zp        (1307)    94015 2016-11-25 15:03:17.000000 rucio-clients-1.9.6/doc/build/doctrees/rest.doctree
--rw-r--r--   0 barisits (51071) zp        (1307)   183082 2016-11-25 15:03:17.000000 rucio-clients-1.9.6/doc/build/doctrees/rse.doctree
--rw-r--r--   0 barisits (51071) zp        (1307)     7070 2016-11-25 15:03:17.000000 rucio-clients-1.9.6/doc/build/doctrees/rucio_cli.doctree
--rw-r--r--   0 barisits (51071) zp        (1307)     5185 2016-11-25 15:03:17.000000 rucio-clients-1.9.6/doc/build/doctrees/rucio_clients.doctree
--rw-r--r--   0 barisits (51071) zp        (1307)    41739 2016-11-25 15:03:17.000000 rucio-clients-1.9.6/doc/build/doctrees/rule.doctree
--rw-r--r--   0 barisits (51071) zp        (1307)     4903 2016-11-25 15:03:17.000000 rucio-clients-1.9.6/doc/build/doctrees/rules_workflow.doctree
--rw-r--r--   0 barisits (51071) zp        (1307)    25243 2016-11-25 15:03:16.000000 rucio-clients-1.9.6/doc/build/doctrees/scope.doctree
--rw-r--r--   0 barisits (51071) zp        (1307)    40983 2016-11-25 15:03:17.000000 rucio-clients-1.9.6/doc/build/doctrees/setup.doctree
--rw-r--r--   0 barisits (51071) zp        (1307)    48626 2016-11-25 15:03:17.000000 rucio-clients-1.9.6/doc/build/doctrees/subscription.doctree
--rw-r--r--   0 barisits (51071) zp        (1307)    28243 2016-11-25 15:03:17.000000 rucio-clients-1.9.6/doc/build/doctrees/usecases.doctree
-drwxr-xr-x   0 barisits (51071) zp        (1307)        0 2017-02-07 09:03:44.000000 rucio-clients-1.9.6/doc/build/html/
-drwxr-xr-x   0 barisits (51071) zp        (1307)        0 2017-02-07 09:03:44.000000 rucio-clients-1.9.6/doc/build/html/_images/
--rw-r--r--   0 barisits (51071) zp        (1307)    71679 2016-11-25 15:03:16.000000 rucio-clients-1.9.6/doc/build/html/_images/RSE_overview.png
--rw-r--r--   0 barisits (51071) zp        (1307)    45789 2016-11-25 15:03:16.000000 rucio-clients-1.9.6/doc/build/html/_images/RSE_sequence_instantiation.png
--rw-r--r--   0 barisits (51071) zp        (1307)    32727 2016-11-25 15:03:16.000000 rucio-clients-1.9.6/doc/build/html/_images/RSE_sequence_usage.png
--rw-r--r--   0 barisits (51071) zp        (1307)    56522 2016-11-25 15:03:16.000000 rucio-clients-1.9.6/doc/build/html/_images/accounts.png
--rw-r--r--   0 barisits (51071) zp        (1307)   136638 2016-11-25 15:03:16.000000 rucio-clients-1.9.6/doc/build/html/_images/architecture.png
--rw-r--r--   0 barisits (51071) zp        (1307)    82950 2016-11-25 15:03:16.000000 rucio-clients-1.9.6/doc/build/html/_images/deployment.png
--rw-r--r--   0 barisits (51071) zp        (1307)   163115 2016-11-25 15:03:16.000000 rucio-clients-1.9.6/doc/build/html/_images/exception_handling.png
--rw-r--r--   0 barisits (51071) zp        (1307)    45282 2016-11-25 15:03:16.000000 rucio-clients-1.9.6/doc/build/html/_images/rucio-get.png
--rw-r--r--   0 barisits (51071) zp        (1307)    73288 2016-11-25 15:03:16.000000 rucio-clients-1.9.6/doc/build/html/_images/rucio-put.png
--rw-r--r--   0 barisits (51071) zp        (1307)    55513 2016-11-25 15:03:16.000000 rucio-clients-1.9.6/doc/build/html/_images/rucio-register.png
--rw-r--r--   0 barisits (51071) zp        (1307)   230376 2016-11-25 15:03:16.000000 rucio-clients-1.9.6/doc/build/html/_images/rucio_schema.png
--rw-r--r--   0 barisits (51071) zp        (1307)    26628 2016-11-25 15:03:16.000000 rucio-clients-1.9.6/doc/build/html/_images/scheduled_transfers.png
-drwxr-xr-x   0 barisits (51071) zp        (1307)        0 2017-02-07 09:03:44.000000 rucio-clients-1.9.6/doc/build/html/_modules/
-drwxr-xr-x   0 barisits (51071) zp        (1307)        0 2017-02-07 09:03:44.000000 rucio-clients-1.9.6/doc/build/html/_modules/rucio/
-drwxr-xr-x   0 barisits (51071) zp        (1307)        0 2017-02-07 09:03:44.000000 rucio-clients-1.9.6/doc/build/html/_modules/rucio/api/
--rw-r--r--   0 barisits (51071) zp        (1307)    16466 2016-11-25 15:03:16.000000 rucio-clients-1.9.6/doc/build/html/_modules/rucio/api/identity.html
--rw-r--r--   0 barisits (51071) zp        (1307)     6844 2016-11-25 15:03:16.000000 rucio-clients-1.9.6/doc/build/html/_modules/rucio/api/permission.html
-drwxr-xr-x   0 barisits (51071) zp        (1307)        0 2017-02-07 09:03:44.000000 rucio-clients-1.9.6/doc/build/html/_modules/rucio/client/
--rw-r--r--   0 barisits (51071) zp        (1307)    59303 2016-11-25 15:03:16.000000 rucio-clients-1.9.6/doc/build/html/_modules/rucio/client/accountclient.html
--rw-r--r--   0 barisits (51071) zp        (1307)    14938 2016-11-25 15:03:16.000000 rucio-clients-1.9.6/doc/build/html/_modules/rucio/client/accountlimitclient.html
--rw-r--r--   0 barisits (51071) zp        (1307)    93774 2016-11-25 15:03:16.000000 rucio-clients-1.9.6/doc/build/html/_modules/rucio/client/didclient.html
--rw-r--r--   0 barisits (51071) zp        (1307)    14712 2016-11-25 15:03:16.000000 rucio-clients-1.9.6/doc/build/html/_modules/rucio/client/lockclient.html
--rw-r--r--   0 barisits (51071) zp        (1307)    23765 2016-11-25 15:03:16.000000 rucio-clients-1.9.6/doc/build/html/_modules/rucio/client/metaclient.html
--rw-r--r--   0 barisits (51071) zp        (1307)    77886 2016-11-25 15:03:16.000000 rucio-clients-1.9.6/doc/build/html/_modules/rucio/client/rseclient.html
--rw-r--r--   0 barisits (51071) zp        (1307)    29268 2016-11-25 15:03:16.000000 rucio-clients-1.9.6/doc/build/html/_modules/rucio/client/ruleclient.html
--rw-r--r--   0 barisits (51071) zp        (1307)    17486 2016-11-25 15:03:16.000000 rucio-clients-1.9.6/doc/build/html/_modules/rucio/client/scopeclient.html
--rw-r--r--   0 barisits (51071) zp        (1307)    29268 2016-11-25 15:03:16.000000 rucio-clients-1.9.6/doc/build/html/_modules/rucio/client/subscriptionclient.html
-drwxr-xr-x   0 barisits (51071) zp        (1307)        0 2017-02-07 09:03:44.000000 rucio-clients-1.9.6/doc/build/html/_modules/rucio/common/
--rw-r--r--   0 barisits (51071) zp        (1307)   100322 2016-11-25 15:03:16.000000 rucio-clients-1.9.6/doc/build/html/_modules/rucio/common/exception.html
-drwxr-xr-x   0 barisits (51071) zp        (1307)        0 2017-02-07 09:03:44.000000 rucio-clients-1.9.6/doc/build/html/_modules/rucio/db/
--rw-r--r--   0 barisits (51071) zp        (1307)    25179 2016-11-25 15:03:16.000000 rucio-clients-1.9.6/doc/build/html/_modules/rucio/db/constants.html
--rw-r--r--   0 barisits (51071) zp        (1307)    20480 2016-11-25 15:03:16.000000 rucio-clients-1.9.6/doc/build/html/_modules/rucio/db/enum.html
-drwxr-xr-x   0 barisits (51071) zp        (1307)        0 2017-02-07 09:03:44.000000 rucio-clients-1.9.6/doc/build/html/_modules/rucio/rse/
-drwxr-xr-x   0 barisits (51071) zp        (1307)        0 2017-02-07 09:03:44.000000 rucio-clients-1.9.6/doc/build/html/_modules/rucio/rse/protocols/
--rw-r--r--   0 barisits (51071) zp        (1307)    51488 2016-11-25 15:03:16.000000 rucio-clients-1.9.6/doc/build/html/_modules/rucio/rse/protocols/protocol.html
-drwxr-xr-x   0 barisits (51071) zp        (1307)        0 2017-02-07 09:03:44.000000 rucio-clients-1.9.6/doc/build/html/_modules/rucio/web/
-drwxr-xr-x   0 barisits (51071) zp        (1307)        0 2017-02-07 09:03:44.000000 rucio-clients-1.9.6/doc/build/html/_modules/rucio/web/rest/
--rw-r--r--   0 barisits (51071) zp        (1307)    11578 2016-11-25 15:03:16.000000 rucio-clients-1.9.6/doc/build/html/_modules/rucio/web/rest/ping.html
--rw-r--r--   0 barisits (51071) zp        (1307)     5307 2016-11-25 15:03:16.000000 rucio-clients-1.9.6/doc/build/html/_modules/index.html
-drwxr-xr-x   0 barisits (51071) zp        (1307)        0 2017-02-07 09:03:44.000000 rucio-clients-1.9.6/doc/build/html/_sources/
-drwxr-xr-x   0 barisits (51071) zp        (1307)        0 2017-02-07 09:03:44.000000 rucio-clients-1.9.6/doc/build/html/_sources/man/
--rw-r--r--   0 barisits (51071) zp        (1307)      770 2016-11-25 15:03:16.000000 rucio-clients-1.9.6/doc/build/html/_sources/man/rucio-admin.txt
--rw-r--r--   0 barisits (51071) zp        (1307)     1744 2016-11-25 15:03:16.000000 rucio-clients-1.9.6/doc/build/html/_sources/man/rucio.txt
-drwxr-xr-x   0 barisits (51071) zp        (1307)        0 2017-02-07 09:03:44.000000 rucio-clients-1.9.6/doc/build/html/_sources/rest/
--rw-r--r--   0 barisits (51071) zp        (1307)      350 2016-11-25 15:03:16.000000 rucio-clients-1.9.6/doc/build/html/_sources/rest/attach_dids_to_dids.txt
-drwxr-xr-x   0 barisits (51071) zp        (1307)        0 2017-02-07 09:03:44.000000 rucio-clients-1.9.6/doc/build/html/_sources/usecases/
--rw-r--r--   0 barisits (51071) zp        (1307)      783 2016-11-25 15:03:16.000000 rucio-clients-1.9.6/doc/build/html/_sources/usecases/add_account_identity.txt
--rw-r--r--   0 barisits (51071) zp        (1307)      708 2016-11-25 15:03:16.000000 rucio-clients-1.9.6/doc/build/html/_sources/usecases/add_metadata_dataset.txt
--rw-r--r--   0 barisits (51071) zp        (1307)      691 2016-11-25 15:03:16.000000 rucio-clients-1.9.6/doc/build/html/_sources/usecases/add_metadata_file.txt
--rw-r--r--   0 barisits (51071) zp        (1307)      655 2016-11-25 15:03:16.000000 rucio-clients-1.9.6/doc/build/html/_sources/usecases/add_scope_to_account.txt
--rw-r--r--   0 barisits (51071) zp        (1307)     1470 2016-11-25 15:03:16.000000 rucio-clients-1.9.6/doc/build/html/_sources/usecases/add_subscription.txt
--rw-r--r--   0 barisits (51071) zp        (1307)     1677 2016-11-25 15:03:16.000000 rucio-clients-1.9.6/doc/build/html/_sources/usecases/authentication.txt
--rw-r--r--   0 barisits (51071) zp        (1307)      804 2016-11-25 15:03:16.000000 rucio-clients-1.9.6/doc/build/html/_sources/usecases/consistency_file_between_storage_and_rucio.txt
--rw-r--r--   0 barisits (51071) zp        (1307)      902 2016-11-25 15:03:16.000000 rucio-clients-1.9.6/doc/build/html/_sources/usecases/delete_file_replica_from_storage.txt
--rw-r--r--   0 barisits (51071) zp        (1307)      795 2016-11-25 15:03:16.000000 rucio-clients-1.9.6/doc/build/html/_sources/usecases/detect_site_reach_watermark.txt
--rw-r--r--   0 barisits (51071) zp        (1307)      940 2016-11-25 15:03:16.000000 rucio-clients-1.9.6/doc/build/html/_sources/usecases/download_all_files_from_a_given_list_of_file_replicas_from_rucio.txt
--rw-r--r--   0 barisits (51071) zp        (1307)      886 2016-11-25 15:03:16.000000 rucio-clients-1.9.6/doc/build/html/_sources/usecases/download_all_files_from_a_given_list_of_files_from_rucio.txt
--rw-r--r--   0 barisits (51071) zp        (1307)      900 2016-11-25 15:03:16.000000 rucio-clients-1.9.6/doc/build/html/_sources/usecases/download_all_files_in_a_dataset_from_rucio.txt
--rw-r--r--   0 barisits (51071) zp        (1307)      794 2016-11-25 15:03:16.000000 rucio-clients-1.9.6/doc/build/html/_sources/usecases/download_files_from_rucio.txt
--rw-r--r--   0 barisits (51071) zp        (1307)      564 2016-11-25 15:03:16.000000 rucio-clients-1.9.6/doc/build/html/_sources/usecases/obsolete_dataset.txt
--rw-r--r--   0 barisits (51071) zp        (1307)      652 2016-11-25 15:03:16.000000 rucio-clients-1.9.6/doc/build/html/_sources/usecases/register_account.txt
--rw-r--r--   0 barisits (51071) zp        (1307)      916 2016-11-25 15:03:16.000000 rucio-clients-1.9.6/doc/build/html/_sources/usecases/register_file_already_on_storage_system.txt
--rw-r--r--   0 barisits (51071) zp        (1307)      646 2016-11-25 15:03:16.000000 rucio-clients-1.9.6/doc/build/html/_sources/usecases/register_transfer_request_file_fts.txt
--rw-r--r--   0 barisits (51071) zp        (1307)      563 2016-11-25 15:03:16.000000 rucio-clients-1.9.6/doc/build/html/_sources/usecases/remove_replication_rules_from_file.txt
--rw-r--r--   0 barisits (51071) zp        (1307)     1270 2016-11-25 15:03:16.000000 rucio-clients-1.9.6/doc/build/html/_sources/usecases/reupload_after_failure.txt
--rw-r--r--   0 barisits (51071) zp        (1307)     1102 2016-11-25 15:03:16.000000 rucio-clients-1.9.6/doc/build/html/_sources/usecases/search.txt
--rw-r--r--   0 barisits (51071) zp        (1307)      409 2016-11-25 15:03:16.000000 rucio-clients-1.9.6/doc/build/html/_sources/usecases/select_unwanted_files_for_deletion.txt
--rw-r--r--   0 barisits (51071) zp        (1307)      590 2016-11-25 15:03:16.000000 rucio-clients-1.9.6/doc/build/html/_sources/usecases/set_replication_rule_to_file.txt
--rw-r--r--   0 barisits (51071) zp        (1307)      852 2016-11-25 15:03:16.000000 rucio-clients-1.9.6/doc/build/html/_sources/usecases/upload_file_with_replication_rule.txt
--rw-r--r--   0 barisits (51071) zp        (1307)     1106 2016-11-25 15:03:16.000000 rucio-clients-1.9.6/doc/build/html/_sources/usecases/usecase_upload_file_into_rucio.txt
--rw-r--r--   0 barisits (51071) zp        (1307)      633 2016-11-25 15:03:16.000000 rucio-clients-1.9.6/doc/build/html/_sources/usecases/where_are_the_replicas_for_a_file.txt
--rw-r--r--   0 barisits (51071) zp        (1307)      325 2016-11-25 15:03:16.000000 rucio-clients-1.9.6/doc/build/html/_sources/Acronyms_and_Abbreviations.txt
--rw-r--r--   0 barisits (51071) zp        (1307)       39 2016-11-25 15:03:16.000000 rucio-clients-1.9.6/doc/build/html/_sources/Architecture.txt
--rw-r--r--   0 barisits (51071) zp        (1307)     2067 2016-11-25 15:03:16.000000 rucio-clients-1.9.6/doc/build/html/_sources/Comparison_matrix.txt
--rw-r--r--   0 barisits (51071) zp        (1307)     7173 2016-11-25 15:03:16.000000 rucio-clients-1.9.6/doc/build/html/_sources/REST_Account.txt
--rw-r--r--   0 barisits (51071) zp        (1307)     1586 2016-11-25 15:03:16.000000 rucio-clients-1.9.6/doc/build/html/_sources/REST_Intro.txt
--rw-r--r--   0 barisits (51071) zp        (1307)     2439 2016-11-25 15:03:16.000000 rucio-clients-1.9.6/doc/build/html/_sources/REST_authentication.txt
--rw-r--r--   0 barisits (51071) zp        (1307)     7945 2016-11-25 15:03:16.000000 rucio-clients-1.9.6/doc/build/html/_sources/REST_did.txt
--rw-r--r--   0 barisits (51071) zp        (1307)      611 2016-11-25 15:03:16.000000 rucio-clients-1.9.6/doc/build/html/_sources/REST_identity.txt
--rw-r--r--   0 barisits (51071) zp        (1307)      637 2016-11-25 15:03:16.000000 rucio-clients-1.9.6/doc/build/html/_sources/REST_lock.txt
--rw-r--r--   0 barisits (51071) zp        (1307)     1444 2016-11-25 15:03:16.000000 rucio-clients-1.9.6/doc/build/html/_sources/REST_meta.txt
--rw-r--r--   0 barisits (51071) zp        (1307)      349 2016-11-25 15:03:16.000000 rucio-clients-1.9.6/doc/build/html/_sources/REST_redirect.txt
--rw-r--r--   0 barisits (51071) zp        (1307)     3702 2016-11-25 15:03:16.000000 rucio-clients-1.9.6/doc/build/html/_sources/REST_replica.txt
--rw-r--r--   0 barisits (51071) zp        (1307)     7502 2016-11-25 15:03:16.000000 rucio-clients-1.9.6/doc/build/html/_sources/REST_rse.txt
--rw-r--r--   0 barisits (51071) zp        (1307)     2509 2016-11-25 15:03:16.000000 rucio-clients-1.9.6/doc/build/html/_sources/REST_rule.txt
--rw-r--r--   0 barisits (51071) zp        (1307)     1034 2016-11-25 15:03:16.000000 rucio-clients-1.9.6/doc/build/html/_sources/REST_scope.txt
--rw-r--r--   0 barisits (51071) zp        (1307)     2512 2016-11-25 15:03:16.000000 rucio-clients-1.9.6/doc/build/html/_sources/REST_subscription.txt
--rw-r--r--   0 barisits (51071) zp        (1307)      662 2016-11-25 15:03:16.000000 rucio-clients-1.9.6/doc/build/html/_sources/Rucio_Project_Meeting.txt
--rw-r--r--   0 barisits (51071) zp        (1307)      163 2016-11-25 15:03:16.000000 rucio-clients-1.9.6/doc/build/html/_sources/account.txt
--rw-r--r--   0 barisits (51071) zp        (1307)      184 2016-11-25 15:03:16.000000 rucio-clients-1.9.6/doc/build/html/_sources/accountlimit.txt
--rw-r--r--   0 barisits (51071) zp        (1307)    16748 2016-11-25 15:03:16.000000 rucio-clients-1.9.6/doc/build/html/_sources/api_curl_examples.txt
--rw-r--r--   0 barisits (51071) zp        (1307)     2265 2016-11-25 15:03:16.000000 rucio-clients-1.9.6/doc/build/html/_sources/atlas_integration_testbed.txt
--rw-r--r--   0 barisits (51071) zp        (1307)        0 2016-11-25 15:03:16.000000 rucio-clients-1.9.6/doc/build/html/_sources/authentication_and_identity.txt
--rw-r--r--   0 barisits (51071) zp        (1307)     4204 2016-11-25 15:03:16.000000 rucio-clients-1.9.6/doc/build/html/_sources/cli_admin_examples.txt
--rw-r--r--   0 barisits (51071) zp        (1307)     4565 2016-11-25 15:03:16.000000 rucio-clients-1.9.6/doc/build/html/_sources/cli_examples.txt
--rw-r--r--   0 barisits (51071) zp        (1307)     1010 2016-11-25 15:03:16.000000 rucio-clients-1.9.6/doc/build/html/_sources/client_examples.txt
--rw-r--r--   0 barisits (51071) zp        (1307)    24065 2016-11-25 15:03:16.000000 rucio-clients-1.9.6/doc/build/html/_sources/client_howto.txt
--rw-r--r--   0 barisits (51071) zp        (1307)    11591 2016-11-25 15:03:16.000000 rucio-clients-1.9.6/doc/build/html/_sources/client_tutorial.txt
--rw-r--r--   0 barisits (51071) zp        (1307)      138 2016-11-25 15:03:16.000000 rucio-clients-1.9.6/doc/build/html/_sources/core_constants.txt
--rw-r--r--   0 barisits (51071) zp        (1307)     4981 2016-11-25 15:03:16.000000 rucio-clients-1.9.6/doc/build/html/_sources/developing.txt
--rw-r--r--   0 barisits (51071) zp        (1307)     3281 2016-11-25 15:03:16.000000 rucio-clients-1.9.6/doc/build/html/_sources/development_guidelines.txt
--rw-r--r--   0 barisits (51071) zp        (1307)      180 2016-11-25 15:03:16.000000 rucio-clients-1.9.6/doc/build/html/_sources/did.txt
--rw-r--r--   0 barisits (51071) zp        (1307)      165 2016-11-25 15:03:16.000000 rucio-clients-1.9.6/doc/build/html/_sources/exception.txt
--rw-r--r--   0 barisits (51071) zp        (1307)      162 2016-11-25 15:03:16.000000 rucio-clients-1.9.6/doc/build/html/_sources/identity.txt
--rw-r--r--   0 barisits (51071) zp        (1307)     2917 2016-11-25 15:03:16.000000 rucio-clients-1.9.6/doc/build/html/_sources/index.txt
--rw-r--r--   0 barisits (51071) zp        (1307)     2167 2016-11-25 15:03:16.000000 rucio-clients-1.9.6/doc/build/html/_sources/installing_atlas_clients.txt
--rw-r--r--   0 barisits (51071) zp        (1307)     1601 2016-11-25 15:03:16.000000 rucio-clients-1.9.6/doc/build/html/_sources/installing_clients.txt
--rw-r--r--   0 barisits (51071) zp        (1307)     1693 2016-11-25 15:03:16.000000 rucio-clients-1.9.6/doc/build/html/_sources/installing_server.txt
--rw-r--r--   0 barisits (51071) zp        (1307)      152 2016-11-25 15:03:16.000000 rucio-clients-1.9.6/doc/build/html/_sources/lock.txt
--rw-r--r--   0 barisits (51071) zp        (1307)      162 2016-11-25 15:03:16.000000 rucio-clients-1.9.6/doc/build/html/_sources/meta-data.txt
--rw-r--r--   0 barisits (51071) zp        (1307)     1180 2016-11-25 15:03:16.000000 rucio-clients-1.9.6/doc/build/html/_sources/naming_convention_db.txt
--rw-r--r--   0 barisits (51071) zp        (1307)      475 2016-11-25 15:03:16.000000 rucio-clients-1.9.6/doc/build/html/_sources/overview_Accounting_and_quota.txt
--rw-r--r--   0 barisits (51071) zp        (1307)      157 2016-11-25 15:03:16.000000 rucio-clients-1.9.6/doc/build/html/_sources/overview_Architecture.txt
--rw-r--r--   0 barisits (51071) zp        (1307)     1905 2016-11-25 15:03:16.000000 rucio-clients-1.9.6/doc/build/html/_sources/overview_Data_Deletion.txt
--rw-r--r--   0 barisits (51071) zp        (1307)      311 2016-11-25 15:03:16.000000 rucio-clients-1.9.6/doc/build/html/_sources/overview_Database_Schema.txt
--rw-r--r--   0 barisits (51071) zp        (1307)      442 2016-11-25 15:03:16.000000 rucio-clients-1.9.6/doc/build/html/_sources/overview_Deployment.txt
--rw-r--r--   0 barisits (51071) zp        (1307)     1514 2016-11-25 15:03:16.000000 rucio-clients-1.9.6/doc/build/html/_sources/overview_Exception_Handling.txt
--rw-r--r--   0 barisits (51071) zp        (1307)     3882 2016-11-25 15:03:16.000000 rucio-clients-1.9.6/doc/build/html/_sources/overview_File_Dataset_Container.txt
--rw-r--r--   0 barisits (51071) zp        (1307)      894 2016-11-25 15:03:16.000000 rucio-clients-1.9.6/doc/build/html/_sources/overview_Meta-data_attributes.txt
--rw-r--r--   0 barisits (51071) zp        (1307)      292 2016-11-25 15:03:16.000000 rucio-clients-1.9.6/doc/build/html/_sources/overview_Notifications.txt
--rw-r--r--   0 barisits (51071) zp        (1307)      251 2016-11-25 15:03:16.000000 rucio-clients-1.9.6/doc/build/html/_sources/overview_Permission_model.txt
--rw-r--r--   0 barisits (51071) zp        (1307)     1911 2016-11-25 15:03:16.000000 rucio-clients-1.9.6/doc/build/html/_sources/overview_Replica_management.txt
--rw-r--r--   0 barisits (51071) zp        (1307)     1652 2016-11-25 15:03:16.000000 rucio-clients-1.9.6/doc/build/html/_sources/overview_Rucio_Storage_Element.txt
--rw-r--r--   0 barisits (51071) zp        (1307)    27901 2016-11-25 15:03:16.000000 rucio-clients-1.9.6/doc/build/html/_sources/overview_Rucio_Storage_Element_Manager.txt
--rw-r--r--   0 barisits (51071) zp        (1307)     1142 2016-11-25 15:03:16.000000 rucio-clients-1.9.6/doc/build/html/_sources/overview_Rucio_account.txt
--rw-r--r--   0 barisits (51071) zp        (1307)    23147 2016-11-25 15:03:16.000000 rucio-clients-1.9.6/doc/build/html/_sources/overview_Rules.txt
--rw-r--r--   0 barisits (51071) zp        (1307)     2701 2016-11-25 15:03:16.000000 rucio-clients-1.9.6/doc/build/html/_sources/overview_Scheduled_Transfers.txt
--rw-r--r--   0 barisits (51071) zp        (1307)     4241 2016-11-25 15:03:16.000000 rucio-clients-1.9.6/doc/build/html/_sources/overview_Subscriptions.txt
--rw-r--r--   0 barisits (51071) zp        (1307)      382 2016-11-25 15:03:16.000000 rucio-clients-1.9.6/doc/build/html/_sources/overview_Traceability_and_logging.txt
--rw-r--r--   0 barisits (51071) zp        (1307)      347 2016-11-25 15:03:16.000000 rucio-clients-1.9.6/doc/build/html/_sources/overview_flow.txt
--rw-r--r--   0 barisits (51071) zp        (1307)      169 2016-11-25 15:03:16.000000 rucio-clients-1.9.6/doc/build/html/_sources/permission.txt
--rw-r--r--   0 barisits (51071) zp        (1307)      155 2016-11-25 15:03:16.000000 rucio-clients-1.9.6/doc/build/html/_sources/replica.txt
--rw-r--r--   0 barisits (51071) zp        (1307)     3376 2016-11-25 15:03:16.000000 rucio-clients-1.9.6/doc/build/html/_sources/replication_rules_examples.txt
--rw-r--r--   0 barisits (51071) zp        (1307)     7948 2016-11-25 15:03:16.000000 rucio-clients-1.9.6/doc/build/html/_sources/rest.txt
--rw-r--r--   0 barisits (51071) zp        (1307)      453 2016-11-25 15:03:16.000000 rucio-clients-1.9.6/doc/build/html/_sources/rse.txt
--rw-r--r--   0 barisits (51071) zp        (1307)      717 2016-11-25 15:03:16.000000 rucio-clients-1.9.6/doc/build/html/_sources/rucio_cli.txt
--rw-r--r--   0 barisits (51071) zp        (1307)      814 2016-11-25 15:03:16.000000 rucio-clients-1.9.6/doc/build/html/_sources/rucio_clients.txt
--rw-r--r--   0 barisits (51071) zp        (1307)      183 2016-11-25 15:03:16.000000 rucio-clients-1.9.6/doc/build/html/_sources/rule.txt
--rw-r--r--   0 barisits (51071) zp        (1307)     1414 2016-11-25 15:03:16.000000 rucio-clients-1.9.6/doc/build/html/_sources/rules_workflow.txt
--rw-r--r--   0 barisits (51071) zp        (1307)      158 2016-11-25 15:03:16.000000 rucio-clients-1.9.6/doc/build/html/_sources/scope.txt
--rw-r--r--   0 barisits (51071) zp        (1307)     6529 2016-11-25 15:03:16.000000 rucio-clients-1.9.6/doc/build/html/_sources/setup.txt
--rw-r--r--   0 barisits (51071) zp        (1307)      190 2016-11-25 15:03:16.000000 rucio-clients-1.9.6/doc/build/html/_sources/subscription.txt
--rw-r--r--   0 barisits (51071) zp        (1307)     2438 2016-11-25 15:03:16.000000 rucio-clients-1.9.6/doc/build/html/_sources/usecases.txt
-drwxr-xr-x   0 barisits (51071) zp        (1307)        0 2017-02-07 09:03:44.000000 rucio-clients-1.9.6/doc/build/html/_static/
--rw-r--r--   0 barisits (51071) zp        (1307)      673 2016-11-25 15:03:16.000000 rucio-clients-1.9.6/doc/build/html/_static/ajax-loader.gif
--rw-r--r--   0 barisits (51071) zp        (1307)     8270 2016-11-25 15:03:16.000000 rucio-clients-1.9.6/doc/build/html/_static/basic.css
--rw-r--r--   0 barisits (51071) zp        (1307)     4125 2016-11-25 15:03:16.000000 rucio-clients-1.9.6/doc/build/html/_static/classic.css
--rw-r--r--   0 barisits (51071) zp        (1307)     3500 2016-11-25 15:03:16.000000 rucio-clients-1.9.6/doc/build/html/_static/comment-bright.png
--rw-r--r--   0 barisits (51071) zp        (1307)     3578 2016-11-25 15:03:16.000000 rucio-clients-1.9.6/doc/build/html/_static/comment-close.png
--rw-r--r--   0 barisits (51071) zp        (1307)     3445 2016-11-25 15:03:16.000000 rucio-clients-1.9.6/doc/build/html/_static/comment.png
--rw-r--r--   0 barisits (51071) zp        (1307)     4041 2016-11-25 15:03:16.000000 rucio-clients-1.9.6/doc/build/html/_static/default.css
--rw-r--r--   0 barisits (51071) zp        (1307)     7377 2016-11-25 15:03:16.000000 rucio-clients-1.9.6/doc/build/html/_static/doctools.js
--rw-r--r--   0 barisits (51071) zp        (1307)      347 2016-11-25 15:03:16.000000 rucio-clients-1.9.6/doc/build/html/_static/down-pressed.png
--rw-r--r--   0 barisits (51071) zp        (1307)      347 2016-11-25 15:03:16.000000 rucio-clients-1.9.6/doc/build/html/_static/down.png
--rw-r--r--   0 barisits (51071) zp        (1307)      358 2016-11-25 15:03:16.000000 rucio-clients-1.9.6/doc/build/html/_static/file.png
--rw-r--r--   0 barisits (51071) zp        (1307)   282766 2016-11-25 15:03:16.000000 rucio-clients-1.9.6/doc/build/html/_static/jquery-1.11.1.js
--rw-r--r--   0 barisits (51071) zp        (1307)    95786 2016-11-25 15:03:16.000000 rucio-clients-1.9.6/doc/build/html/_static/jquery.js
--rw-r--r--   0 barisits (51071) zp        (1307)      173 2016-11-25 15:03:16.000000 rucio-clients-1.9.6/doc/build/html/_static/minus.png
--rw-r--r--   0 barisits (51071) zp        (1307)      173 2016-11-25 15:03:16.000000 rucio-clients-1.9.6/doc/build/html/_static/plus.png
--rw-r--r--   0 barisits (51071) zp        (1307)     3991 2016-11-25 15:03:16.000000 rucio-clients-1.9.6/doc/build/html/_static/pygments.css
--rw-r--r--   0 barisits (51071) zp        (1307)    19563 2016-11-25 15:03:16.000000 rucio-clients-1.9.6/doc/build/html/_static/rucio_logo.png
--rw-r--r--   0 barisits (51071) zp        (1307)    17880 2016-11-25 15:03:16.000000 rucio-clients-1.9.6/doc/build/html/_static/searchtools.js
--rw-r--r--   0 barisits (51071) zp        (1307)     4803 2016-11-25 15:03:16.000000 rucio-clients-1.9.6/doc/build/html/_static/sidebar.js
--rw-r--r--   0 barisits (51071) zp        (1307)    35168 2016-11-25 15:03:16.000000 rucio-clients-1.9.6/doc/build/html/_static/underscore-1.3.1.js
--rw-r--r--   0 barisits (51071) zp        (1307)    12140 2016-11-25 15:03:16.000000 rucio-clients-1.9.6/doc/build/html/_static/underscore.js
--rw-r--r--   0 barisits (51071) zp        (1307)      345 2016-11-25 15:03:16.000000 rucio-clients-1.9.6/doc/build/html/_static/up-pressed.png
--rw-r--r--   0 barisits (51071) zp        (1307)      345 2016-11-25 15:03:16.000000 rucio-clients-1.9.6/doc/build/html/_static/up.png
--rw-r--r--   0 barisits (51071) zp        (1307)    25350 2016-11-25 15:03:16.000000 rucio-clients-1.9.6/doc/build/html/_static/websupport.js
-drwxr-xr-x   0 barisits (51071) zp        (1307)        0 2017-02-07 09:03:44.000000 rucio-clients-1.9.6/doc/build/html/man/
--rw-r--r--   0 barisits (51071) zp        (1307)     7794 2016-11-25 15:03:16.000000 rucio-clients-1.9.6/doc/build/html/man/rucio-admin.html
--rw-r--r--   0 barisits (51071) zp        (1307)     9052 2016-11-25 15:03:16.000000 rucio-clients-1.9.6/doc/build/html/man/rucio.html
-drwxr-xr-x   0 barisits (51071) zp        (1307)        0 2017-02-07 09:03:44.000000 rucio-clients-1.9.6/doc/build/html/rest/
--rw-r--r--   0 barisits (51071) zp        (1307)     5795 2016-11-25 15:03:16.000000 rucio-clients-1.9.6/doc/build/html/rest/attach_dids_to_dids.html
-drwxr-xr-x   0 barisits (51071) zp        (1307)        0 2017-02-07 09:03:44.000000 rucio-clients-1.9.6/doc/build/html/usecases/
--rw-r--r--   0 barisits (51071) zp        (1307)     4477 2016-11-25 15:03:15.000000 rucio-clients-1.9.6/doc/build/html/usecases/add_account_identity.html
--rw-r--r--   0 barisits (51071) zp        (1307)     4558 2016-11-25 15:03:15.000000 rucio-clients-1.9.6/doc/build/html/usecases/add_metadata_dataset.html
--rw-r--r--   0 barisits (51071) zp        (1307)     4543 2016-11-25 15:03:15.000000 rucio-clients-1.9.6/doc/build/html/usecases/add_metadata_file.html
--rw-r--r--   0 barisits (51071) zp        (1307)     4538 2016-11-25 15:03:15.000000 rucio-clients-1.9.6/doc/build/html/usecases/add_scope_to_account.html
--rw-r--r--   0 barisits (51071) zp        (1307)     4872 2016-11-25 15:03:15.000000 rucio-clients-1.9.6/doc/build/html/usecases/add_subscription.html
--rw-r--r--   0 barisits (51071) zp        (1307)     5938 2016-11-25 15:03:15.000000 rucio-clients-1.9.6/doc/build/html/usecases/authentication.html
--rw-r--r--   0 barisits (51071) zp        (1307)     4716 2016-11-25 15:03:15.000000 rucio-clients-1.9.6/doc/build/html/usecases/consistency_file_between_storage_and_rucio.html
--rw-r--r--   0 barisits (51071) zp        (1307)     4557 2016-11-25 15:03:15.000000 rucio-clients-1.9.6/doc/build/html/usecases/delete_file_replica_from_storage.html
--rw-r--r--   0 barisits (51071) zp        (1307)     4701 2016-11-25 15:03:15.000000 rucio-clients-1.9.6/doc/build/html/usecases/detect_site_reach_watermark.html
--rw-r--r--   0 barisits (51071) zp        (1307)     4745 2016-11-25 15:03:15.000000 rucio-clients-1.9.6/doc/build/html/usecases/download_all_files_from_a_given_list_of_file_replicas_from_rucio.html
--rw-r--r--   0 barisits (51071) zp        (1307)     4665 2016-11-25 15:03:15.000000 rucio-clients-1.9.6/doc/build/html/usecases/download_all_files_from_a_given_list_of_files_from_rucio.html
--rw-r--r--   0 barisits (51071) zp        (1307)     4595 2016-11-25 15:03:15.000000 rucio-clients-1.9.6/doc/build/html/usecases/download_all_files_in_a_dataset_from_rucio.html
--rw-r--r--   0 barisits (51071) zp        (1307)     4510 2016-11-25 15:03:15.000000 rucio-clients-1.9.6/doc/build/html/usecases/download_files_from_rucio.html
--rw-r--r--   0 barisits (51071) zp        (1307)     4433 2016-11-25 15:03:15.000000 rucio-clients-1.9.6/doc/build/html/usecases/obsolete_dataset.html
--rw-r--r--   0 barisits (51071) zp        (1307)     4551 2016-11-25 15:03:15.000000 rucio-clients-1.9.6/doc/build/html/usecases/register_account.html
--rw-r--r--   0 barisits (51071) zp        (1307)     4548 2016-11-25 15:03:15.000000 rucio-clients-1.9.6/doc/build/html/usecases/register_file_already_on_storage_system.html
--rw-r--r--   0 barisits (51071) zp        (1307)     4567 2016-11-25 15:03:15.000000 rucio-clients-1.9.6/doc/build/html/usecases/register_transfer_request_file_fts.html
--rw-r--r--   0 barisits (51071) zp        (1307)     4523 2016-11-25 15:03:15.000000 rucio-clients-1.9.6/doc/build/html/usecases/remove_replication_rules_from_file.html
--rw-r--r--   0 barisits (51071) zp        (1307)     4531 2016-11-25 15:03:15.000000 rucio-clients-1.9.6/doc/build/html/usecases/reupload_after_failure.html
--rw-r--r--   0 barisits (51071) zp        (1307)     4876 2016-11-25 15:03:15.000000 rucio-clients-1.9.6/doc/build/html/usecases/search.html
--rw-r--r--   0 barisits (51071) zp        (1307)     4523 2016-11-25 15:03:15.000000 rucio-clients-1.9.6/doc/build/html/usecases/select_unwanted_files_for_deletion.html
--rw-r--r--   0 barisits (51071) zp        (1307)     4549 2016-11-25 15:03:15.000000 rucio-clients-1.9.6/doc/build/html/usecases/set_replication_rule_to_file.html
--rw-r--r--   0 barisits (51071) zp        (1307)     4671 2016-11-25 15:03:15.000000 rucio-clients-1.9.6/doc/build/html/usecases/upload_file_with_replication_rule.html
--rw-r--r--   0 barisits (51071) zp        (1307)     5164 2016-11-25 15:03:15.000000 rucio-clients-1.9.6/doc/build/html/usecases/usecase_upload_file_into_rucio.html
--rw-r--r--   0 barisits (51071) zp        (1307)     4518 2016-11-25 15:03:15.000000 rucio-clients-1.9.6/doc/build/html/usecases/where_are_the_replicas_for_a_file.html
--rw-r--r--   0 barisits (51071) zp        (1307)      230 2016-11-25 15:03:16.000000 rucio-clients-1.9.6/doc/build/html/.buildinfo
--rw-r--r--   0 barisits (51071) zp        (1307)     5302 2016-11-25 15:03:15.000000 rucio-clients-1.9.6/doc/build/html/Acronyms_and_Abbreviations.html
--rw-r--r--   0 barisits (51071) zp        (1307)     4352 2016-11-25 15:03:15.000000 rucio-clients-1.9.6/doc/build/html/Architecture.html
--rw-r--r--   0 barisits (51071) zp        (1307)     6632 2016-11-25 15:03:16.000000 rucio-clients-1.9.6/doc/build/html/Comparison_matrix.html
--rw-r--r--   0 barisits (51071) zp        (1307)    33965 2016-11-25 15:03:16.000000 rucio-clients-1.9.6/doc/build/html/REST_Account.html
--rw-r--r--   0 barisits (51071) zp        (1307)     8155 2016-11-25 15:03:16.000000 rucio-clients-1.9.6/doc/build/html/REST_Intro.html
--rw-r--r--   0 barisits (51071) zp        (1307)    13611 2016-11-25 15:03:15.000000 rucio-clients-1.9.6/doc/build/html/REST_authentication.html
--rw-r--r--   0 barisits (51071) zp        (1307)    36870 2016-11-25 15:03:16.000000 rucio-clients-1.9.6/doc/build/html/REST_did.html
--rw-r--r--   0 barisits (51071) zp        (1307)     7178 2016-11-25 15:03:16.000000 rucio-clients-1.9.6/doc/build/html/REST_identity.html
--rw-r--r--   0 barisits (51071) zp        (1307)     8464 2016-11-25 15:03:16.000000 rucio-clients-1.9.6/doc/build/html/REST_lock.html
--rw-r--r--   0 barisits (51071) zp        (1307)    11196 2016-11-25 15:03:16.000000 rucio-clients-1.9.6/doc/build/html/REST_meta.html
--rw-r--r--   0 barisits (51071) zp        (1307)     6873 2016-11-25 15:03:16.000000 rucio-clients-1.9.6/doc/build/html/REST_redirect.html
--rw-r--r--   0 barisits (51071) zp        (1307)    19743 2016-11-25 15:03:16.000000 rucio-clients-1.9.6/doc/build/html/REST_replica.html
--rw-r--r--   0 barisits (51071) zp        (1307)    36294 2016-11-25 15:03:16.000000 rucio-clients-1.9.6/doc/build/html/REST_rse.html
--rw-r--r--   0 barisits (51071) zp        (1307)    15403 2016-11-25 15:03:16.000000 rucio-clients-1.9.6/doc/build/html/REST_rule.html
--rw-r--r--   0 barisits (51071) zp        (1307)     9564 2016-11-25 15:03:16.000000 rucio-clients-1.9.6/doc/build/html/REST_scope.html
--rw-r--r--   0 barisits (51071) zp        (1307)    15806 2016-11-25 15:03:16.000000 rucio-clients-1.9.6/doc/build/html/REST_subscription.html
--rw-r--r--   0 barisits (51071) zp        (1307)     5982 2016-11-25 15:03:16.000000 rucio-clients-1.9.6/doc/build/html/Rucio_Project_Meeting.html
--rw-r--r--   0 barisits (51071) zp        (1307)    24371 2016-11-25 15:03:15.000000 rucio-clients-1.9.6/doc/build/html/account.html
--rw-r--r--   0 barisits (51071) zp        (1307)     9490 2016-11-25 15:03:16.000000 rucio-clients-1.9.6/doc/build/html/accountlimit.html
--rw-r--r--   0 barisits (51071) zp        (1307)    49271 2016-11-25 15:03:15.000000 rucio-clients-1.9.6/doc/build/html/api_curl_examples.html
--rw-r--r--   0 barisits (51071) zp        (1307)     8847 2016-11-25 15:03:16.000000 rucio-clients-1.9.6/doc/build/html/atlas_integration_testbed.html
--rw-r--r--   0 barisits (51071) zp        (1307)     4218 2016-11-25 15:03:16.000000 rucio-clients-1.9.6/doc/build/html/authentication_and_identity.html
--rw-r--r--   0 barisits (51071) zp        (1307)    20697 2016-11-25 15:03:16.000000 rucio-clients-1.9.6/doc/build/html/cli_admin_examples.html
--rw-r--r--   0 barisits (51071) zp        (1307)    16629 2016-11-25 15:03:15.000000 rucio-clients-1.9.6/doc/build/html/cli_examples.html
--rw-r--r--   0 barisits (51071) zp        (1307)     7763 2016-11-25 15:03:16.000000 rucio-clients-1.9.6/doc/build/html/client_examples.html
--rw-r--r--   0 barisits (51071) zp        (1307)    51571 2016-11-25 15:03:16.000000 rucio-clients-1.9.6/doc/build/html/client_howto.html
--rw-r--r--   0 barisits (51071) zp        (1307)    22174 2016-11-25 15:03:15.000000 rucio-clients-1.9.6/doc/build/html/client_tutorial.html
--rw-r--r--   0 barisits (51071) zp        (1307)    38831 2016-11-25 15:03:16.000000 rucio-clients-1.9.6/doc/build/html/core_constants.html
--rw-r--r--   0 barisits (51071) zp        (1307)    13195 2016-11-25 15:03:15.000000 rucio-clients-1.9.6/doc/build/html/developing.html
--rw-r--r--   0 barisits (51071) zp        (1307)    11262 2016-11-25 15:03:15.000000 rucio-clients-1.9.6/doc/build/html/development_guidelines.html
--rw-r--r--   0 barisits (51071) zp        (1307)    42116 2016-11-25 15:03:15.000000 rucio-clients-1.9.6/doc/build/html/did.html
--rw-r--r--   0 barisits (51071) zp        (1307)    64292 2016-11-25 15:03:16.000000 rucio-clients-1.9.6/doc/build/html/exception.html
--rw-r--r--   0 barisits (51071) zp        (1307)    54351 2016-11-25 15:03:16.000000 rucio-clients-1.9.6/doc/build/html/genindex.html
--rw-r--r--   0 barisits (51071) zp        (1307)    22008 2016-11-25 15:03:16.000000 rucio-clients-1.9.6/doc/build/html/http-routingtable.html
--rw-r--r--   0 barisits (51071) zp        (1307)    13231 2016-11-25 15:03:16.000000 rucio-clients-1.9.6/doc/build/html/identity.html
--rw-r--r--   0 barisits (51071) zp        (1307)    15245 2016-11-25 15:03:16.000000 rucio-clients-1.9.6/doc/build/html/index.html
--rw-r--r--   0 barisits (51071) zp        (1307)    10693 2016-11-25 15:03:16.000000 rucio-clients-1.9.6/doc/build/html/installing_atlas_clients.html
--rw-r--r--   0 barisits (51071) zp        (1307)     9658 2016-11-25 15:03:16.000000 rucio-clients-1.9.6/doc/build/html/installing_clients.html
--rw-r--r--   0 barisits (51071) zp        (1307)     8756 2016-11-25 15:03:15.000000 rucio-clients-1.9.6/doc/build/html/installing_server.html
--rw-r--r--   0 barisits (51071) zp        (1307)     9333 2016-11-25 15:03:16.000000 rucio-clients-1.9.6/doc/build/html/lock.html
--rw-r--r--   0 barisits (51071) zp        (1307)    13990 2016-11-25 15:03:16.000000 rucio-clients-1.9.6/doc/build/html/meta-data.html
--rw-r--r--   0 barisits (51071) zp        (1307)     8368 2016-11-25 15:03:16.000000 rucio-clients-1.9.6/doc/build/html/naming_convention_db.html
--rw-r--r--   0 barisits (51071) zp        (1307)     4611 2016-11-25 15:03:16.000000 rucio-clients-1.9.6/doc/build/html/objects.inv
--rw-r--r--   0 barisits (51071) zp        (1307)     5892 2016-11-25 15:03:16.000000 rucio-clients-1.9.6/doc/build/html/overview_Accounting_and_quota.html
--rw-r--r--   0 barisits (51071) zp        (1307)     5729 2016-11-25 15:03:16.000000 rucio-clients-1.9.6/doc/build/html/overview_Architecture.html
--rw-r--r--   0 barisits (51071) zp        (1307)     7965 2016-11-25 15:03:16.000000 rucio-clients-1.9.6/doc/build/html/overview_Data_Deletion.html
--rw-r--r--   0 barisits (51071) zp        (1307)     5828 2016-11-25 15:03:16.000000 rucio-clients-1.9.6/doc/build/html/overview_Database_Schema.html
--rw-r--r--   0 barisits (51071) zp        (1307)     5590 2016-11-25 15:03:16.000000 rucio-clients-1.9.6/doc/build/html/overview_Deployment.html
--rw-r--r--   0 barisits (51071) zp        (1307)     7307 2016-11-25 15:03:16.000000 rucio-clients-1.9.6/doc/build/html/overview_Exception_Handling.html
--rw-r--r--   0 barisits (51071) zp        (1307)    11254 2016-11-25 15:03:16.000000 rucio-clients-1.9.6/doc/build/html/overview_File_Dataset_Container.html
--rw-r--r--   0 barisits (51071) zp        (1307)     6831 2016-11-25 15:03:16.000000 rucio-clients-1.9.6/doc/build/html/overview_Meta-data_attributes.html
--rw-r--r--   0 barisits (51071) zp        (1307)     5732 2016-11-25 15:03:16.000000 rucio-clients-1.9.6/doc/build/html/overview_Notifications.html
--rw-r--r--   0 barisits (51071) zp        (1307)     5700 2016-11-25 15:03:16.000000 rucio-clients-1.9.6/doc/build/html/overview_Permission_model.html
--rw-r--r--   0 barisits (51071) zp        (1307)     8062 2016-11-25 15:03:16.000000 rucio-clients-1.9.6/doc/build/html/overview_Replica_management.html
--rw-r--r--   0 barisits (51071) zp        (1307)     7173 2016-11-25 15:03:16.000000 rucio-clients-1.9.6/doc/build/html/overview_Rucio_Storage_Element.html
--rw-r--r--   0 barisits (51071) zp        (1307)    46544 2016-11-25 15:03:16.000000 rucio-clients-1.9.6/doc/build/html/overview_Rucio_Storage_Element_Manager.html
--rw-r--r--   0 barisits (51071) zp        (1307)     6701 2016-11-25 15:03:16.000000 rucio-clients-1.9.6/doc/build/html/overview_Rucio_account.html
--rw-r--r--   0 barisits (51071) zp        (1307)    36223 2016-11-25 15:03:15.000000 rucio-clients-1.9.6/doc/build/html/overview_Rules.html
--rw-r--r--   0 barisits (51071) zp        (1307)     8997 2016-11-25 15:03:16.000000 rucio-clients-1.9.6/doc/build/html/overview_Scheduled_Transfers.html
--rw-r--r--   0 barisits (51071) zp        (1307)    14110 2016-11-25 15:03:16.000000 rucio-clients-1.9.6/doc/build/html/overview_Subscriptions.html
--rw-r--r--   0 barisits (51071) zp        (1307)     4733 2016-11-25 15:03:16.000000 rucio-clients-1.9.6/doc/build/html/overview_Traceability_and_logging.html
--rw-r--r--   0 barisits (51071) zp        (1307)     5951 2016-11-25 15:03:16.000000 rucio-clients-1.9.6/doc/build/html/overview_flow.html
--rw-r--r--   0 barisits (51071) zp        (1307)     5808 2016-11-25 15:03:16.000000 rucio-clients-1.9.6/doc/build/html/permission.html
--rw-r--r--   0 barisits (51071) zp        (1307)     7644 2016-11-25 15:03:16.000000 rucio-clients-1.9.6/doc/build/html/py-modindex.html
--rw-r--r--   0 barisits (51071) zp        (1307)     5259 2016-11-25 15:03:16.000000 rucio-clients-1.9.6/doc/build/html/replica.html
--rw-r--r--   0 barisits (51071) zp        (1307)    11051 2016-11-25 15:03:16.000000 rucio-clients-1.9.6/doc/build/html/replication_rules_examples.html
--rw-r--r--   0 barisits (51071) zp        (1307)    22089 2016-11-25 15:03:15.000000 rucio-clients-1.9.6/doc/build/html/rest.html
--rw-r--r--   0 barisits (51071) zp        (1307)    47247 2016-11-25 15:03:16.000000 rucio-clients-1.9.6/doc/build/html/rse.html
--rw-r--r--   0 barisits (51071) zp        (1307)     6001 2016-11-25 15:03:16.000000 rucio-clients-1.9.6/doc/build/html/rucio_cli.html
--rw-r--r--   0 barisits (51071) zp        (1307)     5824 2016-11-25 15:03:16.000000 rucio-clients-1.9.6/doc/build/html/rucio_clients.html
--rw-r--r--   0 barisits (51071) zp        (1307)    13860 2016-11-25 15:03:15.000000 rucio-clients-1.9.6/doc/build/html/rule.html
--rw-r--r--   0 barisits (51071) zp        (1307)     6962 2016-11-25 15:03:16.000000 rucio-clients-1.9.6/doc/build/html/rules_workflow.html
--rw-r--r--   0 barisits (51071) zp        (1307)    10313 2016-11-25 15:03:15.000000 rucio-clients-1.9.6/doc/build/html/scope.html
--rw-r--r--   0 barisits (51071) zp        (1307)     4453 2016-11-25 15:03:16.000000 rucio-clients-1.9.6/doc/build/html/search.html
--rw-r--r--   0 barisits (51071) zp        (1307)    57160 2016-11-25 15:03:16.000000 rucio-clients-1.9.6/doc/build/html/searchindex.js
--rw-r--r--   0 barisits (51071) zp        (1307)    17589 2016-11-25 15:03:16.000000 rucio-clients-1.9.6/doc/build/html/setup.html
--rw-r--r--   0 barisits (51071) zp        (1307)    14675 2016-11-25 15:03:16.000000 rucio-clients-1.9.6/doc/build/html/subscription.html
--rw-r--r--   0 barisits (51071) zp        (1307)    10373 2016-11-25 15:03:16.000000 rucio-clients-1.9.6/doc/build/html/usecases.html
-drwxr-xr-x   0 barisits (51071) zp        (1307)        0 2017-02-07 09:03:44.000000 rucio-clients-1.9.6/doc/design/
-drwxr-xr-x   0 barisits (51071) zp        (1307)        0 2017-02-07 09:03:44.000000 rucio-clients-1.9.6/doc/design/RSE/
--rwxr-xr-x   0 barisits (51071) zp        (1307)     1996 2016-11-25 15:17:13.000000 rucio-clients-1.9.6/doc/design/RSE/meeting-2012-02-23.org
-drwxr-xr-x   0 barisits (51071) zp        (1307)        0 2017-02-07 09:03:44.000000 rucio-clients-1.9.6/doc/design/erd/
--rwxr-xr-x   0 barisits (51071) zp        (1307)    55942 2016-11-25 15:17:13.000000 rucio-clients-1.9.6/doc/design/erd/ERD.graffle
--rwxr-xr-x   0 barisits (51071) zp        (1307)    72676 2016-11-25 15:17:13.000000 rucio-clients-1.9.6/doc/design/erd/ERD.pdf
--rwxr-xr-x   0 barisits (51071) zp        (1307)    60363 2016-11-25 15:17:13.000000 rucio-clients-1.9.6/doc/design/rucio_schema.pdf
-drwxr-xr-x   0 barisits (51071) zp        (1307)        0 2017-02-07 09:03:44.000000 rucio-clients-1.9.6/doc/source/
-drwxr-xr-x   0 barisits (51071) zp        (1307)        0 2017-02-07 09:03:44.000000 rucio-clients-1.9.6/doc/source/_static/
--rwxr-xr-x   0 barisits (51071) zp        (1307)     8270 2016-11-25 15:17:13.000000 rucio-clients-1.9.6/doc/source/_static/basic.css
--rw-r--r--   0 barisits (51071) zp        (1307)     4179 2016-11-25 15:17:13.000000 rucio-clients-1.9.6/doc/source/_static/classic.css
--rwxr-xr-x   0 barisits (51071) zp        (1307)     4041 2016-11-25 15:17:13.000000 rucio-clients-1.9.6/doc/source/_static/default.css
--rwxr-xr-x   0 barisits (51071) zp        (1307)    19563 2016-11-25 15:17:13.000000 rucio-clients-1.9.6/doc/source/_static/rucio_logo.png
-drwxr-xr-x   0 barisits (51071) zp        (1307)        0 2017-02-07 09:03:44.000000 rucio-clients-1.9.6/doc/source/_templates/
--rwxr-xr-x   0 barisits (51071) zp        (1307)      777 2016-11-25 15:17:13.000000 rucio-clients-1.9.6/doc/source/_templates/layout.html
-drwxr-xr-x   0 barisits (51071) zp        (1307)        0 2017-02-07 09:03:44.000000 rucio-clients-1.9.6/doc/source/cli_examples/
--rw-r--r--   0 barisits (51071) zp        (1307)       14 2016-11-25 15:17:13.000000 rucio-clients-1.9.6/doc/source/cli_examples/ping.out
--rwxr-xr-x   0 barisits (51071) zp        (1307)       12 2016-11-25 15:17:13.000000 rucio-clients-1.9.6/doc/source/cli_examples/ping.sh
-drwxr-xr-x   0 barisits (51071) zp        (1307)        0 2017-02-07 09:03:44.000000 rucio-clients-1.9.6/doc/source/curl_examples/
--rw-r--r--   0 barisits (51071) zp        (1307)      235 2016-11-25 15:17:13.000000 rucio-clients-1.9.6/doc/source/curl_examples/del_account.out
--rwxr-xr-x   0 barisits (51071) zp        (1307)      312 2016-11-25 15:17:13.000000 rucio-clients-1.9.6/doc/source/curl_examples/del_account.sh
--rw-r--r--   0 barisits (51071) zp        (1307)      289 2016-11-25 15:17:13.000000 rucio-clients-1.9.6/doc/source/curl_examples/del_location.out
--rwxr-xr-x   0 barisits (51071) zp        (1307)      313 2016-11-25 15:17:13.000000 rucio-clients-1.9.6/doc/source/curl_examples/del_location.sh
--rw-r--r--   0 barisits (51071) zp        (1307)      404 2016-11-25 15:17:13.000000 rucio-clients-1.9.6/doc/source/curl_examples/get_account.out
--rwxr-xr-x   0 barisits (51071) zp        (1307)      309 2016-11-25 15:17:13.000000 rucio-clients-1.9.6/doc/source/curl_examples/get_account.sh
--rw-r--r--   0 barisits (51071) zp        (1307)      673 2016-11-25 15:17:13.000000 rucio-clients-1.9.6/doc/source/curl_examples/get_account_whoami.out
--rwxr-xr-x   0 barisits (51071) zp        (1307)      314 2016-11-25 15:17:13.000000 rucio-clients-1.9.6/doc/source/curl_examples/get_account_whoami.sh
--rw-r--r--   0 barisits (51071) zp        (1307)      253 2016-11-25 15:17:13.000000 rucio-clients-1.9.6/doc/source/curl_examples/get_accounts.out
--rwxr-xr-x   0 barisits (51071) zp        (1307)      305 2016-11-25 15:17:13.000000 rucio-clients-1.9.6/doc/source/curl_examples/get_accounts.sh
--rw-r--r--   0 barisits (51071) zp        (1307)       29 2016-11-25 15:17:13.000000 rucio-clients-1.9.6/doc/source/curl_examples/get_api.out
--rwxr-xr-x   0 barisits (51071) zp        (1307)       34 2016-11-25 15:17:13.000000 rucio-clients-1.9.6/doc/source/curl_examples/get_api.sh
--rw-r--r--   0 barisits (51071) zp        (1307)      289 2016-11-25 15:17:13.000000 rucio-clients-1.9.6/doc/source/curl_examples/get_auth_gss.out
--rwxr-xr-x   0 barisits (51071) zp        (1307)      129 2016-11-25 15:17:13.000000 rucio-clients-1.9.6/doc/source/curl_examples/get_auth_gss.sh
--rw-r--r--   0 barisits (51071) zp        (1307)      289 2016-11-25 15:17:13.000000 rucio-clients-1.9.6/doc/source/curl_examples/get_auth_userpass.out
--rwxr-xr-x   0 barisits (51071) zp        (1307)      168 2016-11-25 15:17:13.000000 rucio-clients-1.9.6/doc/source/curl_examples/get_auth_userpass.sh
--rw-r--r--   0 barisits (51071) zp        (1307)      304 2016-11-25 15:17:13.000000 rucio-clients-1.9.6/doc/source/curl_examples/get_auth_validate.out
--rwxr-xr-x   0 barisits (51071) zp        (1307)      334 2016-11-25 15:17:13.000000 rucio-clients-1.9.6/doc/source/curl_examples/get_auth_validate.sh
--rw-r--r--   0 barisits (51071) zp        (1307)      289 2016-11-25 15:17:13.000000 rucio-clients-1.9.6/doc/source/curl_examples/get_auth_x509.out
--rwxr-xr-x   0 barisits (51071) zp        (1307)      139 2016-11-25 15:17:13.000000 rucio-clients-1.9.6/doc/source/curl_examples/get_auth_x509.sh
--rw-r--r--   0 barisits (51071) zp        (1307)      289 2016-11-25 15:17:13.000000 rucio-clients-1.9.6/doc/source/curl_examples/get_location.out
--rwxr-xr-x   0 barisits (51071) zp        (1307)      310 2016-11-25 15:17:13.000000 rucio-clients-1.9.6/doc/source/curl_examples/get_location.sh
--rw-r--r--   0 barisits (51071) zp        (1307)      239 2016-11-25 15:17:13.000000 rucio-clients-1.9.6/doc/source/curl_examples/get_locations.out
--rwxr-xr-x   0 barisits (51071) zp        (1307)      306 2016-11-25 15:17:13.000000 rucio-clients-1.9.6/doc/source/curl_examples/get_locations.sh
--rw-r--r--   0 barisits (51071) zp        (1307)      356 2016-11-25 15:17:13.000000 rucio-clients-1.9.6/doc/source/curl_examples/get_scopes.out
--rwxr-xr-x   0 barisits (51071) zp        (1307)      309 2016-11-25 15:17:13.000000 rucio-clients-1.9.6/doc/source/curl_examples/get_scopes.sh
--rwxr-xr-x   0 barisits (51071) zp        (1307)      380 2016-11-25 15:17:13.000000 rucio-clients-1.9.6/doc/source/curl_examples/post_account.sh
--rwxr-xr-x   0 barisits (51071) zp        (1307)      332 2016-11-25 15:17:13.000000 rucio-clients-1.9.6/doc/source/curl_examples/post_location.sh
--rw-r--r--   0 barisits (51071) zp        (1307)      370 2016-11-25 15:17:13.000000 rucio-clients-1.9.6/doc/source/curl_examples/post_location_rse.sh
--rw-r--r--   0 barisits (51071) zp        (1307)      257 2016-11-25 15:17:13.000000 rucio-clients-1.9.6/doc/source/curl_examples/put_account.out
--rwxr-xr-x   0 barisits (51071) zp        (1307)      331 2016-11-25 15:17:13.000000 rucio-clients-1.9.6/doc/source/curl_examples/put_account.sh
--rw-r--r--   0 barisits (51071) zp        (1307)      289 2016-11-25 15:17:13.000000 rucio-clients-1.9.6/doc/source/curl_examples/put_location.out
--rwxr-xr-x   0 barisits (51071) zp        (1307)      309 2016-11-25 15:17:13.000000 rucio-clients-1.9.6/doc/source/curl_examples/put_location.sh
--rw-r--r--   0 barisits (51071) zp        (1307)      257 2016-11-25 15:17:13.000000 rucio-clients-1.9.6/doc/source/curl_examples/put_scope.out
--rwxr-xr-x   0 barisits (51071) zp        (1307)      314 2016-11-25 15:17:13.000000 rucio-clients-1.9.6/doc/source/curl_examples/put_scope.sh
-drwxr-xr-x   0 barisits (51071) zp        (1307)        0 2017-02-07 09:03:44.000000 rucio-clients-1.9.6/doc/source/example_outputs/
--rw-r--r--   0 barisits (51071) zp        (1307)     3643 2016-11-25 15:17:13.000000 rucio-clients-1.9.6/doc/source/example_outputs/failure-rucio.tests.test_curl.TestCurlRucio.test_get_accounts_whoami.txt
--rw-r--r--   0 barisits (51071) zp        (1307)      164 2016-11-25 15:17:13.000000 rucio-clients-1.9.6/doc/source/example_outputs/success-rucio.tests.test_bin_rucio.TestBinRucio.test_add_account.txt
--rw-r--r--   0 barisits (51071) zp        (1307)       43 2016-11-25 15:17:13.000000 rucio-clients-1.9.6/doc/source/example_outputs/success-rucio.tests.test_bin_rucio.TestBinRucio.test_rucio_ping.txt
--rw-r--r--   0 barisits (51071) zp        (1307)        0 2016-11-25 15:17:13.000000 rucio-clients-1.9.6/doc/source/example_outputs/success-rucio.tests.test_bin_rucio.TestBinRucio.test_rucio_version.txt
--rw-r--r--   0 barisits (51071) zp        (1307)       46 2016-11-25 15:17:13.000000 rucio-clients-1.9.6/doc/source/example_outputs/success-rucio.tests.test_clients.TestRucioClients.test_ping.txt
--rw-r--r--   0 barisits (51071) zp        (1307)      804 2016-11-25 15:17:13.000000 rucio-clients-1.9.6/doc/source/example_outputs/success-rucio.tests.test_curl.TestCurlRucio.test_get_accounts_whoami.txt
--rw-r--r--   0 barisits (51071) zp        (1307)      429 2016-11-25 15:17:13.000000 rucio-clients-1.9.6/doc/source/example_outputs/success-rucio.tests.test_curl.TestCurlRucio.test_get_auth_GSS.txt
--rw-r--r--   0 barisits (51071) zp        (1307)        0 2016-11-25 15:17:13.000000 rucio-clients-1.9.6/doc/source/example_outputs/success-rucio.tests.test_curl.TestCurlRucio.test_get_auth_proxy.txt
--rw-r--r--   0 barisits (51071) zp        (1307)      466 2016-11-25 15:17:13.000000 rucio-clients-1.9.6/doc/source/example_outputs/success-rucio.tests.test_curl.TestCurlRucio.test_get_auth_userpass.txt
--rw-r--r--   0 barisits (51071) zp        (1307)      454 2016-11-25 15:17:13.000000 rucio-clients-1.9.6/doc/source/example_outputs/success-rucio.tests.test_curl.TestCurlRucio.test_get_auth_validate.txt
--rw-r--r--   0 barisits (51071) zp        (1307)      442 2016-11-25 15:17:13.000000 rucio-clients-1.9.6/doc/source/example_outputs/success-rucio.tests.test_curl.TestCurlRucio.test_get_auth_x509.txt
--rw-r--r--   0 barisits (51071) zp        (1307)      454 2016-11-25 15:17:13.000000 rucio-clients-1.9.6/doc/source/example_outputs/success-rucio.tests.test_curl.TestCurlRucio.test_get_auth_x509_proxy.txt
--rw-r--r--   0 barisits (51071) zp        (1307)       84 2016-11-25 15:17:13.000000 rucio-clients-1.9.6/doc/source/example_outputs/success-rucio.tests.test_curl.TestCurlRucio.test_ping.txt
--rw-r--r--   0 barisits (51071) zp        (1307)      440 2016-11-25 15:17:13.000000 rucio-clients-1.9.6/doc/source/example_outputs/success-rucio.tests.test_curl.TestCurlRucio.test_post_account.txt
--rw-r--r--   0 barisits (51071) zp        (1307)      398 2016-11-25 15:17:13.000000 rucio-clients-1.9.6/doc/source/example_outputs/success-rucio.tests.test_curl.TestCurlRucio.test_post_rse.txt
-drwxr-xr-x   0 barisits (51071) zp        (1307)        0 2017-02-07 09:03:44.000000 rucio-clients-1.9.6/doc/source/images/
-drwxr-xr-x   0 barisits (51071) zp        (1307)        0 2017-02-07 09:03:44.000000 rucio-clients-1.9.6/doc/source/images/deployment.graffle/
-drwxr-xr-x   0 barisits (51071) zp        (1307)        0 2017-02-07 09:03:44.000000 rucio-clients-1.9.6/doc/source/images/deployment.graffle/QuickLook/
--rw-r--r--   0 barisits (51071) zp        (1307)    52757 2016-11-25 15:17:13.000000 rucio-clients-1.9.6/doc/source/images/deployment.graffle/QuickLook/Preview.pdf
--rw-r--r--   0 barisits (51071) zp        (1307)   124440 2016-11-25 15:17:13.000000 rucio-clients-1.9.6/doc/source/images/deployment.graffle/QuickLook/Thumbnail.tiff
--rw-r--r--   0 barisits (51071) zp        (1307)     3172 2016-11-25 15:17:13.000000 rucio-clients-1.9.6/doc/source/images/deployment.graffle/data.plist
--rw-r--r--   0 barisits (51071) zp        (1307)    26954 2016-11-25 15:17:13.000000 rucio-clients-1.9.6/doc/source/images/deployment.graffle/image2.tiff
--rw-r--r--   0 barisits (51071) zp        (1307)    10020 2016-11-25 15:17:13.000000 rucio-clients-1.9.6/doc/source/images/deployment.graffle/image3.tiff
--rwxr-xr-x   0 barisits (51071) zp        (1307)    71679 2016-11-25 15:17:13.000000 rucio-clients-1.9.6/doc/source/images/RSE_overview.png
--rwxr-xr-x   0 barisits (51071) zp        (1307)    45789 2016-11-25 15:17:13.000000 rucio-clients-1.9.6/doc/source/images/RSE_sequence_instantiation.png
--rwxr-xr-x   0 barisits (51071) zp        (1307)    32727 2016-11-25 15:17:13.000000 rucio-clients-1.9.6/doc/source/images/RSE_sequence_usage.png
--rwxr-xr-x   0 barisits (51071) zp        (1307)    56522 2016-11-25 15:17:13.000000 rucio-clients-1.9.6/doc/source/images/accounts.png
--rw-r--r--   0 barisits (51071) zp        (1307)   397841 2016-11-25 15:17:13.000000 rucio-clients-1.9.6/doc/source/images/architecture.graffle
--rw-r--r--   0 barisits (51071) zp        (1307)   136638 2016-11-25 15:17:13.000000 rucio-clients-1.9.6/doc/source/images/architecture.png
--rw-r--r--   0 barisits (51071) zp        (1307)    82950 2016-11-25 15:17:13.000000 rucio-clients-1.9.6/doc/source/images/deployment.png
--rwxr-xr-x   0 barisits (51071) zp        (1307)    70361 2016-11-25 15:17:13.000000 rucio-clients-1.9.6/doc/source/images/exception_handling.graffle
--rwxr-xr-x   0 barisits (51071) zp        (1307)   163115 2016-11-25 15:17:13.000000 rucio-clients-1.9.6/doc/source/images/exception_handling.png
--rwxr-xr-x   0 barisits (51071) zp        (1307)   127009 2016-11-25 15:17:13.000000 rucio-clients-1.9.6/doc/source/images/highLevelRoadmap.graffle
--rwxr-xr-x   0 barisits (51071) zp        (1307)   205501 2016-11-25 15:17:13.000000 rucio-clients-1.9.6/doc/source/images/rucio-get.graffle
--rwxr-xr-x   0 barisits (51071) zp        (1307)    45282 2016-11-25 15:17:13.000000 rucio-clients-1.9.6/doc/source/images/rucio-get.png
--rwxr-xr-x   0 barisits (51071) zp        (1307)   298526 2016-11-25 15:17:13.000000 rucio-clients-1.9.6/doc/source/images/rucio-put.graffle
--rwxr-xr-x   0 barisits (51071) zp        (1307)    73288 2016-11-25 15:17:13.000000 rucio-clients-1.9.6/doc/source/images/rucio-put.png
--rwxr-xr-x   0 barisits (51071) zp        (1307)   232239 2016-11-25 15:17:13.000000 rucio-clients-1.9.6/doc/source/images/rucio-register.graffle
--rwxr-xr-x   0 barisits (51071) zp        (1307)    55513 2016-11-25 15:17:13.000000 rucio-clients-1.9.6/doc/source/images/rucio-register.png
--rw-r--r--   0 barisits (51071) zp        (1307)    62543 2016-11-25 15:17:13.000000 rucio-clients-1.9.6/doc/source/images/rucio_schema.gml
--rwxr-xr-x   0 barisits (51071) zp        (1307)   226250 2016-11-25 15:17:13.000000 rucio-clients-1.9.6/doc/source/images/rucio_schema.pdf
--rwxr-xr-x   0 barisits (51071) zp        (1307)   230376 2016-11-25 15:17:14.000000 rucio-clients-1.9.6/doc/source/images/rucio_schema.png
--rw-r--r--   0 barisits (51071) zp        (1307)     2485 2016-11-25 15:17:14.000000 rucio-clients-1.9.6/doc/source/images/scheduled_transfers.dia
--rw-r--r--   0 barisits (51071) zp        (1307)    26628 2016-11-25 15:17:14.000000 rucio-clients-1.9.6/doc/source/images/scheduled_transfers.png
-drwxr-xr-x   0 barisits (51071) zp        (1307)        0 2017-02-07 09:03:44.000000 rucio-clients-1.9.6/doc/source/man/
--rwxr-xr-x   0 barisits (51071) zp        (1307)      770 2016-11-25 15:17:14.000000 rucio-clients-1.9.6/doc/source/man/rucio-admin.rst
--rwxr-xr-x   0 barisits (51071) zp        (1307)     1744 2016-11-25 15:17:14.000000 rucio-clients-1.9.6/doc/source/man/rucio.rst
-drwxr-xr-x   0 barisits (51071) zp        (1307)        0 2017-02-07 09:03:44.000000 rucio-clients-1.9.6/doc/source/rest/
--rw-r--r--   0 barisits (51071) zp        (1307)      350 2016-11-25 15:17:14.000000 rucio-clients-1.9.6/doc/source/rest/attach_dids_to_dids.rst
-drwxr-xr-x   0 barisits (51071) zp        (1307)        0 2017-02-07 09:03:44.000000 rucio-clients-1.9.6/doc/source/usecases/
--rw-r--r--   0 barisits (51071) zp        (1307)      783 2016-11-25 15:17:14.000000 rucio-clients-1.9.6/doc/source/usecases/add_account_identity.rst
--rw-r--r--   0 barisits (51071) zp        (1307)      708 2016-11-25 15:17:14.000000 rucio-clients-1.9.6/doc/source/usecases/add_metadata_dataset.rst
--rw-r--r--   0 barisits (51071) zp        (1307)      691 2016-11-25 15:17:14.000000 rucio-clients-1.9.6/doc/source/usecases/add_metadata_file.rst
--rw-r--r--   0 barisits (51071) zp        (1307)      655 2016-11-25 15:17:14.000000 rucio-clients-1.9.6/doc/source/usecases/add_scope_to_account.rst
--rw-r--r--   0 barisits (51071) zp        (1307)     1470 2016-11-25 15:17:14.000000 rucio-clients-1.9.6/doc/source/usecases/add_subscription.rst
--rw-r--r--   0 barisits (51071) zp        (1307)     1677 2016-11-25 15:17:14.000000 rucio-clients-1.9.6/doc/source/usecases/authentication.rst
--rw-r--r--   0 barisits (51071) zp        (1307)      804 2016-11-25 15:17:14.000000 rucio-clients-1.9.6/doc/source/usecases/consistency_file_between_storage_and_rucio.rst
--rw-r--r--   0 barisits (51071) zp        (1307)      902 2016-11-25 15:17:14.000000 rucio-clients-1.9.6/doc/source/usecases/delete_file_replica_from_storage.rst
--rw-r--r--   0 barisits (51071) zp        (1307)      795 2016-11-25 15:17:14.000000 rucio-clients-1.9.6/doc/source/usecases/detect_site_reach_watermark.rst
--rw-r--r--   0 barisits (51071) zp        (1307)      940 2016-11-25 15:17:14.000000 rucio-clients-1.9.6/doc/source/usecases/download_all_files_from_a_given_list_of_file_replicas_from_rucio.rst
--rw-r--r--   0 barisits (51071) zp        (1307)      886 2016-11-25 15:17:14.000000 rucio-clients-1.9.6/doc/source/usecases/download_all_files_from_a_given_list_of_files_from_rucio.rst
--rw-r--r--   0 barisits (51071) zp        (1307)      900 2016-11-25 15:17:14.000000 rucio-clients-1.9.6/doc/source/usecases/download_all_files_in_a_dataset_from_rucio.rst
--rw-r--r--   0 barisits (51071) zp        (1307)      794 2016-11-25 15:17:14.000000 rucio-clients-1.9.6/doc/source/usecases/download_files_from_rucio.rst
--rw-r--r--   0 barisits (51071) zp        (1307)      564 2016-11-25 15:17:14.000000 rucio-clients-1.9.6/doc/source/usecases/obsolete_dataset.rst
--rw-r--r--   0 barisits (51071) zp        (1307)      652 2016-11-25 15:17:14.000000 rucio-clients-1.9.6/doc/source/usecases/register_account.rst
--rw-r--r--   0 barisits (51071) zp        (1307)      916 2016-11-25 15:17:14.000000 rucio-clients-1.9.6/doc/source/usecases/register_file_already_on_storage_system.rst
--rw-r--r--   0 barisits (51071) zp        (1307)      646 2016-11-25 15:17:14.000000 rucio-clients-1.9.6/doc/source/usecases/register_transfer_request_file_fts.rst
--rw-r--r--   0 barisits (51071) zp        (1307)      563 2016-11-25 15:17:14.000000 rucio-clients-1.9.6/doc/source/usecases/remove_replication_rules_from_file.rst
--rw-r--r--   0 barisits (51071) zp        (1307)     1270 2016-11-25 15:17:14.000000 rucio-clients-1.9.6/doc/source/usecases/reupload_after_failure.rst
--rw-r--r--   0 barisits (51071) zp        (1307)     1102 2016-11-25 15:17:14.000000 rucio-clients-1.9.6/doc/source/usecases/search.rst
--rw-r--r--   0 barisits (51071) zp        (1307)      409 2016-11-25 15:17:14.000000 rucio-clients-1.9.6/doc/source/usecases/select_unwanted_files_for_deletion.rst
--rw-r--r--   0 barisits (51071) zp        (1307)      590 2016-11-25 15:17:14.000000 rucio-clients-1.9.6/doc/source/usecases/set_replication_rule_to_file.rst
--rw-r--r--   0 barisits (51071) zp        (1307)      852 2016-11-25 15:17:14.000000 rucio-clients-1.9.6/doc/source/usecases/upload_file_with_replication_rule.rst
--rw-r--r--   0 barisits (51071) zp        (1307)     1106 2016-11-25 15:17:14.000000 rucio-clients-1.9.6/doc/source/usecases/usecase_upload_file_into_rucio.rst
--rw-r--r--   0 barisits (51071) zp        (1307)      633 2016-11-25 15:17:14.000000 rucio-clients-1.9.6/doc/source/usecases/where_are_the_replicas_for_a_file.rst
--rwxr-xr-x   0 barisits (51071) zp        (1307)      325 2016-11-25 15:17:13.000000 rucio-clients-1.9.6/doc/source/Acronyms_and_Abbreviations.rst
--rwxr-xr-x   0 barisits (51071) zp        (1307)       39 2016-11-25 15:17:13.000000 rucio-clients-1.9.6/doc/source/Architecture.rst
--rwxr-xr-x   0 barisits (51071) zp        (1307)     2067 2016-11-25 15:17:13.000000 rucio-clients-1.9.6/doc/source/Comparison_matrix.rst
--rw-r--r--   0 barisits (51071) zp        (1307)     7173 2016-11-25 15:17:13.000000 rucio-clients-1.9.6/doc/source/REST_Account.rst
--rw-r--r--   0 barisits (51071) zp        (1307)     1586 2016-11-25 15:17:13.000000 rucio-clients-1.9.6/doc/source/REST_Intro.rst
--rw-r--r--   0 barisits (51071) zp        (1307)     2439 2016-11-25 15:17:13.000000 rucio-clients-1.9.6/doc/source/REST_authentication.rst
--rw-r--r--   0 barisits (51071) zp        (1307)     7945 2016-11-25 15:17:13.000000 rucio-clients-1.9.6/doc/source/REST_did.rst
--rw-r--r--   0 barisits (51071) zp        (1307)      611 2016-11-25 15:17:13.000000 rucio-clients-1.9.6/doc/source/REST_identity.rst
--rw-r--r--   0 barisits (51071) zp        (1307)      637 2016-11-25 15:17:13.000000 rucio-clients-1.9.6/doc/source/REST_lock.rst
--rw-r--r--   0 barisits (51071) zp        (1307)     1444 2016-11-25 15:17:13.000000 rucio-clients-1.9.6/doc/source/REST_meta.rst
--rw-r--r--   0 barisits (51071) zp        (1307)      349 2016-11-25 15:17:13.000000 rucio-clients-1.9.6/doc/source/REST_redirect.rst
--rw-r--r--   0 barisits (51071) zp        (1307)     3702 2016-11-25 15:17:13.000000 rucio-clients-1.9.6/doc/source/REST_replica.rst
--rw-r--r--   0 barisits (51071) zp        (1307)     7502 2016-11-25 15:17:13.000000 rucio-clients-1.9.6/doc/source/REST_rse.rst
--rw-r--r--   0 barisits (51071) zp        (1307)     2509 2016-11-25 15:17:13.000000 rucio-clients-1.9.6/doc/source/REST_rule.rst
--rw-r--r--   0 barisits (51071) zp        (1307)     1034 2016-11-25 15:17:13.000000 rucio-clients-1.9.6/doc/source/REST_scope.rst
--rw-r--r--   0 barisits (51071) zp        (1307)     2512 2016-11-25 15:17:13.000000 rucio-clients-1.9.6/doc/source/REST_subscription.rst
--rw-r--r--   0 barisits (51071) zp        (1307)     6000 2016-11-25 15:17:13.000000 rucio-clients-1.9.6/doc/source/RSE_Expressions.rst
--rwxr-xr-x   0 barisits (51071) zp        (1307)      662 2016-11-25 15:17:13.000000 rucio-clients-1.9.6/doc/source/Rucio_Project_Meeting.rst
--rwxr-xr-x   0 barisits (51071) zp        (1307)      163 2016-11-25 15:17:13.000000 rucio-clients-1.9.6/doc/source/account.rst
--rw-r--r--   0 barisits (51071) zp        (1307)      184 2016-11-25 15:17:13.000000 rucio-clients-1.9.6/doc/source/accountlimit.rst
--rwxr-xr-x   0 barisits (51071) zp        (1307)    19866 2016-11-25 15:17:13.000000 rucio-clients-1.9.6/doc/source/accounts.pdf
--rwxr-xr-x   0 barisits (51071) zp        (1307)    56522 2016-11-25 15:17:13.000000 rucio-clients-1.9.6/doc/source/accounts.png
--rw-r--r--   0 barisits (51071) zp        (1307)    16748 2016-11-25 15:17:13.000000 rucio-clients-1.9.6/doc/source/api_curl_examples.rst
--rw-r--r--   0 barisits (51071) zp        (1307)     2265 2016-11-25 15:17:13.000000 rucio-clients-1.9.6/doc/source/atlas_integration_testbed.rst
--rwxr-xr-x   0 barisits (51071) zp        (1307)        0 2016-11-25 15:17:13.000000 rucio-clients-1.9.6/doc/source/authentication_and_identity.rst
--rw-r--r--   0 barisits (51071) zp        (1307)     4204 2016-11-25 15:17:13.000000 rucio-clients-1.9.6/doc/source/cli_admin_examples.rst
--rw-r--r--   0 barisits (51071) zp        (1307)     4565 2016-11-25 15:17:13.000000 rucio-clients-1.9.6/doc/source/cli_examples.rst
--rw-r--r--   0 barisits (51071) zp        (1307)     1010 2016-11-25 15:17:13.000000 rucio-clients-1.9.6/doc/source/client_examples.rst
--rw-r--r--   0 barisits (51071) zp        (1307)    11591 2016-11-25 15:17:13.000000 rucio-clients-1.9.6/doc/source/client_tutorial.rst
--rwxr-xr-x   0 barisits (51071) zp        (1307)     7556 2016-11-25 15:17:13.000000 rucio-clients-1.9.6/doc/source/conf.py
--rw-r--r--   0 barisits (51071) zp        (1307)      138 2016-11-25 15:17:13.000000 rucio-clients-1.9.6/doc/source/core_constants.rst
--rwxr-xr-x   0 barisits (51071) zp        (1307)     7860 2016-11-25 15:17:13.000000 rucio-clients-1.9.6/doc/source/developing.rst
--rwxr-xr-x   0 barisits (51071) zp        (1307)     3281 2016-11-25 15:17:13.000000 rucio-clients-1.9.6/doc/source/development_guidelines.rst
--rwxr-xr-x   0 barisits (51071) zp        (1307)      180 2016-11-25 15:17:13.000000 rucio-clients-1.9.6/doc/source/did.rst
--rw-r--r--   0 barisits (51071) zp        (1307)      165 2016-11-25 15:17:13.000000 rucio-clients-1.9.6/doc/source/exception.rst
--rwxr-xr-x   0 barisits (51071) zp        (1307)      162 2016-11-25 15:17:13.000000 rucio-clients-1.9.6/doc/source/identity.rst
--rwxr-xr-x   0 barisits (51071) zp        (1307)     3274 2016-11-25 15:17:14.000000 rucio-clients-1.9.6/doc/source/index.rst
--rw-r--r--   0 barisits (51071) zp        (1307)     2167 2016-11-25 15:17:14.000000 rucio-clients-1.9.6/doc/source/installing_atlas_clients.rst
--rw-r--r--   0 barisits (51071) zp        (1307)     1601 2016-11-25 15:17:14.000000 rucio-clients-1.9.6/doc/source/installing_clients.rst
--rw-r--r--   0 barisits (51071) zp        (1307)     1693 2016-11-25 15:17:14.000000 rucio-clients-1.9.6/doc/source/installing_server.rst
--rw-r--r--   0 barisits (51071) zp        (1307)      152 2016-11-25 15:17:14.000000 rucio-clients-1.9.6/doc/source/lock.rst
--rwxr-xr-x   0 barisits (51071) zp        (1307)      162 2016-11-25 15:17:14.000000 rucio-clients-1.9.6/doc/source/meta-data.rst
--rwxr-xr-x   0 barisits (51071) zp        (1307)     1180 2016-11-25 15:17:14.000000 rucio-clients-1.9.6/doc/source/naming_convention_db.rst
--rwxr-xr-x   0 barisits (51071) zp        (1307)      475 2016-11-25 15:17:14.000000 rucio-clients-1.9.6/doc/source/overview_Accounting_and_quota.rst
--rwxr-xr-x   0 barisits (51071) zp        (1307)      157 2016-11-25 15:17:14.000000 rucio-clients-1.9.6/doc/source/overview_Architecture.rst
--rw-r--r--   0 barisits (51071) zp        (1307)     1905 2016-11-25 15:17:14.000000 rucio-clients-1.9.6/doc/source/overview_Data_Deletion.rst
--rwxr-xr-x   0 barisits (51071) zp        (1307)      311 2016-11-25 15:17:14.000000 rucio-clients-1.9.6/doc/source/overview_Database_Schema.rst
--rw-r--r--   0 barisits (51071) zp        (1307)      442 2016-11-25 15:17:14.000000 rucio-clients-1.9.6/doc/source/overview_Deployment.rst
--rwxr-xr-x   0 barisits (51071) zp        (1307)     1514 2016-11-25 15:17:14.000000 rucio-clients-1.9.6/doc/source/overview_Exception_Handling.rst
--rwxr-xr-x   0 barisits (51071) zp        (1307)     3882 2016-11-25 15:17:14.000000 rucio-clients-1.9.6/doc/source/overview_File_Dataset_Container.rst
--rwxr-xr-x   0 barisits (51071) zp        (1307)      894 2016-11-25 15:17:14.000000 rucio-clients-1.9.6/doc/source/overview_Meta-data_attributes.rst
--rwxr-xr-x   0 barisits (51071) zp        (1307)      292 2016-11-25 15:17:14.000000 rucio-clients-1.9.6/doc/source/overview_Notifications.rst
--rwxr-xr-x   0 barisits (51071) zp        (1307)      251 2016-11-25 15:17:14.000000 rucio-clients-1.9.6/doc/source/overview_Permission_model.rst
--rwxr-xr-x   0 barisits (51071) zp        (1307)     1911 2016-11-25 15:17:14.000000 rucio-clients-1.9.6/doc/source/overview_Replica_management.rst
--rwxr-xr-x   0 barisits (51071) zp        (1307)     1652 2016-11-25 15:17:14.000000 rucio-clients-1.9.6/doc/source/overview_Rucio_Storage_Element.rst
--rw-r--r--   0 barisits (51071) zp        (1307)    27901 2016-11-25 15:17:14.000000 rucio-clients-1.9.6/doc/source/overview_Rucio_Storage_Element_Manager.rst
--rwxr-xr-x   0 barisits (51071) zp        (1307)     1142 2016-11-25 15:17:14.000000 rucio-clients-1.9.6/doc/source/overview_Rucio_account.rst
--rw-r--r--   0 barisits (51071) zp        (1307)    23147 2016-11-25 15:17:14.000000 rucio-clients-1.9.6/doc/source/overview_Rules.rst
--rw-r--r--   0 barisits (51071) zp        (1307)     2701 2016-11-25 15:17:14.000000 rucio-clients-1.9.6/doc/source/overview_Scheduled_Transfers.rst
--rwxr-xr-x   0 barisits (51071) zp        (1307)     4241 2016-11-25 15:17:14.000000 rucio-clients-1.9.6/doc/source/overview_Subscriptions.rst
--rwxr-xr-x   0 barisits (51071) zp        (1307)      382 2016-11-25 15:17:14.000000 rucio-clients-1.9.6/doc/source/overview_Traceability_and_logging.rst
--rwxr-xr-x   0 barisits (51071) zp        (1307)      347 2016-11-25 15:17:14.000000 rucio-clients-1.9.6/doc/source/overview_flow.rst
--rwxr-xr-x   0 barisits (51071) zp        (1307)      169 2016-11-25 15:17:14.000000 rucio-clients-1.9.6/doc/source/permission.rst
--rwxr-xr-x   0 barisits (51071) zp        (1307)      155 2016-11-25 15:17:14.000000 rucio-clients-1.9.6/doc/source/replica.rst
--rw-r--r--   0 barisits (51071) zp        (1307)     1509 2016-11-25 15:17:14.000000 rucio-clients-1.9.6/doc/source/replication_rules_examples.rst
--rwxr-xr-x   0 barisits (51071) zp        (1307)     7948 2016-11-25 15:17:14.000000 rucio-clients-1.9.6/doc/source/rest.rst
--rwxr-xr-x   0 barisits (51071) zp        (1307)      453 2016-11-25 15:17:14.000000 rucio-clients-1.9.6/doc/source/rse.rst
--rw-r--r--   0 barisits (51071) zp        (1307)      717 2016-11-25 15:17:14.000000 rucio-clients-1.9.6/doc/source/rucio_cli.rst
--rw-r--r--   0 barisits (51071) zp        (1307)      814 2016-11-25 15:17:14.000000 rucio-clients-1.9.6/doc/source/rucio_clients.rst
--rwxr-xr-x   0 barisits (51071) zp        (1307)      183 2016-11-25 15:17:14.000000 rucio-clients-1.9.6/doc/source/rule.rst
--rw-r--r--   0 barisits (51071) zp        (1307)     1414 2016-11-25 15:17:14.000000 rucio-clients-1.9.6/doc/source/rules_workflow.rst
--rwxr-xr-x   0 barisits (51071) zp        (1307)      158 2016-11-25 15:17:14.000000 rucio-clients-1.9.6/doc/source/scope.rst
--rw-r--r--   0 barisits (51071) zp        (1307)      189 2016-11-25 15:17:14.000000 rucio-clients-1.9.6/doc/source/service_avaibility.rst
--rwxr-xr-x   0 barisits (51071) zp        (1307)      190 2016-11-25 15:17:14.000000 rucio-clients-1.9.6/doc/source/subscription.rst
--rwxr-xr-x   0 barisits (51071) zp        (1307)     2438 2016-11-25 15:17:14.000000 rucio-clients-1.9.6/doc/source/usecases.rst
--rwxr-xr-x   0 barisits (51071) zp        (1307)     4590 2016-11-25 15:17:13.000000 rucio-clients-1.9.6/doc/Makefile
--rwxr-xr-x   0 barisits (51071) zp        (1307)     4513 2016-11-25 15:17:13.000000 rucio-clients-1.9.6/doc/make.bat
-drwxr-xr-x   0 barisits (51071) zp        (1307)        0 2017-02-07 09:03:44.000000 rucio-clients-1.9.6/etc/
-drwxr-xr-x   0 barisits (51071) zp        (1307)        0 2017-02-07 09:03:44.000000 rucio-clients-1.9.6/etc/schemas/
--rw-r--r--   0 barisits (51071) zp        (1307)       97 2016-11-25 15:17:14.000000 rucio-clients-1.9.6/etc/schemas/account.json
--rw-r--r--   0 barisits (51071) zp        (1307)       82 2016-11-25 15:17:14.000000 rucio-clients-1.9.6/etc/schemas/account_type.json
--rw-r--r--   0 barisits (51071) zp        (1307)       82 2016-11-25 15:17:14.000000 rucio-clients-1.9.6/etc/schemas/did_type.json
--rw-r--r--   0 barisits (51071) zp        (1307)      112 2016-11-25 15:17:14.000000 rucio-clients-1.9.6/etc/schemas/mail.json
--rw-r--r--   0 barisits (51071) zp        (1307)      105 2016-11-25 15:17:14.000000 rucio-clients-1.9.6/etc/schemas/rse.json
--rw-r--r--   0 barisits (51071) zp        (1307)      105 2016-11-25 15:17:14.000000 rucio-clients-1.9.6/etc/schemas/scope.json
--rw-r--r--   0 barisits (51071) zp        (1307)      559 2016-11-25 15:17:14.000000 rucio-clients-1.9.6/etc/schemas/sub_filter.json
--rwxr-xr-x   0 barisits (51071) zp        (1307)      543 2016-11-25 15:17:14.000000 rucio-clients-1.9.6/etc/rse-accounts.cfg.template
--rw-r--r--   0 barisits (51071) zp        (1307)      570 2016-11-25 15:17:14.000000 rucio-clients-1.9.6/etc/rucio.cfg.atlas.client.template
--rwxr-xr-x   0 barisits (51071) zp        (1307)     5165 2016-11-28 16:40:28.000000 rucio-clients-1.9.6/etc/rucio.cfg.template
-drwxr-xr-x   0 barisits (51071) zp        (1307)        0 2017-02-07 09:03:44.000000 rucio-clients-1.9.6/lib/
-drwxr-xr-x   0 barisits (51071) zp        (1307)        0 2017-02-07 09:03:44.000000 rucio-clients-1.9.6/lib/rucio/
-drwxr-xr-x   0 barisits (51071) zp        (1307)        0 2017-02-07 09:03:44.000000 rucio-clients-1.9.6/lib/rucio/client/
-drwxr-xr-x   0 barisits (51071) zp        (1307)        0 2017-02-07 09:03:44.000000 rucio-clients-1.9.6/lib/rucio/client/cli/
--rw-r--r--   0 barisits (51071) zp        (1307)      282 2016-11-25 15:17:14.000000 rucio-clients-1.9.6/lib/rucio/client/cli/__init__.py
--rw-r--r--   0 barisits (51071) zp        (1307)      469 2016-11-25 15:17:14.000000 rucio-clients-1.9.6/lib/rucio/client/cli/download.py
--rw-r--r--   0 barisits (51071) zp        (1307)      581 2016-11-25 15:17:14.000000 rucio-clients-1.9.6/lib/rucio/client/cli/upload.py
--rw-r--r--   0 barisits (51071) zp        (1307)      325 2016-11-25 15:17:14.000000 rucio-clients-1.9.6/lib/rucio/client/__init__.py
--rw-r--r--   0 barisits (51071) zp        (1307)    12651 2016-11-25 15:17:14.000000 rucio-clients-1.9.6/lib/rucio/client/accountclient.py
--rw-r--r--   0 barisits (51071) zp        (1307)     2763 2016-11-25 15:17:14.000000 rucio-clients-1.9.6/lib/rucio/client/accountlimitclient.py
--rw-r--r--   0 barisits (51071) zp        (1307)    21749 2016-11-28 16:40:28.000000 rucio-clients-1.9.6/lib/rucio/client/baseclient.py
--rw-r--r--   0 barisits (51071) zp        (1307)     2381 2016-11-25 15:17:14.000000 rucio-clients-1.9.6/lib/rucio/client/client.py
--rw-r--r--   0 barisits (51071) zp        (1307)     3523 2016-11-25 15:17:14.000000 rucio-clients-1.9.6/lib/rucio/client/configclient.py
--rw-r--r--   0 barisits (51071) zp        (1307)    23023 2017-01-20 08:30:56.000000 rucio-clients-1.9.6/lib/rucio/client/didclient.py
--rw-r--r--   0 barisits (51071) zp        (1307)    91449 2016-11-25 16:30:17.000000 rucio-clients-1.9.6/lib/rucio/client/dq2client.py
--rw-r--r--   0 barisits (51071) zp        (1307)     1716 2016-11-25 15:17:14.000000 rucio-clients-1.9.6/lib/rucio/client/fileclient.py
--rw-r--r--   0 barisits (51071) zp        (1307)     2594 2016-11-25 15:17:14.000000 rucio-clients-1.9.6/lib/rucio/client/lockclient.py
--rw-r--r--   0 barisits (51071) zp        (1307)     4717 2016-11-25 15:17:14.000000 rucio-clients-1.9.6/lib/rucio/client/metaclient.py
--rw-r--r--   0 barisits (51071) zp        (1307)     3915 2016-11-25 15:17:14.000000 rucio-clients-1.9.6/lib/rucio/client/objectstoreclient.py
--rw-r--r--   0 barisits (51071) zp        (1307)     1356 2016-11-25 15:17:14.000000 rucio-clients-1.9.6/lib/rucio/client/pingclient.py
--rw-r--r--   0 barisits (51071) zp        (1307)     9888 2016-11-28 16:40:28.000000 rucio-clients-1.9.6/lib/rucio/client/replicaclient.py
--rw-r--r--   0 barisits (51071) zp        (1307)    19164 2016-11-25 15:17:14.000000 rucio-clients-1.9.6/lib/rucio/client/rseclient.py
--rw-r--r--   0 barisits (51071) zp        (1307)     9768 2016-11-28 16:40:28.000000 rucio-clients-1.9.6/lib/rucio/client/ruleclient.py
--rw-r--r--   0 barisits (51071) zp        (1307)     3292 2016-11-25 15:17:14.000000 rucio-clients-1.9.6/lib/rucio/client/scopeclient.py
--rw-r--r--   0 barisits (51071) zp        (1307)     7475 2016-11-25 15:17:14.000000 rucio-clients-1.9.6/lib/rucio/client/subscriptionclient.py
--rw-r--r--   0 barisits (51071) zp        (1307)     4326 2016-11-25 15:17:14.000000 rucio-clients-1.9.6/lib/rucio/client/uploadclient.py
-drwxr-xr-x   0 barisits (51071) zp        (1307)        0 2017-02-07 09:03:44.000000 rucio-clients-1.9.6/lib/rucio/common/
--rw-r--r--   0 barisits (51071) zp        (1307)      282 2016-11-25 15:17:14.000000 rucio-clients-1.9.6/lib/rucio/common/__init__.py
--rw-r--r--   0 barisits (51071) zp        (1307)     1208 2016-11-25 15:17:14.000000 rucio-clients-1.9.6/lib/rucio/common/client.py
--rw-r--r--   0 barisits (51071) zp        (1307)     7763 2016-11-25 15:17:14.000000 rucio-clients-1.9.6/lib/rucio/common/closeness_sorter.py
--rw-r--r--   0 barisits (51071) zp        (1307)     3883 2016-11-25 15:17:14.000000 rucio-clients-1.9.6/lib/rucio/common/config.py
--rw-r--r--   0 barisits (51071) zp        (1307)      938 2016-11-25 15:17:14.000000 rucio-clients-1.9.6/lib/rucio/common/constants.py
--rw-r--r--   0 barisits (51071) zp        (1307)      439 2016-11-25 15:17:14.000000 rucio-clients-1.9.6/lib/rucio/common/constraints.py
--rw-r--r--   0 barisits (51071) zp        (1307)    19606 2016-11-25 15:17:14.000000 rucio-clients-1.9.6/lib/rucio/common/exception.py
--rw-r--r--   0 barisits (51071) zp        (1307)     3288 2016-11-25 15:17:14.000000 rucio-clients-1.9.6/lib/rucio/common/log.py
--rw-r--r--   0 barisits (51071) zp        (1307)    14068 2016-11-28 16:40:28.000000 rucio-clients-1.9.6/lib/rucio/common/objectstore.py
--rw-r--r--   0 barisits (51071) zp        (1307)     8188 2016-11-28 16:40:28.000000 rucio-clients-1.9.6/lib/rucio/common/policy.py
--rw-r--r--   0 barisits (51071) zp        (1307)     5461 2016-11-25 16:30:17.000000 rucio-clients-1.9.6/lib/rucio/common/replicas_selector.py
--rw-r--r--   0 barisits (51071) zp        (1307)     1520 2016-11-25 15:17:14.000000 rucio-clients-1.9.6/lib/rucio/common/rse_attributes.py
--rw-r--r--   0 barisits (51071) zp        (1307)    14030 2017-01-20 08:30:56.000000 rucio-clients-1.9.6/lib/rucio/common/schema.py
--rw-r--r--   0 barisits (51071) zp        (1307)    14270 2016-11-25 16:30:17.000000 rucio-clients-1.9.6/lib/rucio/common/utils.py
-drwxr-xr-x   0 barisits (51071) zp        (1307)        0 2017-02-07 09:03:44.000000 rucio-clients-1.9.6/lib/rucio/rse/
-drwxr-xr-x   0 barisits (51071) zp        (1307)        0 2017-02-07 09:03:44.000000 rucio-clients-1.9.6/lib/rucio/rse/protocols/
--rw-r--r--   0 barisits (51071) zp        (1307)      282 2016-11-25 15:17:14.000000 rucio-clients-1.9.6/lib/rucio/rse/protocols/__init__.py
--rw-r--r--   0 barisits (51071) zp        (1307)     4165 2016-11-28 16:40:28.000000 rucio-clients-1.9.6/lib/rucio/rse/protocols/cache.py
--rw-r--r--   0 barisits (51071) zp        (1307)     3787 2016-11-28 16:40:28.000000 rucio-clients-1.9.6/lib/rucio/rse/protocols/dummy.py
--rw-r--r--   0 barisits (51071) zp        (1307)    17585 2017-01-20 08:30:56.000000 rucio-clients-1.9.6/lib/rucio/rse/protocols/gfal.py
--rw-r--r--   0 barisits (51071) zp        (1307)     1256 2016-11-28 16:40:28.000000 rucio-clients-1.9.6/lib/rucio/rse/protocols/gfalv2.py
--rw-r--r--   0 barisits (51071) zp        (1307)     4680 2017-01-31 11:31:28.000000 rucio-clients-1.9.6/lib/rucio/rse/protocols/gsiftp.py
--rw-r--r--   0 barisits (51071) zp        (1307)     2635 2016-11-28 16:40:28.000000 rucio-clients-1.9.6/lib/rucio/rse/protocols/http_cache.py
--rw-r--r--   0 barisits (51071) zp        (1307)     3741 2016-11-25 15:17:14.000000 rucio-clients-1.9.6/lib/rucio/rse/protocols/mock.py
--rw-r--r--   0 barisits (51071) zp        (1307)     6596 2016-11-28 16:40:28.000000 rucio-clients-1.9.6/lib/rucio/rse/protocols/ngarc.py
--rw-r--r--   0 barisits (51071) zp        (1307)     6199 2016-11-25 15:17:14.000000 rucio-clients-1.9.6/lib/rucio/rse/protocols/posix.py
--rw-r--r--   0 barisits (51071) zp        (1307)    12783 2016-11-28 16:40:28.000000 rucio-clients-1.9.6/lib/rucio/rse/protocols/protocol.py
--rw-r--r--   0 barisits (51071) zp        (1307)     5035 2016-11-25 15:17:14.000000 rucio-clients-1.9.6/lib/rucio/rse/protocols/rfio.py
--rw-r--r--   0 barisits (51071) zp        (1307)     9114 2016-11-28 16:40:28.000000 rucio-clients-1.9.6/lib/rucio/rse/protocols/s3.py
--rw-r--r--   0 barisits (51071) zp        (1307)     9915 2016-11-28 16:40:28.000000 rucio-clients-1.9.6/lib/rucio/rse/protocols/s3boto.py
--rw-r--r--   0 barisits (51071) zp        (1307)     6345 2016-11-28 16:40:28.000000 rucio-clients-1.9.6/lib/rucio/rse/protocols/s3es.py
--rw-r--r--   0 barisits (51071) zp        (1307)     6723 2016-11-25 15:17:14.000000 rucio-clients-1.9.6/lib/rucio/rse/protocols/sftp.py
--rw-r--r--   0 barisits (51071) zp        (1307)    15403 2016-11-28 16:40:28.000000 rucio-clients-1.9.6/lib/rucio/rse/protocols/signeds3.py
--rw-r--r--   0 barisits (51071) zp        (1307)    13829 2016-11-25 15:17:14.000000 rucio-clients-1.9.6/lib/rucio/rse/protocols/srm.py
--rw-r--r--   0 barisits (51071) zp        (1307)    19999 2016-11-28 16:40:28.000000 rucio-clients-1.9.6/lib/rucio/rse/protocols/webdav.py
--rw-r--r--   0 barisits (51071) zp        (1307)     8153 2016-11-28 16:40:28.000000 rucio-clients-1.9.6/lib/rucio/rse/protocols/xrootd.py
--rw-r--r--   0 barisits (51071) zp        (1307)     1937 2016-11-25 15:17:14.000000 rucio-clients-1.9.6/lib/rucio/rse/__init__.py
--rwxr-xr-x   0 barisits (51071) zp        (1307)    25451 2017-01-31 11:31:28.000000 rucio-clients-1.9.6/lib/rucio/rse/rsemanager.py
-drwxr-xr-x   0 barisits (51071) zp        (1307)        0 2017-02-07 09:03:44.000000 rucio-clients-1.9.6/lib/rucio/tests/
-drwxr-xr-x   0 barisits (51071) zp        (1307)        0 2017-02-07 09:03:44.000000 rucio-clients-1.9.6/lib/rucio/tests/emulation/
-drwxr-xr-x   0 barisits (51071) zp        (1307)        0 2017-02-07 09:03:44.000000 rucio-clients-1.9.6/lib/rucio/tests/emulation/usecases/
--rw-r--r--   0 barisits (51071) zp        (1307)      282 2016-11-25 15:17:14.000000 rucio-clients-1.9.6/lib/rucio/tests/emulation/usecases/__init__.py
--rw-r--r--   0 barisits (51071) zp        (1307)      969 2016-11-25 15:17:14.000000 rucio-clients-1.9.6/lib/rucio/tests/emulation/usecases/auth.py
--rw-r--r--   0 barisits (51071) zp        (1307)     1476 2016-11-25 15:17:14.000000 rucio-clients-1.9.6/lib/rucio/tests/emulation/usecases/dummy.py
--rw-r--r--   0 barisits (51071) zp        (1307)     2208 2016-11-25 15:17:14.000000 rucio-clients-1.9.6/lib/rucio/tests/emulation/usecases/dummy_template.py
--rw-r--r--   0 barisits (51071) zp        (1307)     3716 2016-11-25 15:17:14.000000 rucio-clients-1.9.6/lib/rucio/tests/emulation/usecases/jdoe.py
--rwxr-xr-x   0 barisits (51071) zp        (1307)    65781 2016-11-25 15:17:14.000000 rucio-clients-1.9.6/lib/rucio/tests/emulation/usecases/panda.py
--rw-r--r--   0 barisits (51071) zp        (1307)    56221 2016-11-25 15:17:14.000000 rucio-clients-1.9.6/lib/rucio/tests/emulation/usecases/panda_new.py
--rw-r--r--   0 barisits (51071) zp        (1307)    21228 2016-11-25 15:17:14.000000 rucio-clients-1.9.6/lib/rucio/tests/emulation/usecases/tzero.py
--rw-r--r--   0 barisits (51071) zp        (1307)      282 2016-11-25 15:17:14.000000 rucio-clients-1.9.6/lib/rucio/tests/emulation/__init__.py
--rw-r--r--   0 barisits (51071) zp        (1307)     9327 2016-11-25 15:17:14.000000 rucio-clients-1.9.6/lib/rucio/tests/emulation/emulator.py
--rw-r--r--   0 barisits (51071) zp        (1307)    15903 2016-11-25 15:17:14.000000 rucio-clients-1.9.6/lib/rucio/tests/emulation/ucemulator.py
--rw-r--r--   0 barisits (51071) zp        (1307)     8610 2016-11-25 15:17:14.000000 rucio-clients-1.9.6/lib/rucio/tests/emulation/ucprocess.py
--rw-r--r--   0 barisits (51071) zp        (1307)     3278 2016-11-25 15:17:14.000000 rucio-clients-1.9.6/lib/rucio/tests/emulation/worker.py
--rw-r--r--   0 barisits (51071) zp        (1307)      282 2016-11-25 15:17:14.000000 rucio-clients-1.9.6/lib/rucio/tests/__init__.py
--rw-r--r--   0 barisits (51071) zp        (1307)     4684 2016-11-25 15:17:14.000000 rucio-clients-1.9.6/lib/rucio/tests/common.py
--rw-r--r--   0 barisits (51071) zp        (1307)    22701 2016-11-28 16:40:28.000000 rucio-clients-1.9.6/lib/rucio/tests/rsemgr_api_test.py
--rw-r--r--   0 barisits (51071) zp        (1307)    14408 2016-11-25 15:17:14.000000 rucio-clients-1.9.6/lib/rucio/tests/test_account.py
--rw-r--r--   0 barisits (51071) zp        (1307)     4351 2016-11-25 15:17:14.000000 rucio-clients-1.9.6/lib/rucio/tests/test_account_limits.py
--rw-r--r--   0 barisits (51071) zp        (1307)     1245 2016-11-25 15:17:14.000000 rucio-clients-1.9.6/lib/rucio/tests/test_alembic.py
--rw-r--r--   0 barisits (51071) zp        (1307)     2723 2016-11-25 15:17:14.000000 rucio-clients-1.9.6/lib/rucio/tests/test_auditor.py
--rw-r--r--   0 barisits (51071) zp        (1307)     1719 2016-11-25 15:17:14.000000 rucio-clients-1.9.6/lib/rucio/tests/test_auditor_hdfs.py
--rw-r--r--   0 barisits (51071) zp        (1307)     2888 2016-11-25 15:17:14.000000 rucio-clients-1.9.6/lib/rucio/tests/test_auditor_srmdumps.py
--rw-r--r--   0 barisits (51071) zp        (1307)     2292 2016-11-25 15:17:14.000000 rucio-clients-1.9.6/lib/rucio/tests/test_authentication.py
--rw-r--r--   0 barisits (51071) zp        (1307)     3786 2017-01-20 09:22:58.000000 rucio-clients-1.9.6/lib/rucio/tests/test_bb8.py
--rw-r--r--   0 barisits (51071) zp        (1307)    27965 2016-11-28 16:40:28.000000 rucio-clients-1.9.6/lib/rucio/tests/test_bin_rucio.py
--rw-r--r--   0 barisits (51071) zp        (1307)     3210 2016-11-25 16:30:17.000000 rucio-clients-1.9.6/lib/rucio/tests/test_clients.py
--rw-r--r--   0 barisits (51071) zp        (1307)     3120 2016-11-25 15:17:14.000000 rucio-clients-1.9.6/lib/rucio/tests/test_config.py
--rw-r--r--   0 barisits (51071) zp        (1307)     1200 2016-11-28 16:40:28.000000 rucio-clients-1.9.6/lib/rucio/tests/test_conveyor.py
--rw-r--r--   0 barisits (51071) zp        (1307)     4563 2016-11-25 15:17:14.000000 rucio-clients-1.9.6/lib/rucio/tests/test_counter.py
--rw-r--r--   0 barisits (51071) zp        (1307)     6716 2017-01-20 08:30:56.000000 rucio-clients-1.9.6/lib/rucio/tests/test_curl.py
--rw-r--r--   0 barisits (51071) zp        (1307)     2091 2016-11-25 15:17:14.000000 rucio-clients-1.9.6/lib/rucio/tests/test_dataset_replicas.py
--rw-r--r--   0 barisits (51071) zp        (1307)      707 2016-11-25 15:17:14.000000 rucio-clients-1.9.6/lib/rucio/tests/test_db.py
--rw-r--r--   0 barisits (51071) zp        (1307)    37907 2016-11-28 16:40:28.000000 rucio-clients-1.9.6/lib/rucio/tests/test_did.py
--rw-r--r--   0 barisits (51071) zp        (1307)     6392 2016-11-25 15:17:14.000000 rucio-clients-1.9.6/lib/rucio/tests/test_dumper.py
--rw-r--r--   0 barisits (51071) zp        (1307)    16150 2016-11-25 15:17:14.000000 rucio-clients-1.9.6/lib/rucio/tests/test_dumper_consistency.py
--rw-r--r--   0 barisits (51071) zp        (1307)     9790 2016-11-25 15:17:14.000000 rucio-clients-1.9.6/lib/rucio/tests/test_dumper_data_model.py
--rw-r--r--   0 barisits (51071) zp        (1307)     3145 2016-11-25 15:17:14.000000 rucio-clients-1.9.6/lib/rucio/tests/test_dumper_path_parsing.py
--rw-r--r--   0 barisits (51071) zp        (1307)     3362 2016-11-25 15:17:14.000000 rucio-clients-1.9.6/lib/rucio/tests/test_heartbeat.py
--rw-r--r--   0 barisits (51071) zp        (1307)     1657 2016-11-25 15:17:14.000000 rucio-clients-1.9.6/lib/rucio/tests/test_hermes.py
--rw-r--r--   0 barisits (51071) zp        (1307)     2141 2016-11-25 15:17:14.000000 rucio-clients-1.9.6/lib/rucio/tests/test_identity.py
--rw-r--r--   0 barisits (51071) zp        (1307)     3274 2016-11-25 15:17:14.000000 rucio-clients-1.9.6/lib/rucio/tests/test_judge_cleaner.py
--rw-r--r--   0 barisits (51071) zp        (1307)    11109 2017-01-20 08:30:56.000000 rucio-clients-1.9.6/lib/rucio/tests/test_judge_evaluator.py
--rw-r--r--   0 barisits (51071) zp        (1307)     8060 2017-01-30 16:14:23.000000 rucio-clients-1.9.6/lib/rucio/tests/test_judge_injector.py
--rw-r--r--   0 barisits (51071) zp        (1307)    16107 2017-02-01 12:54:56.000000 rucio-clients-1.9.6/lib/rucio/tests/test_judge_repairer.py
--rw-r--r--   0 barisits (51071) zp        (1307)     2086 2016-11-25 16:30:17.000000 rucio-clients-1.9.6/lib/rucio/tests/test_message.py
--rw-r--r--   0 barisits (51071) zp        (1307)     3149 2016-11-25 15:17:14.000000 rucio-clients-1.9.6/lib/rucio/tests/test_meta.py
--rw-r--r--   0 barisits (51071) zp        (1307)     2532 2016-11-25 15:17:14.000000 rucio-clients-1.9.6/lib/rucio/tests/test_meta_did.py
--rw-r--r--   0 barisits (51071) zp        (1307)     1510 2016-11-25 15:17:14.000000 rucio-clients-1.9.6/lib/rucio/tests/test_monitor.py
--rw-r--r--   0 barisits (51071) zp        (1307)     2756 2016-11-25 15:17:14.000000 rucio-clients-1.9.6/lib/rucio/tests/test_naming_convention.py
--rw-r--r--   0 barisits (51071) zp        (1307)    12265 2016-11-25 15:17:14.000000 rucio-clients-1.9.6/lib/rucio/tests/test_objectstore.py
--rw-r--r--   0 barisits (51071) zp        (1307)     3185 2016-11-25 15:17:14.000000 rucio-clients-1.9.6/lib/rucio/tests/test_permission.py
--rw-r--r--   0 barisits (51071) zp        (1307)     1286 2016-11-25 15:17:14.000000 rucio-clients-1.9.6/lib/rucio/tests/test_ping.py
--rw-r--r--   0 barisits (51071) zp        (1307)     1358 2016-11-28 16:40:28.000000 rucio-clients-1.9.6/lib/rucio/tests/test_quarantined_replica.py
--rw-r--r--   0 barisits (51071) zp        (1307)     1307 2016-11-28 16:40:28.000000 rucio-clients-1.9.6/lib/rucio/tests/test_reaper.py
--rw-r--r--   0 barisits (51071) zp        (1307)     5633 2017-01-20 08:30:56.000000 rucio-clients-1.9.6/lib/rucio/tests/test_redirect.py
--rw-r--r--   0 barisits (51071) zp        (1307)    27875 2017-01-20 08:30:56.000000 rucio-clients-1.9.6/lib/rucio/tests/test_replica.py
--rw-r--r--   0 barisits (51071) zp        (1307)    78040 2016-11-25 16:30:17.000000 rucio-clients-1.9.6/lib/rucio/tests/test_rse.py
--rw-r--r--   0 barisits (51071) zp        (1307)    10690 2017-01-20 08:30:56.000000 rucio-clients-1.9.6/lib/rucio/tests/test_rse_expression_parser.py
--rw-r--r--   0 barisits (51071) zp        (1307)    13237 2016-11-25 15:17:14.000000 rucio-clients-1.9.6/lib/rucio/tests/test_rse_protocol_gfal2.py
--rw-r--r--   0 barisits (51071) zp        (1307)     5173 2016-11-25 15:17:14.000000 rucio-clients-1.9.6/lib/rucio/tests/test_rse_protocol_mock.py
--rw-r--r--   0 barisits (51071) zp        (1307)     9728 2016-11-25 15:17:14.000000 rucio-clients-1.9.6/lib/rucio/tests/test_rse_protocol_posix.py
--rw-r--r--   0 barisits (51071) zp        (1307)     9813 2016-11-25 15:17:14.000000 rucio-clients-1.9.6/lib/rucio/tests/test_rse_protocol_s3.py
--rw-r--r--   0 barisits (51071) zp        (1307)    11909 2016-11-25 16:30:17.000000 rucio-clients-1.9.6/lib/rucio/tests/test_rse_protocol_s3boto.py
--rw-r--r--   0 barisits (51071) zp        (1307)     4845 2016-11-28 16:40:28.000000 rucio-clients-1.9.6/lib/rucio/tests/test_rse_protocol_s3es.py
--rw-r--r--   0 barisits (51071) zp        (1307)    10110 2016-11-25 15:17:14.000000 rucio-clients-1.9.6/lib/rucio/tests/test_rse_protocol_sftp.py
--rw-r--r--   0 barisits (51071) zp        (1307)     9909 2016-11-25 15:17:14.000000 rucio-clients-1.9.6/lib/rucio/tests/test_rse_protocol_signeds3.py
--rw-r--r--   0 barisits (51071) zp        (1307)    12805 2016-11-25 15:17:14.000000 rucio-clients-1.9.6/lib/rucio/tests/test_rse_protocol_srm.py
--rw-r--r--   0 barisits (51071) zp        (1307)    11084 2016-11-25 15:17:14.000000 rucio-clients-1.9.6/lib/rucio/tests/test_rse_protocol_webdav.py
--rw-r--r--   0 barisits (51071) zp        (1307)    10967 2016-11-25 15:17:14.000000 rucio-clients-1.9.6/lib/rucio/tests/test_rse_protocol_xrootd.py
--rw-r--r--   0 barisits (51071) zp        (1307)     6537 2016-11-25 15:17:14.000000 rucio-clients-1.9.6/lib/rucio/tests/test_rucio_cache.py
--rw-r--r--   0 barisits (51071) zp        (1307)     4792 2016-11-25 15:17:14.000000 rucio-clients-1.9.6/lib/rucio/tests/test_rucio_server.py
--rw-r--r--   0 barisits (51071) zp        (1307)    51857 2016-11-28 16:40:28.000000 rucio-clients-1.9.6/lib/rucio/tests/test_rule.py
--rw-r--r--   0 barisits (51071) zp        (1307)    10288 2016-11-25 15:17:14.000000 rucio-clients-1.9.6/lib/rucio/tests/test_scope.py
--rw-r--r--   0 barisits (51071) zp        (1307)    20485 2016-11-28 16:40:28.000000 rucio-clients-1.9.6/lib/rucio/tests/test_subscription.py
--rw-r--r--   0 barisits (51071) zp        (1307)     2650 2016-11-28 16:40:28.000000 rucio-clients-1.9.6/lib/rucio/tests/test_temporary_did.py
--rw-r--r--   0 barisits (51071) zp        (1307)     1345 2016-11-25 15:17:14.000000 rucio-clients-1.9.6/lib/rucio/tests/test_trace.py
--rw-r--r--   0 barisits (51071) zp        (1307)     3258 2016-11-25 16:30:17.000000 rucio-clients-1.9.6/lib/rucio/tests/test_undertaker.py
--rw-r--r--   0 barisits (51071) zp        (1307)      337 2016-11-25 15:17:14.000000 rucio-clients-1.9.6/lib/rucio/__init__.py
--rw-r--r--   0 barisits (51071) zp        (1307)      249 2017-02-07 09:03:41.000000 rucio-clients-1.9.6/lib/rucio/vcsversion.py
--rw-r--r--   0 barisits (51071) zp        (1307)     1127 2016-11-25 15:17:14.000000 rucio-clients-1.9.6/lib/rucio/version.py
-drwxr-xr-x   0 barisits (51071) zp        (1307)        0 2017-02-07 09:03:44.000000 rucio-clients-1.9.6/tools/
--rw-r--r--   0 barisits (51071) zp        (1307)     1060 2017-01-20 08:30:56.000000 rucio-clients-1.9.6/tools/pip-requires-client
--rw-r--r--   0 barisits (51071) zp        (1307)      541 2016-11-25 15:17:13.000000 rucio-clients-1.9.6/AUTHORS
--rw-r--r--   0 barisits (51071) zp        (1307)      273 2016-11-25 15:17:13.000000 rucio-clients-1.9.6/ChangeLog
--rw-r--r--   0 barisits (51071) zp        (1307)      274 2016-11-25 15:17:13.000000 rucio-clients-1.9.6/LICENSE
--rw-r--r--   0 barisits (51071) zp        (1307)      737 2017-02-07 09:03:41.000000 rucio-clients-1.9.6/MANIFEST.in
--rw-r--r--   0 barisits (51071) zp        (1307)      298 2017-02-07 09:03:41.000000 rucio-clients-1.9.6/README.rst
--rw-r--r--   0 barisits (51071) zp        (1307)     7396 2016-11-25 15:17:14.000000 rucio-clients-1.9.6/pylintrc
--rw-r--r--   0 barisits (51071) zp        (1307)     1014 2016-11-25 15:17:14.000000 rucio-clients-1.9.6/setup.cfg
--rw-r--r--   0 barisits (51071) zp        (1307)     6401 2017-02-07 09:03:41.000000 rucio-clients-1.9.6/setup.py
--rw-r--r--   0 barisits (51071) zp        (1307)      759 2017-02-07 09:03:44.000000 rucio-clients-1.9.6/PKG-INFO
+drwxr-xr-x   0 root         (0) root         (0)        0 2023-07-26 15:14:47.805165 rucio-clients-32.0.0rc1/
+-rwxr-xr-x   0 root         (0) root         (0)     4383 2023-07-26 12:29:16.000000 rucio-clients-32.0.0rc1/AUTHORS.rst
+-rw-r--r--   0 root         (0) root         (0)       87 2023-07-26 12:29:16.000000 rucio-clients-32.0.0rc1/ChangeLog
+-rw-r--r--   0 root         (0) root         (0)    11357 2023-07-26 12:29:16.000000 rucio-clients-32.0.0rc1/LICENSE
+-rw-r--r--   0 root         (0) root         (0)      598 2023-07-26 15:14:41.000000 rucio-clients-32.0.0rc1/MANIFEST.in
+-rw-r--r--   0 root         (0) root         (0)      936 2023-07-26 15:14:47.805165 rucio-clients-32.0.0rc1/PKG-INFO
+-rw-r--r--   0 root         (0) root         (0)     1982 2023-07-26 12:29:16.000000 rucio-clients-32.0.0rc1/README.rst
+drwxr-xr-x   0 root         (0) root         (0)        0 2023-07-26 15:14:47.752163 rucio-clients-32.0.0rc1/bin/
+-rwxr-xr-x   0 root         (0) root         (0)   128576 2023-07-26 12:29:16.000000 rucio-clients-32.0.0rc1/bin/rucio
+-rwxr-xr-x   0 root         (0) root         (0)   133738 2023-07-26 12:29:16.000000 rucio-clients-32.0.0rc1/bin/rucio-admin
+drwxr-xr-x   0 root         (0) root         (0)        0 2023-07-26 15:14:47.753163 rucio-clients-32.0.0rc1/etc/
+-rwxr-xr-x   0 root         (0) root         (0)      543 2023-07-26 12:29:16.000000 rucio-clients-32.0.0rc1/etc/rse-accounts.cfg.template
+-rw-r--r--   0 root         (0) root         (0)     1311 2023-07-26 12:29:16.000000 rucio-clients-32.0.0rc1/etc/rucio.cfg.atlas.client.template
+-rw-r--r--   0 root         (0) root         (0)     8534 2023-07-26 12:29:16.000000 rucio-clients-32.0.0rc1/etc/rucio.cfg.template
+drwxr-xr-x   0 root         (0) root         (0)        0 2023-07-26 15:14:47.748163 rucio-clients-32.0.0rc1/lib/
+drwxr-xr-x   0 root         (0) root         (0)        0 2023-07-26 15:14:47.754163 rucio-clients-32.0.0rc1/lib/rucio/
+-rw-r--r--   0 root         (0) root         (0)      684 2023-07-26 12:29:16.000000 rucio-clients-32.0.0rc1/lib/rucio/__init__.py
+-rw-r--r--   0 root         (0) root         (0)      714 2023-07-26 12:29:16.000000 rucio-clients-32.0.0rc1/lib/rucio/alembicrevision.py
+drwxr-xr-x   0 root         (0) root         (0)        0 2023-07-26 15:14:47.761163 rucio-clients-32.0.0rc1/lib/rucio/client/
+-rw-r--r--   0 root         (0) root         (0)      684 2023-07-26 12:29:16.000000 rucio-clients-32.0.0rc1/lib/rucio/client/__init__.py
+-rw-r--r--   0 root         (0) root         (0)    16412 2023-07-26 12:29:16.000000 rucio-clients-32.0.0rc1/lib/rucio/client/accountclient.py
+-rw-r--r--   0 root         (0) root         (0)     6020 2023-07-26 12:29:16.000000 rucio-clients-32.0.0rc1/lib/rucio/client/accountlimitclient.py
+-rw-r--r--   0 root         (0) root         (0)    47219 2023-07-26 12:29:16.000000 rucio-clients-32.0.0rc1/lib/rucio/client/baseclient.py
+-rw-r--r--   0 root         (0) root         (0)     3072 2023-07-26 12:29:16.000000 rucio-clients-32.0.0rc1/lib/rucio/client/client.py
+-rw-r--r--   0 root         (0) root         (0)     4460 2023-07-26 12:29:16.000000 rucio-clients-32.0.0rc1/lib/rucio/client/configclient.py
+-rw-r--r--   0 root         (0) root         (0)     2048 2023-07-26 12:29:16.000000 rucio-clients-32.0.0rc1/lib/rucio/client/credentialclient.py
+-rw-r--r--   0 root         (0) root         (0)    28223 2023-07-26 12:29:16.000000 rucio-clients-32.0.0rc1/lib/rucio/client/didclient.py
+-rw-r--r--   0 root         (0) root         (0)     1991 2023-07-26 12:29:16.000000 rucio-clients-32.0.0rc1/lib/rucio/client/diracclient.py
+-rw-r--r--   0 root         (0) root         (0)    86127 2023-07-26 12:29:16.000000 rucio-clients-32.0.0rc1/lib/rucio/client/downloadclient.py
+-rw-r--r--   0 root         (0) root         (0)     1634 2023-07-26 12:29:16.000000 rucio-clients-32.0.0rc1/lib/rucio/client/exportclient.py
+-rw-r--r--   0 root         (0) root         (0)     1669 2023-07-26 12:29:16.000000 rucio-clients-32.0.0rc1/lib/rucio/client/fileclient.py
+-rw-r--r--   0 root         (0) root         (0)     1528 2023-07-26 12:29:16.000000 rucio-clients-32.0.0rc1/lib/rucio/client/importclient.py
+-rw-r--r--   0 root         (0) root         (0)     2881 2023-07-26 12:29:16.000000 rucio-clients-32.0.0rc1/lib/rucio/client/lifetimeclient.py
+-rw-r--r--   0 root         (0) root         (0)     3973 2023-07-26 12:29:16.000000 rucio-clients-32.0.0rc1/lib/rucio/client/lockclient.py
+-rw-r--r--   0 root         (0) root         (0)     4722 2023-07-26 12:29:16.000000 rucio-clients-32.0.0rc1/lib/rucio/client/metaclient.py
+-rw-r--r--   0 root         (0) root         (0)     1414 2023-07-26 12:29:16.000000 rucio-clients-32.0.0rc1/lib/rucio/client/pingclient.py
+-rw-r--r--   0 root         (0) root         (0)    19343 2023-07-26 12:29:16.000000 rucio-clients-32.0.0rc1/lib/rucio/client/replicaclient.py
+-rw-r--r--   0 root         (0) root         (0)     4256 2023-07-26 12:29:16.000000 rucio-clients-32.0.0rc1/lib/rucio/client/requestclient.py
+-rw-r--r--   0 root         (0) root         (0)    27183 2023-07-26 12:29:16.000000 rucio-clients-32.0.0rc1/lib/rucio/client/rseclient.py
+-rw-r--r--   0 root         (0) root         (0)    12741 2023-07-26 12:29:16.000000 rucio-clients-32.0.0rc1/lib/rucio/client/ruleclient.py
+-rw-r--r--   0 root         (0) root         (0)     3206 2023-07-26 12:29:16.000000 rucio-clients-32.0.0rc1/lib/rucio/client/scopeclient.py
+-rw-r--r--   0 root         (0) root         (0)     7980 2023-07-26 12:29:16.000000 rucio-clients-32.0.0rc1/lib/rucio/client/subscriptionclient.py
+-rw-r--r--   0 root         (0) root         (0)     2701 2023-07-26 12:29:16.000000 rucio-clients-32.0.0rc1/lib/rucio/client/touchclient.py
+-rw-r--r--   0 root         (0) root         (0)    45907 2023-07-26 12:29:16.000000 rucio-clients-32.0.0rc1/lib/rucio/client/uploadclient.py
+drwxr-xr-x   0 root         (0) root         (0)        0 2023-07-26 15:14:47.766163 rucio-clients-32.0.0rc1/lib/rucio/common/
+-rw-r--r--   0 root         (0) root         (0)      642 2023-07-26 12:29:16.000000 rucio-clients-32.0.0rc1/lib/rucio/common/__init__.py
+-rw-r--r--   0 root         (0) root         (0)     2297 2023-07-26 12:29:16.000000 rucio-clients-32.0.0rc1/lib/rucio/common/cache.py
+-rw-r--r--   0 root         (0) root         (0)    24286 2023-07-26 12:29:16.000000 rucio-clients-32.0.0rc1/lib/rucio/common/config.py
+-rw-r--r--   0 root         (0) root         (0)     3314 2023-07-26 12:29:16.000000 rucio-clients-32.0.0rc1/lib/rucio/common/constants.py
+-rw-r--r--   0 root         (0) root         (0)      750 2023-07-26 12:29:16.000000 rucio-clients-32.0.0rc1/lib/rucio/common/constraints.py
+-rwxr-xr-x   0 root         (0) root         (0)     6323 2023-07-26 12:29:16.000000 rucio-clients-32.0.0rc1/lib/rucio/common/didtype.py
+-rw-r--r--   0 root         (0) root         (0)    32369 2023-07-26 12:29:16.000000 rucio-clients-32.0.0rc1/lib/rucio/common/exception.py
+-rw-r--r--   0 root         (0) root         (0)     1422 2023-07-26 12:29:16.000000 rucio-clients-32.0.0rc1/lib/rucio/common/extra.py
+-rw-r--r--   0 root         (0) root         (0)    15070 2023-07-26 12:29:16.000000 rucio-clients-32.0.0rc1/lib/rucio/common/logging.py
+-rw-r--r--   0 root         (0) root         (0)    45324 2023-07-26 12:29:16.000000 rucio-clients-32.0.0rc1/lib/rucio/common/pcache.py
+-rw-r--r--   0 root         (0) root         (0)     3037 2023-07-26 12:29:16.000000 rucio-clients-32.0.0rc1/lib/rucio/common/policy.py
+drwxr-xr-x   0 root         (0) root         (0)        0 2023-07-26 15:14:47.769164 rucio-clients-32.0.0rc1/lib/rucio/common/schema/
+-rw-r--r--   0 root         (0) root         (0)     5176 2023-07-26 12:29:16.000000 rucio-clients-32.0.0rc1/lib/rucio/common/schema/__init__.py
+-rw-r--r--   0 root         (0) root         (0)    15628 2023-07-26 12:29:16.000000 rucio-clients-32.0.0rc1/lib/rucio/common/schema/atlas.py
+-rw-r--r--   0 root         (0) root         (0)    15445 2023-07-26 12:29:16.000000 rucio-clients-32.0.0rc1/lib/rucio/common/schema/belleii.py
+-rw-r--r--   0 root         (0) root         (0)    18575 2023-07-26 12:29:16.000000 rucio-clients-32.0.0rc1/lib/rucio/common/schema/cms.py
+-rw-r--r--   0 root         (0) root         (0)    15026 2023-07-26 12:29:16.000000 rucio-clients-32.0.0rc1/lib/rucio/common/schema/domatpc.py
+-rw-r--r--   0 root         (0) root         (0)    15965 2023-07-26 12:29:16.000000 rucio-clients-32.0.0rc1/lib/rucio/common/schema/escape.py
+-rw-r--r--   0 root         (0) root         (0)    16280 2023-07-26 12:29:16.000000 rucio-clients-32.0.0rc1/lib/rucio/common/schema/generic.py
+-rw-r--r--   0 root         (0) root         (0)    15507 2023-07-26 12:29:16.000000 rucio-clients-32.0.0rc1/lib/rucio/common/schema/generic_multi_vo.py
+-rw-r--r--   0 root         (0) root         (0)    15316 2023-07-26 12:29:16.000000 rucio-clients-32.0.0rc1/lib/rucio/common/schema/icecube.py
+-rw-r--r--   0 root         (0) root         (0)    15924 2023-07-26 12:29:16.000000 rucio-clients-32.0.0rc1/lib/rucio/common/schema/lsst.py
+-rw-r--r--   0 root         (0) root         (0)     5438 2023-07-26 12:29:16.000000 rucio-clients-32.0.0rc1/lib/rucio/common/stomp_utils.py
+-rw-r--r--   0 root         (0) root         (0)     1665 2023-07-26 12:29:16.000000 rucio-clients-32.0.0rc1/lib/rucio/common/stopwatch.py
+-rw-r--r--   0 root         (0) root         (0)     4929 2023-07-26 12:29:16.000000 rucio-clients-32.0.0rc1/lib/rucio/common/test_rucio_server.py
+-rw-r--r--   0 root         (0) root         (0)     3281 2023-07-26 12:29:16.000000 rucio-clients-32.0.0rc1/lib/rucio/common/types.py
+-rw-r--r--   0 root         (0) root         (0)    69433 2023-07-26 12:29:16.000000 rucio-clients-32.0.0rc1/lib/rucio/common/utils.py
+drwxr-xr-x   0 root         (0) root         (0)        0 2023-07-26 15:14:47.769164 rucio-clients-32.0.0rc1/lib/rucio/rse/
+-rw-r--r--   0 root         (0) root         (0)     3327 2023-07-26 12:29:16.000000 rucio-clients-32.0.0rc1/lib/rucio/rse/__init__.py
+drwxr-xr-x   0 root         (0) root         (0)        0 2023-07-26 15:14:47.774164 rucio-clients-32.0.0rc1/lib/rucio/rse/protocols/
+-rw-r--r--   0 root         (0) root         (0)      642 2023-07-26 12:29:16.000000 rucio-clients-32.0.0rc1/lib/rucio/rse/protocols/__init__.py
+-rw-r--r--   0 root         (0) root         (0)     4627 2023-07-26 12:29:16.000000 rucio-clients-32.0.0rc1/lib/rucio/rse/protocols/cache.py
+-rw-r--r--   0 root         (0) root         (0)     4249 2023-07-26 12:29:16.000000 rucio-clients-32.0.0rc1/lib/rucio/rse/protocols/dummy.py
+-rw-r--r--   0 root         (0) root         (0)    29035 2023-07-26 12:29:16.000000 rucio-clients-32.0.0rc1/lib/rucio/rse/protocols/gfal.py
+-rw-r--r--   0 root         (0) root         (0)     9704 2023-07-26 12:29:16.000000 rucio-clients-32.0.0rc1/lib/rucio/rse/protocols/globus.py
+-rw-r--r--   0 root         (0) root         (0)     3433 2023-07-26 12:29:16.000000 rucio-clients-32.0.0rc1/lib/rucio/rse/protocols/gsiftp.py
+-rw-r--r--   0 root         (0) root         (0)     2999 2023-07-26 12:29:16.000000 rucio-clients-32.0.0rc1/lib/rucio/rse/protocols/http_cache.py
+-rw-r--r--   0 root         (0) root         (0)     4519 2023-07-26 12:29:16.000000 rucio-clients-32.0.0rc1/lib/rucio/rse/protocols/mock.py
+-rw-r--r--   0 root         (0) root         (0)     7248 2023-07-26 12:29:16.000000 rucio-clients-32.0.0rc1/lib/rucio/rse/protocols/ngarc.py
+-rw-r--r--   0 root         (0) root         (0)    10418 2023-07-26 12:29:16.000000 rucio-clients-32.0.0rc1/lib/rucio/rse/protocols/posix.py
+-rw-r--r--   0 root         (0) root         (0)    22818 2023-07-26 12:29:16.000000 rucio-clients-32.0.0rc1/lib/rucio/rse/protocols/protocol.py
+-rw-r--r--   0 root         (0) root         (0)    15276 2023-07-26 12:29:16.000000 rucio-clients-32.0.0rc1/lib/rucio/rse/protocols/rclone.py
+-rw-r--r--   0 root         (0) root         (0)     5554 2023-07-26 12:29:16.000000 rucio-clients-32.0.0rc1/lib/rucio/rse/protocols/rfio.py
+-rw-r--r--   0 root         (0) root         (0)    14671 2023-07-26 12:29:16.000000 rucio-clients-32.0.0rc1/lib/rucio/rse/protocols/srm.py
+-rw-r--r--   0 root         (0) root         (0)    17487 2023-07-26 12:29:16.000000 rucio-clients-32.0.0rc1/lib/rucio/rse/protocols/ssh.py
+-rw-r--r--   0 root         (0) root         (0)     8151 2023-07-26 12:29:16.000000 rucio-clients-32.0.0rc1/lib/rucio/rse/protocols/storm.py
+-rw-r--r--   0 root         (0) root         (0)    22219 2023-07-26 12:29:16.000000 rucio-clients-32.0.0rc1/lib/rucio/rse/protocols/webdav.py
+-rw-r--r--   0 root         (0) root         (0)    12489 2023-07-26 12:29:16.000000 rucio-clients-32.0.0rc1/lib/rucio/rse/protocols/xrootd.py
+-rw-r--r--   0 root         (0) root         (0)    36928 2023-07-26 12:29:16.000000 rucio-clients-32.0.0rc1/lib/rucio/rse/rsemanager.py
+-rw-r--r--   0 root         (0) root         (0)      243 2023-07-26 15:02:05.000000 rucio-clients-32.0.0rc1/lib/rucio/vcsversion.py
+-rw-r--r--   0 root         (0) root         (0)     1573 2023-07-26 12:29:16.000000 rucio-clients-32.0.0rc1/lib/rucio/version.py
+drwxr-xr-x   0 root         (0) root         (0)        0 2023-07-26 15:14:47.805165 rucio-clients-32.0.0rc1/lib/rucio_clients.egg-info/
+-rw-r--r--   0 root         (0) root         (0)     5113 2023-07-26 15:14:47.000000 rucio-clients-32.0.0rc1/lib/rucio_clients.egg-info/SOURCES.txt
+-rw-r--r--   0 root         (0) root         (0)    16384 2023-07-26 12:29:16.000000 rucio-clients-32.0.0rc1/pylintrc
+-rw-r--r--   0 root         (0) root         (0)       90 2023-07-26 12:29:16.000000 rucio-clients-32.0.0rc1/pyproject.toml
+-rw-r--r--   0 root         (0) root         (0)     5083 2023-07-26 12:29:16.000000 rucio-clients-32.0.0rc1/requirements.txt
+-rw-r--r--   0 root         (0) root         (0)      206 2023-07-26 15:14:47.806166 rucio-clients-32.0.0rc1/setup.cfg
+-rw-r--r--   0 root         (0) root         (0)     3074 2023-07-26 15:14:41.000000 rucio-clients-32.0.0rc1/setup.py
+-rw-r--r--   0 root         (0) root         (0)     4749 2023-07-26 12:29:16.000000 rucio-clients-32.0.0rc1/setuputil.py
+drwxr-xr-x   0 root         (0) root         (0)        0 2023-07-26 15:14:47.804165 rucio-clients-32.0.0rc1/tests/
+-rw-r--r--   0 root         (0) root         (0)     3708 2023-07-26 12:29:16.000000 rucio-clients-32.0.0rc1/tests/test_abacus_account.py
+-rw-r--r--   0 root         (0) root         (0)    10771 2023-07-26 12:29:16.000000 rucio-clients-32.0.0rc1/tests/test_abacus_collection_replica.py
+-rw-r--r--   0 root         (0) root         (0)     3255 2023-07-26 12:29:16.000000 rucio-clients-32.0.0rc1/tests/test_abacus_rse.py
+-rw-r--r--   0 root         (0) root         (0)    15997 2023-07-26 12:29:16.000000 rucio-clients-32.0.0rc1/tests/test_account.py
+-rw-r--r--   0 root         (0) root         (0)     9834 2023-07-26 12:29:16.000000 rucio-clients-32.0.0rc1/tests/test_account_limits.py
+-rw-r--r--   0 root         (0) root         (0)    20192 2023-07-26 12:29:16.000000 rucio-clients-32.0.0rc1/tests/test_api_external_representation.py
+-rw-r--r--   0 root         (0) root         (0)    13376 2023-07-26 12:29:16.000000 rucio-clients-32.0.0rc1/tests/test_archive.py
+-rw-r--r--   0 root         (0) root         (0)     4120 2023-07-26 12:29:16.000000 rucio-clients-32.0.0rc1/tests/test_auditor.py
+-rw-r--r--   0 root         (0) root         (0)     2213 2023-07-26 12:29:16.000000 rucio-clients-32.0.0rc1/tests/test_auditor_hdfs.py
+-rw-r--r--   0 root         (0) root         (0)     4225 2023-07-26 12:29:16.000000 rucio-clients-32.0.0rc1/tests/test_auditor_srmdumps.py
+-rw-r--r--   0 root         (0) root         (0)    15239 2023-07-26 12:29:16.000000 rucio-clients-32.0.0rc1/tests/test_authentication.py
+-rw-r--r--   0 root         (0) root         (0)     3488 2023-07-26 12:29:16.000000 rucio-clients-32.0.0rc1/tests/test_automatix.py
+-rw-r--r--   0 root         (0) root         (0)    23339 2023-07-26 12:29:16.000000 rucio-clients-32.0.0rc1/tests/test_bad_replica.py
+-rw-r--r--   0 root         (0) root         (0)    12468 2023-07-26 12:29:16.000000 rucio-clients-32.0.0rc1/tests/test_bb8.py
+-rw-r--r--   0 root         (0) root         (0)     4707 2023-07-26 12:29:16.000000 rucio-clients-32.0.0rc1/tests/test_belleii.py
+-rwxr-xr-x   0 root         (0) root         (0)   107211 2023-07-26 12:29:16.000000 rucio-clients-32.0.0rc1/tests/test_bin_rucio.py
+-rw-r--r--   0 root         (0) root         (0)     2089 2023-07-26 12:29:16.000000 rucio-clients-32.0.0rc1/tests/test_boolean.py
+-rw-r--r--   0 root         (0) root         (0)     8248 2023-07-26 12:29:16.000000 rucio-clients-32.0.0rc1/tests/test_clients.py
+-rw-r--r--   0 root         (0) root         (0)     1889 2023-07-26 12:29:16.000000 rucio-clients-32.0.0rc1/tests/test_common_types.py
+-rw-r--r--   0 root         (0) root         (0)     6966 2023-07-26 12:29:16.000000 rucio-clients-32.0.0rc1/tests/test_config.py
+-rw-r--r--   0 root         (0) root         (0)    85465 2023-07-26 12:29:16.000000 rucio-clients-32.0.0rc1/tests/test_conveyor.py
+-rw-r--r--   0 root         (0) root         (0)    22228 2023-07-26 12:29:16.000000 rucio-clients-32.0.0rc1/tests/test_conveyor_submitter.py
+-rw-r--r--   0 root         (0) root         (0)     6971 2023-07-26 12:29:16.000000 rucio-clients-32.0.0rc1/tests/test_counter.py
+-rw-r--r--   0 root         (0) root         (0)     7703 2023-07-26 12:29:16.000000 rucio-clients-32.0.0rc1/tests/test_credential.py
+-rw-r--r--   0 root         (0) root         (0)     7361 2023-07-26 12:29:16.000000 rucio-clients-32.0.0rc1/tests/test_curl.py
+-rw-r--r--   0 root         (0) root         (0)     2674 2023-07-26 12:29:16.000000 rucio-clients-32.0.0rc1/tests/test_daemons.py
+-rw-r--r--   0 root         (0) root         (0)    28205 2023-07-26 12:29:16.000000 rucio-clients-32.0.0rc1/tests/test_dataset_replicas.py
+-rw-r--r--   0 root         (0) root         (0)     2759 2023-07-26 12:29:16.000000 rucio-clients-32.0.0rc1/tests/test_db.py
+-rw-r--r--   0 root         (0) root         (0)    55592 2023-07-26 12:29:16.000000 rucio-clients-32.0.0rc1/tests/test_did.py
+-rw-r--r--   0 root         (0) root         (0)    29541 2023-07-26 12:29:16.000000 rucio-clients-32.0.0rc1/tests/test_did_meta_plugins.py
+-rw-r--r--   0 root         (0) root         (0)     4209 2023-07-26 12:29:16.000000 rucio-clients-32.0.0rc1/tests/test_didtype.py
+-rw-r--r--   0 root         (0) root         (0)    35378 2023-07-26 12:29:16.000000 rucio-clients-32.0.0rc1/tests/test_download.py
+-rw-r--r--   0 root         (0) root         (0)     7203 2023-07-26 12:29:16.000000 rucio-clients-32.0.0rc1/tests/test_dumper.py
+-rw-r--r--   0 root         (0) root         (0)    16658 2023-07-26 12:29:16.000000 rucio-clients-32.0.0rc1/tests/test_dumper_consistency.py
+-rw-r--r--   0 root         (0) root         (0)    10907 2023-07-26 12:29:16.000000 rucio-clients-32.0.0rc1/tests/test_dumper_data_model.py
+-rw-r--r--   0 root         (0) root         (0)     3158 2023-07-26 12:29:16.000000 rucio-clients-32.0.0rc1/tests/test_dumper_path_parsing.py
+-rw-r--r--   0 root         (0) root         (0)    38325 2023-07-26 12:29:16.000000 rucio-clients-32.0.0rc1/tests/test_filter_engine.py
+-rw-r--r--   0 root         (0) root         (0)     7977 2023-07-26 12:29:16.000000 rucio-clients-32.0.0rc1/tests/test_heartbeat.py
+-rw-r--r--   0 root         (0) root         (0)    10138 2023-07-26 12:29:16.000000 rucio-clients-32.0.0rc1/tests/test_hermes.py
+-rw-r--r--   0 root         (0) root         (0)     5843 2023-07-26 12:29:16.000000 rucio-clients-32.0.0rc1/tests/test_identity.py
+-rw-r--r--   0 root         (0) root         (0)    18578 2023-07-26 12:29:16.000000 rucio-clients-32.0.0rc1/tests/test_impl_upload_download.py
+-rw-r--r--   0 root         (0) root         (0)    57799 2023-07-26 12:29:16.000000 rucio-clients-32.0.0rc1/tests/test_import_export.py
+-rw-r--r--   0 root         (0) root         (0)     5281 2023-07-26 12:29:16.000000 rucio-clients-32.0.0rc1/tests/test_judge_cleaner.py
+-rw-r--r--   0 root         (0) root         (0)    22299 2023-07-26 12:29:16.000000 rucio-clients-32.0.0rc1/tests/test_judge_evaluator.py
+-rw-r--r--   0 root         (0) root         (0)    11070 2023-07-26 12:29:16.000000 rucio-clients-32.0.0rc1/tests/test_judge_injector.py
+-rw-r--r--   0 root         (0) root         (0)    20482 2023-07-26 12:29:16.000000 rucio-clients-32.0.0rc1/tests/test_judge_repairer.py
+-rw-r--r--   0 root         (0) root         (0)    12205 2023-07-26 12:29:16.000000 rucio-clients-32.0.0rc1/tests/test_lifetime.py
+-rw-r--r--   0 root         (0) root         (0)     5879 2023-07-26 12:29:16.000000 rucio-clients-32.0.0rc1/tests/test_message.py
+-rw-r--r--   0 root         (0) root         (0)     6270 2023-07-26 12:29:16.000000 rucio-clients-32.0.0rc1/tests/test_meta.py
+-rw-r--r--   0 root         (0) root         (0)     1790 2023-07-26 12:29:16.000000 rucio-clients-32.0.0rc1/tests/test_meta_did.py
+-rw-r--r--   0 root         (0) root         (0)      983 2023-07-26 12:29:16.000000 rucio-clients-32.0.0rc1/tests/test_module_import.py
+-rw-r--r--   0 root         (0) root         (0)     2058 2023-07-26 12:29:16.000000 rucio-clients-32.0.0rc1/tests/test_monitor.py
+-rw-r--r--   0 root         (0) root         (0)    54354 2023-07-26 12:29:16.000000 rucio-clients-32.0.0rc1/tests/test_multi_vo.py
+-rw-r--r--   0 root         (0) root         (0)     2911 2023-07-26 12:29:16.000000 rucio-clients-32.0.0rc1/tests/test_naming_convention.py
+-rw-r--r--   0 root         (0) root         (0)    11606 2023-07-26 12:29:16.000000 rucio-clients-32.0.0rc1/tests/test_oauthmanager.py
+-rw-r--r--   0 root         (0) root         (0)   107576 2023-07-26 12:29:16.000000 rucio-clients-32.0.0rc1/tests/test_oidc.py
+-rw-r--r--   0 root         (0) root         (0)     4529 2023-07-26 12:29:16.000000 rucio-clients-32.0.0rc1/tests/test_permission.py
+-rw-r--r--   0 root         (0) root         (0)     4147 2023-07-26 12:29:16.000000 rucio-clients-32.0.0rc1/tests/test_pfns.py
+-rw-r--r--   0 root         (0) root         (0)     1165 2023-07-26 12:29:16.000000 rucio-clients-32.0.0rc1/tests/test_ping.py
+-rw-r--r--   0 root         (0) root         (0)     7607 2023-07-26 12:29:16.000000 rucio-clients-32.0.0rc1/tests/test_preparer.py
+-rw-r--r--   0 root         (0) root         (0)     2765 2023-07-26 12:29:16.000000 rucio-clients-32.0.0rc1/tests/test_qos.py
+-rw-r--r--   0 root         (0) root         (0)     2053 2023-07-26 12:29:16.000000 rucio-clients-32.0.0rc1/tests/test_quarantined_replica.py
+-rw-r--r--   0 root         (0) root         (0)    27057 2023-07-26 12:29:16.000000 rucio-clients-32.0.0rc1/tests/test_reaper.py
+-rw-r--r--   0 root         (0) root         (0)     6464 2023-07-26 12:29:16.000000 rucio-clients-32.0.0rc1/tests/test_redirect.py
+-rw-r--r--   0 root         (0) root         (0)    62407 2023-07-26 12:29:16.000000 rucio-clients-32.0.0rc1/tests/test_replica.py
+-rw-r--r--   0 root         (0) root         (0)    32236 2023-07-26 12:29:16.000000 rucio-clients-32.0.0rc1/tests/test_replica_recoverer.py
+-rw-r--r--   0 root         (0) root         (0)    22852 2023-07-26 12:29:16.000000 rucio-clients-32.0.0rc1/tests/test_replica_sorting.py
+-rw-r--r--   0 root         (0) root         (0)    13879 2023-07-26 12:29:16.000000 rucio-clients-32.0.0rc1/tests/test_request.py
+-rw-r--r--   0 root         (0) root         (0)    13074 2023-07-26 12:29:16.000000 rucio-clients-32.0.0rc1/tests/test_root_proxy.py
+-rw-r--r--   0 root         (0) root         (0)    75437 2023-07-26 12:29:16.000000 rucio-clients-32.0.0rc1/tests/test_rse.py
+-rw-r--r--   0 root         (0) root         (0)    13230 2023-07-26 12:29:16.000000 rucio-clients-32.0.0rc1/tests/test_rse_expression_parser.py
+-rw-r--r--   0 root         (0) root         (0)     8966 2023-07-26 12:29:16.000000 rucio-clients-32.0.0rc1/tests/test_rse_lfn2path.py
+-rw-r--r--   0 root         (0) root         (0)     4300 2023-07-26 12:29:16.000000 rucio-clients-32.0.0rc1/tests/test_rse_protocol_gfal2.py
+-rw-r--r--   0 root         (0) root         (0)     3845 2023-07-26 12:29:16.000000 rucio-clients-32.0.0rc1/tests/test_rse_protocol_gfal2_impl.py
+-rw-r--r--   0 root         (0) root         (0)     3290 2023-07-26 12:29:16.000000 rucio-clients-32.0.0rc1/tests/test_rse_protocol_posix.py
+-rw-r--r--   0 root         (0) root         (0)     3713 2023-07-26 12:29:16.000000 rucio-clients-32.0.0rc1/tests/test_rse_protocol_rclone.py
+-rw-r--r--   0 root         (0) root         (0)     4822 2023-07-26 12:29:16.000000 rucio-clients-32.0.0rc1/tests/test_rse_protocol_rsync.py
+-rw-r--r--   0 root         (0) root         (0)     4002 2023-07-26 12:29:16.000000 rucio-clients-32.0.0rc1/tests/test_rse_protocol_srm.py
+-rw-r--r--   0 root         (0) root         (0)     4678 2023-07-26 12:29:16.000000 rucio-clients-32.0.0rc1/tests/test_rse_protocol_ssh.py
+-rw-r--r--   0 root         (0) root         (0)     3038 2023-07-26 12:29:16.000000 rucio-clients-32.0.0rc1/tests/test_rse_protocol_webdav.py
+-rw-r--r--   0 root         (0) root         (0)     3803 2023-07-26 12:29:16.000000 rucio-clients-32.0.0rc1/tests/test_rse_protocol_xrootd.py
+-rw-r--r--   0 root         (0) root         (0)     7770 2023-07-26 12:29:16.000000 rucio-clients-32.0.0rc1/tests/test_rse_selector.py
+-rw-r--r--   0 root         (0) root         (0)      891 2023-07-26 12:29:16.000000 rucio-clients-32.0.0rc1/tests/test_rucio_server.py
+-rw-r--r--   0 root         (0) root         (0)    90900 2023-07-26 12:29:16.000000 rucio-clients-32.0.0rc1/tests/test_rule.py
+-rw-r--r--   0 root         (0) root         (0)    13873 2023-07-26 12:29:16.000000 rucio-clients-32.0.0rc1/tests/test_schema_cms.py
+-rw-r--r--   0 root         (0) root         (0)     7510 2023-07-26 12:29:16.000000 rucio-clients-32.0.0rc1/tests/test_scope.py
+-rw-r--r--   0 root         (0) root         (0)    48744 2023-07-26 12:29:16.000000 rucio-clients-32.0.0rc1/tests/test_subscription.py
+-rw-r--r--   0 root         (0) root         (0)     2771 2023-07-26 12:29:16.000000 rucio-clients-32.0.0rc1/tests/test_temporary_did.py
+-rw-r--r--   0 root         (0) root         (0)    55640 2023-07-26 12:29:16.000000 rucio-clients-32.0.0rc1/tests/test_throttler.py
+-rw-r--r--   0 root         (0) root         (0)     5045 2023-07-26 12:29:16.000000 rucio-clients-32.0.0rc1/tests/test_tpc.py
+-rw-r--r--   0 root         (0) root         (0)     3204 2023-07-26 12:29:16.000000 rucio-clients-32.0.0rc1/tests/test_trace.py
+-rw-r--r--   0 root         (0) root         (0)    19610 2023-07-26 12:29:16.000000 rucio-clients-32.0.0rc1/tests/test_transfer.py
+-rw-r--r--   0 root         (0) root         (0)     8478 2023-07-26 12:29:16.000000 rucio-clients-32.0.0rc1/tests/test_undertaker.py
+-rw-r--r--   0 root         (0) root         (0)    15673 2023-07-26 12:29:16.000000 rucio-clients-32.0.0rc1/tests/test_upload.py
+-rw-r--r--   0 root         (0) root         (0)     6375 2023-07-26 12:29:16.000000 rucio-clients-32.0.0rc1/tests/test_utils.py
+drwxr-xr-x   0 root         (0) root         (0)        0 2023-07-26 15:14:47.805165 rucio-clients-32.0.0rc1/tools/
+-rw-r--r--   0 root         (0) root         (0)     6123 2023-07-26 12:29:16.000000 rucio-clients-32.0.0rc1/tools/merge_rucio_configs.py
```

### filetype from file(1)

```diff
@@ -1 +1 @@
-POSIX tar archive (GNU)
+POSIX tar archive
```

### Comparing `rucio-clients-1.9.6/bin/rucio` & `rucio-clients-32.0.0rc1/bin/rucio`

 * *Files 10% similar despite different names*

```diff
@@ -1,283 +1,275 @@
 #!/usr/bin/env python
-
-# Copyright European Organization for Nuclear Research (CERN)
+# -*- coding: utf-8 -*-
+# Copyright European Organization for Nuclear Research (CERN) since 2012
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
-# You may not use this file except in compliance with the License.
-# You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#    http://www.apache.org/licenses/LICENSE-2.0
 #
-# Authors:
-# - Mario Lassnig, <mario.lassnig@cern.ch>, 2012-2014, 2016-2017
-# - Vincent Garonne, <vincent.garonne@cern.ch>, 2012-2016
-# - Thomas Beermann, <thomas.beermann@cern.ch>, 2012
-# - Yun-Pin Sun, <yun-pin.sun@cern.ch>, 2013
-# - Cedric Serfon <cedric.serfon@cern.ch>, 2013-2016
-# - Martin Barisits <martin.barisits@cern.ch>, 2013-2016
-# - Joaquin Bogado <joaquin.bogado@cern.ch>, 2014, 2015
-# - Evangelia Liotiri <evangelia.liotiri@cern.ch>, 2015
-
-"""
-Rucio CLI.
-"""
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 
-import argcomplete
 import argparse
-import hashlib
-import json
+import errno
+import itertools
 import logging
+import math
 import os
-import random
-import requests
+import re
 import signal
-import socket
 import subprocess
 import sys
-import tabulate
 import time
 import traceback
+import unittest
 import uuid
-
-from ConfigParser import NoOptionError, NoSectionError
 from copy import deepcopy
+from datetime import datetime
 from functools import wraps
-from multiprocessing import Process
-from Queue import Queue, Empty
-from threading import Thread, Event
 
-from rucio.client import Client
+from configparser import NoOptionError, NoSectionError
+from tabulate import tabulate
+
+# rucio module has the same name as this executable module, so this rule fails. pylint: disable=no-name-in-module
 from rucio import version
-from rucio.common.config import config_get
-from rucio.common.exception import (DataIdentifierAlreadyExists, Duplicate, FileAlreadyExists, AccessDenied, ResourceTemporaryUnavailable,
-                                    DataIdentifierNotFound, InvalidObject, RSENotFound, InvalidRSEExpression, DuplicateContent, RSEProtocolNotSupported,
-                                    RuleNotFound, CannotAuthenticate, MissingDependency, UnsupportedOperation, FileConsistencyMismatch,
-                                    RucioException)
-from rucio.common.utils import adler32, generate_uuid, execute, chunks, sizefmt, Color
-from rucio.rse import rsemanager as rsemgr
+from rucio.client import Client
+from rucio.common.config import config_get, config_get_float
+from rucio.common.exception import (DataIdentifierAlreadyExists, AccessDenied, DataIdentifierNotFound, InvalidObject,
+                                    RSENotFound, InvalidRSEExpression, InputValidationError, DuplicateContent,
+                                    RuleNotFound, CannotAuthenticate, MissingDependency, UnsupportedOperation,
+                                    RucioException, DuplicateRule, InvalidType, DuplicateCriteriaInDIDFilter,
+                                    DIDFilterSyntaxError)
+from rucio.common.extra import import_extras
+from rucio.common.test_rucio_server import TestRucioServer
+from rucio.common.utils import sizefmt, Color, detect_client_location, chunks, parse_did_filter_from_string, \
+    parse_did_filter_from_string_fe, extract_scope, setup_logger, StoreAndDeprecateWarningAction
+from rucio.common.constants import ReplicaState
+from rucio.rse.protocols.protocol import RSEProtocol
+
+EXTRA_MODULES = import_extras(['argcomplete'])
+
+if EXTRA_MODULES['argcomplete']:
+    import argcomplete  # pylint: disable=E0401
 
 SUCCESS = 0
 FAILURE = 1
 
 DEFAULT_SECURE_PORT = 443
 DEFAULT_PORT = 80
 
-STOP_REQUEST = Event()
-
-logger = logging.getLogger("user")
+logger = logging.log
 gfal2_logger = logging.getLogger("gfal2")
 tablefmt = 'psql'
 
 
-def setup_logger(logger):
-    logger.setLevel(logging.INFO)
-    hdlr = logging.StreamHandler()
-
-    def emit_decorator(fnc):
-        def func(*args):
-            if 'RUCIO_LOGGING_FORMAT' not in os.environ:
-                levelno = args[0].levelno
-                if levelno >= logging.CRITICAL:
-                    color = '\033[31;1m'
-                elif levelno >= logging.ERROR:
-                    color = '\033[31;1m'
-                elif levelno >= logging.WARNING:
-                    color = '\033[33;1m'
-                elif levelno >= logging.INFO:
-                    color = '\033[32;1m'
-                elif levelno >= logging.DEBUG:
-                    color = '\033[36;1m'
-                else:
-                    color = '\033[0m'
-                formatter = logging.Formatter('{0}%(asctime)s\t%(levelname)s\t%(message)s\033[0m'.format(color))
-            else:
-                formatter = logging.Formatter(os.environ['RUCIO_LOGGING_FORMAT'])
-            hdlr.setFormatter(formatter)
-            return fnc(*args)
-        return func
-    hdlr.emit = emit_decorator(hdlr.emit)
-    logger.addHandler(hdlr)
-
-
 def setup_gfal2_logger(logger):
     logger.setLevel(logging.CRITICAL)
     logger.addHandler(logging.StreamHandler())
 
-setup_logger(logger)
+
 setup_gfal2_logger(gfal2_logger)
 
 
 def signal_handler(sig, frame):
     logger.warning('You pressed Ctrl+C! Exiting gracefully')
     child_processes = subprocess.Popen('ps -o pid --ppid %s --noheaders' % os.getpid(), shell=True, stdout=subprocess.PIPE)
     child_processes = child_processes.stdout.read()
     for pid in child_processes.split("\n")[:-1]:
         try:
             os.kill(int(pid), signal.SIGTERM)
         except Exception:
-            print 'Cannot kill child process'
-    STOP_REQUEST.set()
+            print('Cannot kill child process')
     sys.exit(1)
 
+
 signal.signal(signal.SIGINT, signal_handler)
 
 
-def extract_scope(did):
-    # Try to extract the scope from the DSN
-    if did.find(':') > -1:
-        if len(did.split(':')) > 2:
-            raise RucioException('Too many colons. Cannot extract scope and name')
-        scope, name = did.split(':')[0], did.split(':')[1]
-        if name.endswith('/'):
-            name = name[:-1]
+def get_scope(did, client):
+    try:
+        scope, name = extract_scope(did)
         return scope, name
-    else:
-        scope = did.split('.')[0]
-        if did.startswith('user') or did.startswith('group'):
-            scope = ".".join(did.split('.')[0:2])
-        if did.endswith('/'):
-            did = did[:-1]
-        return scope, did
-
-
-def send_trace(trace, trace_endpoint, user_agent, retries=5, threadnb=None, total_threads=None):
-    if user_agent.startswith('pilot'):
-        logger.debug('pilot detected - not sending trace')
-        return 0
-    else:
-        if threadnb is not None and total_threads is not None:
-            logger.debug('Thread %s/%s : sending trace' % (threadnb, total_threads))
-        else:
-            logger.debug('sending trace')
-
-    for dummy in xrange(retries):
-        try:
-            requests.post(trace_endpoint + '/traces/', verify=False, data=json.dumps(trace))
-            return 0
-        except Exception, error:
-            if threadnb is not None and total_threads is not None:
-                logger.debug('Thread %s/%s : %s' % (threadnb, total_threads, error))
-            else:
-                logger.debug(error)
-    return 1
+    except TypeError:
+        scopes = client.list_scopes()
+        scope, name = extract_scope(did, scopes)
+        return scope, name
+    return None, did
 
 
 def exception_handler(function):
     @wraps(function)
     def new_funct(*args, **kwargs):
         try:
             return function(*args, **kwargs)
-        except InvalidObject, error:
+        except InvalidObject as error:
             logger.error(error)
-            return FAILURE
-        except DataIdentifierNotFound, error:
+            return error.error_code
+        except DataIdentifierNotFound as error:
             logger.error(error)
             logger.debug('This means that the Data IDentifier you provided is not known by Rucio.')
-            return FAILURE
-        except AccessDenied, error:
+            return error.error_code
+        except AccessDenied as error:
             logger.error(error)
             logger.debug('This error is a permission issue. You cannot run this command with your account.')
-            return FAILURE
-        except DataIdentifierAlreadyExists, error:
+            return error.error_code
+        except DataIdentifierAlreadyExists as error:
             logger.error(error)
             logger.debug('This means that the Data IDentifier you try to add is already registered in Rucio.')
-            return FAILURE
-        except RSENotFound, error:
+            return error.error_code
+        except RSENotFound as error:
             logger.error(error)
             logger.debug('This means that the Rucio Storage Element you provided is not known by Rucio.')
-            return FAILURE
-        except InvalidRSEExpression, error:
+            return error.error_code
+        except InvalidRSEExpression as error:
             logger.error(error)
             logger.debug('This means the RSE expression you provided is not syntactically correct.')
-            return FAILURE
-        except DuplicateContent, error:
+            return error.error_code
+        except DuplicateContent as error:
             logger.error(error)
             logger.debug('This means that the DID you want to attach is already in the target DID.')
-            return FAILURE
-        except TypeError, error:
+            return error.error_code
+        except TypeError as error:
             logger.error(error)
             logger.debug('This means the parameter you passed has a wrong type.')
             return FAILURE
-        except RuleNotFound, error:
+        except RuleNotFound as error:
             logger.error(error)
             logger.debug('This means the rule you specified does not exist.')
-            return FAILURE
-        except UnsupportedOperation, error:
+            return error.error_code
+        except UnsupportedOperation as error:
             logger.error(error)
-            logger.debug('That means you cannot change the status of the DID.')
-            return FAILURE
-        except MissingDependency, error:
+            logger.debug('This means you cannot change the status of the DID.')
+            return error.error_code
+        except MissingDependency as error:
             logger.error(error)
             logger.debug('This means one dependency is missing.')
-            return FAILURE
-        except KeyError, error:
-            if 'x-rucio-auth-token' in error:
-                logger.error(error)
-                logger.error('This means that your RUCIO_ACCOUNT environment variable not match with a registered identity to your account.')
+            return error.error_code
+        except KeyError as error:
+            if 'x-rucio-auth-token' in str(error):
+                used_account = None
+                try:  # get the configured account from the configuration file
+                    used_account = '%s (from rucio.cfg)' % config_get('client', 'account')
+                except:
+                    pass
+                try:  # are we overriden by the environment?
+                    used_account = '%s (from RUCIO_ACCOUNT)' % os.environ['RUCIO_ACCOUNT']
+                except:
+                    pass
+                logger.error('Specified account %s does not have an associated identity.' % used_account)
             else:
-                logger.error(error)
                 logger.debug(traceback.format_exc())
-                logger.error("This means that the object doesn't have the property " + str(error) +
-                             ". This should never happen. Please rerun the last command with the '-v'" +
-                             " option and submit a ticket with all the necessary information at https://its.cern.ch/jira/browse/RUCIO")
-            return FAILURE
-        except RucioException, error:
-            logger.error(error)
-            return FAILURE
-        except IOError, error:
-            logger.error(error)
+                contact = config_get('policy', 'support', raise_exception=False)
+                support = ('Please follow up with all relevant information at: ' + contact) if contact else ''
+                logger.error('\nThe object is missing this property: %s\n'
+                             'This should never happen. Please rerun the last command with the "-v" option to gather more information.\n'
+                             '%s' % (str(error), support))
             return FAILURE
-        except Exception, error:
+        except RucioException as error:
             logger.error(error)
-            logger.error("Rucio exited with an unexpected/unknown error. Please rerun the last command with the '-v' option and submit a ticket with all the necessary information at https://its.cern.ch/jira/browse/RUCIO")
+            return error.error_code
+        except Exception as error:
+            if isinstance(error, IOError) and getattr(error, 'errno', None) == errno.EPIPE:
+                # Ignore Broken Pipe
+                # While in python3 we can directly catch 'BrokenPipeError', in python2 it doesn't exist.
+
+                # Python flushes standard streams on exit; redirect remaining output
+                # to devnull to avoid another BrokenPipeError at shutdown
+                devnull = os.open(os.devnull, os.O_WRONLY)
+                os.dup2(devnull, sys.stdout.fileno())
+                return SUCCESS
             logger.debug(traceback.format_exc())
+            logger.error(error)
+            contact = config_get('policy', 'support', raise_exception=False)
+            support = ("If it's a problem concerning your experiment or if you're unsure what to do, please follow up at: %s\n" % contact) if contact else ''
+            contact = config_get('policy', 'support_rucio', default='https://github.com/rucio/rucio/issues')
+            support += "If you're sure there is a problem with Rucio itself, please follow up at: " + contact
+            logger.error('\nRucio exited with an unexpected/unknown error.\n'
+                         'Please rerun the last command with the "-v" option to gather more information.\n'
+                         '%s' % support)
             return FAILURE
     return new_funct
 
 
 def get_client(args):
     """
     Returns a new client object.
     """
-    if args.auth_strategy == 'userpass':
+    if not args.auth_strategy:
+        if 'RUCIO_AUTH_TYPE' in os.environ:
+            auth_type = os.environ['RUCIO_AUTH_TYPE'].lower()
+        else:
+            try:
+                auth_type = config_get('client', 'auth_type').lower()
+            except (NoOptionError, NoSectionError):
+                logger.error('Cannot get AUTH_TYPE')
+                sys.exit(FAILURE)
+    else:
+        auth_type = args.auth_strategy.lower()
+
+    if auth_type in ['userpass', 'saml'] and args.username is not None and args.password is not None:
         creds = {'username': args.username, 'password': args.password}
+    elif auth_type == 'oidc':
+        if args.oidc_issuer:
+            args.oidc_issuer = args.oidc_issuer.lower()
+        creds = {'oidc_auto': args.oidc_auto,
+                 'oidc_scope': args.oidc_scope,
+                 'oidc_audience': args.oidc_audience,
+                 'oidc_polling': args.oidc_polling,
+                 'oidc_refresh_lifetime': args.oidc_refresh_lifetime,
+                 'oidc_issuer': args.oidc_issuer,
+                 'oidc_username': args.oidc_username,
+                 'oidc_password': args.oidc_password}
     else:
         creds = None
 
     try:
         client = Client(rucio_host=args.host, auth_host=args.auth_host,
                         account=args.account,
-                        auth_type=args.auth_strategy, creds=creds,
+                        auth_type=auth_type, creds=creds,
                         ca_cert=args.ca_certificate, timeout=args.timeout,
-                        user_agent=args.user_agent)
-    except CannotAuthenticate, error:
+                        user_agent=args.user_agent, vo=args.vo,
+                        logger=logger)
+    except CannotAuthenticate as error:
         logger.error(error)
-        if not args.auth_strategy:
-            if 'RUCIO_AUTH_TYPE' in os.environ:
-                auth_type = os.environ['RUCIO_AUTH_TYPE']
-            else:
-                try:
-                    auth_type = config_get('client', 'auth_type')
-                except (NoOptionError, NoSectionError):
-                    logger.error('Cannot get AUTH_TYPE')
-                    sys.exit(FAILURE)
-        if auth_type == 'x509_proxy':
+        if 'alert certificate expired' in str(error):
+            logger.error('The server certificate expired.')
+        elif auth_type.lower() == 'x509_proxy':
             logger.error('Please verify that your proxy is still valid and renew it if needed.')
         sys.exit(FAILURE)
     return client
 
 
+def __resolve_containers_to_datasets(scope, name, client):
+    """
+    Helper function to resolve a container into its dataset content.
+    """
+    datasets = []
+    for did in client.list_content(scope, name):
+        if did['type'] == 'DATASET':
+            datasets.append({'scope': did['scope'], 'name': did['name']})
+        elif did['type'] == 'CONTAINER':
+            datasets.extend(__resolve_containers_to_datasets(did['scope'], did['name'], client))
+    return datasets
+
+
 @exception_handler
 def ping(args):
     """
     Pings a Rucio server.
     """
     client = get_client(args)
     server_info = client.ping()
     if server_info:
-        print server_info['version']
+        print(server_info['version'])
         return SUCCESS
     logger.error('Ping failed')
     return FAILURE
 
 
 @exception_handler
 def whoami_account(args):
@@ -285,396 +277,463 @@
     %(prog)s show [options] <field1=value1 field2=value2 ...>
 
     Show extended information of a given account
     """
     client = get_client(args)
     info = client.whoami()
     for k in info:
-        print k.ljust(10) + ' : ' + str(info[k])
-    return SUCCESS
-
-
-@exception_handler
-def add_file(args):
-    """
-    %(prog)s show [options] <field1=value1 field2=value2 ...>
-
-    Add file.
-    """
-    client = get_client(args)
-    scope = args.scope
-    lfn = args.lfn
-    if scope is None:
-        scope, lfn = extract_scope(lfn)
-    client.add_file(rse=args.rse, scope=scope, lfn=lfn)
-    print 'Added new file replica: %s-%s' % (args.rse, args.lfn)
+        print(k.ljust(10) + ' : ' + str(info[k]))
     return SUCCESS
 
 
 @exception_handler
 def list_dataset_replicas(args):
     """
     %(prog)s list [options] <field1=value1 field2=value2 ...>
 
     List dataset replicas
     """
     client = get_client(args)
-    datasets, result = {}, {}
-    scope, name = extract_scope(args.did)
-    meta = client.get_metadata(scope, name)
-    if meta['did_type'] != 'DATASET':
-        dids = client.scope_list(scope=scope, name=name, recursive=True)
-        for did in dids:
-            if did['type'] == 'FILE':
-                dsn = '%s:%s' % (did['parent']['scope'], did['parent']['name'])
-                if dsn not in datasets:
-                    datasets[dsn] = 0
-                datasets[dsn] += 1
-    else:
-        datasets['%s:%s' % (scope, name)] = 0
-    for dsn in datasets:
-        scope, name = extract_scope(dsn)
-        result[dsn] = {}
-        for rep in client.list_dataset_replicas(scope, name, args.deep):
-            result[dsn][rep['rse']] = [rep['rse'], rep['available_length'], rep['length']]
+    result = {}
+    datasets = []
+
+    def _append_to_datasets(scope, name):
+        filedid = {'scope': scope, 'name': name}
+        if filedid not in datasets:
+            datasets.append(filedid)
+
+    def _fetch_datasets_for_meta(meta):
+        """Internal function to fetch datasets and recurse into files."""
+        if meta['did_type'] != 'DATASET':
+            dids = client.scope_list(scope=meta['scope'], name=meta['name'], recursive=True)
+            for did in dids:
+                if did['type'] == 'FILE':
+                    _append_to_datasets(did['parent']['scope'], did['parent']['name'])
+        else:
+            _append_to_datasets(meta['scope'], meta['name'])
+
+    def _append_result(dsn, replica):
+        if dsn not in result:
+            result[dsn] = {}
+        result[dsn][replica['rse']] = [replica['rse'], replica['available_length'], replica['length']]
+
+    if len(args.dids) == 1:
+        scope, name = get_scope(args.dids[0], client)
+        dmeta = client.get_metadata(scope, name)
+        _fetch_datasets_for_meta(meta=dmeta)
+    else:
+        extractdids = (get_scope(did, client) for did in args.dids)
+        splitdids = [{'scope': scope, 'name': name} for scope, name in extractdids]
+        for dmeta in client.get_metadata_bulk(dids=splitdids):
+            _fetch_datasets_for_meta(meta=dmeta)
+
+    if args.deep or len(datasets) < 2:
+        for did in datasets:
+            dsn = "%s:%s" % (did['scope'], did['name'])
+            for rep in client.list_dataset_replicas(scope=did['scope'], name=did['name'], deep=args.deep):
+                _append_result(dsn=dsn, replica=rep)
+    else:
+        for rep in client.list_dataset_replicas_bulk(dids=datasets):
+            dsn = "%s:%s" % (rep['scope'], rep['name'])
+            _append_result(dsn=dsn, replica=rep)
+
     if args.csv:
         for dsn in result:
-            for rse in result[dsn].values():
-                print "{0}, {1}, {2}".format(rse[0], rse[1], rse[2])
+            for rse in list(result[dsn].values()):
+                print(rse[0], rse[1], rse[2], sep=',')
     else:
         for dsn in result:
-            print '\nDATASET: %s' % (dsn)
-            print tabulate.tabulate(result[dsn].values(), tablefmt=tablefmt, headers=['RSE', 'FOUND', 'TOTAL'])
+            print('\nDATASET: %s' % (dsn))
+            print(tabulate(list(result[dsn].values()), tablefmt=tablefmt, headers=['RSE', 'FOUND', 'TOTAL']))
     return SUCCESS
 
 
 @exception_handler
 def list_file_replicas(args):
     """
     %(prog)s list [options] <field1=value1 field2=value2 ...>
 
     List file replicas
     """
+    if args.missing:
+        args.all_states = True
     client = get_client(args)
+
     protocols = None
     if args.protocols:
         protocols = args.protocols.split(',')
 
     table = []
     dids = []
-    rse_dict = {}
-    if args.missing and not args.selected_rse:
-        print 'Cannot use --missing without specifying a RSE'
+    if args.missing and not args.rses:
+        print('Cannot use --missing without specifying a RSE')
         return FAILURE
-    if args.selected_rse and args.list_collections:
-        print 'Cannot use --rse with --list_collections option'
-        return FAILURE
-    if args.list_collections and len(args.dids) > 1:
-        print 'Cannot use --list_collections option with multiple dids'
+    if args.link and ':' not in args.link:
+        print('The substitution parameter must equal --link="/pfn/dir:/dst/dir"')
         return FAILURE
 
     for did in args.dids:
-        scope, name = extract_scope(did)
+        scope, name = get_scope(did, client)
+        client.get_metadata(scope=scope, name=name)  # break with Exception before streaming replicas if DID does not exist
         dids.append({'scope': scope, 'name': name})
+
     replicas = client.list_replicas(dids, schemes=protocols,
+                                    ignore_availability=True,
                                     all_states=args.all_states,
-                                    rse_expression=args.rse_expression)
+                                    rse_expression=args.rses,
+                                    metalink=args.metalink,
+                                    client_location=detect_client_location(),
+                                    sort=args.sort, domain=args.domain,
+                                    resolve_archives=not args.no_resolve_archives)
+    rses = [rse["rse"] for rse in client.list_rses(rse_expression=args.rses)]
+
+    if args.metalink:
+        print(replicas[:-1])  # last character is newline, no need to print that
+        return SUCCESS
+
     if args.missing:
-        for replica in replicas:
-            rses = replica['rses'].keys()
-            if args.selected_rse not in rses:
-                table.append([replica['scope'], replica['name']])
-        print tabulate.tabulate(table, tablefmt=tablefmt, headers=['SCOPE', 'NAME'])
+        for replica, rse in itertools.product(replicas, rses):
+            if 'states' in replica and rse in replica['states'] and replica['states'].get(rse) != 'AVAILABLE':
+                table.append([replica['scope'], replica['name'], "({0}) {1}".format(ReplicaState[replica['states'].get(rse)].value, rse)])
+        print(tabulate(table, tablefmt=tablefmt, headers=['SCOPE', 'NAME', '(STATE) RSE']))
+    elif args.link:
+        pfn_dir, dst_dir = args.link.split(':')
+        if args.rses:
+            for replica, rse in itertools.product(replicas, rses):
+                if rse in list(replica['rses'].keys()) and replica['rses'][rse]:
+                    for pfn in replica['rses'][rse]:
+                        os.symlink(dst_dir + pfn.rsplit(pfn_dir)[-1], replica['name'])
+        else:
+            for replica in replicas:
+                for rse in replica['rses']:
+                    if replica['rses'][rse]:
+                        for pfn in replica['rses'][rse]:
+                            os.symlink(dst_dir + pfn.rsplit(pfn_dir)[-1], replica['name'])
+    elif args.pfns:
+        if args.rses:
+            for replica, rse in itertools.product(replicas, rses):
+                if rse in list(replica['rses'].keys()) and replica['rses'][rse]:
+                    for pfn in replica['rses'][rse]:
+                        print(pfn)
+        else:
+            for replica in replicas:
+                for rse in replica['rses']:
+                    if replica['rses'][rse]:
+                        for pfn in replica['rses'][rse]:
+                            print(pfn)
     else:
+        if args.all_states:
+            header = ['SCOPE', 'NAME', 'FILESIZE', 'ADLER32', '(STATE) RSE: REPLICA']
+        else:
+            header = ['SCOPE', 'NAME', 'FILESIZE', 'ADLER32', 'RSE: REPLICA']
         for replica in replicas:
             if 'bytes' in replica:
                 for rse in replica['rses']:
-                    if args.list_collections:
-                        if rse not in rse_dict:
-                            rse_dict[rse] = 0
-                        if replica['rses'][rse] != []:
-                            rse_dict[rse] += 1
-                    else:
-                        for pfn in replica['rses'][rse]:
-                            if args.selected_rse:
-                                if rse == args.selected_rse:
-                                    table.append([replica['scope'], replica['name'], sizefmt(replica['bytes'], args.human), replica['adler32'], '{0}: {1}'.format(rse, pfn)])
-                            else:
-                                table.append([replica['scope'], replica['name'], sizefmt(replica['bytes'], args.human), replica['adler32'], '{0}: {1}'.format(rse, pfn)])
-            elif not args.list_collections:
-                table.append([replica['scope'], replica['name'], '???', '???', 'Unavailable'])
-        if args.list_collections:
-            nbfiles = len([i for i in client.list_files(dids[0]['scope'], dids[0]['name'])])
-            sorted_key = rse_dict.keys()
-            sorted_key.sort()
-            table = []
-            for rse in sorted_key:
-                table.append([rse, rse_dict[rse], nbfiles])
-        if args.list_collections:
-            logger.warning('This option is deprecated. Please use rucio list-dataset-replicas instead.')
-            print tabulate.tabulate(table, tablefmt=tablefmt, headers=['RSE', 'Found', 'Total'])
-        else:
-            print tabulate.tabulate(table, tablefmt=tablefmt, headers=['SCOPE', 'NAME', 'FILESIZE', 'ADLER32', 'RSE: REPLICA'])
+                    for pfn in replica['rses'][rse]:
+                        if args.all_states:
+                            rse_string = '({2}) {0}: {1}'.format(rse, pfn, ReplicaState[replica['states'][rse]].value)
+                        else:
+                            rse_string = '{0}: {1}'.format(rse, pfn)
+                        if args.rses:
+                            for selected_rse in rses:
+                                if rse == selected_rse:
+                                    table.append([replica['scope'], replica['name'], sizefmt(replica['bytes'], args.human), replica['adler32'], rse_string])
+                        else:
+                            table.append([replica['scope'], replica['name'], sizefmt(replica['bytes'], args.human), replica['adler32'], rse_string])
+        print(tabulate(table, tablefmt=tablefmt, headers=header, disable_numparse=True))
     return SUCCESS
 
 
 @exception_handler
 def add_dataset(args):
     """
     %(prog)s add-dataset [options] <dsn>
 
     Add a dataset identifier.
     """
     client = get_client(args)
-    scope, name = extract_scope(args.did)
+    scope, name = get_scope(args.did, client)
     client.add_dataset(scope=scope, name=name, statuses={'monotonic': args.monotonic}, lifetime=args.lifetime)
-    print 'Added %s:%s' % (scope, name)
+    print('Added %s:%s' % (scope, name))
     return SUCCESS
 
 
 @exception_handler
 def add_container(args):
     """
     %(prog)s add-container [options] <dsn>
 
     Add a container identifier.
     """
     client = get_client(args)
-    scope, name = extract_scope(args.did)
+    scope, name = get_scope(args.did, client)
     client.add_container(scope=scope, name=name, statuses={'monotonic': args.monotonic}, lifetime=args.lifetime)
-    print 'Added %s:%s' % (scope, name)
+    print('Added %s:%s' % (scope, name))
     return SUCCESS
 
 
 @exception_handler
 def attach(args):
     """
     %(prog)s attach [options] <field1=value1 field2=value2 ...>
 
     Attach a data identifier.
     """
     client = get_client(args)
-    scope, name = extract_scope(args.todid)
-    dids = []
+    scope, name = get_scope(args.todid, client)
+    dids = args.dids
+    limit = 499
+
     if args.fromfile:
-        if len(args.dids) > 1:
+        if len(dids) > 1:
             logger.error("If --fromfile option is active, only one file is supported. The file should contain a list of dids, one per line.")
             return FAILURE
         try:
-            f = open(args.dids[0], 'r')
-            for did in f.readlines():
-                cscope, cname = extract_scope(did.rstrip())
-                dids.append({'scope': cscope, 'name': cname})
+            f = open(dids[0], 'r')
+            dids = [did.rstrip() for did in f.readlines()]
         except IOError:
-            logger.error("Can't open file '" + args.dids[0] + "'.")
+            logger.error("Can't open file '" + dids[0] + "'.")
             return FAILURE
+
+    dids = [{'scope': get_scope(did, client)[0], 'name': get_scope(did, client)[1]} for did in dids]
+    if len(dids) <= limit:
+        client.attach_dids(scope=scope, name=name, dids=dids)
     else:
-        for did in args.dids:
-            cscope, cname = extract_scope(did)
-            dids.append({'scope': cscope, 'name': cname})
-    client.attach_dids(scope=scope, name=name, dids=dids)
-    print 'DIDs successfully attached to %s:%s' % (scope, name)
+        logger.warning("You are trying to attach too much DIDs. Therefore they will be chunked and attached in multiple commands.")
+        missing_dids = []
+        for i, chunk in enumerate(chunks(dids, limit)):
+            logger.info("Try to attach chunk {0}/{1}".format(i, int(math.ceil(float(len(dids)) / float(limit)))))
+            try:
+                client.attach_dids(scope=scope, name=name, dids=chunk)
+            except Exception:
+                content = [{'scope': did['scope'], 'name': did['name']} for did in client.list_content(scope=scope, name=name)]
+                missing_dids += [did for did in chunk if did not in content]
+
+        if missing_dids:
+            for chunk in chunks(missing_dids, limit):
+                client.attach_dids(scope=scope, name=name, dids=chunk)
+
+    print('DIDs successfully attached to %s:%s' % (scope, name))
     return SUCCESS
 
 
 @exception_handler
 def detach(args):
     """
     %(prog)s detach [options] <field1=value1 field2=value2 ...>
 
     Detach data identifier.
     """
     client = get_client(args)
-    scope, name = extract_scope(args.fromdid)
+    scope, name = get_scope(args.fromdid, client)
     dids = []
     for did in args.dids:
-        cscope, cname = extract_scope(did)
+        cscope, cname = get_scope(did, client)
         dids.append({'scope': cscope, 'name': cname})
     client.detach_dids(scope=scope, name=name, dids=dids)
+    print('DIDs successfully detached from %s:%s' % (scope, name))
     return SUCCESS
 
 
 @exception_handler
 def list_dids(args):
     """
-    %(prog)s list-dids scope[:*|:name] [--filter 'key=value' | --recursive]
+    %(prog)s list-dids scope[:*|:name] [--filter 'value' | --recursive]
 
     List the data identifiers for a given scope.
     """
     client = get_client(args)
     filters = {}
-    type = 'collection'
+    type_ = 'collection'
     table = []
-    if args.filter:
-        if args.recursive:
-            logger.error('Option recursive and filter cannot be used together')
-            return FAILURE
-        else:
-            try:
-                for key, value in [(a.split('=')[0], a.split('=')[1]) for a in args.filter.split(',')]:
-                    if key == 'type':
-                        if value.upper() in ['ALL', 'COLLECTION', 'CONTAINER', 'DATASET', 'FILE']:
-                            type = value.lower()
-                        else:
-                            logger.error('{0} is not a valid type. Valid types are {1}'.format(value, ['ALL', 'COLLECTION', 'CONTAINER', 'DATASET', 'FILE']))
-                            return FAILURE
-                    else:
-                        if value.lower() == 'true':
-                            value = '1'
-                        elif value.lower() == 'false':
-                            value = '0'
-                        filters[key] = value
-            except Exception:
-                logger.error("Invalid Filter. Filter must be 'key=value'")
-                return FAILURE
+
     try:
-        scope, name = extract_scope(args.did[0])
+        scope, name = get_scope(args.did[0], client)
         if name == '':
             name = '*'
     except InvalidObject:
         scope = args.did[0]
         name = '*'
+
     if scope not in client.list_scopes():
         logger.error('Scope not found')
         return FAILURE
-    if name.find('*') > -1:
-        if args.recursive:
-            logger.error('Option recursive cannot be used with wildcards')
-            return FAILURE
-        elif ('name' in filters) and (name != '*'):
-            logger.error('You cannot use a wildcard query and a filter by name')
-            return FAILURE
-        filters['name'] = name
-        for did in client.list_dids(scope, filters=filters, type=type, long=True):
-            table.append(['%s:%s' % (did['scope'], did['name']), did['did_type']])
+
+    if args.recursive and '*' in name:
+        logger.error('Option recursive cannot be used with wildcards')
+        return FAILURE
     else:
-        dids = client.scope_list(scope=scope, name=name, recursive=args.recursive)
-        no_result = True
-        for did in dids:
-            table.append(['%s:%s' % (did['scope'], did['name']), did['type'].upper()])
-        if no_result:
-            did_info = client.get_did(scope=scope, name=name)
-            table.append(['%s:%s' % (did_info['scope'], did_info['name']), did_info['type'].upper()])
+        if filters:
+            if ('name' in filters) and (name != '*'):
+                logger.error('Must have a wildcard in did name if filtering by name')
+                return FAILURE
+
+    try:
+        filters, type_ = parse_did_filter_from_string_fe(args.filter, name)
+    except InvalidType as error:
+        logger.error(error)
+        return FAILURE
+    except DuplicateCriteriaInDIDFilter as error:
+        logger.error(error)
+        return FAILURE
+    except DIDFilterSyntaxError as error:
+        logger.error(error)
+        return FAILURE
+    except ValueError as error:
+        logger.error(error)
+        return FAILURE
+    except Exception as e:
+        logger.error(e)
+        return FAILURE
+
+    for did in client.list_dids(scope, filters=filters, did_type=type_, long=True, recursive=args.recursive):
+        table.append(['%s:%s' % (did['scope'], did['name']), did['did_type']])
 
     if args.short:
-        for did, dummy in table:
-            print did
+        for did, _ in table:
+            print(did)
     else:
-        print tabulate.tabulate(table, tablefmt=tablefmt, headers=['SCOPE:NAME', '[DID TYPE]'])
+        print(tabulate(table, tablefmt=tablefmt, headers=['SCOPE:NAME', '[DID TYPE]']))
+
     return SUCCESS
 
 
 @exception_handler
+def list_dids_extended(args):
+    """
+    %(prog)s list-dids-extended scope[:*|:name] [--filter 'key=value' | --recursive]
+
+    List the data identifiers for a given scope (DEPRECATED).
+    """
+    logger.error("This command has been deprecated. Please use list_dids instead.")
+    return FAILURE
+
+
+@exception_handler
 def list_scopes(args):
     """
     %(prog)s list-scopes <scope>
 
     List scopes.
     """
     # For the moment..
     client = get_client(args)
     scopes = client.list_scopes()
     for scope in scopes:
-        print scope
+        print(scope)
     return SUCCESS
 
 
 @exception_handler
 def list_files(args):
     """
     %(prog)s list-files [options] <field1=value1 field2=value2 ...>
 
     List data identifier contents.
     """
     client = get_client(args)
     if args.csv:
         for did in args.dids:
-            scope, name = extract_scope(did)
+            scope, name = get_scope(did, client)
             for f in client.list_files(scope=scope, name=name):
                 guid = f['guid']
-                guid = '%s-%s-%s-%s-%s' % (guid[0:8], guid[8:12], guid[12:16], guid[16:20], guid[20:32])
-                print "{0}:{1},{2},{3},{4},{5}".format(f['scope'], f['name'], guid, f['adler32'], sizefmt(f['bytes'], args.human), f['events'])
+                if guid:
+                    guid = '%s-%s-%s-%s-%s' % (guid[0:8], guid[8:12], guid[12:16], guid[16:20], guid[20:32])
+                else:
+                    guid = '(None)'
+                print('{}:{}'.format(f['scope'], f['name']), guid, f['adler32'], sizefmt(f['bytes'], args.human), f['events'], sep=',')
         return SUCCESS
     elif args.LOCALPATH:
 
-        print '''<?xml version="1.0" encoding="UTF-8" standalone="no" ?>
+        print('''<?xml version="1.0" encoding="UTF-8" standalone="no" ?>
 <!DOCTYPE POOLFILECATALOG SYSTEM "InMemory">
-<POOLFILECATALOG>'''
+<POOLFILECATALOG>''')
 
         file_str = ''' <File ID="%s">
   <physical>
    <pfn filetype="ROOT_All" name="%s/%s"/>
   </physical>
   <logical>
    <lfn name="%s"/>
   </logical>
  </File>'''
 
         for did in args.dids:
-            scope, name = extract_scope(did)
+            scope, name = get_scope(did, client)
             for f in client.list_files(scope=scope, name=name):
-                guid = '%s-%s-%s-%s-%s' % (f['guid'][0:8], f['guid'][8:12], f['guid'][12:16], f['guid'][16:20], f['guid'][20:32])
-                print file_str % (guid, args.LOCALPATH, f['name'], f['name'])
+                guid = f['guid']
+                if guid:
+                    guid = '%s-%s-%s-%s-%s' % (guid[0:8], guid[8:12], guid[12:16], guid[16:20], guid[20:32])
+                else:
+                    guid = '(None)'
+                print(file_str % (guid, args.LOCALPATH, f['name'], f['name']))
 
-        print '</POOLFILECATALOG>'
+        print('</POOLFILECATALOG>')
         return SUCCESS
     else:
         table = []
         for did in args.dids:
             totfiles = 0
             totsize = 0
             totevents = 0
-            scope, name = extract_scope(did)
+            scope, name = get_scope(did, client)
             for file in client.list_files(scope=scope, name=name):
                 totfiles += 1
                 totsize += int(file['bytes'])
                 if file['events']:
                     totevents += int(file.get('events', 0))
                 guid = file['guid']
-                guid = '%s-%s-%s-%s-%s' % (guid[0:8], guid[8:12], guid[12:16], guid[16:20], guid[20:32])
+                if guid:
+                    guid = '%s-%s-%s-%s-%s' % (guid[0:8], guid[8:12], guid[12:16], guid[16:20], guid[20:32])
+                else:
+                    guid = '(None)'
                 table.append(['%s:%s' % (file['scope'], file['name']), guid, 'ad:%s' % file['adler32'], sizefmt(file['bytes'], args.human), file['events']])
-            print tabulate.tabulate(table, tablefmt=tablefmt, headers=['SCOPE:NAME', 'GUID', 'ADLER32', 'FILESIZE', 'EVENTS'])
-            print 'Total files : %s' % totfiles
-            print 'Total size : %s' % sizefmt(totsize, args.human)
+            print(tabulate(table, tablefmt=tablefmt, headers=['SCOPE:NAME', 'GUID', 'ADLER32', 'FILESIZE', 'EVENTS'], disable_numparse=True))
+            print('Total files : %s' % totfiles)
+            print('Total size : %s' % sizefmt(totsize, args.human))
             if totevents:
-                print 'Total events : %s' % totevents
+                print('Total events : %s' % totevents)
         return SUCCESS
 
 
 @exception_handler
 def list_content(args):
     """
     %(prog)s list-content [options] <field1=value1 field2=value2 ...>
 
     List data identifier contents.
     """
     client = get_client(args)
     table = []
     for did in args.dids:
-        scope, name = extract_scope(did)
+        scope, name = get_scope(did, client)
         for content in client.list_content(scope=scope, name=name):
             table.append(['%s:%s' % (content['scope'], content['name']), content['type'].upper()])
-    print tabulate.tabulate(table, tablefmt=tablefmt, headers=['SCOPE:NAME', '[DID TYPE]'])
+    if args.short:
+        for did, dummy in table:
+            print(did)
+    else:
+        print(tabulate(table, tablefmt=tablefmt, headers=['SCOPE:NAME', '[DID TYPE]']))
     return SUCCESS
 
 
 @exception_handler
 def list_content_history(args):
     """
     %(prog)s list-content-history [options] <field1=value1 field2=value2 ...>
 
     List data identifier contents.
     """
     client = get_client(args)
     table = []
     for did in args.dids:
-        scope, name = extract_scope(did)
+        scope, name = get_scope(did, client)
         for content in client.list_content_history(scope=scope, name=name):
             table.append(['%s:%s' % (content['scope'], content['name']), content['type'].upper()])
-    print tabulate.tabulate(table, tablefmt=tablefmt, headers=['SCOPE:NAME', '[DID TYPE]'])
+    print(tabulate(table, tablefmt=tablefmt, headers=['SCOPE:NAME', '[DID TYPE]']))
     return SUCCESS
 
 
 @exception_handler
 def list_parent_dids(args):
     """
     %(prog)s list-parent-dids
@@ -688,1068 +747,551 @@
             for key in res:
                 if key not in dict_datasets:
                     dict_datasets[key] = []
                 for rule in client.list_associated_rules_for_file(res[key]['scope'], res[key]['name']):
                     if '%s:%s' % (rule['scope'], rule['name']) not in dict_datasets[key]:
                         dict_datasets[key].append('%s:%s' % (rule['scope'], rule['name']))
         for pfn in dict_datasets:
-            print 'PFN: ', pfn
-            print 'Parents: ', ','.join(dict_datasets[pfn])
+            print('PFN: ', pfn)
+            print('Parents: ', ','.join(dict_datasets[pfn]))
     elif args.guids:
         guids = []
-        for input in args.guids:
+        for input_ in args.guids:
             try:
-                uuid.UUID(input)
+                uuid.UUID(input_)
             except ValueError:
                 continue
         dict_datasets = {}
         for guid in guids:
             for did in client.get_dataset_by_guid(guid):
                 if guid not in dict_datasets:
                     dict_datasets[guid] = []
                 for rule in client.list_associated_rules_for_file(did['scope'], did['name']):
                     if '%s:%s' % (rule['scope'], rule['name']) not in dict_datasets[guid]:
                         dict_datasets[guid].append('%s:%s' % (rule['scope'], rule['name']))
         for guid in dict_datasets:
-            print 'GUID: ', guid
-            print 'Parents : ', ','.join(dict_datasets[guid])
-    else:
+            print('GUID: ', guid)
+            print('Parents : ', ','.join(dict_datasets[guid]))
+    elif args.did:
         table = []
-        scope, name = extract_scope(args.did)
+        scope, name = get_scope(args.did, client)
         for dataset in client.list_parent_dids(scope=scope, name=name):
             table.append(['%s:%s' % (dataset['scope'], dataset['name']), dataset['type']])
-        print tabulate.tabulate(table, tablefmt=tablefmt, headers=['SCOPE:NAME', '[DID TYPE]'])
+        print(tabulate(table, tablefmt=tablefmt, headers=['SCOPE:NAME', '[DID TYPE]']))
+    else:
+        print('At least one option has to be given. Use -h to list the options.')
+        return FAILURE
     return SUCCESS
 
 
 @exception_handler
 def close(args):
     """
     %(prog)s close [options] <field1=value1 field2=value2 ...>
 
     Close a dataset or container.
     """
     client = get_client(args)
     for did in args.dids:
-        scope, name = extract_scope(did)
+        scope, name = get_scope(did, client)
         client.set_status(scope=scope, name=name, open=False)
-        print '%(scope)s:%(name)s has been closed.' % locals()
+        print('%(scope)s:%(name)s has been closed.' % locals())
     return SUCCESS
 
 
 @exception_handler
 def reopen(args):
     """
     %(prog)s reopen [options] <field1=value1 field2=value2 ...>
 
     Reopen a dataset or container (only for privileged users).
     """
     client = get_client(args)
     for did in args.dids:
-        scope, name = extract_scope(did)
+        scope, name = get_scope(did, client)
         client.set_status(scope=scope, name=name, open=True)
-        print '%(scope)s:%(name)s has been reopened.' % locals()
+        print('%(scope)s:%(name)s has been reopened.' % locals())
     return SUCCESS
 
 
 @exception_handler
 def stat(args):
     """
     %(prog)s stat [options] <field1=value1 field2=value2 ...>
 
     List attributes and statuses about data identifiers..
     """
     client = get_client(args)
-    for did in args.dids:
-        scope, name = extract_scope(did)
-        info = client.get_did(scope=scope, name=name)
-        for key, val in info.iteritems():
-            print '%(key)s: %(val)s' % locals()
+    for i, did in enumerate(args.dids):
+        if i > 0:
+            print('------')
+        scope, name = get_scope(did, client)
+        info = client.get_did(scope=scope, name=name, dynamic_depth='DATASET')
+        table = [(k + ':', str(v)) for (k, v) in sorted(info.items())]
+        print(tabulate(table, tablefmt='plain', disable_numparse=True))
     return SUCCESS
 
 
 def erase(args):
     """
     %(prog)s erase [options] <field1=value1 field2=value2 ...>
 
     Delete data identifier.
     """
     client = get_client(args)
     for did in args.dids:
-        scope, name = extract_scope(did)
+        if '*' in did:
+            logger.warning("This command doesn't support wildcards! Skipping DID: %s" % did)
+            continue
+        try:
+            scope, name = get_scope(did, client)
+        except RucioException as error:
+            logger.warning('DID is in wrong format: %s' % did)
+            logger.debug('Error: %s' % error)
+            continue
+
         if args.undo:
             try:
                 client.set_metadata(scope=scope, name=name, key='lifetime', value=None)
                 logger.info('Erase undo for DID: {0}:{1}'.format(scope, name))
             except Exception:
                 logger.warning('Cannot undo erase operation on DID. DID not existent or grace period of 24 hours already expired.')
                 logger.warning('    DID: {0}:{1}'.format(scope, name))
         else:
-            # set lifetime to expire in 24 hours (value is in seconds).
-            client.set_metadata(scope=scope, name=name, key='lifetime', value=86400)
-            logger.info('CAUTION! erase operation is irreversible after 24 hours. To cancel this operation you can run the following command:')
-            print "rucio erase --undo {0}:{1}".format(scope, name)
+            try:
+                # set lifetime to expire in 24 hours (value is in seconds).
+                client.set_metadata(scope=scope, name=name, key='lifetime', value=86400)
+                logger.info('CAUTION! erase operation is irreversible after 24 hours. To cancel this operation you can run the following command:')
+                print("rucio erase --undo {0}:{1}".format(scope, name))
+            except RucioException as error:
+                logger.warning('Failed to erase DID: %s' % did)
+                logger.debug('Error: %s' % error)
     return SUCCESS
 
 
-def __get_dataset(args):
-    '''Parse helper for upload'''
-    dsscope = None
-    dsname = None
-    for item in args:
-        if item.count(':') == 1:
-            if dsscope:
-                raise Exception("Only one dataset should be given")
-            else:
-                dsscope, dsname = item.split(':')
-    return dsscope, dsname
+@exception_handler
+def list_impls(args):
+    """
+    %(prog)s list-impls
+
+    List protocol implementations.
+    """
 
+    PROTOCOL_DIRECTORY = '/opt/rucio/lib/rucio/rse/protocols'
 
-def __get_files(args):
-    '''Parse helper for upload'''
-    files = []
-    for item in args:
-        # Skip things that look like a scope:datasetname
-        if item.count(':') == 1:
-            logger.warning("{0} cannot be distinguished from scope:datasetname. Skipping it.".format(item))
+    for filename in os.listdir(PROTOCOL_DIRECTORY):
+        if os.path.isdir(os.path.join(PROTOCOL_DIRECTORY, filename)) or filename in ["__init__.py", "cache.py", "http_cache.py", "dummy.py", "protocol.py"]:
             continue
-        if os.path.isdir(item):
-            dname, subdirs, fnames = os.walk(item).next()
-            # Check if there are files in the directory
-            if fnames:
-                for fname in fnames:
-                    files.append(os.path.join(dname, fname))
-            # No files, but subdirectories. Needed to be added one-by-one
-            # Maybe change so we look through the subdirs and add those files?
-            elif subdirs:
-                raise Exception("Directory ({directory}) has no files in it. Please add subdirectories individually.".format(directory=dname))
-            else:
-                raise Exception("Directory ({directory}) is empty.".format(directory=dname))
-        elif os.path.isfile(item):
-            files.append(item)
         else:
-            logger.warning('{0} is not a directory or file or does not exist'.format(item))
-    return files
+            filename = re.sub(r".py", r"", filename)
+            __import__("rucio.rse.protocols", fromlist=[filename])
+
+    impls = []
+
+    def get_subclasses(cls):
+        for subclass in cls.__subclasses__():
+            if (str(subclass)[28:-2]).endswith('Default'):
+                class_name = str(subclass)[28:-10]
+            else:
+                class_name = str(subclass)[28:-2]
+            impls.append([class_name, subclass.__doc__])
+            get_subclasses(subclass)
+
+    get_subclasses(RSEProtocol)
+    impls = sorted(impls)
+    table = []
+    for impl in impls:
+        table.append([impl[0], impl[1]])
+
+    print(tabulate(table, tablefmt=tablefmt, headers=['impl', 'DESCRIPTION']))
+    return SUCCESS
 
 
 @exception_handler
 def upload(args):
     """
     rucio upload [scope:datasetname] [folder/] [files1 file2 file3]
     %(prog)s upload [options] <field1=value1 field2=value2 ...>
 
     Upload files into Rucio
     """
-    client = get_client(args)
-    try:
-        dsscope, dsname = __get_dataset(args.args)    # None, None if no dataset given
-    except Exception:
-        logger.error('rucio upload only allows to upload files to one dataset, more than one provided.')
-        return FAILURE
-    # Checking if the user can add to the scope given for the dataset
-    if dsscope and dsscope not in client.list_scopes_for_account(client.account):
-        logger.error("Cannot add to scope {scope}.".format(scope=dsscope))
-        return FAILURE
-    files = __get_files(args.args)               # a list of file names (even if a directory is given)
-    if not files:
-        return FAILURE
-    list_files = []
-    files_to_list = []
-    lfns = {}
-    revert_dict = {}
-    if args.scope:
-        fscope = args.scope
-    else:
-        fscope = 'user.' + client.account
-    if args.no_register is False and fscope not in client.list_scopes_for_account(client.account):
-        logger.error("Cannot guess the scope for the files. You must specify one with --scope option. The scope must be one of: " + ', '.join(client.list_scopes_for_account(client.account)))
+    if args.lifetime and args.expiration_date:
+        logger.error("--lifetime and --expiration-date cannot be specified at the same time.")
         return FAILURE
-
-    trace = {}
-    trace['hostname'] = socket.getfqdn()
-    trace['scope'] = fscope
-    trace['uuid'] = generate_uuid()
-    for name in files:
-        try:
-            size = os.stat(name).st_size
-            checksum = adler32(name)
-            logger.debug('Extracting filesize (%s) and checksum (%s) for file %s:%s' % (str(size), checksum, fscope, os.path.basename(name)))
-            files_to_list.append({'scope': fscope, 'name': os.path.basename(name)})
-            if not args.guid and 'pool.root' in name.lower():  # is a root file, getting the GUID
-                status, output, err = execute('pool_extractFileIdentifier {0}'.format(name))
-                if status != 0:
-                    logger.error('Trying to upload ROOT files but pool_extractFileIdentifier tool can not be found.')
-                    logger.error('Setup your ATHENA environment and try again.')
-                    return FAILURE
-                try:
-                    logger.debug('Extracting GUID from POOL file: {0}'.format(output.splitlines()[-1].split()[0].replace('-', '').lower()))
-                    guid = output.splitlines()[-1].split()[0].replace('-', '').lower()
-                except Exception:
-                    logger.error('Error during GUID extraction. Failing. None of the files will be uploaded.')
-                    return FAILURE
-                list_files.append({'scope': fscope, 'name': os.path.basename(name), 'bytes': size, 'adler32': checksum, 'state': 'C', 'meta': {'guid': guid}})
-            elif args.guid:
-                logger.info('Manually set GUID: %s' % args.guid.replace('-', ''))
-                list_files.append({'scope': fscope, 'name': os.path.basename(name), 'bytes': size, 'adler32': checksum, 'state': 'C', 'meta': {'guid': args.guid.replace('-', '')}})
-            else:
-                logger.debug('Automatically setting new GUID')
-                list_files.append({'scope': fscope, 'name': os.path.basename(name), 'bytes': size, 'adler32': checksum, 'state': 'C', 'meta': {'guid': generate_uuid()}})
-            if not os.path.dirname(name) in lfns:
-                lfns[os.path.dirname(name)] = []
-            lfns[os.path.dirname(name)].append({'name': os.path.basename(name), 'scope': fscope, 'adler32': checksum, 'filesize': size})
-            revert_dict[fscope, os.path.basename(name)] = os.path.dirname(name)
-
-        except OSError, error:
-            logger.error(error)
-            logger.error("No operation will be performed. Exiting!")
+    elif args.expiration_date:
+        expiration_date = datetime.strptime(args.expiration_date, "%Y-%m-%d-%H:%M:%S")
+        if expiration_date < datetime.utcnow():
+            logger.error("The specified expiration date should be in the future!")
             return FAILURE
+        args.lifetime = (expiration_date - datetime.utcnow()).total_seconds()
 
-    rse_settings = rsemgr.get_rse_info(args.rse)
-    if rse_settings['availability_write'] != 1:
-        logger.critical('RSE is not available for write now')
-        return FAILURE
-
-    if args.protocol:
-        try:
-            logger.debug('Forcing protocol : %s' % args.protocol)
-            logger.debug('Forcing RSE : %s' % args.rse)
-            protocol = rsemgr.select_protocol(rse_settings, operation='read', scheme=args.protocol)
-        except RSEProtocolNotSupported, error:
-            logger.error('The protocol specfied (%s) is not supported by %s' % (args.protocol, args.rse))
-            logger.debug(error)
-            return FAILURE
-        rse_settings['protocols'] = [protocol, ]
-
-    if args.account is None:
-        account = client.whoami()['account']
-    else:
-        account = args.account
-    logger.debug('Using account %s' % (account))
-
-    trace['account'] = client.account
-    trace['dataset'] = ''
-    trace['datasetScope'] = ''
-    trace['eventType'] = 'upload'
-    trace['eventVersion'] = version.RUCIO_VERSION[0]
-    if dsscope and dsname:
-        if files_to_list.count({'scope': dsscope, 'name': dsname}) > 0:
-            # There is a file with the name of the destination dataset.
-            logger.error('scope:name for the files must be different from scope:name for the destination dataset. {0}:{1}'.format(dsscope, dsname))
-            return FAILURE
-        try:
-            client.add_dataset(scope=dsscope, name=dsname, rules=[{'account': client.account, 'copies': 1, 'rse_expression': args.rse, 'grouping': 'DATASET', 'lifetime': args.lifetime}])
-            logger.info('Dataset successfully created')
-            trace['dataset'] = dsname
-            trace['datasetScope'] = dsscope
-        except DataIdentifierAlreadyExists:
-            # TODO: Need to check the rules thing!!
-            logger.warning("The dataset name already exist")
-
-    # Adding files to the catalog
-    for f in list_files:
-        try:  # If the did already exist in the catalog, only should be upload if the checksum is the same
-            meta = client.get_metadata(f['scope'], f['name'])
-            replicastate = [rep for rep in client.list_replicas([{'scope': f['scope'], 'name': f['name']}], all_states=True)]
-            if args.rse not in replicastate[0]['rses']:
-                client.add_replicas(files=[f], rse=args.rse)
-            # logger.warning("The file {0}:{1} already exist in the catalog and will not be added.".format(f['scope'], f['name']))
-            if rsemgr.exists(rse_settings=rse_settings, files={'name': f['name'], 'scope': f['scope']}):
-                logger.warning('File {0}:{1} already exists on RSE. Will not try to reupload'.format(f['scope'], f['name']))
-            else:
-                if meta['adler32'] == f['adler32']:
-                    logger.info('Local files and file %s:%s recorded in Rucio have the same checksum. Will try the upload' % (f['scope'], f['name']))
-                    directory = revert_dict[f['scope'], f['name']]
-                    trace['remoteSite'] = rse_settings['rse']
-                    trace['protocol'] = rse_settings['protocols'][0]['scheme']
-                    trace['filesize'] = f['bytes']
-                    trace['transferStart'] = time.time()
-                    rsemgr.upload(rse_settings=rse_settings, lfns=[{'name': f['name'], 'scope': f['scope'], 'adler32': f['adler32'], 'filesize': f['bytes']}], source_dir=directory)
-                    trace['transferEnd'] = time.time()
-                    trace['clientState'] = 'DONE'
-                    logger.info('File %s:%s successfully uploaded on the storage' % (f['scope'], f['name']))
-                    send_trace(trace, client.host, args.user_agent)
-                else:
-                    raise DataIdentifierAlreadyExists
-        except NotImplementedError, error:
-            for proto in rse_settings['protocols']:
-                if proto['domains']['wan']['read'] == 1:
-                    prot = proto['scheme']
-            logger.error('Protocol {0} for RSE {1} not supported!'.format(prot, args.rse))
-            return FAILURE
-        except DataIdentifierNotFound:
-            try:
-                if args.no_register is False:
-                    # Skiping registration for pilot
-                    logger.info('Adding replicas in Rucio catalog')
-                    client.add_replicas(files=[f], rse=args.rse)
-                    logger.info('Replicas successfully added')
-                    if not dsscope:
-                        # only need to add rules for files if no dataset is given
-                        logger.info('Adding replication rule on RSE {0} for the file {1}:{2}'.format(args.rse, f['scope'], f['name']))
-                        client.add_replication_rule([f], copies=1, rse_expression=args.rse, lifetime=args.lifetime)
-                directory = revert_dict[f['scope'], f['name']]
-                trace['remoteSite'] = rse_settings['rse']
-                trace['protocol'] = rse_settings['protocols'][0]['scheme']
-                trace['filesize'] = f['bytes']
-                trace['transferStart'] = time.time()
-                rsemgr.upload(rse_settings=rse_settings, lfns=[{'name': f['name'], 'scope': f['scope'], 'adler32': f['adler32'], 'filesize': f['bytes']}], source_dir=directory)
-                trace['transferEnd'] = time.time()
-                trace['clientState'] = 'DONE'
-                logger.info('File {0}:{1} successfully uploaded on the storage'.format(f['scope'], f['name']))
-                send_trace(trace, client.host, args.user_agent)
-            except (Duplicate, FileAlreadyExists), error:
-                logger.warning(error)
-                return FAILURE
-            except ResourceTemporaryUnavailable, error:
-                logger.error(error)
-                return FAILURE
-        except DataIdentifierAlreadyExists, error:
-            logger.debug(error)
-            logger.error("Some of the files already exist in the catalog. No one will be added.")
-    if dsname:
-        # A dataset is provided. Must add the files to the dataset.
-        for f in list_files:
-            try:
-                client.add_files_to_dataset(scope=dsscope, name=dsname, files=[f])
-            except Exception, error:
-                logger.warning('Failed to attach file {0} to the dataset'.format(f))
-                logger.warning(error)
-                logger.warning("Continuing with the next one")
-
-    replicas = []
-    replica_dictionary = {}
-    for chunk_files_to_list in chunks(files_to_list, 50):
-        for rep in client.list_replicas(chunk_files_to_list):
-            replica_dictionary[rep['scope'], rep['name']] = rep['rses'].keys()
-    for file in list_files:
-        if (file['scope'], file['name']) not in replica_dictionary:
-            file['state'] = 'A'
-            replicas.append(file)
-        elif args.rse not in replica_dictionary[file['scope'], file['name']]:
-            file['state'] = 'A'
-            replicas.append(file)
-    if args.no_register is False and replicas != []:
-        logger.info('Will update the file replicas states')
-        for chunk_replicas in chunks(replicas, 20):
-            try:
-                client.update_replicas_states(rse=args.rse, files=chunk_replicas)
-            except AccessDenied, error:
-                logger.error(error)
-                return FAILURE
-        logger.info('File replicas states successfully updated')
+    dsscope = None
+    dsname = None
+    for arg in args.args:
+        did = arg.split(':')
+        if not dsscope and len(did) == 2:
+            dsscope = did[0]
+            dsname = did[1]
+        elif len(did) == 2:
+            logger.warning('Ignoring input {} because dataset DID is already set {}:{}'.format(arg, dsscope, dsname))
+
+    items = []
+    for arg in args.args:
+        if arg.count(':') > 0:
+            continue
+        if args.pfn:
+            if args.impl:
+                logger.warning('Ignoring --impl option because --pfn option given')
+                args.impl = None
+        items.append({'path': arg,
+                      'rse': args.rse,
+                      'did_scope': args.scope,
+                      'did_name': args.name,
+                      'impl': args.impl,
+                      'dataset_scope': dsscope,
+                      'dataset_name': dsname,
+                      'force_scheme': args.protocol,
+                      'pfn': args.pfn,
+                      'no_register': args.no_register,
+                      'lifetime': args.lifetime,
+                      'register_after_upload': args.register_after_upload,
+                      'transfer_timeout': args.transfer_timeout,
+                      'guid': args.guid,
+                      'recursive': args.recursive})
+
+    if len(items) < 1:
+        raise InputValidationError('No files could be extracted from the given arguments')
+
+    if len(items) > 1 and args.guid:
+        logger.error("A single GUID was specified on the command line, but there are multiple files to upload.")
+        logger.error("If GUID auto-detection is not used, only one file may be uploaded at a time")
+        raise InputValidationError('Invalid input argument composition')
+    if len(items) > 1 and args.name:
+        logger.error("A single LFN was specified on the command line, but there are multiple files to upload.")
+        logger.error("If LFN auto-detection is not used, only one file may be uploaded at a time")
+        raise InputValidationError('Invalid input argument composition')
+
+    if args.recursive and args.pfn:
+        logger.error("It is not possible to create the folder structure into collections with a non-deterministic way.")
+        logger.error("If PFN is specified, you cannot use --recursive")
+        raise InputValidationError('Invalid input argument composition')
+
+    client = get_client(args)
+    from rucio.client.uploadclient import UploadClient
+    upload_client = UploadClient(client, logger=logger)
+    summary_file_path = 'rucio_upload.json' if args.summary else None
+    upload_client.upload(items, summary_file_path)
     return SUCCESS
 
 
-def _downloader2(total_number, scope, name, files, tape_endpoints, is_admin, last_chosen_rse, rse_dict, dest_dir, trace, client, user_agent, exit=False):
-    for file in files:
-        logger.info('Starting the download of %s:%s' % (file['scope'], file['name']))
-        stime = time.time()
-        trace['scope'] = file['scope']
-        trace['filename'] = file['name']
-        if scope == file['scope'] and name == file['name']:
-            trace['dataset'] = ''
-            trace['datasetScope'] = ''
-        else:
-            trace['dataset'] = name
-            trace['datasetScope'] = scope
-        rses = file['rses'].keys()
-        if rses == []:
-            logger.warning('File %s:%s has no available replicas. Cannot be downloaded.' % (file['scope'], file['name']))
-            trace['clientState'] = 'FILE_NOT_FOUND'
-            trace['eventType'] = 'download'
-            trace['eventVersion'] = version.RUCIO_VERSION[0]
-            send_trace(trace, client.host, user_agent)
-        else:
-            trace['filesize'] = file['bytes']
-        trace['eventType'] = 'download'
-        trace['eventVersion'] = version.RUCIO_VERSION[0]
-        rse_to_remove = []
-        for rse in rses:
-            if str(rse) in tape_endpoints:
-                rse_to_remove.append(rse)
-        if len(rses) == len(rse_to_remove):
-            if not is_admin:
-                logger.warning('File %s:%s has no replicas available on disk endpoints and cannot be downloaded. Please ask for a replication' % (file['scope'], file['name']))
-                sys.exit(1)
-            else:
-                logger.warning('File %s:%s has no replicas available on disk endpoints. Will be downloaded from TAPE.' % (file['scope'], file['name']))
-        if last_chosen_rse and last_chosen_rse in rses:
-            idx = rses.index(last_chosen_rse)
-            rses[idx], rses[0] = rses[0], rses[idx]
-        else:
-            random.shuffle(rses)
-        logger.debug('Potential sources : %s' % str(rses))
-        index = 0
-        download_ok = 0
-        for rse in rses:
-            if rse not in rse_dict:
-                rse_dict[rse] = rsemgr.get_rse_info(rse)
-            if args.protocol:
-                try:
-                    protocol = rsemgr.select_protocol(rse_dict[rse], operation='read', scheme=args.protocol)
-                except RSEProtocolNotSupported, error:
-                    logger.error('The protocol specfied (%s) is not supported by %s' % (args.protocol, rse))
-                    logger.debug(error)
-                    return FAILURE
-                rse_dict[rse]['protocols'] = [protocol, ]
-            if rse_dict[rse]['availability_read']:
-                logger.debug('Getting file %s:%s from %s' % (file['scope'], file['name'], rse))
-                trace['remoteSite'] = rse_dict[rse]['rse']
-                trace['protocol'] = rse_dict[rse]['protocols'][0]['scheme']
-                trace['transferStart'] = time.time()
-                trace['clientState'] = 'DOWNLOAD_ATTEMPT'
-                success = False
-                retries = 5
-                for attempt in xrange(0, retries):
-                    try:
-                        logger.info('Trying %s' % rse)
-                        rsemgr.download(rse_dict[rse],
-                                        files=[{'name': file['name'],
-                                                'scope': file['scope'],
-                                                'adler32': file['adler32']}, ],
-                                        dest_dir=dest_dir)
-                        logger.info('File %s:%s successfully downloaded from %s' % (file['scope'],
-                                                                                    file['name'],
-                                                                                    rse))
-                        download_ok = 1
-                        success = True
-                        last_chosen_rse = rse
-                        break
-                    except FileConsistencyMismatch, error:
-                        logger.warning(str(error))
-                        try:
-                            pfns_dict = rsemgr.lfns2pfns(rse_dict[rse],
-                                                         lfns=[{'name': file['name'],
-                                                                'scope': file['scope']}, ],
-                                                         operation='read',
-                                                         scheme=args.protocol)
-                            pfn = pfns_dict['%s:%s' % (file['scope'], file['name'])]
-                            client.declare_suspicious_file_replicas([pfn, ], reason='Corrupted')
-                        except Exception, error:
-                            logger.debug(str(error))
-                        trace['clientState'] = 'FAIL_VALIDATE'
-                        logger.debug('Failed attempt %s/%s' % (attempt + 1, retries))
-                    except Exception, error:
-                        logger.warning(str(error))
-                        trace['clientState'] = str(type(error).__name__)
-                        logger.debug('Failed attempt %s/%s' % (attempt + 1, retries))
-                if success:
-                    trace['clientState'] = 'DONE'
-                trace['transferEnd'] = time.time()
-                send_trace(trace, client.host, args.user_agent)
-                if success:
-                    break
-                index += 1
-                if index != len(rses):
-                    logger.debug('Will retry download on an other RSE')
-        if not download_ok:
-            logger.error('Cannot download file %s:%s' % (file['scope'], file['name']))
-        else:
-            logger.info('File %s:%s successfully downloaded. %s bytes downloaded in %s seconds' % (file['scope'], file['name'], sizefmt(file['bytes'], args.human), time.time() - stime))
-    if exit:
-        sys.exit(0)
-    return
-
-
-def _downloader1(input_queue, output_queue, threadnb, total_threads, tape_endpoints, is_admin, trace_endpoint, trace_pattern, user_agent):
-    last_chosen_rse = None
-    rse_dict = {}
-    try:
-        while(True):
-            trace = deepcopy(trace_pattern)
-            stime = time.time()
-            file = input_queue.get_nowait()
-            logger.info('Thread %s/%s : Starting the download of %s:%s' % (threadnb, total_threads, file['scope'], file['name']))
-            trace.update({'scope': file['scope'], 'name': file['name'], 'datasetScope': file['datasetScope'], 'dataset': file['datasetName'], 'eventType': 'download', 'eventVersion': version.RUCIO_VERSION[0], 'filesize': file['bytes']})
-            rses = file['rses'].keys()
-            dest_dir = file['dest_dir']
-            if rses == []:
-                logger.warning('Thread %s/%s : File %s:%s has no available replicas. Cannot be downloaded.' % (threadnb, total_threads, file['scope'], file['name']))
-                trace['clientState'] = 'FILE_NOT_FOUND'
-                send_trace(trace, trace_endpoint, args.user_agent)
-            rse_to_remove = []
-            for rse in rses:
-                if str(rse) in tape_endpoints:
-                    rse_to_remove.append(rse)
-            if len(rse_to_remove) > 0:
-                logger.debug('Thread %s/%s : Excluding TAPE endpoints: %s' % (threadnb, total_threads, rse_to_remove))
-            if len(rses) == len(rse_to_remove):
-                if not is_admin:
-                    logger.warning('Thread %s/%s : File %s:%s has no replicas available on disk endpoints and cannot be downloaded. Go to https://rucio-ui.cern.ch/ and request a replication.' % (threadnb, total_threads, file['scope'], file['name']))
-                    input_queue.task_done()
-                    return
-                else:
-                    logger.warning('Thread %s/%s : File %s:%s has no replicas available on disk endpoints. Admin override: Downloaded from TAPE enabled.' % (threadnb, total_threads, file['scope'], file['name']))
-            if last_chosen_rse and last_chosen_rse in rses:
-                idx = rses.index(last_chosen_rse)
-                rses[idx], rses[0] = rses[0], rses[idx]
-            else:
-                random.shuffle(rses)
-            logger.debug('Thread %s/%s : Potential sources : %s' % (threadnb, total_threads, str(rses)))
-            index = 0
-            download_ok = 0
-            for rse in rses:
-                if rse not in rse_dict:
-                    rse_dict[rse] = rsemgr.get_rse_info(rse)
-                if args.protocol:
-                    try:
-                        protocol = rsemgr.select_protocol(rse_dict[rse], operation='read', scheme=args.protocol)
-                    except RSEProtocolNotSupported, error:
-                        logger.error('Thread %s/%s : The protocol specfied (%s) is not supported by %s' % (threadnb, total_threads, args.protocol, rse))
-                        logger.debug(error)
-                        return FAILURE
-                    rse_dict[rse]['protocols'] = [protocol, ]
-                if rse_dict[rse]['availability_read']:
-                    trace.update({'remoteSite': rse_dict[rse]['rse'], 'protocol': rse_dict[rse]['protocols'][0]['scheme'], 'transferStart': time.time(), 'clientState': 'DOWNLOAD_ATTEMPT'})
-                    success = False
-                    retries = 5
-                    for attempt in xrange(0, retries):
-                        try:
-                            logger.info('Thread %s/%s : File %s:%s trying from %s' % (threadnb, total_threads, file['scope'], file['name'], rse))
-                            if args.pfn:
-                                rsemgr.download(rse_dict[rse],
-                                                files=[{'name': file['name'],
-                                                        'scope': file['scope'],
-                                                        'adler32': file['adler32'],
-                                                        'pfn': args.pfn}, ],
-                                                dest_dir=dest_dir,
-                                                force_scheme=args.pfn.split(':')[0])
-                            else:
-                                rsemgr.download(rse_dict[rse],
-                                                files=[{'name': file['name'],
-                                                        'scope': file['scope'],
-                                                        'adler32': file['adler32']}, ],
-                                                dest_dir=dest_dir)
-                            logger.info('Thread %s/%s : File %s:%s successfully downloaded from %s' % (threadnb,
-                                                                                                       total_threads,
-                                                                                                       file['scope'],
-                                                                                                       file['name'],
-                                                                                                       rse))
-                            download_ok = 1
-                            success = True
-                            last_chosen_rse = rse
-                            break
-                        except KeyboardInterrupt:
-                            logger.warning('You pressed Ctrl+C! Exiting gracefully')
-                            os.kill(os.getpgid(), signal.SIGINT)
-                            break
-                        except FileConsistencyMismatch, error:
-                            logger.warning(str(error))
-                            try:
-                                pfns_dict = rsemgr.lfns2pfns(rse_dict[rse], lfns=[{'name': file['name'], 'scope': file['scope']}, ], operation='read', scheme=args.protocol)
-                                pfn = pfns_dict['%s:%s' % (file['scope'], file['name'])]
-                                output_queue.put({'dataset_scope': file['datasetScope'], 'dataset_name': file['datasetName'], 'scope': file['scope'], 'name': file['name'], 'clientState': 'CORRUPTED', 'attemptnr': attempt + 1, 'pfn': pfn})
-                            except Exception, error:
-                                logger.debug('Thread %s/%s : %s' % (threadnb, total_threads, str(error)))
-                            trace['clientState'] = 'FAIL_VALIDATE'
-                            logger.debug('Thread %s/%s : Failed attempt %s/%s' % (threadnb, total_threads, attempt + 1, retries))
-                        except Exception, error:
-                            logger.warning(str(error))
-                            trace['clientState'] = str(type(error).__name__)
-                            logger.debug('Thread %s/%s : Failed attempt %s/%s' % (threadnb, total_threads, attempt + 1, retries))
-
-                    if success:
-                        trace['clientState'] = 'DONE'
-                        output_queue.put({'dataset_scope': file['datasetScope'], 'dataset_name': file['datasetName'], 'scope': file['scope'], 'name': file['name'], 'clientState': 'DONE', 'attemptnr': attempt + 1})
-                    trace['transferEnd'] = time.time()
-
-                    send_trace(trace, trace_endpoint, user_agent, threadnb=threadnb, total_threads=total_threads)
-
-                    if success:
-                        break
-                    index += 1
-
-                    if index != len(rses):
-                        logger.debug('Thread %s/%s : Will retry download on an other RSE' % (threadnb, total_threads))
-                else:
-                    logger.info('Thread %s/%s : %s is blacklisted for reading' % (threadnb, total_threads, rse))
-
-            if not download_ok:
-                logger.error('Thread %s/%s : Cannot download file %s:%s' % (threadnb, total_threads, file['scope'], file['name']))
-            else:
-                duration = round(time.time() - stime, 2)
-                logger.info('Thread %s/%s : File %s:%s successfully downloaded. %s in %s seconds = %s MBps' % (threadnb, total_threads,
-                                                                                                               file['scope'], file['name'],
-                                                                                                               sizefmt(file['bytes'], args.human),
-                                                                                                               duration,
-                                                                                                               round((file['bytes'] / duration) * 1e-6, 2)))
-
-            input_queue.task_done()
-
-    except Empty:
-        return
-
-
-def _file_exists(type, scope, name, directory, dsn=None, no_subdir=False):
-    file_exists = False
-    dest_dir = None
-    if no_subdir:
-        dest_dir = '%s' % (directory)
-    else:
-        if type != 'FILE':
-            dest_dir = '%s/%s' % (directory, dsn)
-            if os.path.isfile('%s/%s' % (dest_dir, name)):
-                file_exists = True
-        else:
-            dest_dir = '%s/%s' % (directory, scope)
-            if os.path.isfile('%s/%s' % (dest_dir, name)):
-                file_exists = True
-    return file_exists, dest_dir
-
-
+@exception_handler
 def download(args):
     """
     %(prog)s download [options] <field1=value1 field2=value2 ...>
 
-    Download files from Rucio
+    Download files from Rucio using new threaded model and RSE expression support
     """
-    if args.old:
-        logger.debug('Using old multi-process download model')
-        return download2(args)
+    # Input validation
+    if not args.dids and not args.filter and not args.metalink_file:
+        logger.error('At least one did is mandatory')
+        return FAILURE
+    elif not args.dids and args.filter and not args.scope:
+        logger.error('The argument scope is mandatory')
+        return FAILURE
 
-    return download1(args)
+    if args.filter and args.metalink_file:
+        logger.error('Arguments filter and metalink cannot be used together.')
+        return FAILURE
 
+    if args.dids and args.metalink_file:
+        logger.error('Arguments dids and metalink cannot be used together.')
+        return FAILURE
 
-def download1(args):
-    """
-    %(prog)s download [options] <field1=value1 field2=value2 ...>
+    if args.ignore_checksum and args.check_local_with_filesize_only:
+        logger.error('Arguments ignore-checksum and check-local-with-filesize-only cannot be used together.')
+        return FAILURE
 
-    Download files from Rucio using new threaded model and RSE expression support
-    """
-    client = get_client(args)
-    trace_endpoint = client.host
-    summary = {}
-    trace_pattern = {'hostname': socket.getfqdn(), 'account': client.account, 'uuid': generate_uuid(), 'eventType': 'download', 'eventVersion': version.RUCIO_VERSION[0]}
-    nbfiles_to_download = {}
+    trace_pattern = {}
 
-    tape_endpoints = []
-    try:
-        tape_endpoints = [str(rse['rse']) for rse in client.list_rses('rse_type=TAPE')]
-    except InvalidRSEExpression, error:
-        logger.warning(error)
-
-    account_attributes = [acc for acc in client.list_account_attributes(client.account)]
-    is_admin = False
-    for attr in account_attributes[0]:
-        if attr['key'] == 'admin' and attr['value'] is True:
-            logger.debug('Admin mode enabled')
-            is_admin = True
-            break
+    if args.trace_appid:
+        trace_pattern['appid'] = args.trace_appid
+    if args.trace_dataset:
+        trace_pattern['dataset'] = args.trace_dataset
+    if args.trace_datasetscope:
+        trace_pattern['datasetScope'] = args.trace_datasetscope
+    if args.trace_eventtype:
+        trace_pattern['eventType'] = args.trace_eventtype
+    if args.trace_pq:
+        trace_pattern['pq'] = args.trace_pq
+    if args.trace_taskid:
+        trace_pattern['taskid'] = args.trace_taskid
+    if args.trace_usrdn:
+        trace_pattern['usrdn'] = args.trace_usrdn
+    deactivate_file_download_exceptions = args.deactivate_file_download_exceptions if args.deactivate_file_download_exceptions is not None else False
+
+    client = get_client(args)
+    from rucio.client.downloadclient import DownloadClient
+    download_client = DownloadClient(client=client, logger=logger, check_admin=args.allow_tape)
+
+    result = None
+    item_defaults = {}
+    item_defaults['rse'] = args.rses
+    item_defaults['base_dir'] = args.dir
+    item_defaults['no_subdir'] = args.no_subdir
+    item_defaults['transfer_timeout'] = args.transfer_timeout
+    item_defaults['no_resolve_archives'] = args.no_resolve_archives
+    item_defaults['ignore_checksum'] = args.ignore_checksum
+    item_defaults['check_local_with_filesize_only'] = args.check_local_with_filesize_only
+    archive_did = args.archive_did
+    if archive_did:
+        logger.warning("Archives are treated transparently. --archive-did option is being obsoleted.")  # TODO
 
-    # Extract the scope, name from the did(s)
-    dids = []
-    for did in args.dids:
+    # Get filters
+    filters = {}
+    type_ = 'all'
+    if args.filter:
         try:
-            scope, name = extract_scope(did)
-            if name.find('*') > -1:
-                for dsn in client.list_dids(scope, filters={'name': name}):
-                    dids.append((scope, dsn))
-            else:
-                dids.append((scope, name))
-        except ValueError, error:
-            logger.error('ERROR cannot extract the scope and name from %s : [%s]' % (did, error))
+            filters, type_ = parse_did_filter_from_string(args.filter)
+            if args.scope:
+                filters['scope'] = args.scope
+        except InvalidType as error:
+            logger.error(error)
             return FAILURE
-
-    total_workers = 1
-    if args.ndownloader:
-        total_workers = args.ndownloader
-        nlimit = 5
-        if total_workers > nlimit:
-            logger.warning('Cannot use more than %s parallel downloader.' % nlimit)
-            total_workers = nlimit
-
-    input_queue = Queue()
-    output_queue = Queue()
-    for scope, name in dids:
-        try:
-            summary['%s:%s' % (scope, name)] = {}
-            logger.debug('Checking validity of DID')
-            did_info = client.get_did(scope, name)
-            did_type = did_info['type']
-            dataset_scope = '' if did_type == 'FILE' else scope
-            dataset_name = '' if did_type == 'FILE' else name
-
-            logger.debug('Getting the list of replicas from Rucio servers')
-            replicas = [f for f in client.list_replicas([{'scope': scope, 'name': name}],
-                                                        rse_expression=args.rse)]
-
-            if args.nrandom:
-                files_to_download = replicas
-                random.shuffle(files_to_download)
-                files_to_download = files_to_download[0:args.nrandom]
-            else:
-                files_to_download = replicas
-            nbfiles_to_download['%s:%s' % (scope, name)] = len(files_to_download)
-
-            logger.info('Starting download for %s:%s with %s files' % (scope, name, len(files_to_download)))
-            for file in files_to_download:
-                file_download = file
-                dest_dir = None
-                file_exists, dest_dir = _file_exists(did_type, file['scope'], file['name'], args.dir, dsn=name, no_subdir=args.no_subdir)
-                if file_exists:
-                    logger.info('File %s:%s already exists locally' % (file['scope'], file['name']))
-                    summary['%s:%s' % (scope, name)]['%s:%s' % (file['scope'], file['name'])] = 2
-                    trace = deepcopy(trace_pattern)
-                    # Filling and sending the trace
-                    trace.update({'scope': file['scope'], 'filename': file['name'], 'datasetScope': dataset_scope, 'datasetName': dataset_name,
-                                 'filesize': file['bytes'], 'transferStart': time.time(), 'transferEnd': time.time(), 'clientState': 'ALREADY_DONE'})
-                    send_trace(trace, trace_endpoint, args.user_agent)
-                    output_queue.put({'dataset_scope': dataset_scope, 'dataset_name': dataset_name, 'scope': file['scope'], 'name': file['name'], 'clientState': 'ALREADY_DONE'})
-                else:
-                    logger.debug('Queueing file %s:%s for download' % (file['scope'], file['name']))
-                    if not os.path.isdir(dest_dir):
-                        os.mkdir(dest_dir)
-                    if args.no_subdir is True and os.path.isfile('%s/%s' % (dest_dir, file['name'])):
-                        # Overwrite the files
-                        os.remove("%s/%s" % (dest_dir, file['name']))
-                    file_download['datasetScope'] = dataset_scope
-                    file_download['datasetName'] = dataset_name
-                    file_download['dest_dir'] = dest_dir
-                    input_queue.put(file_download)
-
-        except Exception, error:
-            logger.error('Failed to download %(scope)s:%(name)s' % locals())
+        except ValueError as error:
             logger.error(error)
+            return FAILURE
+        except Exception as error:
+            logger.error(error)
+            logger.error("Invalid Filter. Filter must be 'key=value', 'key>=value', 'key>value', 'key<=value', 'key<value'")
+            return FAILURE
+        item_defaults['filters'] = filters
 
-    threads = []
-    for worker in range(total_workers):
-        thread = Thread(target=_downloader1, kwargs={'input_queue': input_queue, 'output_queue': output_queue, 'threadnb': worker + 1, 'total_threads': total_workers,
-                                                     'tape_endpoints': tape_endpoints, 'is_admin': is_admin, 'trace_endpoint': trace_endpoint, 'trace_pattern': trace_pattern,
-                                                     'user_agent': args.user_agent})
-        thread.start()
-        threads.append(thread)
-
-    try:
-        logger.debug('Waiting for threads to finish')
-        input_queue.join()
-    except KeyboardInterrupt:
-        logger.warning('You pressed Ctrl+C! Exiting gracefully')
-        graceful_stop = True
-        for thread in threads:
-            thread.kill_received = True
-    logger.debug('All threads finished')
+    if not args.pfn:
+        item_defaults['impl'] = args.impl
+        item_defaults['force_scheme'] = args.protocol
+        item_defaults['nrandom'] = args.nrandom
+        item_defaults['transfer_speed_timeout'] = args.transfer_speed_timeout \
+            if args.transfer_speed_timeout is not None \
+            else config_get_float('download', 'transfer_speed_timeout', False, 500)
+        items = []
+        if args.dids:
+            for did in args.dids:
+                item = {'did': did}
+                item.update(item_defaults)
+                items.append(item)
+        else:
+            items.append(item_defaults)
 
-    while(True):
-        try:
-            item = output_queue.get_nowait()
-            output_queue.task_done()
-            if '%s:%s' % (item['dataset_scope'], item['dataset_name']) in summary or '%s:%s' % (item['scope'], item['name']) in summary:
-                if item['dataset_scope'] == '':
-                    summary['%s:%s' % (item['scope'], item['name'])]['%s:%s' % (item['scope'], item['name'])] = item['clientState']
-                else:
-                    summary['%s:%s' % (item['dataset_scope'], item['dataset_name'])]['%s:%s' % (item['scope'], item['name'])] = item['clientState']
-                if item['clientState'] == 'CORRUPTED':
-                    client.declare_suspicious_file_replicas([item['pfn'], ], reason='Corrupted')
-        except Empty:
-            break
-
-    not_downloaded_files = 0
-    print '----------------------------------'
-    print 'Download summary'
-    if summary:
-        for did in summary:
-            print '-' * 40
-            print 'DID %s' % (did)
-            downloaded_files = 0
-            not_downloaded_files = 0
-            local_files = 0
-            for file in summary[did]:
-                if summary[did][file] == 'DONE':
-                    downloaded_files += 1
-                elif summary[did][file] == 'ALREADY_DONE':
-                    local_files += 1
-            not_downloaded_files = nbfiles_to_download[did] - downloaded_files - local_files
-            print '{0:40} {1:6d}'.format('Total files : ', nbfiles_to_download[did])
-            print '{0:40} {1:6d}'.format('Downloaded files : ', downloaded_files)
-            print '{0:40} {1:6d}'.format('Files already found locally : ', local_files)
-            print '{0:40} {1:6d}'.format('Files that cannot be downloaded : ', not_downloaded_files)
+        if args.aria:
+            result = download_client.download_aria2c(items, trace_pattern, deactivate_file_download_exceptions=deactivate_file_download_exceptions, sort=args.sort)
+        elif args.metalink_file:
+            result = download_client.download_from_metalink_file(items[0], args.metalink_file, deactivate_file_download_exceptions=deactivate_file_download_exceptions)
+            if args.sort:
+                logger.warning('Ignoring --replica-selection option because --metalink option given')
+        else:
+            result = download_client.download_dids(items, args.ndownloader, trace_pattern, deactivate_file_download_exceptions=deactivate_file_download_exceptions, sort=args.sort)
     else:
-        print '-' * 40
-        print 'No DID matching the pattern'
-    return not_downloaded_files
-
+        if args.aria:
+            logger.warning('Ignoring --aria option because --pfn option given')
+        if args.impl:
+            logger.warning('Ignoring --impl option because --pfn option given')
+        if args.protocol:
+            logger.warning('Ignoring --protocol option because --pfn option given')
+        if args.transfer_speed_timeout:
+            logger.warning("Download with --pfn doesn't support --transfer-speed-timeout")
+        num_dids = len(args.dids)
+        did_str = args.dids[0]
+        if num_dids > 1:
+            logger.warning('Download with --pfn option only supports one DID but {} DIDs were given. Considering only first DID: {}'.format(num_dids, did_str))
+            logger.debug(args.dids)
+        item_defaults['pfn'] = args.pfn
+        item_defaults['did'] = did_str
+        result = download_client.download_pfns([item_defaults], 1, trace_pattern, deactivate_file_download_exceptions=deactivate_file_download_exceptions)
 
-def download2(args):
-    """
-    %(prog)s download [options] <field1=value1 field2=value2 ...>
+    if not result:
+        raise RucioException('Download API failed')
 
-    Download files from Rucio, the ancient way with RSE expression support
-    """
-    client = get_client(args)
-    rse_dict = {}
     summary = {}
-    trace = {}
-    trace_uuid = generate_uuid()
-    trace['hostname'] = socket.getfqdn()
-    trace['account'] = client.account
-    trace['uuid'] = trace_uuid
-    nbfiles_to_download = {}
-
-    try:
-        tape_endpoints = [str(rse['rse']) for rse in client.list_rses('rse_type=TAPE')]
-    except InvalidRSEExpression, error:
-        logger.warning(error)
-        tape_endpoints = []
-
-    account_attributes = [acc for acc in client.list_account_attributes(client.account)]
-    is_admin = False
-    for attr in account_attributes[0]:
-        if attr['key'] == 'admin' and attr['value'] is True:
-            is_admin = True
-            break
-
-    dids = []
-    for did in args.dids:
-        try:
-            scope, name = extract_scope(did)
-            if name.find('*') > -1:
-                for dsn in client.list_dids(scope, filters={'name': name}):
-                    dids.append((scope, dsn))
-            else:
-                dids.append((scope, name))
-        except ValueError, error:
-            logger.error('ERROR cannot extract the scope and name from %s : [%s]' % (did, error))
-            return FAILURE
-
-    for scope, name in dids:
-        try:
-            did_info = client.get_did(scope, name)
-            files_to_download = [f for f in client.list_files(scope=scope, name=name)]
-            if args.nrandom:
-                files_to_download = files_to_download[0:args.nrandom]
-            nbfiles_to_download['%s:%s' % (scope, name)] = len(files_to_download)
-            logger.info('Starting download for %s:%s with %s files' % (scope, name, len(files_to_download)))
-            summary['%s:%s' % (scope, name)] = {}
-            logger.debug('Getting the list of replicas')
-            files = client.list_replicas([{'scope': scope, 'name': name}],
-                                         rse_expression=args.rse)
-
-            if args.nrandom:
-                files_to_download = [f for f in files]
-                random.shuffle(files_to_download)
-                files_to_download = files_to_download[0:args.nrandom]
-            else:
-                files_to_download = files
-            last_chosen_rse = None
-            dict_files_to_download = {}
-            dict_files_to_download[0] = []
-            total_workers = 1
-            if args.ndownloader:
-                total_workers = args.ndownloader
-                nlimit = 5
-                if total_workers > nlimit:
-                    logger.warning('Cannot use more than %s parallel downloader.' % nlimit)
-                    total_workers = nlimit
-                for i in xrange(total_workers):
-                    dict_files_to_download[i] = []
-            for file in files_to_download:
-                dest_dir = None
-                file_exists, dest_dir = _file_exists(did_info['type'], file['scope'], file['name'], args.dir, dsn=name, no_subdir=args.no_subdir)
-                if file_exists:
-                    logger.info('File %s:%s already exists locally' % (file['scope'], file['name']))
-                    summary['%s:%s' % (scope, name)]['%s:%s' % (file['scope'], file['name'])] = 2
-                    # Filling and sending the trace
-                    trace['scope'] = file['scope']
-                    trace['filename'] = file['name']
-                    if scope == file['scope'] and name == file['name']:
-                        trace['dataset'] = ''
-                        trace['datasetScope'] = ''
-                    else:
-                        trace['dataset'] = name
-                        trace['datasetScope'] = scope
-                    trace['filesize'] = file['bytes']
-                    trace['eventType'] = 'download'
-                    trace['eventVersion'] = version.RUCIO_VERSION[0]
-                    trace['transferStart'] = time.time()
-                    trace['transferEnd'] = time.time()
-                    trace['clientState'] = 'ALREADY_DONE'
-                    for dummy in xrange(5):
-                        try:
-                            requests.post(client.host + '/traces/', verify=False, data=json.dumps(trace))
-                            break
-                        except Exception:
-                            pass
-                else:
-                    logger.debug('Will start downloading file %s:%s' % (file['scope'], file['name']))
-                    if not os.path.isdir(dest_dir):
-                        os.mkdir(dest_dir)
-                    if args.no_subdir is True and os.path.isfile('%s/%s' % (dest_dir, file['name'])):
-                        # Overwrite the files
-                        os.remove("%s/%s" % (dest_dir, file['name']))
-                    hash_md5 = hashlib.md5()
-                    hash_md5.update('%s:%s' % (file['scope'], file['name']))
-                    worker_number = int(hash_md5.hexdigest(), 16) % total_workers
-                    dict_files_to_download[worker_number].append(file)
-
-            if total_workers == 1 and dict_files_to_download:
-                _downloader2(total_workers, scope, name, dict_files_to_download[0],
-                             tape_endpoints, is_admin, last_chosen_rse,
-                             rse_dict, dest_dir, trace, client, args.user_agent, exit=False)
-            else:
-                pool_list = []
-                for worker_number in dict_files_to_download:
-                    proc = Process(target=_downloader2, args=(total_workers, scope, name,
-                                   dict_files_to_download[worker_number], tape_endpoints,
-                                   is_admin, last_chosen_rse, rse_dict, dest_dir, trace,
-                                   client, args.user_agent))
-                    proc.start()
-                    pool_list.append(proc)
-                active_workers = pool_list
-
-                while active_workers:
-                    time.sleep(2)
-                    active_workers = [worker for worker in pool_list if worker.is_alive()]
-
-            for worker in dict_files_to_download:
-                for file in dict_files_to_download[worker]:
-                    file_exists, dest_dir = _file_exists(did_info['type'], file['scope'], file['name'], args.dir, dsn=name, no_subdir=args.no_subdir)
-                    # If no_subdir is set, "Files already found locally" should be 0, truly existing files will be overwriten...
-                    if file_exists:
-                        summary['%s:%s' % (scope, name)]['%s:%s' % (file['scope'], file['name'])] = 1
-                    elif os.path.isfile('%s/%s' % (dest_dir, file['name'])):
-                        summary['%s:%s' % (scope, name)]['%s:%s' % (file['scope'], file['name'])] = 1
-
-        except DataIdentifierAlreadyExists, error:
-            logger.error(error)
-            return FAILURE
-        except Exception, error:
-            logger.error('Failed to download %(scope)s:%(name)s' % locals())
-            logger.error(error)
-
-    logger.info('Download operation for %s:%s done' % (scope, name))
-    not_downloaded_files = 0
-    print '----------------------------------'
-    print 'Download summary'
-    if summary:
-        for did in summary:
-            print '-' * 40
-            print 'DID %s' % (did)
-            downloaded_files = 0
-            not_downloaded_files = 0
-            local_files = 0
-            for file in summary[did]:
-                if summary[did][file] == 1:
-                    downloaded_files += 1
-                elif summary[did][file] == 2:
-                    local_files += 1
-            not_downloaded_files = nbfiles_to_download[did] - downloaded_files - local_files
-            print '{0:40} {1:6d}'.format('Total files : ', nbfiles_to_download[did])
-            print '{0:40} {1:6d}'.format('Downloaded files : ', downloaded_files)
-            print '{0:40} {1:6d}'.format('Files already found locally : ', local_files)
-            print '{0:40} {1:6d}'.format('Files that cannot be downloaded : ', not_downloaded_files)
-    else:
-        print '-' * 40
-        print 'No DID matching the pattern'
+    for item in result:
+        for did, did_stats in item.get('input_dids', {}).items():
+            did_summary = summary.setdefault(did, {'length': did_stats.get('length'), 'DONE': 0, 'ALREADY_DONE': 0, '_total': 0})
+            did_summary['_total'] += 1
+            state = item['clientState'].upper()
+            if state in did_summary:
+                did_summary[state] += 1
+
+    print('----------------------------------')
+    print('Download summary')
+    if not len(summary):
+        print('-' * 40)
+        print('No DID matching the pattern')
+
+    for summary_key, did_summary in summary.items():
+        print('-' * 40)
+        print('DID %s' % summary_key)
+        length = did_summary['length']
+        ds_total = did_summary['_total']
+        downloaded_files = did_summary['DONE']
+        local_files = did_summary['ALREADY_DONE']
+        not_downloaded_files = ds_total - downloaded_files - local_files
+
+        if length:
+            print('{0:40} {1:6d}'.format('Total files (DID): ', length))
+            print('{0:40} {1:6d}'.format('Total files (filtered):   ', ds_total))
+        else:
+            print('{0:40} {1:6d}'.format('Total files:   ', ds_total))
+        print('{0:40} {1:6d}'.format('Downloaded files: ', downloaded_files))
+        print('{0:40} {1:6d}'.format('Files already found locally: ', local_files))
+        print('{0:40} {1:6d}'.format('Files that cannot be downloaded: ', not_downloaded_files))
 
-    return not_downloaded_files
+    return SUCCESS
 
 
 @exception_handler
 def get_metadata(args):
     """
     %(prog)s get_metadata [options] <field1=value1 field2=value2 ...>
 
     Get data identifier metadata
     """
     client = get_client(args)
-    for did in args.dids:
-        scope, name = extract_scope(did)
-        meta = client.get_metadata(scope=scope, name=name)
-        for k in meta:
-            print '%s: %s' % (k, meta[k])
+    if args.plugin:
+        plugin = args.plugin
+    else:
+        plugin = config_get('client', 'metadata_default_plugin', default='DID_COLUMN')
+
+    for i, did in enumerate(args.dids):
+        if i > 0:
+            print('------')
+        scope, name = get_scope(did, client)
+        meta = client.get_metadata(scope=scope, name=name, plugin=plugin)
+        table = [(k + ':', str(v)) for (k, v) in sorted(meta.items())]
+        print(tabulate(table, tablefmt='plain', disable_numparse=True))
     return SUCCESS
 
 
 @exception_handler
 def set_metadata(args):
     """
     %(prog)s set_metadata [options] <field1=value1 field2=value2 ...>
 
     Set data identifier metadata
     """
     client = get_client(args)
     value = args.value
     if args.key == 'lifetime':
         value = float(args.value)
-    scope, name = extract_scope(args.did)
+    scope, name = get_scope(args.did, client)
     client.set_metadata(scope=scope, name=name, key=args.key, value=value)
     return SUCCESS
 
 
+@exception_handler
 def delete_metadata(args):
     """
     %(prog)s set_metadata [options] <field1=value1 field2=value2 ...>
 
     Delete data identifier metadata
     """
-    # For the moment..
-    raise NotImplementedError
+    client = get_client(args)
+    scope, name = get_scope(args.did, client)
+    client.delete_metadata(scope=scope, name=name, key=args.key)
+    return SUCCESS
 
 
 @exception_handler
 def add_rule(args):
     """
     %(prog)s add-rule <did> <copies> <rse-expression> [options]
 
     Add a rule to a did.
     """
     client = get_client(args)
     dids = []
+    rule_ids = []
     for did in args.dids:
-        scope, name = extract_scope(did)
+        scope, name = get_scope(did, client)
         dids.append({'scope': scope, 'name': name})
-    rule_ids = client.add_replication_rule(dids=dids,
-                                           copies=args.copies,
-                                           rse_expression=args.rse_expression,
-                                           weight=args.weight,
-                                           lifetime=args.lifetime,
-                                           grouping=args.grouping,
-                                           account=args.rule_account,
-                                           locked=args.locked,
-                                           source_replica_expression=args.source_replica_expression,
-                                           notify=args.notify,
-                                           activity=args.activity,
-                                           comment=args.comment,
-                                           ask_approval=args.ask_approval,
-                                           asynchronous=args.asynchronous)
+    try:
+        rule_ids = client.add_replication_rule(dids=dids,
+                                               copies=args.copies,
+                                               rse_expression=args.rse_expression,
+                                               weight=args.weight,
+                                               lifetime=args.lifetime,
+                                               grouping=args.grouping,
+                                               account=args.rule_account,
+                                               locked=args.locked,
+                                               source_replica_expression=args.source_replica_expression,
+                                               notify=args.notify,
+                                               activity=args.activity,
+                                               comment=args.comment,
+                                               ask_approval=args.ask_approval,
+                                               asynchronous=args.asynchronous,
+                                               delay_injection=args.delay_injection)
+    except DuplicateRule as error:
+        if args.ignore_duplicate:
+            for did in dids:
+                try:
+                    rule_id = client.add_replication_rule(dids=[did],
+                                                          copies=args.copies,
+                                                          rse_expression=args.rse_expression,
+                                                          weight=args.weight,
+                                                          lifetime=args.lifetime,
+                                                          grouping=args.grouping,
+                                                          account=args.rule_account,
+                                                          locked=args.locked,
+                                                          source_replica_expression=args.source_replica_expression,
+                                                          notify=args.notify,
+                                                          activity=args.activity,
+                                                          comment=args.comment,
+                                                          ask_approval=args.ask_approval,
+                                                          asynchronous=args.asynchronous,
+                                                          delay_injection=args.delay_injection)
+                    rule_ids.extend(rule_id)
+                except DuplicateRule:
+                    print('Duplicate rule for %s:%s found; Skipping.' % (did['scope'], did['name']))
+        else:
+            raise error
+
     for rule in rule_ids:
-        print rule
+        print(rule)
     return SUCCESS
 
 
 @exception_handler
 def delete_rule(args):
     """
     %(prog)s delete-rule [options] <ruleid>
 
     Delete a rule.
     """
     client = get_client(args)
+
     try:
         # Test if the rule_id is a real rule_id
         uuid.UUID(args.rule_id)
         client.delete_replication_rule(rule_id=args.rule_id, purge_replicas=args.purge_replicas)
     except ValueError:
         # Otherwise, trying to extract the scope, name from args.rule_id
-        if not args.rse_expression:
+        if not args.rses:
             logger.error('A RSE expression must be specified if you do not provide a rule_id but a DID')
             return FAILURE
-        scope, name = extract_scope(args.rule_id)
+        scope, name = get_scope(args.rule_id, client)
         rules = client.list_did_rules(scope=scope, name=name)
         if args.rule_account is None:
             account = client.account
         else:
             account = args.rule_account
         deletion_success = False
         for rule in rules:
             if args.delete_all:
                 account_checked = True
             else:
                 account_checked = rule['account'] == account
-            if rule['rse_expression'] == args.rse_expression and account_checked:
+            if rule['rse_expression'] == args.rses and account_checked:
                 client.delete_replication_rule(rule_id=rule['id'], purge_replicas=args.purge_replicas)
                 deletion_success = True
         if not deletion_success:
             logger.error('No replication rule was deleted from the DID')
             return FAILURE
     return SUCCESS
 
@@ -1762,284 +1304,389 @@
     Update a rule.
     """
     client = get_client(args)
     options = {}
     if args.lifetime:
         options['lifetime'] = None if args.lifetime.lower() == "none" else int(args.lifetime)
     if args.locked:
-        if args.locked == "True":
+        if args.locked.title() == "True":
             options['locked'] = True
-        elif args.locked == "False":
+        elif args.locked.title() == "False":
             options['locked'] = False
+        else:
+            logger.error('Locked must be True or False')
+            return FAILURE
+    if args.comment:
+        options['comment'] = args.comment
     if args.rule_account:
         options['account'] = args.rule_account
     if args.state_stuck:
         options['state'] = 'STUCK'
     if args.state_suspended:
         options['state'] = 'SUSPENDED'
     if args.rule_activity:
         options['activity'] = args.rule_activity
     if args.source_replica_expression:
         options['source_replica_expression'] = None if args.source_replica_expression.lower() == 'none' else args.source_replica_expression
     if args.cancel_requests:
+        if 'state' not in options:
+            logger.error('--stuck or --suspend must be specified when running --cancel-requests')
+            return FAILURE
         options['cancel_requests'] = True
     if args.priority:
         options['priority'] = int(args.priority)
     if args.child_rule_id:
-        options['child_rule_id'] = args.child_rule_id
+        if args.child_rule_id.lower() == 'none':
+            options['child_rule_id'] = None
+        else:
+            options['child_rule_id'] = args.child_rule_id
+    if args.boost_rule:
+        options['boost_rule'] = args.boost_rule
     client.update_replication_rule(rule_id=args.rule_id, options=options)
-    print 'Updated Rule'
+    print('Updated Rule')
+    return SUCCESS
+
+
+@exception_handler
+def move_rule(args):
+    """
+    %(prog)s move-rule [options] <ruleid> <rse_expression>
+
+    Update a rule.
+    """
+    client = get_client(args)
+
+    override = {}
+    if args.activity:
+        override['activity'] = args.activity
+    if args.source_replica_expression:
+        override['source_replica_expression'] = None if args.source_replica_expression.lower() == "none" else args.source_replica_expression
+
+    print(client.move_replication_rule(rule_id=args.rule_id,
+                                       rse_expression=args.rse_expression,
+                                       override=override))
     return SUCCESS
 
 
 @exception_handler
 def info_rule(args):
     """
     %(prog)s rule-info [options] <ruleid>
 
     Retrieve information about a rule.
     """
+    if args.estimate_ttc:
+        logger.error('"--estimate-ttc" is deprecated!')
+        return FAILURE
     client = get_client(args)
     if args.examine:
         analysis = client.examine_replication_rule(rule_id=args.rule_id)
-        print 'Status of the replication rule: %s' % analysis['rule_error']
+        print('Status of the replication rule: %s' % analysis['rule_error'])
         if analysis['transfers']:
-            print 'STUCK Requests:'
+            print('STUCK Requests:')
             for transfer in analysis['transfers']:
-                print '  %s:%s' % (transfer['scope'], transfer['name'])
-                print '    RSE:                  %s' % str(transfer['rse'])
-                print '    Attempts:             %s' % str(transfer['attempts'])
-                print '    Last Retry:           %s' % str(transfer['last_time'])
-                print '    Last error:           %s' % str(transfer['last_error'])
-                print '    Last source:          %s' % str(transfer['last_source'])
-                print '    Available sources:    %s' % ', '.join([source[0] for source in transfer['sources'] if source[1]])
-                print '    Blacklisted sources:  %s' % ', '.join([source[0] for source in transfer['sources'] if not source[1]])
+                print('  %s:%s' % (transfer['scope'], transfer['name']))
+                print('    RSE:                  %s' % str(transfer['rse']))
+                print('    Attempts:             %s' % str(transfer['attempts']))
+                print('    Last Retry:           %s' % str(transfer['last_time']))
+                print('    Last error:           %s' % str(transfer['last_error']))
+                print('    Last source:          %s' % str(transfer['last_source']))
+                print('    Available sources:    %s' % ', '.join([source[0] for source in transfer['sources'] if source[1]]))
+                print('    Blocklisted sources:  %s' % ', '.join([source[0] for source in transfer['sources'] if not source[1]]))
     else:
         rule = client.get_replication_rule(rule_id=args.rule_id)
-        print "Id:                         %s" % rule['id']
-        print "Account:                    %s" % rule['account']
-        print "Scope:                      %s" % rule['scope']
-        print "Name:                       %s" % rule['name']
-        print "RSE Expression:             %s" % rule['rse_expression']
-        print "Copies:                     %s" % rule['copies']
-        print "State:                      %s" % rule['state']
-        print "Locks OK/REPLICATING/STUCK: %s/%s/%s" % (rule['locks_ok_cnt'], rule['locks_replicating_cnt'], rule['locks_stuck_cnt'])
-        print "Grouping:                   %s" % rule['grouping']
-        print "Expires at:                 %s" % rule['expires_at']
-        print "Locked:                     %s" % rule['locked']
-        print "Weight:                     %s" % rule['weight']
-        print "Created at:                 %s" % rule['created_at']
-        print "Updated at:                 %s" % rule['updated_at']
-        print "Error:                      %s" % rule['error']
-        print "Subscription Id:            %s" % rule['subscription_id']
-        print "Source replica expression:  %s" % rule['source_replica_expression']
-        print "Activity:                   %s" % rule['activity']
-        print "Comment:                    %s" % rule['comments']
-        print "Ignore Quota:               %s" % rule['ignore_account_limit']
-        print "Ignore Availability:        %s" % rule['ignore_availability']
-        print "Purge replicas:             %s" % rule['purge_replicas']
-        print "Notification:               %s" % rule['notification']
-        print "End of life:                %s" % rule['eol_at']
+        print("Id:                         %s" % rule['id'])
+        print("Account:                    %s" % rule['account'])
+        print("Scope:                      %s" % rule['scope'])
+        print("Name:                       %s" % rule['name'])
+        print("RSE Expression:             %s" % rule['rse_expression'])
+        print("Copies:                     %s" % rule['copies'])
+        print("State:                      %s" % rule['state'])
+        print("Locks OK/REPLICATING/STUCK: %s/%s/%s" % (rule['locks_ok_cnt'], rule['locks_replicating_cnt'], rule['locks_stuck_cnt']))
+        print("Grouping:                   %s" % rule['grouping'])
+        print("Expires at:                 %s" % rule['expires_at'])
+        print("Locked:                     %s" % rule['locked'])
+        print("Weight:                     %s" % rule['weight'])
+        print("Created at:                 %s" % rule['created_at'])
+        print("Updated at:                 %s" % rule['updated_at'])
+        print("Error:                      %s" % rule['error'])
+        print("Subscription Id:            %s" % rule['subscription_id'])
+        print("Source replica expression:  %s" % rule['source_replica_expression'])
+        print("Activity:                   %s" % rule['activity'])
+        print("Comment:                    %s" % rule['comments'])
+        print("Ignore Quota:               %s" % rule['ignore_account_limit'])
+        print("Ignore Availability:        %s" % rule['ignore_availability'])
+        print("Purge replicas:             %s" % rule['purge_replicas'])
+        print("Notification:               %s" % rule['notification'])
+        print("End of life:                %s" % rule['eol_at'])
+        print("Child Rule Id:              %s" % rule['child_rule_id'])
     return SUCCESS
 
 
 @exception_handler
 def list_rules(args):
     """
     %(prog)s list-rules ...
 
     List rules.
     """
     client = get_client(args)
     if args.rule_id:
         rules = [client.get_replication_rule(args.rule_id)]
     elif args.file:
-        scope, name = extract_scope(args.file)
+        scope, name = get_scope(args.file, client)
         rules = client.list_associated_rules_for_file(scope=scope, name=name)
     elif args.traverse:
-        scope, name = extract_scope(args.did)
+        scope, name = get_scope(args.did, client)
         locks = client.get_dataset_locks(scope=scope, name=name)
         rules = []
         for rule_id in list(set([lock['rule_id'] for lock in locks])):
             rules.append(client.get_replication_rule(rule_id))
     elif args.did:
-        scope, name = extract_scope(args.did)
+        scope, name = get_scope(args.did, client)
         meta = client.get_metadata(scope=scope, name=name)
         rules = client.list_did_rules(scope=scope, name=name)
         try:
-            rules.next()
+            next(rules)
             rules = client.list_did_rules(scope=scope, name=name)
         except StopIteration:
             rules = []
             # looking for other rules
-            if meta['did_type'] == u'CONTAINER':
+            if meta['did_type'] == 'CONTAINER':
                 for dsn in client.list_content(scope, name):
                     rules.extend(client.list_did_rules(scope=dsn['scope'], name=dsn['name']))
-            if meta['did_type'] == u'DATASET':
+                if rules:
+                    print('No rules found, listing rules for content')
+            if meta['did_type'] == 'DATASET':
                 for container in client.list_parent_dids(scope, name):
                     rules.extend(client.list_did_rules(scope=container['scope'], name=container['name']))
+                if rules:
+                    print('No rules found, listing rules for parents')
     elif args.rule_account:
         rules = client.list_account_rules(account=args.rule_account)
     elif args.subscription:
         account = args.subscription[0]
         name = args.subscription[1]
         rules = client.list_subscription_rules(account=account, name=name)
     else:
-        print 'At least one option has to be given. Use -h to list the options.'
+        print('At least one option has to be given. Use -h to list the options.')
         return FAILURE
     if args.csv:
         for rule in rules:
-            print "{0}, {1}, {2}, {3}, {4}, {5}, {6}".format(rule['id'],
-                                                             rule['account'],
-                                                             '%s:%s' % (rule['scope'], rule['name']),
-                                                             '%s[%d/%d/%d]' % (rule['state'], rule['locks_ok_cnt'], rule['locks_replicating_cnt'], rule['locks_stuck_cnt']),
-                                                             rule['rse_expression'],
-                                                             rule['copies'],
-                                                             rule['expires_at'])
+            print(rule['id'],
+                  rule['account'],
+                  '%s:%s' % (rule['scope'], rule['name']),
+                  '%s[%d/%d/%d]' % (rule['state'], rule['locks_ok_cnt'], rule['locks_replicating_cnt'], rule['locks_stuck_cnt']),
+                  rule['rse_expression'],
+                  rule['copies'],
+                  rule['expires_at'],
+                  rule['created_at'],
+                  sep=',')
     else:
         table = []
         for rule in rules:
             table.append([rule['id'],
                           rule['account'],
                           '%s:%s' % (rule['scope'], rule['name']),
                           '%s[%d/%d/%d]' % (rule['state'], rule['locks_ok_cnt'], rule['locks_replicating_cnt'], rule['locks_stuck_cnt']),
                           rule['rse_expression'],
                           rule['copies'],
-                          rule['expires_at']])
-        print tabulate.tabulate(table, tablefmt='simple', headers=['ID', 'ACCOUNT', 'SCOPE:NAME', 'STATE[OK/REPL/STUCK]', 'RSE_EXPRESSION', 'COPIES', 'EXPIRES (UTC)'])
+                          rule['expires_at'],
+                          rule['created_at']])
+        print(tabulate(table, tablefmt='simple', headers=['ID', 'ACCOUNT', 'SCOPE:NAME', 'STATE[OK/REPL/STUCK]', 'RSE_EXPRESSION', 'COPIES', 'EXPIRES (UTC)', 'CREATED (UTC)'], disable_numparse=True))
     return SUCCESS
 
 
 @exception_handler
 def list_rules_history(args):
     """
     %(prog)s list-rules_history ...
 
     List replication rules history for a DID.
     """
     rule_dict = []
     client = get_client(args)
-    scope, name = extract_scope(args.did)
+    scope, name = get_scope(args.did, client)
     for rule in client.list_replication_rule_full_history(scope, name):
         if rule['rule_id'] not in rule_dict:
             rule_dict.append(rule['rule_id'])
-            print '-' * 40
-            print 'Rule insertion'
-            print 'Account : %s' % rule['account']
-            print 'RSE expression : %s' % (rule['rse_expression'])
-            print 'Time : %s' % (rule['created_at'])
+            print('-' * 40)
+            print('Rule insertion')
+            print('Account : %s' % rule['account'])
+            print('RSE expression : %s' % (rule['rse_expression']))
+            print('Time : %s' % (rule['created_at']))
         else:
             rule_dict.remove(rule['rule_id'])
-            print '-' * 40
-            print 'Rule deletion'
-            print 'Account : %s' % rule['account']
-            print 'RSE expression : %s' % (rule['rse_expression'])
-            print 'Time : %s' % (rule['updated_at'])
+            print('-' * 40)
+            print('Rule deletion')
+            print('Account : %s' % rule['account'])
+            print('RSE expression : %s' % (rule['rse_expression']))
+            print('Time : %s' % (rule['updated_at']))
     return SUCCESS
 
 
 @exception_handler
 def list_rses(args):
     """
     %(prog)s list-rses [options] <field1=value1 field2=value2 ...>
 
     List rses.
 
     """
     client = get_client(args)
+
+    rses = client.list_rses(args.rses)
+    for rse in rses:
+        print('%(rse)s' % rse)
+    return SUCCESS
+
+
+@exception_handler
+def list_suspicious_replicas(args):
+    """
+    %(prog)s list-suspicious-replicas [options] <field1=value1 field2=value2 ...>
+
+    List replicas marked as suspicious.
+
+    """
+    client = get_client(args)
     rse_expression = None
+    younger_than = None
+    nattempts = None
     if args.rse_expression:
         rse_expression = args.rse_expression
-    rses = client.list_rses(rse_expression)
-    for rse in rses:
-        print '%(rse)s' % rse
+    if args.younger_than:
+        younger_than = args.younger_than
+    if args.nattempts:
+        nattempts = args.nattempts
+    # Generator is a list with one entry, which itself is a list of lists.
+    replicas_gen = client.list_suspicious_replicas(rse_expression, younger_than, nattempts)
+    for i in replicas_gen:
+        replicas = i
+    table = []
+    for rep in replicas:
+        table.append([rep['rse'], rep['scope'], rep['created_at'], rep['cnt'], rep['name']])
+    print(tabulate(table, headers=(['RSE Expression:', 'Scope:', 'Created at:', 'Nattempts:', 'File Name:'])))
     return SUCCESS
 
 
 @exception_handler
 def list_rse_attributes(args):
     """
     %(prog)s list-rse-attributes [options] <field1=value1 field2=value2 ...>
 
     List rses.
 
     """
     client = get_client(args)
     attributes = client.list_rse_attributes(rse=args.rse)
-    for k in attributes:
-        print '  ' + k + ': ' + str(attributes[k])
+    table = [(k + ':', str(v)) for (k, v) in sorted(attributes.items())]  # columns hav mixed datatypes
+    print(tabulate(table, tablefmt='plain', disable_numparse=True))  # disabling number parsing
     return SUCCESS
 
 
 @exception_handler
 def list_rse_usage(args):
     """
     %(prog)s list-rse-usage [options] <rse>
 
     Show the space usage of a given rse
 
     """
     client = get_client(args)
-    usages = client.get_rse_usage(rse=args.rse)
-    print 'USAGE:'
-    for usage in usages:
-        print '------'
-        print_free = False
-        if usage['source'] in ['srm']:
-            print_free = True
+    all_usages = client.get_rse_usage(rse=args.rse, filters={'per_account': args.show_accounts})
+    select_usages = [u for u in all_usages if u['source'] not in ('srm', 'gsiftp', 'webdav')]
+    print('USAGE:')
+    for usage in select_usages:
+        print('------')
         for elem in usage:
-            if not (elem == 'free' or elem == 'total') or print_free:
-                if elem == 'free' or elem == 'total' or elem == 'used':
-                    print '  {0}: {1}'.format(elem, sizefmt(usage[elem], args.human))
+            if (elem in ['free', 'total'] and usage['source'] != 'storage' or elem == 'files' and usage['source'] != 'rucio'):
+                continue
+            elif elem in ['used', 'free', 'total']:
+                print('  {0}: {1}'.format(elem, sizefmt(usage[elem], args.human)))
+            elif elem == 'account_usages':
+                account_usages_title = '  per account:'
+                if not usage[elem]:
+                    account_usages_title += ' no usage'
                 else:
-                    print '  {0}: {1}'.format(elem, usage[elem])
-    print '------'
+                    print(account_usages_title)
+                    print('  ------')
+                    col_width = max(len(str(entry[1])) for account in usage[elem] for entry in list(account.items())) + 16
+                    for account in usage[elem]:
+                        base_string = '    '
+                        used_string = 'used: {0}'.format(sizefmt(account['used'], args.human))
+                        account_string = 'account: {0}'.format(account['account'])
+                        percentage_string = 'percentage: {0}'.format(account['percentage'])
+                        print(base_string + account_string.ljust(col_width) + used_string.ljust(col_width) + percentage_string.ljust(col_width))
+                        print('  ------')
+            else:
+                print('  {0}: {1}'.format(elem, usage[elem]))
+    print('------')
     return SUCCESS
 
 
 @exception_handler
 def list_account_limits(args):
     """
     %(prog)s list [options] <field1=value1 field2=value2 ...>
 
     List account limits.
 
     """
     client = get_client(args)
     table = []
     if args.rse:
-        limits = client.get_account_limit(account=args.limit_account, rse=args.rse)
+        limits = client.get_local_account_limit(account=args.limit_account, rse=args.rse)
     else:
-        limits = client.get_account_limits(account=args.limit_account)
-    for limit in limits.items():
+        limits = client.get_local_account_limits(account=args.limit_account)
+    for limit in list(limits.items()):
         table.append([limit[0], sizefmt(limit[1], args.human)])
     table.sort()
-    print tabulate.tabulate(table, tablefmt=tablefmt, headers=['RSE', 'LIMIT'])
+    print(tabulate(table, tablefmt=tablefmt, headers=['RSE', 'LIMIT']))
+
+    table = []
+    limits = client.get_global_account_limits(account=args.limit_account)
+    for limit in list(limits.items()):
+        if (args.rse and args.rse in limit[1]['resolved_rses']) or not args.rse:
+            table.append([limit[0], sizefmt(limit[1]['limit'], args.human)])
+    table.sort()
+    print(tabulate(table, tablefmt=tablefmt, headers=['RSE EXPRESSION', 'LIMIT']))
+
     return SUCCESS
 
 
 @exception_handler
 def list_account_usage(args):
     """
     %(prog)s list [options] <field1=value1 field2=value2 ...>
 
     List account usage.
 
     """
     client = get_client(args)
     table = []
     if args.rse:
-        usage = client.get_account_usage(account=args.usage_account, rse=args.rse)
+        usage = client.get_local_account_usage(account=args.usage_account, rse=args.rse)
     else:
-        usage = client.get_account_usage(account=args.usage_account)
+        usage = client.get_local_account_usage(account=args.usage_account)
     for item in usage:
         remaining = 0 if float(item['bytes_remaining']) < 0 else float(item['bytes_remaining'])
         table.append([item['rse'], sizefmt(item['bytes'], args.human), sizefmt(item['bytes_limit'], args.human), sizefmt(remaining, args.human)])
     table.sort()
-    print tabulate.tabulate(table, tablefmt=tablefmt, headers=['RSE', 'USAGE', 'LIMIT', 'QUOTA LEFT'])
+    print(tabulate(table, tablefmt=tablefmt, headers=['RSE', 'USAGE', 'LIMIT', 'QUOTA LEFT']))
+
+    table = []
+    usage = client.get_global_account_usage(account=args.usage_account)
+    for item in usage:
+        if (args.rse and args.rse in item['rse_expression']) or not args.rse:
+            remaining = 0 if float(item['bytes_remaining']) < 0 else float(item['bytes_remaining'])
+            table.append([item['rse_expression'], sizefmt(item['bytes'], args.human), sizefmt(item['bytes_limit'], args.human), sizefmt(remaining, args.human)])
+    table.sort()
+    print(tabulate(table, tablefmt=tablefmt, headers=['RSE EXPRESSION', 'USAGE', 'LIMIT', 'QUOTA LEFT']))
+
     return SUCCESS
 
 
 @exception_handler
 def list_datasets_rse(args):
     """
     %(prog)s list [options] <field1=value1 field2=value2 ...>
@@ -2049,458 +1696,842 @@
     """
     client = get_client(args)
     if args.long:
         table = []
         for dsn in client.list_datasets_per_rse(args.rse):
             table.append(['%s:%s' % (dsn['scope'], dsn['name']), '%s/%s' % (str(dsn['available_length']), str(dsn['length'])), '%s/%s' % (str(dsn['available_bytes']), str(dsn['bytes']))])
         table.sort()
-        print tabulate.tabulate(table, tablefmt=tablefmt, headers=['DID', 'LOCAL FILES/TOTAL FILES', 'LOCAL BYTES/TOTAL BYTES'])
+        print(tabulate(table, tablefmt=tablefmt, headers=['DID', 'LOCAL FILES/TOTAL FILES', 'LOCAL BYTES/TOTAL BYTES']))
     else:
         dsns = list(set(['%s:%s' % (dsn['scope'], dsn['name']) for dsn in client.list_datasets_per_rse(args.rse)]))
         dsns.sort()
-        print "SCOPE:NAME"
-        print '----------'
+        print("SCOPE:NAME")
+        print('----------')
         for dsn in dsns:
-            print dsn
+            print(dsn)
     return SUCCESS
 
 
 @exception_handler
-def list_parent_datasets(args):
+def add_lifetime_exception(args):
     """
-    %(prog)s list [options] <field1=value1 field2=value2 ...>
+    %(prog)s add_lifetime_exception [options] <field1=value1 field2=value2 ...>
 
-    List the possible PFN for a file at a site.
+    Declare a lifetime model exception.
 
     """
-    print ''
-    print 'The functionality of this command has been replicated to ' + Color.BOLD + 'rucio list-parent-dids' + Color.END
-    print 'The ' + Color.BOLD + 'rucio list-parent-datasets' + Color.END + ' command will be deactivated with Rucio version 1.11.0'
-    print ''
-
     client = get_client(args)
-    inputs = args.inputs.split(',')
-    guids = []
-    pfns = []
-    for input in inputs:
-        try:
-            uuid.UUID(input)
-            guids.append(input)
-        except ValueError:
-            pfns.append(input)
-
-    dict_datasets = {}
-    if pfns:
-        for res in client.get_did_from_pfns(pfns):
-            for key in res:
-                if key not in dict_datasets:
-                    dict_datasets[key] = []
-                for rule in client.list_associated_rules_for_file(res[key]['scope'], res[key]['name']):
-                    if '%s:%s' % (rule['scope'], rule['name']) not in dict_datasets[key]:
-                        dict_datasets[key].append('%s:%s' % (rule['scope'], rule['name']))
-    for guid in guids:
-        for did in client.get_dataset_by_guid(guid):
-            if guid not in dict_datasets:
-                dict_datasets[guid] = []
-            for rule in client.list_associated_rules_for_file(did['scope'], did['name']):
-                if '%s:%s' % (rule['scope'], rule['name']) not in dict_datasets[guid]:
-                    dict_datasets[guid].append('%s:%s' % (rule['scope'], rule['name']))
-
-    for pfn in dict_datasets:
-        print 'PFN or GUID : ', pfn
-        print 'Parents : ', ','.join(dict_datasets[pfn])
+    if not args.reason:
+        logger.error('reason for the extension is mandatory')
+        return FAILURE
+    reason = args.reason
+    if not args.expiration:
+        logger.error('expiration is mandatory')
+        return FAILURE
+    try:
+        expiration = datetime.strptime(args.expiration, "%Y-%m-%d")
+    except Exception as err:
+        logger.error(err)
+        return FAILURE
+
+    if not args.inputfile:
+        logger.error('inputfile is mandatory')
+        return FAILURE
+    with open(args.inputfile) as infile:
+        dids = list(set(line.strip() for line in infile))
+
+    dids_list = []
+    containers = []
+    datasets = []
+    error_types = ['Total DIDs',
+                   'DID not submitted because it is a file',
+                   'DID that are containers and were resolved',
+                   'DID not submitted because it is not part of the lifetime campaign',
+                   'DID successfully submitted including the one from containers resolved']
+    for did in dids:
+        scope, name = get_scope(did, client)
+        dids_list.append({'scope': scope, 'name': name})
+    summary = {0: len(dids_list), 1: 0, 2: 0, 3: 0, 4: 0}
+    chunk_limit = 500  # Server should be able to accept 1000
+    dids_list_copy = deepcopy(dids_list)
+    for chunk in chunks(dids_list_copy, chunk_limit):
+        for meta in client.get_metadata_bulk(chunk):
+            scope, name = meta['scope'], meta['name']
+            dids_list.remove({'scope': scope, 'name': name})
+            if meta['did_type'] == 'FILE':
+                logger.warning('%s:%s is a file. Will be ignored' % (scope, name))
+                summary[1] += 1
+            elif meta['did_type'] == 'CONTAINER':
+                logger.warning('%s:%s is a container. It needs to be resolved' % (scope, name))
+                containers.append({'scope': scope, 'name': name})
+                summary[2] += 1
+            elif not meta['eol_at']:
+                logger.warning('%s:%s is not affected by the lifetime model' % (scope, name))
+                summary[3] += 1
+            else:
+                logger.info('%s:%s will be declared' % (scope, name))
+                datasets.append({'scope': scope, 'name': name})
+                summary[4] += 1
+
+    for did in dids_list:
+        scope = did['scope']
+        name = did['name']
+        logger.warning('%s:%s does not exist' % (scope, name))
+
+    if containers:
+        logger.warning('One or more DIDs are containers. They will be resolved into a list of datasets to request exception. Full list below')
+        for container in containers:
+            logger.info('Resolving %s:%s into datasets :' % (container['scope'], container['name']))
+            list_datasets = __resolve_containers_to_datasets(container['scope'], container['name'], client)
+            for chunk in chunks(list_datasets, chunk_limit):
+                for meta in client.get_metadata_bulk(chunk):
+                    scope, name = meta['scope'], meta['name']
+                    logger.debug('%s:%s' % (scope, name))
+                    if not meta['eol_at']:
+                        logger.warning('%s:%s is not affected by the lifetime model' % (scope, name))
+                        summary[3] += 1
+                    else:
+                        logger.info('%s:%s will be declared' % (scope, name))
+                        datasets.append({'scope': scope, 'name': name})
+
+    if not datasets:
+        logger.error('Nothing to submit')
+        return SUCCESS
+    try:
+        client.add_exception(dids=datasets, account=client.account, pattern='', comments=reason, expires_at=expiration)
+    except UnsupportedOperation as err:
+        logger.error(err)
+        return FAILURE
+    except Exception:
+        logger.error('Failure to submit exception. Please retry')
+        logger.debug(traceback.format_exc())
+        return FAILURE
+
+    logger.info('Exception successfully submitted. Summary below')
+    for cnt, error in enumerate(error_types):
+        print('{0:100} {1:6d}'.format(error, summary[cnt]))
     return SUCCESS
 
 
 def test_server(args):
     """"
     %(prog)s test-server [options] <field1=value1 field2=value2 ...>
     Test the client against a server.
     """
-    import nose
-    config = nose.config.Config()
-    config.verbosity = 2
-    nose.run(argv=sys.argv[1:], defaultTest='rucio.tests.test_rucio_server', config=config)
+    suite = unittest.TestLoader().loadTestsFromTestCase(TestRucioServer)
+    unittest.TextTestRunner(verbosity=2).run(suite)
     return SUCCESS
 
 
-if __name__ == '__main__':
-    usage = """
-usage: %(prog)s <command> [options] [args]
+def touch(args):
+    """
+    %(prog)s touch [options] <did1 did2 ...>
+    """
 
-Commands:
+    client = get_client(args)
 
-    help <command>  Output help for one of the commands below
+    for did in args.dids:
+        scope, name = get_scope(did, client)
+        client.touch(scope, name, args.rse)
 
 
-"""
+def rse_completer(prefix, parsed_args, **kwargs):
+    """
+    Completes the argument with a list of RSEs
+    """
+    client = get_client(parsed_args)
+    return ["%(rse)s" % rse for rse in client.list_rses()]
+
+
+def get_parser():
+    """
+    Returns the argparse parser.
+    """
     oparser = argparse.ArgumentParser(prog=os.path.basename(sys.argv[0]), add_help=True)
     subparsers = oparser.add_subparsers()
 
     # Main arguments
     oparser.add_argument('--version', action='version', version='%(prog)s ' + version.version_string())
+    oparser.add_argument('--config', dest="config", help="The Rucio configuration file to use.")
     oparser.add_argument('--verbose', '-v', default=False, action='store_true', help="Print more verbose output.")
     oparser.add_argument('-H', '--host', dest="host", metavar="ADDRESS", help="The Rucio API host.")
     oparser.add_argument('--auth-host', dest="auth_host", metavar="ADDRESS", help="The Rucio Authentication host.")
     oparser.add_argument('-a', '--account', dest="account", metavar="ACCOUNT", help="Rucio account to use.")
-    oparser.add_argument('-S', '--auth-strategy', dest="auth_strategy", default=None, help="Authentication strategy (userpass or x509)")
+    oparser.add_argument('-S', '--auth-strategy', dest="auth_strategy", default=None, help="Authentication strategy (userpass, x509...)")
     oparser.add_argument('-T', '--timeout', dest="timeout", type=float, default=None, help="Set all timeout values to seconds.")
     oparser.add_argument('--robot', '-R', dest="human", default=True, action='store_false', help="All output in bytes and without the units. This output format is preferred by parsers and scripts.")
     oparser.add_argument('--user-agent', '-U', dest="user_agent", default='rucio-clients', action='store', help="Rucio User Agent")
+    oparser.add_argument('--vo', dest="vo", metavar="VO", default=None, help="VO to authenticate at. Only used in multi-VO mode.")
 
-    # Options for the userpass auth_strategy
+    # Options for the userpass or OIDC auth_strategy
     oparser.add_argument('-u', '--user', dest='username', default=None, help='username')
     oparser.add_argument('-pwd', '--password', dest='password', default=None, help='password')
+    # Options for defining remaining OIDC parameters
+    oparser.add_argument('--oidc-user', dest='oidc_username', default=None, help='OIDC username')
+    oparser.add_argument('--oidc-password', dest='oidc_password', default=None, help='OIDC password')
+    oparser.add_argument('--oidc-scope', dest='oidc_scope', default='openid profile', help='Defines which (OIDC) information user will share with Rucio. '
+                         + 'Rucio requires at least -sc="openid profile". To request refresh token for Rucio, scope must include "openid offline_access" and '  # NOQA: W503
+                         + 'there must be no active access token saved on the side of the currently used Rucio Client.')  # NOQA: W503
+    oparser.add_argument('--oidc-audience', dest='oidc_audience', default=None, help='Defines which audience are tokens requested for.')
+    oparser.add_argument('--oidc-auto', dest='oidc_auto', default=False, action='store_true', help='If not specified, username and password credentials are not required and users will be given a URL '
+                         + 'to use in their browser. If specified, the users explicitly trust Rucio with their IdP credentials.')  # NOQA: W503
+    oparser.add_argument('--oidc-polling', dest='oidc_polling', default=False, action='store_true', help='If not specified, user will be asked to enter a code returned by the browser to the command line. '
+                         + 'If --polling is set, Rucio Client should get the token without any further interaction of the user. This option is active only if --auto is *not* specified.')  # NOQA: W503
+    oparser.add_argument('--oidc-refresh-lifetime', dest='oidc_refresh_lifetime', default=None, help='Max lifetime in hours for this an access token will be refreshed by asynchronous Rucio daemon. '
+                         + 'If not specified, refresh will be stopped after 4 days. This option is effective only if --oidc-scope includes offline_access scope for a refresh token to be granted to Rucio.')  # NOQA: W503
+    oparser.add_argument('--oidc-issuer', dest='oidc_issuer', default=None,
+                         help='Defines which Identity Provider is going to be used. The issuer string must correspond '
+                         + 'to the keys configured in the /etc/idpsecrets.json auth server configuration file.')  # NOQA: W503
 
     # Options for the x509  auth_strategy
     oparser.add_argument('--certificate', dest='certificate', default=None, help='Client certificate file.')
     oparser.add_argument('--ca-certificate', dest='ca_certificate', default=None, help='CA certificate to verify peer against (SSL).')
 
     # Ping command
-    ping_parser = subparsers.add_parser('ping', help='Ping Rucio server.')
-    ping_parser.set_defaults(which='ping')
+    ping_parser = subparsers.add_parser('ping', formatter_class=argparse.RawDescriptionHelpFormatter, help='Ping Rucio server.',
+                                        epilog='Usage example\n'
+                                               '"""""""""""""\n'
+                                               '\n'
+                                               'To ping the server::\n'
+                                               '\n'
+                                               '    $ rucio ping\n'
+                                               '    1.14.8\n'
+                                               '\n'
+                                               'The returned value is the version of Rucio installed on the server.'
+                                               '\n')
+    ping_parser.set_defaults(function=ping)
 
     # The whoami command
-    whoami_parser = subparsers.add_parser('whoami', help='Get information about account whose token is used.')
-    whoami_parser.set_defaults(which='whoami_account')
+    whoami_parser = subparsers.add_parser('whoami', help='Get information about account whose token is used.', formatter_class=argparse.RawDescriptionHelpFormatter,
+                                          epilog='''Usage example
+"""""""""""""
+::
+
+    $ rucio whoami
+    jdoe
+
+The returned value is the account currently used.
+                                                 ''')
+
+    whoami_parser.set_defaults(function=whoami_account)
 
     # The list-file-replicas command
-    list_file_replicas_parser = subparsers.add_parser('list-file-replicas', help='List the replicas of a DID and it\'s PFNs.', description='This method allows to list all the replicas of a given Data IDentifier (DID).\
-                                                 The only mandatory parameter is the DID which can be a container/dataset/files. By default all the files replicas in state available are returned.')
-    list_file_replicas_parser.set_defaults(which='list_file_replicas')
+    list_file_replicas_parser = subparsers.add_parser('list-file-replicas', help='List the replicas of a DID and its PFNs.', description='This method allows to list all the replicas of a given Data IDentifier (DID). \
+The only mandatory parameter is the DID which can be a container/dataset/files. By default all the files replicas in state available are returned.', formatter_class=argparse.RawDescriptionHelpFormatter,
+                                                      epilog='''Usage example
+^^^^^^^^^^^^^
+
+To list the file replicas for a given dataset::
+
+    $ rucio list-file-replicas user.jdoe:user.jdoe.test.data.1234.1
+    +-----------+---------------------------------+------------+-----------+-----------------------------------------------------------------------------------+
+    | SCOPE     | NAME                            | FILESIZE   | ADLER32   | RSE: REPLICA                                                                      |
+    |-----------+---------------------------------+------------+-----------+-----------------------------------------------------------------------------------|
+    | user.jdoe | user.jdoe.test.data.1234.file.1 | 94.835 MB  | 5d000974  | SITE1_DISK: srm://blahblih/path/to/file/user.jdoe/user.jdoe.test.data.1234.file.1 |
+    | user.jdoe | user.jdoe.test.data.1234.file.1 | 94.835 MB  | 5d000974  | SITE2_DISK: file://another/path/to/file/user.jdoe/user.jdoe.test.data.1234.file.1 |
+    | user.jdoe | user.jdoe.test.data.1234.file.2 | 82.173 MB  | 01e56f23  | SITE2_DISK: file://another/path/to/file/user.jdoe/user.jdoe.test.data.1234.file.2 |
+    +-----------+---------------------------------+------------+-----------+-----------------------------------------------------------------------------------+
+
+To list the missing replica of a dataset of a given RSE-expression::
+
+    $ rucio list-file-replicas --rses SITE1_DISK user.jdoe:user.jdoe.test.data.1234.1
+    +-----------+---------------------------------+------------+-----------+-----------------------------------------------------------------------------------+
+    | SCOPE     | NAME                            | FILESIZE   | ADLER32   | RSE: REPLICA                                                                      |
+    |-----------+---------------------------------+------------+-----------+-----------------------------------------------------------------------------------|
+    | user.jdoe | user.jdoe.test.data.1234.file.1 | 94.835 MB  | 5d000974  | SITE1_DISK: srm://blahblih/path/to/file/user.jdoe/user.jdoe.test.data.1234.file.1 |
+    +-----------+---------------------------------+------------+-----------+-----------------------------------------------------------------------------------+
+    ''')
+    list_file_replicas_parser.set_defaults(function=list_file_replicas)
     list_file_replicas_parser.add_argument('--protocols', dest='protocols', action='store', help='List of comma separated protocols. (i.e. https, root, srm).', required=False)
-    list_file_replicas_parser.add_argument('--all-states', dest='all_states', action='store_true', default=False, help='To select all replicas (including unavailable ones).', required=False)
-    list_file_replicas_parser.add_argument('--list-collections', dest='list_collections', action='store_true', default=False, help='To have the number of files of the dataset by site\
-        (' + Color.BOLD + 'DEPRECATED: ' + Color.END + 'please use list-dataset-replicas instead.)', required=False)
+    list_file_replicas_parser.add_argument('--all-states', dest='all_states', action='store_true', default=False, help='To select all replicas (including unavailable ones).\
+            Also gets information about the current state of a DID in each RSE.\
+            Legend: ' + ', '.join(["{0} = {1}".format(state.value, state.name) for state in ReplicaState]), required=False)
     list_file_replicas_parser.add_argument(dest='dids', nargs='+', action='store', help='List of space separated data identifiers.')
-    list_file_replicas_parser.add_argument('--rse', dest='selected_rse', default=False, action='store', help='Show only results for this RSE', required=False)
-    list_file_replicas_parser.add_argument('--missing', dest='missing', default=False, action='store_true', help='To list missing replicas at a RSE.', required=False)
-    list_file_replicas_parser.add_argument('--expression', dest='rse_expression', default=None, action='store', help='The RSE filter expression. A comprehensive help about RSE expressions\
-            can be found in ' + Color.BOLD + 'http://rucio.cern.ch/client_tutorial.html#adding-rules-for-replication' + Color.END)
-
-    # The list_parent_datasets command
-    list_parent_datasets_parser = subparsers.add_parser('list-parent-datasets', help='List the datasets associated to a PFN.')
-    list_parent_datasets_parser.set_defaults(which='list_parent_datasets')
-    list_parent_datasets_parser.add_argument(dest='inputs', action='store', help='The PFN of the DID is required. Must be a SURL or GUID.')
+    list_file_replicas_parser.add_argument('--pfns', default=False, action='store_true', help='Show only the PFNs.', required=False)
+    list_file_replicas_parser.add_argument('--domain', default=None, action='store', help='Force the networking domain. Available options: wan, lan, all.', required=False)
+    list_file_replicas_parser.add_argument('--link', dest='link', default=None, action='store', help='Symlink PFNs with directory substitution.', required=False)
+    list_file_replicas_parser.add_argument('--missing', dest='missing', default=False, action='store_true', help='To list missing replicas at a RSE-Expression. Must be used with --rses option', required=False)
+    list_file_replicas_parser.add_argument('--metalink', dest='metalink', default=False, action='store_true', help='Output available replicas as metalink.', required=False)
+    list_file_replicas_parser.add_argument('--no-resolve-archives', dest='no_resolve_archives', default=False, action='store_true', help='Do not resolve archives which may contain the files.', required=False)
+    list_file_replicas_parser.add_argument('--sort', dest='sort', default=None, action='store', help='Replica sort algorithm. Available options: geoip (default), random', required=False)
+    list_file_replicas_parser.add_argument('--rses', dest='rses', default=None, action='store', help='The RSE filter expression. A comprehensive help about RSE expressions\
+            can be found in ' + Color.BOLD + 'http://rucio.cern.ch/documentation/rse_expressions/' + Color.END)
 
     # The list-dataset-replicas command
-    list_dataset_replicas_parser = subparsers.add_parser('list-dataset-replicas', help='List the dataset replicas.')
-    list_dataset_replicas_parser.set_defaults(which='list_dataset_replicas')
-    list_dataset_replicas_parser.add_argument(dest='did', action='store', help='The name of the DID to search.')
+    list_dataset_replicas_parser = subparsers.add_parser('list-dataset-replicas', help='List the dataset replicas.',
+                                                         formatter_class=argparse.RawDescriptionHelpFormatter,
+                                                         epilog='''Usage example
+"""""""""""""
+::
+
+    $ rucio list-dataset-replicas user.jdoe:user.jdoe.test.data.1234.1
+
+    DATASET: user.jdoe:user.jdoe.test.data.1234.1
+    +------------+---------+---------+
+    | RSE        |   FOUND |   TOTAL |
+    |------------+---------+---------|
+    | SITE1_DISK |       1 |       2 |
+    | SITE2_DISK |       2 |       2 |
+    +------------+---------+---------+
+    ''')
+    list_dataset_replicas_parser.set_defaults(function=list_dataset_replicas)
+    list_dataset_replicas_parser.add_argument(dest='dids', action='store', nargs='+', help='The name of the DID to search.')
     list_dataset_replicas_parser.add_argument('--deep', action='store_true', help='Make a deep check.')
-    list_dataset_replicas_parser.add_argument('--csv', dest='csv', action='store_true', default=False, help='Comma Separated Value output.')
+    list_dataset_replicas_parser.add_argument('--csv', dest='csv', action='store_true', default=False, help='Comma Separated Value output.',)
 
     # The add-dataset command
-    add_dataset_parser = subparsers.add_parser('add-dataset', help='Add a dataset to Rucio Catalog. ')
-    add_dataset_parser.set_defaults(which='add_dataset')
+    add_dataset_parser = subparsers.add_parser('add-dataset', help='Add a dataset to Rucio Catalog.',
+                                               formatter_class=argparse.RawDescriptionHelpFormatter, epilog='''Usage example
+"""""""""""""
+::
+
+    $ rucio add-dataset user.jdoe:user.jdoe.test.data.1234.1
+    Added user.jdoe:user.jdoe.test.data.1234.1
+
+    ''')
+
+    add_dataset_parser.set_defaults(function=add_dataset)
     add_dataset_parser.add_argument('--monotonic', action='store_true', help='Monotonic status to True.')
     add_dataset_parser.add_argument(dest='did', action='store', help='The name of the dataset to add.')
-    add_dataset_parser.add_argument('--lifetime', dest='lifetime', action='store', help='Lifetime in seconds.')
+    add_dataset_parser.add_argument('--lifetime', dest='lifetime', action='store', type=int, help='Lifetime in seconds.')
 
     # The add-container command
-    add_container_parser = subparsers.add_parser('add-container', help='Add a container to Rucio Catalog.')
-    add_container_parser.set_defaults(which='add_container')
+    add_container_parser = subparsers.add_parser('add-container', help='Add a container to Rucio Catalog.',
+                                                 formatter_class=argparse.RawDescriptionHelpFormatter, epilog='''Usage example
+"""""""""""""
+::
+
+    $ rucio add-container user.jdoe:user.jdoe.test.cont.1234.1
+    Added user.jdoe:user.jdoe.test.cont.1234.1
+
+    ''')
+
+    add_container_parser.set_defaults(function=add_container)
     add_container_parser.add_argument('--monotonic', action='store_true', help='Monotonic status to True.')
     add_container_parser.add_argument(dest='did', action='store', help='The name of the container to add.')
-    add_container_parser.add_argument('--lifetime', dest='lifetime', action='store', help='Lifetime in seconds.')
+    add_container_parser.add_argument('--lifetime', dest='lifetime', action='store', type=int, help='Lifetime in seconds.')
 
     # The attach command
     attach_parser = subparsers.add_parser('attach', help='Attach a list of DIDs to a parent DID.',
-                                          description='Attach a list of Data IDentifiers (file, dataset or container) to an other Data IDentifier (dataset or container).')
-    attach_parser.set_defaults(which='attach')
+                                          description='Attach a list of Data IDentifiers (file, dataset or container) to an other Data IDentifier (dataset or container).',
+                                          formatter_class=argparse.RawDescriptionHelpFormatter, epilog='''Usage example
+"""""""""""""
+::
+
+    $ rucio attach user.jdoe:user.jdoe.test.cont.1234.1 user.jdoe:user.jdoe.test.data.1234.1
+    DIDs successfully attached to user.jdoe:user.jdoe.test.cont.1234.1
+
+    ''')
+
+    attach_parser.set_defaults(function=attach)
     attach_parser.add_argument(dest='todid', action='store', help='Destination Data IDentifier (either dataset or container).')
     attach_parser.add_argument('-f', '--from-file', dest='fromfile', action='store_true', default=False, help='Attach the DIDs contained in a file. The file should contain one did per line.')
     attach_parser.add_argument(dest='dids', nargs='+', action='store', help='List of space separated data identifiers (or a file containing one did per line, if -f is present).')
 
     # The detach command
     detach_parser = subparsers.add_parser('detach', help='Detach a list of DIDs from a parent DID.',
-                                          description='Detach a list of Data Identifiers (file, dataset or container) from an other Data Identifier (dataset or container).')
-    detach_parser.set_defaults(which='detach')
+                                          description='Detach a list of Data Identifiers (file, dataset or container) from an other Data Identifier (dataset or container).',
+                                          formatter_class=argparse.RawDescriptionHelpFormatter, epilog='''Usage example
+"""""""""""""
+::
+
+    $ rucio detach user.jdoe:user.jdoe.test.cont.1234.1 user.jdoe:user.jdoe.test.data.1234.1
+    DIDs successfully detached from user.jdoe:user.jdoe.test.cont.1234.1
+
+    ''')
+
+    detach_parser.set_defaults(function=detach)
     detach_parser.add_argument(dest='fromdid', action='store', help='Target Data IDentifier (must be a dataset or container).')
     detach_parser.add_argument(dest='dids', nargs='+', action='store', help='List of space separated data identifiers.')
 
     # The list command
-    ls_parser = subparsers.add_parser('ls', help='List the data identifiers matching some metadata (synonym for list-dids).', description='List the Data IDentifiers matching certain pattern.\
-                                         Only the collections (i.e. dataset or container) are returned by default.\
-                                         With the filter option, you can specify a list of metadata that the Data IDentifier should match.')
-    ls_parser.set_defaults(which='list_dids')
+    ls_parser = subparsers.add_parser('ls', help='List the data identifiers matching some metadata (synonym for list-dids).', description='List the Data IDentifiers matching certain pattern. \
+Only the collections (i.e. dataset or container) are returned by default. With the filter option, you can specify a list of metadata that the Data IDentifier should match.',
+                                      formatter_class=argparse.RawDescriptionHelpFormatter, epilog='''Usage example
+"""""""""""""
+You can query the DIDs matching a certain pattern. It always requires to specify the scope in which you want to search::
+
+    $ rucio ls user.jdoe:*
+    +-------------------------------------------+--------------+
+    | SCOPE:NAME                                | [DID TYPE]   |
+    |-------------------------------------------+--------------|
+    | user.jdoe:user.jdoe.test.container.1234.1 | CONTAINER    |
+    | user.jdoe:user.jdoe.test.container.1234.2 | CONTAINER    |
+    | user.jdoe:user.jdoe.test.cont.1234.2      | CONTAINER    |
+    | user.jdoe:user.jdoe.test.dataset.1        | DATASET      |
+    | user.jdoe:user.jdoe.test.dataset.2        | DATASET      |
+    | user.jdoe:user.jdoe.test.data.1234.1      | DATASET      |
+    | user.jdoe:test.file.1                     | FILE         |
+    | user.jdoe:test.file.2                     | FILE         |
+    | user.jdoe:test.file.3                     | FILE         |
+    +-------------------------------------------+--------------+
+
+You can filter by key/value, e.g.::
+
+    $ rucio ls --filter type=CONTAINER
+    +-------------------------------------------+--------------+
+    | SCOPE:NAME                                | [DID TYPE]   |
+    |-------------------------------------------+--------------|
+    | user.jdoe:user.jdoe.test.container.1234.1 | CONTAINER    |
+    | user.jdoe:user.jdoe.test.container.1234.2 | CONTAINER    |
+    | user.jdoe:user.jdoe.test.cont.1234.2      | CONTAINER    |
+    +-------------------------------------------+--------------+
+    ''')
+
+    ls_parser.set_defaults(function=list_dids)
     ls_parser.add_argument('-r', '--recursive', dest='recursive', action='store_true', default=False, help='List data identifiers recursively.')
     ls_parser.add_argument('--filter', dest='filter', action='store', help='Filter arguments in form `key=value,another_key=next_value`. Valid keys are name, type.')
     ls_parser.add_argument('--short', dest='short', action='store_true', help='Just dump the list of DIDs.')
     ls_parser.add_argument(dest='did', nargs=1, action='store', default=None, help='Data IDentifier pattern.')
 
-    list_parser = subparsers.add_parser('list-dids', help='List the data identifiers matching some metadata (synonym for ls).', description='List the Data IDentifiers matching certain pattern.\
-                                         Only the collections (i.e. dataset or container) are returned by default.\
-                                         With the filter option, you can specify a list of metadata that the Data IDentifier should match.')
-    list_parser.set_defaults(which='list_dids')
+    list_parser = subparsers.add_parser('list-dids',
+                                        help='List the data identifiers matching some metadata (synonym for ls).',
+                                        description='''List the Data IDentifiers matching certain pattern.
+Only the collections (i.e. dataset or container) are returned by default.
+With the filter option, you can specify a list of metadata that the Data IDentifier should match.
+Please use the filter option `--filter type=all` to find all types of Data IDentifiers.''',
+                                        formatter_class=argparse.RawDescriptionHelpFormatter, epilog='''Usage example
+"""""""""""""
+
+You can query the DIDs matching a certain pattern. It always requires to specify the scope in which you want to search::
+
+    $ rucio list-dids --filter 'type=all' user.jdoe:*
+    +-------------------------------------------+--------------+
+    | SCOPE:NAME                                | [DID TYPE]   |
+    |-------------------------------------------+--------------|
+    | user.jdoe:user.jdoe.test.container.1234.1 | CONTAINER    |
+    | user.jdoe:user.jdoe.test.container.1234.2 | CONTAINER    |
+    | user.jdoe:user.jdoe.test.cont.1234.2      | CONTAINER    |
+    | user.jdoe:user.jdoe.test.dataset.1        | DATASET      |
+    | user.jdoe:user.jdoe.test.dataset.2        | DATASET      |
+    | user.jdoe:user.jdoe.test.data.1234.1      | DATASET      |
+    | user.jdoe:test.file.1                     | FILE         |
+    | user.jdoe:test.file.2                     | FILE         |
+    | user.jdoe:test.file.3                     | FILE         |
+    +-------------------------------------------+--------------+
+
+You can filter by key/value, e.g.::
+
+    $ rucio list-dids --filter 'type=CONTAINER'
+    +-------------------------------------------+--------------+
+    | SCOPE:NAME                                | [DID TYPE]   |
+    |-------------------------------------------+--------------|
+    | user.jdoe:user.jdoe.test.container.1234.1 | CONTAINER    |
+    | user.jdoe:user.jdoe.test.container.1234.2 | CONTAINER    |
+    | user.jdoe:user.jdoe.test.cont.1234.2      | CONTAINER    |
+    +-------------------------------------------+--------------+''')
+
+    list_parser.set_defaults(function=list_dids)
     list_parser.add_argument('--recursive', dest='recursive', action='store_true', default=False, help='List data identifiers recursively.')
-    list_parser.add_argument('--filter', dest='filter', action='store', help='Filter arguments in form `key=value,another_key=next_value`. Valid keys are name, type.')
+    list_parser.add_argument('--filter', dest='filter', action='store', help='Single or logically combined filtering expression(s) either in the form <key><operator><value> or <value1><operator1><key><operator2><value2> (compound inequality). Keys are equivalent to columns in the DID table. Operators must belong to the set of (<=, >=, ==, !=, >, <). The following conventions for combining expressions are used: ";" represents the logical OR operator; "," represents the logical AND operator.')  # noqa: E501
     list_parser.add_argument('--short', dest='short', action='store_true', help='Just dump the list of DIDs.')
     list_parser.add_argument(dest='did', nargs=1, action='store', default=None, help='Data IDentifier pattern')
 
+    # The extended version of list_dids that goes through the plugin mechanism
+    list_extended_parser = subparsers.add_parser('list-dids-extended',
+                                                 help='List the data identifiers matching some metadata (extended version to include metadata from various resources).',
+                                                 description='''List the Data IDentifiers matching certain pattern.
+Only the collections (i.e. dataset or container) are returned by default.
+With the filter option, you can specify a list of metadata that the Data IDentifier should match.
+Please use the filter option `--filter type=all` to find all types of Data IDentifiers.''',
+                                                 formatter_class=argparse.RawDescriptionHelpFormatter, epilog='''Usage example
+"""""""""""""
+
+You can query the DIDs matching a certain pattern. It always requires to specify the scope in which you want to search::
+
+    $ rucio list-dids --filter 'type=all' user.jdoe:*
+    +-------------------------------------------+--------------+
+    | SCOPE:NAME                                | [DID TYPE]   |
+    |-------------------------------------------+--------------|
+    | user.jdoe:user.jdoe.test.container.1234.1 | CONTAINER    |
+    | user.jdoe:user.jdoe.test.container.1234.2 | CONTAINER    |
+    | user.jdoe:user.jdoe.test.cont.1234.2      | CONTAINER    |
+    | user.jdoe:user.jdoe.test.dataset.1        | DATASET      |
+    | user.jdoe:user.jdoe.test.dataset.2        | DATASET      |
+    | user.jdoe:user.jdoe.test.data.1234.1      | DATASET      |
+    | user.jdoe:test.file.1                     | FILE         |
+    | user.jdoe:test.file.2                     | FILE         |
+    | user.jdoe:test.file.3                     | FILE         |
+    +-------------------------------------------+--------------+
+
+You can filter by key/value, e.g.::
+
+    $ rucio list-dids --filter 'type=CONTAINER'
+    +-------------------------------------------+--------------+
+    | SCOPE:NAME                                | [DID TYPE]   |
+    |-------------------------------------------+--------------|
+    | user.jdoe:user.jdoe.test.container.1234.1 | CONTAINER    |
+    | user.jdoe:user.jdoe.test.container.1234.2 | CONTAINER    |
+    | user.jdoe:user.jdoe.test.cont.1234.2      | CONTAINER    |
+    +-------------------------------------------+--------------+''')
+
+    list_extended_parser.set_defaults(function=list_dids_extended)
+
     # The list parent-dids command
-    list_parent_parser = subparsers.add_parser('list-parent-dids', help='List parent DIDs for a given DID', description='List all parents Data IDentifier that contains the target Data IDentifier.')
-    list_parent_parser.set_defaults(which='list_parent_dids')
+    list_parent_parser = subparsers.add_parser('list-parent-dids', help='List parent DIDs for a given DID', description='List all parents Data IDentifier that contains the target Data IDentifier.',
+                                               formatter_class=argparse.RawDescriptionHelpFormatter, epilog='''Usage example
+"""""""""""""
+::
+
+    $ rucio list-parent-dids user.jdoe:user.jdoe.test.data.1234.1
+    +--------------------------------------+--------------+
+    | SCOPE:NAME                           | [DID TYPE]   |
+    |--------------------------------------+--------------|
+    | user.jdoe:user.jdoe.test.cont.1234.2 | CONTAINER    |
+    +--------------------------------------+--------------+
+
+    ''')
+    list_parent_parser.set_defaults(function=list_parent_dids)
     list_parent_parser.add_argument(dest='did', action='store', nargs='?', default=None, help='Data identifier.')
     list_parent_parser.add_argument('--pfn', dest='pfns', action='store', nargs='+', help='List parent dids for these pfns.')
     list_parent_parser.add_argument('--guid', dest='guids', action='store', nargs='+', help='List parent dids for these guids.')
 
+    # argparse 2.7 does not allow aliases for commands, thus the list-parent-datasets is a copy&paste from list-parent-dids
+    list_parent_datasets_parser = subparsers.add_parser('list-parent-datasets', help='List parent DIDs for a given DID', description='List all parents Data IDentifier that contains the target Data IDentifier.',
+                                                        formatter_class=argparse.RawDescriptionHelpFormatter, epilog='''Usage example
+"""""""""""""
+::
+
+    $ rucio list-parent-datasets user.jdoe:user.jdoe.test.data.1234.1
+    +--------------------------------------+--------------+
+    | SCOPE:NAME                           | [DID TYPE]   |
+    |--------------------------------------+--------------|
+    | user.jdoe:user.jdoe.test.cont.1234.2 | CONTAINER    |
+    +--------------------------------------+--------------+
+
+    ''')
+
+    list_parent_datasets_parser.set_defaults(function=list_parent_dids)
+    list_parent_datasets_parser.add_argument(dest='did', action='store', nargs='?', default=None, help='Data identifier.')
+    list_parent_datasets_parser.add_argument('--pfn', dest='pfns', action='store', nargs='+', help='List parent dids for these pfns.')
+    list_parent_datasets_parser.add_argument('--guid', dest='guids', action='store', nargs='+', help='List parent dids for these guids.')
+
     # The list-scopes command
-    scope_list_parser = subparsers.add_parser('list-scopes', help='List all available scopes.')
-    scope_list_parser.set_defaults(which='list_scopes')
+    scope_list_parser = subparsers.add_parser('list-scopes', help='List all available scopes.',
+                                              formatter_class=argparse.RawDescriptionHelpFormatter, epilog='''Usage example
+"""""""""""""
+::
+
+    $ rucio list-scopes
+    mc
+    data
+    user.jdoe
+    user.janedoe
+
+    ''')
+
+    scope_list_parser.set_defaults(function=list_scopes)
 
     # The close command
     close_parser = subparsers.add_parser('close', help='Close a dataset or container.')
-    close_parser.set_defaults(which='close')
+    close_parser.set_defaults(function=close)
     close_parser.add_argument(dest='dids', nargs='+', action='store', help='List of space separated data identifiers.')
 
     # The reopen command
     reopen_parser = subparsers.add_parser('reopen', help='Reopen a dataset or container (only for privileged users).')
-    reopen_parser.set_defaults(which='reopen')
+    reopen_parser.set_defaults(function=reopen)
     reopen_parser.add_argument(dest='dids', nargs='+', action='store', help='List of space separated data identifiers.')
 
     # The stat command
     stat_parser = subparsers.add_parser('stat', help='List attributes and statuses about data identifiers.')
-    stat_parser.set_defaults(which='stat')
+    stat_parser.set_defaults(function=stat)
     stat_parser.add_argument(dest='dids', nargs='+', action='store', help='List of space separated data identifiers.')
 
     # The erase command
     erase_parser = subparsers.add_parser('erase', help='Delete a data identifier.', description='This command sets the lifetime of the DID in order to expire in the next 24 hours.\
             After this time, the dataset is eligible for deletion. The deletion is not reversible after 24 hours grace time period expired.')
-    erase_parser.set_defaults(which='erase')
+    erase_parser.set_defaults(function=erase)
     erase_parser.add_argument('--undo', dest='undo', action='store_true', default=False, help='Undo erase DIDs. Only works if has been less than 24 hours since erase operation.')
     erase_parser.add_argument(dest='dids', nargs='+', action='store', help='List of space separated data identifiers.')
 
     # The list_files command
     list_files_parser = subparsers.add_parser('list-files', help='List DID contents', description='List all the files in a Data IDentifier. The DID can be a container, dataset or a file.\
-                                                                  What is returned is a list of files in the DID with : <scope>:<name>\t<filesize>\t<checksum>\t<guid>')
-    list_files_parser.set_defaults(which='list_files')
+                                                                  What is returned is a list of files in the DID with : <scope>:<name>\t<guid>\t<checksum>\t<filesize>')
+    list_files_parser.set_defaults(function=list_files)
     list_files_parser.add_argument('--csv', dest='csv', action='store_true', default=False, help='Comma Separated Value output. This output format is preferred for easy parsing and scripting.')
     list_files_parser.add_argument('--pfc', dest='LOCALPATH', action='store', default=False, help='Outputs the list of files in the dataset with the LOCALPATH prepended as a PoolFileCatalog')
     list_files_parser.add_argument(dest='dids', nargs='+', action='store', help='List of space separated data identifiers.')
 
     # The list_content command
     list_content_parser = subparsers.add_parser('list-content', help='List the content of a collection.')
-    list_content_parser.set_defaults(which='list_content')
+    list_content_parser.set_defaults(function=list_content)
     list_content_parser.add_argument(dest='dids', nargs='+', action='store', help='List of space separated data identifiers.')
+    list_content_parser.add_argument('--short', dest='short', action='store_true', help='Just dump the list of DIDs.')
 
     # The list_content_history command
     list_content_history_parser = subparsers.add_parser('list-content-history', help='List the content history of a collection.')
-    list_content_history_parser.set_defaults(which='list_content_history')
+    list_content_history_parser.set_defaults(function=list_content_history)
     list_content_history_parser.add_argument(dest='dids', nargs='+', action='store', help='List of space separated data identifiers.')
 
-    # The upload subparser
+    # The list-impls command
+    list_impls_parser = subparsers.add_parser('list-impls', help='List all supported protocol implementations.')
+    list_impls_parser.set_defaults(function=list_impls)
+
+# The upload subparser
     upload_parser = subparsers.add_parser('upload', help='Upload method.')
-    upload_parser.set_defaults(which='upload')
-    upload_parser.add_argument('--rse', dest='rse', action='store', help='Rucio Storage Element (RSE) name.', required=True)
+    upload_parser.set_defaults(function=upload)
+    upload_parser.add_argument('--rse', dest='rse', action='store', help='Rucio Storage Element (RSE) name.', required=True).completer = rse_completer
     upload_parser.add_argument('--lifetime', type=int, action='store', help='Lifetime of the rule in seconds.')
+    upload_parser.add_argument('--expiration-date', action='store', help='The date when the rule expires in UTC, format: <year>-<month>-<day>-<hour>:<minute>:<second>. E.g. 2022-10-20-20:00:00')
     upload_parser.add_argument('--scope', dest='scope', action='store', help='Scope name.')
+    upload_parser.add_argument('--impl', dest='impl', action='store', help='Transfer protocol implementation to use (e.g: xrootd, gfal.NoRename, webdav, ssh.Rsync, rclone).')
     # The --no-register option is hidden. This is pilot ONLY. Users should not use this. Will lead to unregistered data on storage!
     upload_parser.add_argument('--no-register', dest='no_register', action='store_true', default=False, help=argparse.SUPPRESS)
-    upload_parser.add_argument('--guid', dest='guid', action='store', help="Manually specify the GUID for the file.")
+    upload_parser.add_argument('--register-after-upload', dest='register_after_upload', action='store_true', default=False, help='Register the file only after successful upload.')
+    upload_parser.add_argument('--summary', dest='summary', action='store_true', default=False, help='Create rucio_upload.json summary file')
+    upload_parser.add_argument('--guid', dest='guid', action='store', help='Manually specify the GUID for the file.')
     upload_parser.add_argument('--protocol', action='store', help='Force the protocol to use')
+    upload_parser.add_argument('--pfn', dest='pfn', action='store', help='Specify the exact PFN for the upload.')
+    upload_parser.add_argument('--name', dest='name', action='store', help='Specify the exact LFN for the upload.')
+    upload_parser.add_argument('--transfer-timeout', dest='transfer_timeout', type=float, action='store', default=config_get_float('upload', 'transfer_timeout', False, 360), help='Transfer timeout (in seconds).')
     upload_parser.add_argument(dest='args', action='store', nargs='+', help='files and datasets.')
+    upload_parser.add_argument('--recursive', dest='recursive', action='store_true', default=False, help='Convert recursively the folder structure into collections')
 
-    # The download subparser
+    # The download and get subparser
     get_parser = subparsers.add_parser('get', help='Download method (synonym for download)')
-    get_parser.set_defaults(which='download')
-    get_parser.add_argument('--dir', dest='dir', default='.', action='store', help='The directory to store the downloaded file.')
-    get_parser.add_argument(dest='dids', nargs='+', action='store', help='List of space separated data identifiers.')
-    get_parser.add_argument('--rse', action='store', help='RSE Expression to specify allowed sources.')
-    get_parser.add_argument('--protocol', action='store', help='Force the protocol to use.')
-    get_parser.add_argument('--nrandom', type=int, action='store', help='Download N random files from the DID.')
-    get_parser.add_argument('--ndownloader', type=int, default=3, action='store', help='Choose the number of parallel processes for download.')
-    get_parser.add_argument('--no-subdir', action='store_true', default=False, help="Don't create a subdirectory for the scope of the files. Existing files in the directory will be overwritten.")
-    get_parser.add_argument('--old', action='store_true', default=False, help="Choose the old download thread model.")
-    get_parser.add_argument('--pfn', dest='pfn', action='store', help="Specify the exact PFN for the download.")
-
     download_parser = subparsers.add_parser('download', help='Download method (synonym for get)')
-    download_parser.set_defaults(which='download')
-    download_parser.add_argument('--dir', dest='dir', default='.', action='store', help='The directory to store the downloaded file.')
-    download_parser.add_argument(dest='dids', nargs='+', action='store', help='List of space separated data identifiers.')
-    download_parser.add_argument('--rse', action='store', help='RSE Expression to specify allowed sources')
-    download_parser.add_argument('--protocol', action='store', help='Force the protocol to use.')
-    download_parser.add_argument('--nrandom', type=int, action='store', help='Download N random files from the DID.')
-    download_parser.add_argument('--ndownloader', type=int, default=3, action='store', help='Choose the number of parallel processes for download.')
-    download_parser.add_argument('--no-subdir', action='store_true', default=False, help="Don't create a subdirectory for the scope of the files. Existing files in the directory will be overwritten.")
-    download_parser.add_argument('--old', action='store_true', default=False, help="Choose the old download thread model.")
-    download_parser.add_argument('--pfn', dest='pfn', action='store', help="Specify the exact PFN for the download.")
+    for selected_parser in [get_parser, download_parser]:
+        selected_parser.set_defaults(function=download)
+        selected_parser.add_argument('--dir', dest='dir', default='.', action='store', help='The directory to store the downloaded file.')
+        selected_parser.add_argument(dest='dids', nargs='*', action='store', help='List of space separated data identifiers.')
+        selected_parser.add_argument('--allow-tape', action='store_true', default=False, help="Also consider tape endpoints as source of the download.")
+        selected_parser.add_argument('--rses', action='store', help='RSE Expression to specify allowed sources')
+        selected_parser.add_argument('--impl', dest='impl', action='store', help='Transfer protocol implementation to use (e.g: xrootd, gfal.NoRename, webdav, ssh.Rsync, rclone).')
+        selected_parser.add_argument('--protocol', action='store', help='Force the protocol to use.')
+        selected_parser.add_argument('--nrandom', type=int, action='store', help='Download N random files from the DID.')
+        selected_parser.add_argument('--ndownloader', type=int, default=3, action='store', help='Choose the number of parallel processes for download.')
+        selected_parser.add_argument('--no-subdir', action='store_true', default=False, help="Don't create a subdirectory for the scope of the files.")
+        selected_parser.add_argument('--pfn', dest='pfn', action='store', help="Specify the exact PFN for the download.")
+        selected_parser.add_argument('--archive-did', action='store', dest='archive_did', help="Download from archive is transparent. This option is obsolete.")
+        selected_parser.add_argument('--no-resolve-archives', action='store_true', default=False, help="If set archives will not be considered for download.")
+        selected_parser.add_argument('--ignore-checksum', action='store_true', default=False, help="Don't validate checksum for downloaded files.")
+        selected_parser.add_argument('--check-local-with-filesize-only', action='store_true', default=False, help="Don't use checksum verification for already downloaded files, use filesize instead.")
+        selected_parser.add_argument('--transfer-timeout', dest='transfer_timeout', type=float, action='store', default=config_get_float('download', 'transfer_timeout', False, None), help='Transfer timeout (in seconds). Default: computed dynamically from --transfer-speed-timeout. If set to any value >= 0, --transfer-speed-timeout is ignored.')  # NOQA: E501
+        selected_parser.add_argument('--transfer-speed-timeout', dest='transfer_speed_timeout', type=float, action='store', default=None, help='Minimum allowed average transfer speed (in KBps). Default: 500. Used to dynamically compute the timeout if --transfer-timeout not set. Is not supported for --pfn.')  # NOQA: E501
+        selected_parser.add_argument('--aria', action='store_true', default=False, help="Use aria2c utility if possible. (EXPERIMENTAL)")
+        selected_parser.add_argument('--trace_appid', '--trace-appid', new_option_string='--trace-appid', dest='trace_appid', action=StoreAndDeprecateWarningAction, default=os.environ.get('RUCIO_TRACE_APPID', None), help=argparse.SUPPRESS)
+        selected_parser.add_argument('--trace_dataset', '--trace-dataset', new_option_string='--trace-dataset', dest='trace_dataset', action=StoreAndDeprecateWarningAction, default=os.environ.get('RUCIO_TRACE_DATASET', None), help=argparse.SUPPRESS)
+        selected_parser.add_argument('--trace_datasetscope', '--trace-datasetscope', new_option_string='--trace-datasetscope', dest='trace_datasetscope', action=StoreAndDeprecateWarningAction, default=os.environ.get('RUCIO_TRACE_DATASETSCOPE', None), help=argparse.SUPPRESS)  # NOQA: E501
+        selected_parser.add_argument('--trace_eventtype', '--trace-eventtype', new_option_string='--trace-eventtype', dest='trace_eventtype', action=StoreAndDeprecateWarningAction, default=os.environ.get('RUCIO_TRACE_EVENTTYPE', None), help=argparse.SUPPRESS)  # NOQA: E501
+        selected_parser.add_argument('--trace_pq', '--trace-pq', new_option_string='--trace-pq', dest='trace_pq', action=StoreAndDeprecateWarningAction, default=os.environ.get('RUCIO_TRACE_PQ', None), help=argparse.SUPPRESS)
+        selected_parser.add_argument('--trace_taskid', '--trace-taskid', new_option_string='--trace-taskid', dest='trace_taskid', action=StoreAndDeprecateWarningAction, default=os.environ.get('RUCIO_TRACE_TASKID', None), help=argparse.SUPPRESS)
+        selected_parser.add_argument('--trace_usrdn', '--trace-usrdn', new_option_string='--trace-usrdn', dest='trace_usrdn', action=StoreAndDeprecateWarningAction, default=os.environ.get('RUCIO_TRACE_USRDN', None), help=argparse.SUPPRESS)
+        selected_parser.add_argument('--filter', dest='filter', action='store', help='Filter files by key-value pairs like guid=2e2232aafac8324db452070304f8d745.')
+        selected_parser.add_argument('--scope', dest='scope', action='store', help='Scope if you are using the filter option and no full DID.')
+        selected_parser.add_argument('--metalink', dest='metalink_file', action='store', help='Path to a metalink file.')
+        selected_parser.add_argument('--deactivate-file-download-exceptions', dest='deactivate_file_download_exceptions', action='store_true', help='Does not raise NoFilesDownloaded, NotAllFilesDownloaded or incorrect number of output queue files Exception.')  # NOQA: E501
+        selected_parser.add_argument('--replica-selection', dest='sort', action='store', help='Select the best replica using a replica sorting algorithm provided by replica sorter (e.g., random, geoip).')
 
     # The get-metadata subparser
     get_metadata_parser = subparsers.add_parser('get-metadata', help='Get metadata for DIDs.')
-    get_metadata_parser.set_defaults(which='get_metadata')
+    get_metadata_parser.set_defaults(function=get_metadata)
     get_metadata_parser.add_argument(dest='dids', nargs='+', action='store', help='List of space separated data identifiers.')
+    get_metadata_parser.add_argument('--plugin', dest='plugin', action='store', help='Filter down to metadata from specific metadata plugin', required=False)
 
     # The set-metadata subparser
     set_metadata_parser = subparsers.add_parser('set-metadata', help='set-metadata method')
-    set_metadata_parser.set_defaults(which='set_metadata')
-    set_metadata_parser.add_argument('--did', dest='did', action='store', help='Data identifier to add', required=True)
+    set_metadata_parser.set_defaults(function=set_metadata)
+    set_metadata_parser.add_argument('--did', dest='did', action='store', help='Data identifier whose metadata will be set', required=True)
     set_metadata_parser.add_argument('--key', dest='key', action='store', help='Attribute key', required=True)
     set_metadata_parser.add_argument('--value', dest='value', action='store', help='Attribute value', required=True)
 
-    # The delete-metadata subparser
-    # delete_metadata_parser = subparsers.add_parser('delete-metadata', help='Delete metadata')
-    # delete_metadata_parser.set_defaults(which='delete_metadata')
+    # delete-did-meta subparser
+    delete_metadata_parser = subparsers.add_parser('delete-metadata', help='delete metadata')
+    delete_metadata_parser.set_defaults(function=delete_metadata)
+    delete_metadata_parser.add_argument('--did', dest='did', action='store', help='Data identifier to delete', required=True)
+    delete_metadata_parser.add_argument('--key', dest='key', action='store', help='Attribute key', required=True)
 
     # The list-rse-usage subparser
     list_rse_usage_parser = subparsers.add_parser('list-rse-usage', help='Shows the total/free/used space for a given RSE. This values can differ for different RSE source.')
-    list_rse_usage_parser.set_defaults(which='list_rse_usage')
-    list_rse_usage_parser.add_argument(dest='rse', action='store', help='Rucio Storage Element (RSE) name.')
+    list_rse_usage_parser.set_defaults(function=list_rse_usage)
+    list_rse_usage_parser.add_argument(dest='rse', action='store', help='Rucio Storage Element (RSE) name.').completer = rse_completer
     list_rse_usage_parser.add_argument('--history', dest='history', default=False, action='store', help='List RSE usage history. [Unimplemented]')
+    list_rse_usage_parser.add_argument('--show-accounts', dest='show_accounts', action='store_true', default=False, help='List accounts usages of RSE')
 
     # The list-account-usage subparser
     list_account_usage_parser = subparsers.add_parser('list-account-usage', help='Shows the space used, the quota limit and the quota left for an account for every RSE where the user have quota.')
-    list_account_usage_parser.set_defaults(which='list_account_usage')
+    list_account_usage_parser.set_defaults(function=list_account_usage)
     list_account_usage_parser.add_argument(dest='usage_account', action='store', help='Account name.')
     list_account_usage_parser.add_argument('--rse', action='store', help='Show usage for only for this RSE.')
 
     # The list-account-limits subparser
     list_account_limits_parser = subparsers.add_parser('list-account-limits', help='List quota limits for an account in every RSEs.')
-    list_account_limits_parser.set_defaults(which='list_account_limits')
+    list_account_limits_parser.set_defaults(function=list_account_limits)
     list_account_limits_parser.add_argument('limit_account', action='store', help='The account name.')
-    list_account_limits_parser.add_argument('--rse', dest='rse', action='store', help='If this option is given, the results are restricted to only this RSE.')
+    list_account_limits_parser.add_argument('--rse', dest='rse', action='store', help='If this option is given, the results are restricted to only this RSE.').completer = rse_completer
 
     # Add replication rule subparser
     add_rule_parser = subparsers.add_parser('add-rule', help='Add replication rule.')
-    add_rule_parser.set_defaults(which='add_rule')
+    add_rule_parser.set_defaults(function=add_rule)
     add_rule_parser.add_argument(dest='dids', action='store', nargs='+', help='DID(s) to apply the rule to')
     add_rule_parser.add_argument(dest='copies', action='store', type=int, help='Number of copies')
     add_rule_parser.add_argument(dest='rse_expression', action='store', help='RSE Expression')
     add_rule_parser.add_argument('--weight', dest='weight', action='store', help='RSE Weight')
     add_rule_parser.add_argument('--lifetime', dest='lifetime', action='store', type=int, help='Rule lifetime (in seconds)')
     add_rule_parser.add_argument('--grouping', dest='grouping', action='store', choices=['DATASET', 'ALL', 'NONE'], help='Rule grouping')
     add_rule_parser.add_argument('--locked', dest='locked', action='store_true', help='Rule locking')
-    add_rule_parser.add_argument('--source-replica-expression', dest='source_replica_expression', action='store', help='Source Replica Expression')
+    add_rule_parser.add_argument('--source-replica-expression', dest='source_replica_expression', action='store', help='RSE Expression for RSEs to be considered for source replicas')
     add_rule_parser.add_argument('--notify', dest='notify', action='store', help='Notification strategy : Y (Yes), N (No), C (Close)')
-    add_rule_parser.add_argument('--activity', dest='activity', action='store', help='Activity to be used (e.g. User, Data Consolidation')
+    add_rule_parser.add_argument('--activity', dest='activity', action='store', help='Activity to be used (e.g. User, Data Consolidation)')
     add_rule_parser.add_argument('--comment', dest='comment', action='store', help='Comment about the replication rule')
     add_rule_parser.add_argument('--ask-approval', dest='ask_approval', action='store_true', help='Ask for rule approval')
     add_rule_parser.add_argument('--asynchronous', dest='asynchronous', action='store_true', help='Create rule asynchronously')
+    add_rule_parser.add_argument('--delay-injection', dest='delay_injection', action='store', type=int, help='Delay (in seconds) to wait before starting applying the rule. This option implies --asynchronous.')
     add_rule_parser.add_argument('--account', dest='rule_account', action='store', help='The account owning the rule')
+    add_rule_parser.add_argument('--skip-duplicates', dest='ignore_duplicate', action='store_true', help='Skip duplicate rules')
 
     # Delete replication rule subparser
     delete_rule_parser = subparsers.add_parser('delete-rule', help='Delete replication rule.')
-    delete_rule_parser.set_defaults(which='delete_rule')
+    delete_rule_parser.set_defaults(function=delete_rule)
     delete_rule_parser.add_argument(dest='rule_id', action='store', help='Rule id or DID. If DID, the RSE expression is mandatory.')
     delete_rule_parser.add_argument('--purge-replicas', dest='purge_replicas', action='store_true', help='Purge rule replicas')
     delete_rule_parser.add_argument('--all', dest='delete_all', action='store_true', default=False, help='Delete all the rules, even the ones that are not owned by the account')
-    delete_rule_parser.add_argument('--rse_expression', dest='rse_expression', action='store', help='The RSE expression. Must be specified if a DID is provided.')
+    delete_rule_parser.add_argument('--rses', dest='rses', action='store', help='The RSE expression. Must be specified if a DID is provided.')
     delete_rule_parser.add_argument('--account', dest='rule_account', action='store', help='The account of the rule that must be deleted')
 
     # Info replication rule subparser
     info_rule_parser = subparsers.add_parser('rule-info', help='Retrieve information about a rule.')
-    info_rule_parser.set_defaults(which='info_rule')
+    info_rule_parser.set_defaults(function=info_rule)
     info_rule_parser.add_argument(dest='rule_id', action='store', help='The rule ID')
     info_rule_parser.add_argument('--examine', dest='examine', action='store_true', help='Detailed analysis of transfer errors')
+    info_rule_parser.add_argument('--estimate-ttc', dest='estimate_ttc', action='store_true', help='Deprecated, specifying this will cause the program to fail.')
 
     # The list_rules command
-    list_rules_parser = subparsers.add_parser('list-rules', help='List replication rules.')
-    list_rules_parser.set_defaults(which='list_rules')
+    list_rules_parser = subparsers.add_parser('list-rules', help='List replication rules.', formatter_class=argparse.RawDescriptionHelpFormatter, epilog='''Usage example
+"""""""""""""
+
+You can list the rules for a particular DID::
+
+    $ rucio list-rules user.jdoe:user.jdoe.test.container.1234.1
+    ID                                ACCOUNT    SCOPE:NAME                                 STATE[OK/REPL/STUCK]    RSE_EXPRESSION        COPIES  EXPIRES (UTC)
+    --------------------------------  ---------  -----------------------------------------  ----------------------  ------------------  --------  -------------------
+    a12e5664555a4f12b3cc6991db5accf9  jdoe       user.jdoe:user.jdoe.test.container.1234.1  OK[3/0/0]               tier=1&disk=1       1         2018-02-09 03:57:46
+    b0fcde2acbdb489b874c3c4537595adc  janedoe    user.jdoe:user.jdoe.test.container.1234.1  REPLICATING[4/1/1]      tier=1&tape=1       2
+    4a6bd85c13384bd6836fbc06e8b316d7  mc         user.jdoe:user.jdoe.test.container.1234.1  OK[3/0/0]               tier=1&tape=1       2
+
+You can filter by account::
+
+    $ rucio list-rules --account jdoe
+    ID                                ACCOUNT    SCOPE:NAME                                 STATE[OK/REPL/STUCK]    RSE_EXPRESSION        COPIES  EXPIRES (UTC)
+    --------------------------------  ---------  -----------------------------------------  ----------------------  ------------------  --------  -------------------
+    a12e5664555a4f12b3cc6991db5accf9  jdoe       user.jdoe:user.jdoe.test.container.1234.1  OK[3/0/0]               tier=1&disk=1       1         2018-02-09 03:57:46
+    08537b2176843d92e05317938a89d148  jdoe       user.jdoe:user.jdoe.test.data.1234.1       OK[2/0/0]               SITE2_DISK          1
+
+                                              ''')
+
+    list_rules_parser.set_defaults(function=list_rules)
     list_rules_parser.add_argument(dest='did', action='store', nargs='?', default=None, help='List by did')
     list_rules_parser.add_argument('--id', dest='rule_id', action='store', help='List by rule id')
     list_rules_parser.add_argument('--traverse', dest='traverse', action='store_true', help='Traverse the did tree and search for rules affecting this did')
     list_rules_parser.add_argument('--csv', dest='csv', action='store_true', default=False, help='Comma Separated Value output')
     list_rules_parser.add_argument('--file', dest='file', action='store', help='List associated rules of an affected file')
     list_rules_parser.add_argument('--account', dest='rule_account', action='store', help='List by account')
     list_rules_parser.add_argument('--subscription', dest='subscription', action='store', help='List by account and subscription name', metavar=('ACCOUNT', 'SUBSCRIPTION'), nargs=2)
 
     # The list_rules_history command
     list_rules_history_parser = subparsers.add_parser('list-rules-history', help='List replication rules history for a DID.')
-    list_rules_history_parser.set_defaults(which='list_rules_history')
+    list_rules_history_parser.set_defaults(function=list_rules_history)
     list_rules_history_parser.add_argument(dest='did', action='store', help='The Data IDentifier.')
 
     # The update_rule command
     update_rule_parser = subparsers.add_parser('update-rule', help='Update replication rule.')
-    update_rule_parser.set_defaults(which='update_rule')
+    update_rule_parser.set_defaults(function=update_rule)
     update_rule_parser.add_argument(dest='rule_id', action='store', help='Rule id')
     update_rule_parser.add_argument('--lifetime', dest='lifetime', action='store', help='Lifetime in seconds.')
     update_rule_parser.add_argument('--locked', dest='locked', action='store', help='Locked (True/False).')
     update_rule_parser.add_argument('--account', dest='rule_account', action='store', help='Account to change.')
     update_rule_parser.add_argument('--stuck', dest='state_stuck', action='store_true', help='Set state to STUCK.')
     update_rule_parser.add_argument('--suspend', dest='state_suspended', action='store_true', help='Set state to SUSPENDED.')
     update_rule_parser.add_argument('--activity', dest='rule_activity', action='store', help='Activity of the rule.')
     update_rule_parser.add_argument('--source-replica-expression', dest='source_replica_expression', action='store', help='Source replica expression of the rule.')
+    update_rule_parser.add_argument('--comment', dest='comment', action='store', help="Update comment for the rule")
     update_rule_parser.add_argument('--cancel-requests', dest='cancel_requests', action='store_true', help='Cancel requests when setting rules to stuck.')
     update_rule_parser.add_argument('--priority', dest='priority', action='store', help='Priority of the requests of the rule.')
-    update_rule_parser.add_argument('--child-rule-id', dest='child_rule_id', action='store', help='Child rule id of the rule.')
+    update_rule_parser.add_argument('--child-rule-id', dest='child_rule_id', action='store', help='Child rule id of the rule. Use "None" to remove an existing parent/child relationship.')
+    update_rule_parser.add_argument('--boost-rule', dest='boost_rule', action='store_true', help='Quickens the transition of a rule from STUCK to REPLICATING.')
+
+    # The move_rule command
+    move_rule_parser = subparsers.add_parser('move-rule', help='Move a replication rule to another RSE.')
+    move_rule_parser.set_defaults(function=move_rule)
+    move_rule_parser.add_argument(dest='rule_id', action='store', help='Rule id')
+    move_rule_parser.add_argument(dest='rse_expression', action='store', help='RSE expression of new rule')
+    move_rule_parser.add_argument('--activity', dest='activity', action='store', help='Update activity for moved rule.')
+    move_rule_parser.add_argument('--source-replica-expression', dest='source_replica_expression', action='store', help='Update source-replica-expression for moved rule. Use "None" to remove the old value.')
 
     # The list-rses command
     list_rses_parser = subparsers.add_parser('list-rses', help='Show the list of all the registered Rucio Storage Elements (RSEs).')
-    list_rses_parser.set_defaults(which='list_rses')
-    list_rses_parser.add_argument('--expression', dest='rse_expression', action='store', help='The RSE filter expression. A comprehensive help about RSE expressions\
-            can be found in ' + Color.BOLD + 'http://rucio.cern.ch/client_tutorial.html#adding-rules-for-replication' + Color.END)
+    list_rses_parser.set_defaults(function=list_rses)
+    list_rses_parser.add_argument('--rses', dest='rses', action='store', help='The RSE filter expression. A comprehensive help about RSE expressions \
+can be found in ' + Color.BOLD + 'http://rucio.cern.ch/documentation/rse_expressions/' + Color.END)
+
+    # The list-suspicious-replicas command
+    list_suspicious_replicas_parser = subparsers.add_parser('list-suspicious-replicas', help='Show the list of all replicas marked "suspicious".')
+    list_suspicious_replicas_parser.set_defaults(function=list_suspicious_replicas)
+    list_suspicious_replicas_parser.add_argument('--expression', dest='rse_expression', action='store', help='The RSE filter expression. A comprehensive help about RSE expressions \
+can be found in ' + Color.BOLD + 'http://rucio.cern.ch/documentation/rse_expressions/' + Color.END)
+    list_suspicious_replicas_parser.add_argument('--younger_than', '--younger-than', new_option_string='--younger-than', dest='younger_than', action=StoreAndDeprecateWarningAction, help='List files that have been marked suspicious since the date "younger_than", e.g. 2021-11-29T00:00:00.')  # NOQA: E501
+    list_suspicious_replicas_parser.add_argument('--nattempts', dest='nattempts', action='store', help='Minimum number of failed attempts to access a suspicious file.')
 
     # The list-rses-attributes command
     list_rse_attributes_parser = subparsers.add_parser('list-rse-attributes', help='List the attributes of an RSE.', description='This command is useful to create RSE filter expressions.')
-    list_rse_attributes_parser.set_defaults(which='list_rse-attributes')
-    list_rse_attributes_parser.add_argument(dest='rse', action='store', help='The RSE name')
+    list_rse_attributes_parser.set_defaults(function=list_rse_attributes)
+    list_rse_attributes_parser.add_argument(dest='rse', action='store', help='The RSE name').completer = rse_completer
 
     # The list-datasets-rse command
     list_datasets_rse_parser = subparsers.add_parser('list-datasets-rse', help='List all the datasets at a RSE', description='This method allows to list all the datasets on a given Rucio Storage Element.\
         ' + Color.BOLD + 'Warning: ' + Color.END + 'This command can take a long time depending on the number of datasets in the RSE.')
-    list_datasets_rse_parser.set_defaults(which='list_datasets_rse')
-    list_datasets_rse_parser.add_argument(dest='rse', action='store', default=None, help='The RSE name')
+    list_datasets_rse_parser.set_defaults(function=list_datasets_rse)
+    list_datasets_rse_parser.add_argument(dest='rse', action='store', default=None, help='The RSE name').completer = rse_completer
     list_datasets_rse_parser.add_argument('--long', dest='long', action='store_true', default=False, help='The long option')
 
     # The test-server command
-    test_server_parser = subparsers.add_parser('test-server', help='Test Server', description='Run a battery of nosetests against the Rucio Servers.')
-    test_server_parser.set_defaults(which='test_server')
+    test_server_parser = subparsers.add_parser('test-server', help='Test Server', description='Run a bunch of tests against the Rucio Servers.')
+    test_server_parser.set_defaults(function=test_server)
+
+    # The get-metadata subparser
+    touch_parser = subparsers.add_parser('touch', help='Touch one or more DIDs and set the last accessed date to the current date')
+    touch_parser.set_defaults(function=touch)
+    touch_parser.add_argument(dest='dids', nargs='+', action='store', help='List of space separated data identifiers.')
+    touch_parser.add_argument('--rse', dest='rse', action='store', help="The RSE of the DIDs that are touched.").completer = rse_completer
+
+    # The add-lifetime-exception command
+    add_lifetime_exception_parser = subparsers.add_parser('add-lifetime-exception', help='Add an exception to the lifetime model.',
+                                                          formatter_class=argparse.RawDescriptionHelpFormatter, epilog='''Usage example
+"""""""""""""
+::
+
+    $ rucio add-lifetime-exception --inputfile myfile.txt --reason "Needed for my analysis" --expiration 2015-10-30
+
+    ''')
+
+    add_lifetime_exception_parser.set_defaults(function=add_lifetime_exception)
+    add_lifetime_exception_parser.add_argument('--inputfile', action='store', help='File where the list of datasets requested to be extended are located.', required=True)
+    add_lifetime_exception_parser.add_argument('--reason', action='store', help='The reason for the extension.', required=True)
+    add_lifetime_exception_parser.add_argument('--expiration', action='store', help='The expiration date format YYYY-MM-DD', required=True)
 
-    argcomplete.autocomplete(oparser)
+    return oparser
+
+
+if __name__ == '__main__':
+    arguments = sys.argv[1:]
+    # set the configuration before anything else, if the config parameter is present
+    for argi in range(len(arguments)):
+        if arguments[argi] == '--config' and (argi + 1) < len(arguments):
+            os.environ['RUCIO_CONFIG'] = arguments[argi + 1]
+
+    oparser = get_parser()
+    if EXTRA_MODULES['argcomplete']:
+        argcomplete.autocomplete(oparser)
 
     if len(sys.argv) == 1:
         oparser.print_help()
         sys.exit(FAILURE)
 
-    args = oparser.parse_args(sys.argv[1:])
+    args = oparser.parse_args(arguments)
 
-    commands = {'add_container': add_container,
-                'add_dataset': add_dataset,
-                'add_rule': add_rule,
-                'attach': attach,
-                'close': close,
-                'reopen': reopen,
-                'erase': erase,
-                'delete_rule': delete_rule,
-                'detach': detach,
-                'download': download,
-                'get': download,
-                'get_metadata': get_metadata,
-                'info_rule': info_rule,
-                'list_account_limits': list_account_limits,
-                'list_account_usage': list_account_usage,
-                'list_datasets_rse': list_datasets_rse,
-                'list_parent_dids': list_parent_dids,
-                'list_files': list_files,
-                'list_content': list_content,
-                'list_content_history': list_content_history,
-                'list_dids': list_dids,
-                'list_dataset_replicas': list_dataset_replicas,
-                'list_file_replicas': list_file_replicas,
-                'list_parent_datasets': list_parent_datasets,
-                'list_rses': list_rses,
-                'list_rse-attributes': list_rse_attributes,
-                'list_rse_usage': list_rse_usage,
-                'list_rules': list_rules,
-                'list_rules_history': list_rules_history,
-                'list_scopes': list_scopes,
-                'ping': ping,
-                'set_metadata': set_metadata,
-                'stat': stat,
-                'test_server': test_server,
-                'update_rule': update_rule,
-                'upload': upload,
-                'whoami_account': whoami_account}
-
-    try:
-        if args.verbose:
-            logger.setLevel(logging.DEBUG)
-        start_time = time.time()
-        command = commands.get(args.which)
-        result = command(args)
-        end_time = time.time()
-        if args.verbose:
-            print "Completed in %-0.4f sec." % (end_time - start_time)
-        sys.exit(result)
-    except Exception, error:
-        logger.error("Strange error {0}".format(error))
-        sys.exit(FAILURE)
+    logger = setup_logger(module_name=__name__, logger_name='user', verbose=args.verbose)
+    start_time = time.time()
+    result = args.function(args)
+    end_time = time.time()
+    if args.verbose:
+        print("Completed in %-0.4f sec." % (end_time - start_time))
+    sys.exit(result)
```

### Comparing `rucio-clients-1.9.6/etc/rse-accounts.cfg.template` & `rucio-clients-32.0.0rc1/etc/rse-accounts.cfg.template`

 * *Files identical despite different names*

### Comparing `rucio-clients-1.9.6/lib/rucio/client/accountclient.py` & `rucio-clients-32.0.0rc1/lib/rucio/client/accountclient.py`

 * *Files 16% similar despite different names*

```diff
@@ -1,330 +1,413 @@
-# Copyright European Organization for Nuclear Research (CERN)
+# -*- coding: utf-8 -*-
+# Copyright European Organization for Nuclear Research (CERN) since 2012
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
-# You may not use this file except in compliance with the License.
-# You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
 #
-# Authors:
-# - Thomas Beermann, <thomas.beermann@cern.ch>, 2012-2013
-# - Vincent Garonne, <vincent.garonne@cern.ch>, 2012-2015
-# - Cedric Serfon, <cedric.serfon@cern.ch>, 2014
-# - Martin Barisits, <martin.barisits@cern.ch>, 2014
-# - Cheng-Hsi Chao, <cheng-hsi.chao@cern.ch>, 2014
-# - Ralph Vigne, <ralph.vigne@cern.ch>, 2015
-# - Joaquin Bogado, <joaquin.bogado@cern.ch>, 2015
+#    http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 
 from json import dumps
+from urllib.parse import quote_plus
+
 from requests.status_codes import codes
 
 from rucio.client.baseclient import BaseClient
 from rucio.client.baseclient import choice
 from rucio.common.utils import build_url
 
 
 class AccountClient(BaseClient):
 
     """Account client class for working with rucio accounts"""
 
     ACCOUNTS_BASEURL = 'accounts'
 
-    def __init__(self, rucio_host=None, auth_host=None, account=None, ca_cert=None, auth_type=None, creds=None, timeout=None, user_agent='rucio-clients'):
-        super(AccountClient, self).__init__(rucio_host, auth_host, account, ca_cert, auth_type, creds, timeout, user_agent)
-
-    def add_account(self, account, type, email):
+    def add_account(self, account, type_, email):
         """
         Sends the request to create a new account.
 
         :param account: the name of the account.
-        :param type: The account type
+        :param type_: The account type
         :param email: The Email address associated with the account.
 
         :return: True if account was created successfully else False.
         :raises Duplicate: if account already exists.
         """
 
-        data = dumps({'type': type, 'email': email})
+        data = dumps({'type': type_, 'email': email})
         path = '/'.join([self.ACCOUNTS_BASEURL, account])
         url = build_url(choice(self.list_hosts), path=path)
 
-        r = self._send_request(url, type='POST', data=data)
-        if r.status_code == codes.created:
+        res = self._send_request(url, type_='POST', data=data)
+        if res.status_code == codes.created:
             return True
-        else:
-            exc_cls, exc_msg = self._get_exception(headers=r.headers, status_code=r.status_code, data=r.content)
-            raise exc_cls(exc_msg)
+        exc_cls, exc_msg = self._get_exception(headers=res.headers, status_code=res.status_code, data=res.content)
+        raise exc_cls(exc_msg)
 
     def delete_account(self, account):
         """
         Sends the request to disable an account.
 
         :param account: the name of the account.
         :return: True is account was disabled successfully. False otherwise.
         :raises AccountNotFound: if account doesn't exist.
         """
 
         path = '/'.join([self.ACCOUNTS_BASEURL, account])
         url = build_url(choice(self.list_hosts), path=path)
 
-        r = self._send_request(url, type='DEL')
+        res = self._send_request(url, type_='DEL')
 
-        if r.status_code == codes.ok:
+        if res.status_code == codes.ok:
             return True
-        else:
-            exc_cls, exc_msg = self._get_exception(headers=r.headers, status_code=r.status_code, data=r.content)
-            raise exc_cls(exc_msg)
+        exc_cls, exc_msg = self._get_exception(headers=res.headers, status_code=res.status_code, data=res.content)
+        raise exc_cls(exc_msg)
 
     def get_account(self, account):
         """
         Sends the request to get information about a given account.
 
         :param account: the name of the account.
         :return: a list of attributes for the account. None if failure.
         :raises AccountNotFound: if account doesn't exist.
         """
 
         path = '/'.join([self.ACCOUNTS_BASEURL, account])
         url = build_url(choice(self.list_hosts), path=path)
 
-        r = self._send_request(url)
-        if r.status_code == codes.ok:
-            acc = self._load_json_data(r)
-            return acc.next()
-        else:
-            exc_cls, exc_msg = self._get_exception(headers=r.headers, status_code=r.status_code, data=r.content)
-            raise exc_cls(exc_msg)
+        res = self._send_request(url)
+        if res.status_code == codes.ok:
+            acc = self._load_json_data(res)
+            return next(acc)
+        exc_cls, exc_msg = self._get_exception(headers=res.headers, status_code=res.status_code, data=res.content)
+        raise exc_cls(exc_msg)
 
-    def set_account_status(self, account, status):
-        """ Set the status of an account.
+    def update_account(self, account, key, value):
+        """ Update a property of an account.
 
         :param account: Name of the account.
-        :param status: The status for the account.
+        :param key: Account property like status.
+        :param value: Property value.
         """
-        data = dumps({'status': status})
+        data = dumps({key: value})
         path = '/'.join([self.ACCOUNTS_BASEURL, account])
         url = build_url(choice(self.list_hosts), path=path)
 
-        r = self._send_request(url, type='PUT', data=data)
+        res = self._send_request(url, type_='PUT', data=data)
 
-        if r.status_code == codes.ok:
+        if res.status_code == codes.ok:
             return True
         else:
-            exc_cls, exc_msg = self._get_exception(headers=r.headers, status_code=r.status_code, data=r.content)
+            exc_cls, exc_msg = self._get_exception(headers=res.headers, status_code=res.status_code, data=res.content)
             raise exc_cls(exc_msg)
 
-    def list_accounts(self, account_type=None, identity=None):
+    def list_accounts(self, account_type=None, identity=None, filters=None):
         """
         Sends the request to list all rucio accounts.
 
         :param type: The account type
         :param identity: The identity key name. For example x509 DN, or a username.
+        :param filters: A dictionnary key:account attribute to use for the filtering
 
         :return: a list containing account info dictionary for all rucio accounts.
         :raises AccountNotFound: if account doesn't exist.
         """
         path = '/'.join([self.ACCOUNTS_BASEURL])
         url = build_url(choice(self.list_hosts), path=path)
         params = {}
         if account_type:
             params['account_type'] = account_type
         if identity:
             params['identity'] = identity
+        if filters:
+            for key in filters:
+                params[key] = filters[key]
 
-        r = self._send_request(url, params=params)
+        res = self._send_request(url, params=params)
 
-        if r.status_code == codes.ok:
-            accounts = self._load_json_data(r)
+        if res.status_code == codes.ok:
+            accounts = self._load_json_data(res)
             return accounts
         else:
-            exc_cls, exc_msg = self._get_exception(headers=r.headers, status_code=r.status_code, data=r.content)
+            exc_cls, exc_msg = self._get_exception(headers=res.headers, status_code=res.status_code, data=res.content)
             raise exc_cls(exc_msg)
 
     def whoami(self):
         """
         Get information about account whose token is used
 
         :return: a list of attributes for the account. None if failure.
         :raises AccountNotFound: if account doesn't exist.
         """
         return self.get_account('whoami')
 
-    def add_identity(self, account, identity, authtype, email, default=False):
+    def add_identity(self, account, identity, authtype, email, default=False, password=None):
         """
         Adds a membership association between identity and account.
 
         :param account: The account name.
         :param identity: The identity key name. For example x509 DN, or a username.
         :param authtype: The type of the authentication (x509, gss, userpass).
         :param default: If True, the account should be used by default with the provided identity.
         :param email: The Email address associated with the identity.
+        :param password: Password if authtype is userpass.
         """
 
-        data = dumps({'identity': identity, 'authtype': authtype, 'default': default, 'email': email})
+        data = dumps({'identity': identity, 'authtype': authtype, 'default': default, 'email': email, 'password': password})
         path = '/'.join([self.ACCOUNTS_BASEURL, account, 'identities'])
 
         url = build_url(choice(self.list_hosts), path=path)
 
-        r = self._send_request(url, type='POST', data=data)
+        res = self._send_request(url, type_='POST', data=data)
 
-        if r.status_code == codes.created:
+        if res.status_code == codes.created:
             return True
         else:
-            exc_cls, exc_msg = self._get_exception(headers=r.headers, status_code=r.status_code, data=r.content)
+            exc_cls, exc_msg = self._get_exception(headers=res.headers, status_code=res.status_code, data=res.content)
             raise exc_cls(exc_msg)
 
-    def del_identity(self, account, identity, authtype, default=False):
+    def del_identity(self, account, identity, authtype):
         """
         Delete an identity's membership association with an account.
 
         :param account: The account name.
         :param identity: The identity key name. For example x509 DN, or a username.
         :param authtype: The type of the authentication (x509, gss, userpass).
         :param default: If True, the account should be used by default with the provided identity.
         """
 
-        data = dumps({'identity': identity, 'authtype': authtype, 'default': default})
+        data = dumps({'identity': identity, 'authtype': authtype})
         path = '/'.join([self.ACCOUNTS_BASEURL, account, 'identities'])
 
         url = build_url(choice(self.list_hosts), path=path)
 
-        r = self._send_request(url, type='DEL', data=data)
+        res = self._send_request(url, type_='DEL', data=data)
 
-        if r.status_code == codes.ok:
+        if res.status_code == codes.ok:
             return True
         else:
-            exc_cls, exc_msg = self._get_exception(headers=r.headers, status_code=r.status_code, data=r.content)
+            exc_cls, exc_msg = self._get_exception(headers=res.headers, status_code=res.status_code, data=res.content)
             raise exc_cls(exc_msg)
 
     def list_identities(self, account):
         """
         List all identities on an account.
 
         :param account: The account name.
         """
         path = '/'.join([self.ACCOUNTS_BASEURL, account, 'identities'])
         url = build_url(choice(self.list_hosts), path=path)
-        r = self._send_request(url)
-        if r.status_code == codes.ok:
-            identities = self._load_json_data(r)
+        res = self._send_request(url)
+        if res.status_code == codes.ok:
+            identities = self._load_json_data(res)
             return identities
         else:
-            exc_cls, exc_msg = self._get_exception(headers=r.headers, status_code=r.status_code, data=r.content)
+            exc_cls, exc_msg = self._get_exception(headers=res.headers, status_code=res.status_code, data=res.content)
             raise exc_cls(exc_msg)
 
     def list_account_rules(self, account):
         """
         List the associated rules of an account.
 
         :param account: The account name.
         """
 
         path = '/'.join([self.ACCOUNTS_BASEURL, account, 'rules'])
         url = build_url(choice(self.list_hosts), path=path)
-        r = self._send_request(url, type='GET')
-        if r.status_code == codes.ok:
-            return self._load_json_data(r)
+        res = self._send_request(url, type_='GET')
+        if res.status_code == codes.ok:
+            return self._load_json_data(res)
         else:
-            exc_cls, exc_msg = self._get_exception(headers=r.headers, status_code=r.status_code, data=r.content)
+            exc_cls, exc_msg = self._get_exception(headers=res.headers, status_code=res.status_code, data=res.content)
             raise exc_cls(exc_msg)
 
-    def get_account_limits(self, account):
+    def get_account_limits(self, account, rse_expression, locality):
+        """
+        Return the correct account limits for the given locality.
+
+        :param account:        The account name.
+        :param rse_expression: Valid RSE expression
+        :param locality:       The scope of the account limit. 'local' or 'global'.
+        """
+
+        if locality == 'local':
+            return self.get_local_account_limit(account, rse_expression)
+        elif locality == 'global':
+            return self.get_global_account_limit(account, rse_expression)
+        else:
+            from rucio.common.exception import UnsupportedOperation
+            raise UnsupportedOperation('The provided locality (%s) for the account limit was invalid' % locality)
+
+    def get_global_account_limit(self, account, rse_expression):
+        """
+        List the account limit for the specific RSE expression.
+
+        :param account:        The account name.
+        :param rse_expression: The rse expression.
+        """
+
+        path = '/'.join([self.ACCOUNTS_BASEURL, account, 'limits', 'global', quote_plus(rse_expression)])
+        url = build_url(choice(self.list_hosts), path=path)
+        res = self._send_request(url, type_='GET')
+        if res.status_code == codes.ok:
+            return next(self._load_json_data(res))
+        exc_cls, exc_msg = self._get_exception(headers=res.headers, status_code=res.status_code, data=res.content)
+        raise exc_cls(exc_msg)
+
+    def get_global_account_limits(self, account):
+        """
+        List all RSE expression limits of this account.
+
+        :param account: The account name.
+        """
+
+        path = '/'.join([self.ACCOUNTS_BASEURL, account, 'limits', 'global'])
+        url = build_url(choice(self.list_hosts), path=path)
+        res = self._send_request(url, type_='GET')
+        if res.status_code == codes.ok:
+            return next(self._load_json_data(res))
+        exc_cls, exc_msg = self._get_exception(headers=res.headers, status_code=res.status_code, data=res.content)
+        raise exc_cls(exc_msg)
+
+    def get_local_account_limits(self, account):
         """
         List the account rse limits of this account.
 
         :param account: The account name.
         """
 
-        path = '/'.join([self.ACCOUNTS_BASEURL, account, 'limits'])
+        path = '/'.join([self.ACCOUNTS_BASEURL, account, 'limits', 'local'])
         url = build_url(choice(self.list_hosts), path=path)
-        r = self._send_request(url, type='GET')
-        if r.status_code == codes.ok:
-            return self._load_json_data(r).next()
-        else:
-            exc_cls, exc_msg = self._get_exception(headers=r.headers, status_code=r.status_code, data=r.content)
-            raise exc_cls(exc_msg)
+        res = self._send_request(url, type_='GET')
+        if res.status_code == codes.ok:
+            return next(self._load_json_data(res))
+        exc_cls, exc_msg = self._get_exception(headers=res.headers, status_code=res.status_code, data=res.content)
+        raise exc_cls(exc_msg)
 
-    def get_account_limit(self, account, rse):
+    def get_local_account_limit(self, account, rse):
         """
         List the account rse limits of this account for the specific rse.
 
         :param account: The account name.
         :param rse:     The rse name.
         """
 
-        path = '/'.join([self.ACCOUNTS_BASEURL, account, 'limits', rse])
+        path = '/'.join([self.ACCOUNTS_BASEURL, account, 'limits', 'local', rse])
         url = build_url(choice(self.list_hosts), path=path)
-        r = self._send_request(url, type='GET')
-        if r.status_code == codes.ok:
-            return self._load_json_data(r).next()
-        else:
-            exc_cls, exc_msg = self._get_exception(headers=r.headers, status_code=r.status_code, data=r.content)
-            raise exc_cls(exc_msg)
+        res = self._send_request(url, type_='GET')
+        if res.status_code == codes.ok:
+            return next(self._load_json_data(res))
+        exc_cls, exc_msg = self._get_exception(headers=res.headers, status_code=res.status_code, data=res.content)
+        raise exc_cls(exc_msg)
 
-    def get_account_usage(self, account, rse=None):
+    def get_local_account_usage(self, account, rse=None):
         """
         List the account usage for one or all rses of this account.
 
         :param account: The account name.
         :param rse:     The rse name.
         """
         if rse:
-            path = '/'.join([self.ACCOUNTS_BASEURL, account, 'usage', rse])
+            path = '/'.join([self.ACCOUNTS_BASEURL, account, 'usage', 'local', rse])
+        else:
+            path = '/'.join([self.ACCOUNTS_BASEURL, account, 'usage', 'local'])
+        url = build_url(choice(self.list_hosts), path=path)
+        res = self._send_request(url, type_='GET')
+        if res.status_code == codes.ok:
+            return self._load_json_data(res)
         else:
-            path = '/'.join([self.ACCOUNTS_BASEURL, account, 'usage/'])
+            exc_cls, exc_msg = self._get_exception(headers=res.headers, status_code=res.status_code, data=res.content)
+            raise exc_cls(exc_msg)
+
+    def get_global_account_usage(self, account, rse_expression=None):
+        """
+        List the account usage for one or all RSE expressions of this account.
+
+        :param account:        The account name.
+        :param rse_expression: The rse expression.
+        """
+        if rse_expression:
+            path = '/'.join([self.ACCOUNTS_BASEURL, account, 'usage', 'global', quote_plus(rse_expression)])
+        else:
+            path = '/'.join([self.ACCOUNTS_BASEURL, account, 'usage', 'global'])
+        url = build_url(choice(self.list_hosts), path=path)
+        res = self._send_request(url, type_='GET')
+        if res.status_code == codes.ok:
+            return self._load_json_data(res)
+        else:
+            exc_cls, exc_msg = self._get_exception(headers=res.headers, status_code=res.status_code, data=res.content)
+            raise exc_cls(exc_msg)
+
+    def get_account_usage_history(self, account, rse):
+        """
+        List the account usage history of this account on rse.
+
+        :param account: The account name.
+        :param rse:     The rse name.
+        """
+        path = '/'.join([self.ACCOUNTS_BASEURL, account, 'usage/history', rse])
         url = build_url(choice(self.list_hosts), path=path)
-        r = self._send_request(url, type='GET')
-        if r.status_code == codes.ok:
-            return self._load_json_data(r)
+        res = self._send_request(url, type_='GET')
+        if res.status_code == codes.ok:
+            return next(self._load_json_data(res))
         else:
-            exc_cls, exc_msg = self._get_exception(headers=r.headers, status_code=r.status_code, data=r.content)
+            exc_cls, exc_msg = self._get_exception(headers=res.headers, status_code=res.status_code, data=res.content)
             raise exc_cls(exc_msg)
 
     def list_account_attributes(self, account):
         """
         List the attributes for an account.
 
         :param account: The account name.
         """
         path = '/'.join([self.ACCOUNTS_BASEURL, account, 'attr/'])
         url = build_url(choice(self.list_hosts), path=path)
-        r = self._send_request(url, type='GET')
-        if r.status_code == codes.ok:
-            return self._load_json_data(r)
+        res = self._send_request(url, type_='GET')
+        if res.status_code == codes.ok:
+            return self._load_json_data(res)
         else:
-            exc_cls, exc_msg = self._get_exception(headers=r.headers, status_code=r.status_code, data=r.content)
+            exc_cls, exc_msg = self._get_exception(headers=res.headers, status_code=res.status_code, data=res.content)
             raise exc_cls(exc_msg)
 
     def add_account_attribute(self, account, key, value):
         """
         Adds an attribute to an account.
 
         :param account: The account name.
         :param key: The attribute key.
         :param value: The attribute value.
         """
 
         data = dumps({'key': key, 'value': value})
         path = '/'.join([self.ACCOUNTS_BASEURL, account, 'attr', key])
         url = build_url(choice(self.list_hosts), path=path)
-        r = self._send_request(url, type='POST', data=data)
-        if r.status_code == codes.created:
+        res = self._send_request(url, type_='POST', data=data)
+        if res.status_code == codes.created:
             return True
         else:
-            exc_cls, exc_msg = self._get_exception(headers=r.headers, status_code=r.status_code, data=r.content)
+            exc_cls, exc_msg = self._get_exception(headers=res.headers, status_code=res.status_code, data=res.content)
             raise exc_cls(exc_msg)
 
     def delete_account_attribute(self, account, key):
         """
         Delete an attribute for an account.
 
         :param account: The account name.
         :param key: The attribute key.
         """
 
         path = '/'.join([self.ACCOUNTS_BASEURL, account, 'attr', key])
         url = build_url(choice(self.list_hosts), path=path)
-        r = self._send_request(url, type='DEL', data=None)
-        if r.status_code == codes.ok:
+        res = self._send_request(url, type_='DEL', data=None)
+        if res.status_code == codes.ok:
             return True
         else:
-            exc_cls, exc_msg = self._get_exception(headers=r.headers, status_code=r.status_code, data=r.content)
+            exc_cls, exc_msg = self._get_exception(headers=res.headers, status_code=res.status_code, data=res.content)
             raise exc_cls(exc_msg)
```

### Comparing `rucio-clients-1.9.6/lib/rucio/client/accountlimitclient.py` & `rucio-clients-32.0.0rc1/lib/rucio/client/scopeclient.py`

 * *Files 25% similar despite different names*

```diff
@@ -1,73 +1,88 @@
-# Copyright European Organization for Nuclear Research (CERN)
+# -*- coding: utf-8 -*-
+# Copyright European Organization for Nuclear Research (CERN) since 2012
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
-# You may not use this file except in compliance with the License.
-# You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
 #
-# Authors:
-# - Mario Lassnig, <mario.lassnig@cern.ch>, 2013
-# - Cedric Serfon, <cedric.serfon@cern.ch>, 2014
-# - Martin Barisits, <martin.barisits@cern.ch>, 2014
-# - Ralph Vigne, <ralph.vigne@cern.ch>, 2015
-# - Vincent Garonne, <vincent.garonne@cern.ch>, 2012-2015
+#    http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+from json import loads
+from urllib.parse import quote_plus
 
-from json import dumps
 from requests.status_codes import codes
 
 from rucio.client.baseclient import BaseClient
 from rucio.client.baseclient import choice
 from rucio.common.utils import build_url
 
 
-class AccountLimitClient(BaseClient):
-
-    """Account limit client class for working with account limits"""
+class ScopeClient(BaseClient):
 
-    ACCOUNTLIMIT_BASEURL = 'accountlimits'
+    """Scope client class for working with rucio scopes"""
 
-    def __init__(self, rucio_host=None, auth_host=None, account=None, ca_cert=None, auth_type=None, creds=None, timeout=None, user_agent='rucio-clients'):
-        super(AccountLimitClient, self).__init__(rucio_host, auth_host, account, ca_cert, auth_type, creds, timeout, user_agent)
+    SCOPE_BASEURL = 'accounts'
 
-    def set_account_limit(self, account, rse, bytes):
+    def add_scope(self, account, scope):
         """
-        Sends the request to set an account limit for an account.
+        Sends the request to add a new scope.
 
-        :param account: The name of the account.
-        :param rse:     The rse name.
-        :param bytes:   An integer with the limit in bytes.
-        :return:        True if quota was created successfully else False.
+        :param account: the name of the account to add the scope to.
+        :param scope: the name of the new scope.
+        :return: True if scope was created successfully.
+        :raises Duplicate: if scope already exists.
+        :raises AccountNotFound: if account doesn't exist.
         """
 
-        data = dumps({'bytes': bytes})
-        path = '/'.join([self.ACCOUNTLIMIT_BASEURL, account, rse])
+        path = '/'.join([self.SCOPE_BASEURL, account, 'scopes', quote_plus(scope)])
         url = build_url(choice(self.list_hosts), path=path)
-
-        r = self._send_request(url, type='POST', data=data)
-
+        r = self._send_request(url, type_='POST')
         if r.status_code == codes.created:
             return True
         else:
             exc_cls, exc_msg = self._get_exception(headers=r.headers, status_code=r.status_code, data=r.content)
             raise exc_cls(exc_msg)
 
-    def delete_account_limit(self, account, rse):
+    def list_scopes(self):
         """
-        Sends the request to remove an account limit.
+        Sends the request to list all scopes.
 
-        :param account: The name of the account.
-        :param rse:     The rse name.
+        :return: a list containing the names of all scopes.
+        """
 
-        :return: True if quota was removed successfully. False otherwise.
+        path = '/'.join(['scopes/'])
+        url = build_url(choice(self.list_hosts), path=path)
+        r = self._send_request(url)
+        if r.status_code == codes.ok:
+            scopes = loads(r.text)
+            return scopes
+        else:
+            exc_cls, exc_msg = self._get_exception(headers=r.headers, status_code=r.status_code, data=r.content)
+            raise exc_cls(exc_msg)
+
+    def list_scopes_for_account(self, account):
+        """
+        Sends the request to list all scopes for a rucio account.
+
+        :param account: the rucio account to list scopes for.
+        :return: a list containing the names of all scopes for a rucio account.
         :raises AccountNotFound: if account doesn't exist.
+        :raises ScopeNotFound: if no scopes exist for account.
         """
 
-        path = '/'.join([self.ACCOUNTLIMIT_BASEURL, account, rse])
+        path = '/'.join([self.SCOPE_BASEURL, account, 'scopes/'])
         url = build_url(choice(self.list_hosts), path=path)
 
-        r = self._send_request(url, type='DEL')
-
+        r = self._send_request(url)
         if r.status_code == codes.ok:
-            return True
+            scopes = loads(r.text)
+            return scopes
         else:
             exc_cls, exc_msg = self._get_exception(headers=r.headers, status_code=r.status_code, data=r.content)
             raise exc_cls(exc_msg)
```

### Comparing `rucio-clients-1.9.6/lib/rucio/client/baseclient.py` & `rucio-clients-32.0.0rc1/lib/rucio/rse/protocols/webdav.py`

 * *Files 24% similar despite different names*

```diff
@@ -1,510 +1,547 @@
-"""
-  Copyright European Organization for Nuclear Research (CERN)
+# -*- coding: utf-8 -*-
+# Copyright European Organization for Nuclear Research (CERN) since 2012
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#    http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 
-  Licensed under the Apache License, Version 2.0 (the "License");
-  You may not use this file except in compliance with the License.
-  You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0
-
-  Authors:
-  - Vincent Garonne, <vincent.garonne@cern.ch>, 2012-2016
-  - Thomas Beermann, <thomas.beermann@cern.ch>, 2012-2013
-  - Cedric Serfon, <cedric.serfon@cern.ch>, 2014-2015
-  - Ralph Vigne, <ralph.vigne@cern.ch>, 2015
-
-Client class for callers of the Rucio system
-"""
-import random
+import os
 import sys
+import xml.etree.ElementTree as ET
+from typing import Optional
+from urllib.parse import urlparse
+
+import requests
+from dataclasses import dataclass
+from requests.adapters import HTTPAdapter
+from urllib3.poolmanager import PoolManager
 
 from rucio.common import exception
-from rucio.common.config import config_get
-from rucio.common.exception import CannotAuthenticate, ClientProtocolNotSupported, NoAuthInformation, MissingClientParameter
-from rucio.common.utils import build_url, get_tmp_dir, my_key_generator, parse_response
-from rucio import version
-
-from logging import getLogger, StreamHandler, ERROR
-from os import environ, fdopen, path, makedirs
-from shutil import move
-from tempfile import mkstemp
-from urlparse import urlparse
-
-from ConfigParser import NoOptionError, NoSectionError
-from dogpile.cache import make_region
-from requests import session
-from requests.status_codes import codes, _codes
-from requests.exceptions import SSLError
-from requests_kerberos import HTTPKerberosAuth
-# See https://github.com/kennethreitz/requests/issues/2214
-from requests.packages.urllib3 import disable_warnings
-disable_warnings()
-
-
-LOG = getLogger(__name__)
-SH = StreamHandler()
-SH.setLevel(ERROR)
-LOG.addHandler(SH)
-
-
-REGION = make_region(function_key_generator=my_key_generator).configure(
-    'dogpile.cache.memory',
-    expiration_time=60,
-)
-
-
-@REGION.cache_on_arguments(namespace='host_to_choose')
-def choice(hosts):
-    """
-    Select randomly a host
-
-    :param hosts: Lost of hosts
-    :return: A randomly selected host.
-    """
-    return random.choice(hosts)
-
-
-class BaseClient(object):
-
-    """Main client class for accessing Rucio resources. Handles the authentication."""
-
-    AUTH_RETRIES, REQUEST_RETRIES = 2, 3
-    TOKEN_PATH_PREFIX = get_tmp_dir() + '/.rucio_'
-    TOKEN_PREFIX = 'auth_token_'
-
-    def __init__(self, rucio_host=None, auth_host=None, account=None, ca_cert=None, auth_type=None, creds=None, timeout=None, user_agent='rucio-clients'):
-        """
-        Constructor of the BaseClient.
-        :param rucio_host: the address of the rucio server, if None it is read from the config file.
-        :param rucio_port: the port of the rucio server, if None it is read from the config file.
-        :param auth_host: the address of the rucio authentication server, if None it is read from the config file.
-        :param auth_port: the port of the rucio authentication server, if None it is read from the config file.
-        :param account: the account to authenticate to rucio.
-        :param use_ssl: enable or disable ssl for commucation. Default is enabled.
-        :param ca_cert: the path to the rucio server certificate.
-        :param auth_type: the type of authentication (e.g.: 'userpass', 'kerberos' ...)
-        :param creds: a dictionary with credentials needed for authentication.
-        :param user_agent: indicates the client
-        """
-
-        self.host = rucio_host
-        self.list_hosts = []
-        self.auth_host = auth_host
-        self.session = session()
-        self.user_agent = "%s/%s" % (user_agent, version.version_string())  # e.g. "rucio-clients/0.2.13"
-        sys.argv[0] = sys.argv[0].split('/')[-1]
-        self.script_id = '::'.join(sys.argv[0:2])
-        if self.script_id == '':  # Python interpreter used
-            self.script_id = 'python'
-        try:
-            if self.host is None:
-                self.host = config_get('client', 'rucio_host')
-            if self.auth_host is None:
-                self.auth_host = config_get('client', 'auth_host')
-        except (NoOptionError, NoSectionError) as error:
-            raise MissingClientParameter('Section client and Option \'%s\' cannot be found in config file' % error.args[0])
-
-        self.account = account
-        self.ca_cert = ca_cert
-        self.auth_type = auth_type
-        self.creds = creds
-        self.auth_token = None
-        self.headers = {}
-        self.timeout = timeout
-        self.request_retries = self.REQUEST_RETRIES
-
-        if auth_type is None:
-            LOG.debug('no auth_type passed. Trying to get it from the environment variable RUCIO_AUTH_TYPE and config file.')
-            if 'RUCIO_AUTH_TYPE' in environ:
-                if environ['RUCIO_AUTH_TYPE'] not in ('userpass', 'x509', 'x509_proxy', 'gss'):
-                    raise MissingClientParameter('Possible RUCIO_AUTH_TYPE values: userpass, x509, x509_proxy, gss vs. ' + environ['RUCIO_AUTH_TYPE'])
-                self.auth_type = environ['RUCIO_AUTH_TYPE']
-            else:
-                try:
-                    self.auth_type = config_get('client', 'auth_type')
-                except (NoOptionError, NoSectionError) as error:
-                    raise MissingClientParameter('Option \'%s\' cannot be found in config file' % error.args[0])
-
-        if creds is None:
-            LOG.debug('no creds passed. Trying to get it from the config file.')
-            self.creds = {}
-            try:
-                if self.auth_type == 'userpass':
-                    self.creds['username'] = config_get('client', 'username')
-                    self.creds['password'] = config_get('client', 'password')
-                elif self.auth_type == 'x509':
-                    self.creds['client_cert'] = path.abspath(path.expanduser(path.expandvars(config_get('client', 'client_cert'))))
-                    self.creds['client_key'] = path.abspath(path.expanduser(path.expandvars(config_get('client', 'client_key'))))
-                elif self.auth_type == 'x509_proxy':
-                    self.creds['client_proxy'] = path.abspath(path.expanduser(path.expandvars(config_get('client', 'client_x509_proxy'))))
-            except (NoOptionError, NoSectionError) as error:
-                if error.args[0] != 'client_key':
-                    raise MissingClientParameter('Option \'%s\' cannot be found in config file' % error.args[0])
-
-        rucio_scheme = urlparse(self.host).scheme
-        auth_scheme = urlparse(self.auth_host).scheme
+from rucio.rse.protocols import protocol
 
-        if rucio_scheme != 'http' and rucio_scheme != 'https':
-            raise ClientProtocolNotSupported('\'%s\' not supported' % rucio_scheme)
 
-        if auth_scheme != 'http' and auth_scheme != 'https':
-            raise ClientProtocolNotSupported('\'%s\' not supported' % auth_scheme)
+class TLSHTTPAdapter(HTTPAdapter):
+    '''
+    Class to force the SSL protocol to latest TLS
+    '''
+    def init_poolmanager(self, connections, maxsize, block=False):
+        self.poolmanager = PoolManager(num_pools=connections,
+                                       maxsize=maxsize,
+                                       block=block,
+                                       cert_reqs="CERT_REQUIRED",
+                                       ca_cert_dir="/etc/grid-security/certificates")
+
+
+class UploadInChunks(object):
+    '''
+    Class to upload by chunks.
+    '''
+
+    def __init__(self, filename, chunksize, progressbar=False):
+        self.__totalsize = os.path.getsize(filename)
+        self.__readsofar = 0
+        self.__filename = filename
+        self.__chunksize = chunksize
+        self.__progressbar = progressbar
 
-        if (rucio_scheme == 'https' or auth_scheme == 'https') and ca_cert is None:
-            LOG.debug('no ca_cert passed. Trying to get it from the config file.')
-            try:
-                self.ca_cert = path.expandvars(config_get('client', 'ca_cert'))
-            except (NoOptionError, NoSectionError) as error:
-                raise MissingClientParameter('Option \'%s\' cannot be found in config file' % error.args[0])
+    def __iter__(self):
+        try:
+            with open(self.__filename, 'rb') as file_in:
+                while True:
+                    data = file_in.read(self.__chunksize)
+                    if not data:
+                        if self.__progressbar:
+                            sys.stdout.write("\n")
+                        break
+                    self.__readsofar += len(data)
+                    if self.__progressbar:
+                        percent = self.__readsofar * 100 / self.__totalsize
+                        sys.stdout.write("\r{percent:3.0f}%".format(percent=percent))
+                    yield data
+        except OSError as error:
+            raise exception.SourceNotFound(error)
+
+    def __len__(self):
+        return self.__totalsize
+
+
+class IterableToFileAdapter(object):
+    '''
+    Class IterableToFileAdapter
+    '''
+    def __init__(self, iterable):
+        self.iterator = iter(iterable)
+        self.length = len(iterable)
+
+    def read(self, size=-1):   # TBD: add buffer for `len(data) > size` case
+        nextvar = next(self.iterator, b'')
+        return nextvar
+
+    def __len__(self):
+        return self.length
+
+
+@dataclass(frozen=True)
+class _PropfindFile:
+    """Contains the properties of one file from a PROPFIND response."""
+
+    href: str
+    size: Optional[int]
+
+    @classmethod
+    def from_xml_node(cls, node: ET.Element):
+        """Extract file properties from a `<{DAV:}response>` node."""
+
+        xml_href = node.find('./{DAV:}href')
+        if xml_href is None or xml_href.text is None:
+            raise ValueError('Response is missing mandatory field "href".')
+        else:
+            href = xml_href.text
 
-        self.list_hosts = [self.host]
+        xml_size = node.find('./{DAV:}propstat/{DAV:}prop/{DAV:}getcontentlength')
+        if xml_size is None or xml_size.text is None:
+            size = None
+        else:
+            size = int(xml_size.text)
 
-        if account is None:
-            LOG.debug('no account passed. Trying to get it from the config file.')
-            try:
-                self.account = config_get('client', 'account')
-            except (NoOptionError, NoSectionError):
-                try:
-                    self.account = environ['RUCIO_ACCOUNT']
-                except KeyError:
-                    raise MissingClientParameter('Option \'account\' cannot be found in config file and RUCIO_ACCOUNT is not set.')
-
-        token_path = self.TOKEN_PATH_PREFIX + self.account
-        self.token_file = token_path + '/' + self.TOKEN_PREFIX + self.account
-        self.__authenticate()
+        return cls(href=href, size=size)  # type: ignore
 
-        try:
-            self.request_retries = int(config_get('client', 'request_retries'))
-        except NoOptionError:
-            LOG.debug('request_retries not specified in config file. Taking default.')
-        except ValueError:
-            LOG.debug('request_retries must be an integer. Taking default.')
 
-    def _get_exception(self, headers, status_code=None, data=None):
-        """
-        Helper method to parse an error string send by the server and transform it into the corresponding rucio exception.
+@dataclass(frozen=True)
+class _PropfindResponse:
+    """Contains all the files from a PROPFIND response."""
+
+    files: tuple[_PropfindFile]
+
+    @classmethod
+    def parse(cls, document: str):
+        """Parses the XML document of a WebDAV PROPFIND response.
 
-        :param headers: The http response header containing the Rucio exception details.
-        :param status_code: The http status code.
-        :param data: The data with the ExceptionMessage.
+        The PROPFIND response is described in RFC 4918.
+        This method expects the document root to be a node with tag `{DAV:}multistatus`.
 
-        :return: A rucio exception class and an error string.
+        :param document: XML document to parse.
+        :raises ValueError: if the XML document couldn't be parsed.
+        :returns: The parsed response.
         """
-        data = parse_response(data)
-        if 'ExceptionClass' not in data:
-            if 'ExceptionMessage' not in data:
-                human_http_code = _codes.get(status_code, None)  # NOQA, pylint: disable-msg=W0612
-                return getattr(exception, 'RucioException'), 'no error information passed (http status code: %(status_code)s %(human_http_code)s)' % locals()
-            return getattr(exception, 'RucioException'), data['ExceptionMessage']
 
-        exc_cls = None
         try:
-            exc_cls = getattr(exception, data['ExceptionClass'])
-        except AttributeError:
-            return getattr(exception, 'RucioException'), data['ExceptionMessage']
+            xml = ET.fromstring(document)
+        except ET.ParseError as ex:
+            raise ValueError("Couldn't parse XML document") from ex
 
-        return exc_cls, data['ExceptionMessage']
+        if xml.tag != '{DAV:}multistatus':
+            raise ValueError('Root element is not "{DAV:}multistatus".')
 
-    def _load_json_data(self, response):
-        """
-        Helper method to correctly load json data based on the content type of the http response.
+        files = []
+        for xml_response in xml.findall('./{DAV:}response'):
+            files.append(_PropfindFile.from_xml_node(xml_response))
 
-        :param response: the response received from the server.
-        """
-        if 'content-type' in response.headers and response.headers['content-type'] == 'application/x-json-stream':
-            for line in response.iter_lines():
-                if line:
-                    yield parse_response(line)
-        elif 'content-type' in response.headers and response.headers['content-type'] == 'application/json':
-            yield parse_response(response.text)
-        else:  # Exception ?
-            yield response.text
+        return cls(files=tuple(files))  # type: ignore
 
-    def _send_request(self, url, headers=None, type='GET', data=None, params=None):
-        """
-        Helper method to send requests to the rucio server. Gets a new token and retries if an unauthorized error is returned.
 
-        :param url: the http url to use.
-        :param headers: additional http headers to send.
-        :param type: the http request type to use.
-        :param data: post data.
-        :param params: (optional) Dictionary or bytes to be sent in the url query string.
-        :return: the HTTP return body.
-        """
-        result, retry = None, 0
-        hds = {'X-Rucio-Auth-Token': self.auth_token, 'X-Rucio-Account': self.account,
-               'Connection': 'Keep-Alive', 'User-Agent': self.user_agent,
-               'X-Rucio-Script': self.script_id}
+class Default(protocol.RSEProtocol):
 
-        if headers is not None:
-            hds.update(headers)
+    """ Implementing access to RSEs using the webDAV protocol."""
 
-        while retry <= self.request_retries:
-            try:
-                if type == 'GET':
-                    result = self.session.get(url, headers=hds, verify=self.ca_cert, timeout=self.timeout, params=params, stream=True)
-                elif type == 'PUT':
-                    result = self.session.put(url, headers=hds, data=data, verify=self.ca_cert, timeout=self.timeout)
-                elif type == 'POST':
-                    result = self.session.post(url, headers=hds, data=data, verify=self.ca_cert, timeout=self.timeout, stream=True)
-                elif type == 'DEL':
-                    result = self.session.delete(url, headers=hds, data=data, verify=self.ca_cert, timeout=self.timeout)
-                else:
-                    return
-#             except ConnectionError, e:
-#                 LOG.warning('ConnectionError: ' + str(e))
-#                 retry += 1
-#                 if retry > self.request_retries:
-#                     raise
-#                 continue
-            except SSLError as error:
-                LOG.warning('SSLError: ' + str(error))
-                retry += 1
-                self.ca_cert = False
-                if retry > self.request_retries:
-                    raise
-                continue
+    def connect(self, credentials={}):
+        """ Establishes the actual connection to the referred RSE.
 
-            if result is not None and result.status_code == codes.unauthorized:  # pylint: disable-msg=E1101
-                self.__get_token()
-                hds['X-Rucio-Auth-Token'] = self.auth_token
-                retry += 1
-            else:
-                break
-        return result
+            :param credentials: Provides information to establish a connection
+                to the referred storage system. For WebDAV connections these are
+                ca_cert, cert, auth_type, timeout
 
-    def __get_token_userpass(self):
+            :raises RSEAccessDenied
         """
-        Sends a request to get an auth token from the server and stores it as a class attribute. Uses username/password.
+        try:
+            parse_url = urlparse(self.path2pfn(''))
+            self.server = f'{parse_url.scheme}://{parse_url.netloc}'
+        except KeyError:
+            raise exception.RSEAccessDenied('No specified Server')
 
-        :returns: True if the token was successfully received. False otherwise.
-        """
+        try:
+            self.ca_cert = credentials['ca_cert']
+        except KeyError:
+            self.ca_cert = None
 
-        headers = {'X-Rucio-Account': self.account, 'X-Rucio-Username': self.creds['username'], 'X-Rucio-Password': self.creds['password']}
-        url = build_url(self.auth_host, path='auth/userpass')
+        try:
+            self.auth_type = credentials['auth_type']
+        except KeyError:
+            self.auth_type = 'cert'
 
-        retry = 0
-        while retry <= self.AUTH_RETRIES:
-            try:
-                result = self.session.get(url, headers=headers, verify=self.ca_cert)
-            except SSLError as error:
-                LOG.warning('SSLError: ' + str(error))
-                self.ca_cert = False
-                retry += 1
-                if retry > self.request_retries:
-                    raise
-                continue
-            break
+        try:
+            self.cert = credentials['cert']
+        except KeyError:
+            x509 = os.getenv('X509_USER_PROXY')
+            if not x509:
+                # Trying to get the proxy from the default location
+                proxy_path = '/tmp/x509up_u%s' % os.geteuid()
+                if os.path.isfile(proxy_path):
+                    x509 = proxy_path
+                elif self.auth_token:
+                    pass
+                else:
+                    raise exception.RSEAccessDenied('X509_USER_PROXY is not set')
+            self.cert = (x509, x509)
+
+        try:
+            self.timeout = credentials['timeout']
+        except KeyError:
+            self.timeout = 300
+        self.session = requests.Session()
+        self.session.mount('https://', TLSHTTPAdapter())
+        if self.auth_token:
+            self.session.headers.update({'Authorization': 'Bearer ' + self.auth_token})
+        # "ping" to see if the server is available
+        try:
+            res = self.session.request('HEAD', self.path2pfn(''), verify=False, timeout=self.timeout, cert=self.cert)
+            if res.status_code != 200:
+                raise exception.ServiceUnavailable('Problem to connect %s : %s' % (self.path2pfn(''), res.text))
+        except requests.exceptions.ConnectionError as error:
+            raise exception.ServiceUnavailable('Problem to connect %s : %s' % (self.path2pfn(''), error))
+        except requests.exceptions.ReadTimeout as error:
+            raise exception.ServiceUnavailable(error)
+
+    def close(self):
+        self.session.close()
+
+    def path2pfn(self, path):
+        """
+            Returns a fully qualified PFN for the file referred by path.
 
-        if retry == 2:
-            LOG.error('cannot get auth_token')
-            return False
-
-        if result.status_code != codes.ok:  # pylint: disable-msg=E1101
-            exc_cls, exc_msg = self._get_exception(headers=result.headers,
-                                                   status_code=result.status_code,
-                                                   data=result.content)
-            raise exc_cls(exc_msg)
-
-        self.auth_token = result.headers['x-rucio-auth-token']
-        LOG.debug('got new token \'%s\'' % self.auth_token)
-        return True
-
-    def __get_token_x509(self):
-        """
-        Sends a request to get an auth token from the server and stores it as a class attribute. Uses x509 authentication.
-
-        :returns: True if the token was successfully received. False otherwise.
-        """
-
-        headers = {'X-Rucio-Account': self.account}
-
-        client_cert = None
-        client_key = None
-        if self.auth_type == 'x509':
-            url = build_url(self.auth_host, path='auth/x509')
-            client_cert = self.creds['client_cert']
-            if 'client_key' in self.creds:
-                client_key = self.creds['client_key']
-        elif self.auth_type == 'x509_proxy':
-            url = build_url(self.auth_host, path='auth/x509_proxy')
-            client_cert = self.creds['client_proxy']
-
-        if not path.exists(client_cert):
-            LOG.error('given client cert (%s) doesn\'t exist' % client_cert)
-            return False
-        if client_key is not None and not path.exists(client_key):
-            LOG.error('given client key (%s) doesn\'t exist' % client_key)
+            :param path: The path to the file.
 
-        retry = 0
-        result = None
+            :returns: Fully qualified PFN.
 
-        if client_key is None:
-            cert = client_cert
+        """
+        if not path.startswith('https'):
+            return '%s://%s:%s%s%s' % (self.attributes['scheme'], self.attributes['hostname'], str(self.attributes['port']), self.attributes['prefix'], path)
         else:
-            cert = (client_cert, client_key)
+            return path
 
-        while retry <= self.AUTH_RETRIES:
-            try:
-                result = self.session.get(url, headers=headers, cert=cert,
-                                          verify=self.ca_cert)
-            except SSLError as error:
-                if 'alert certificate expired' in str(error):
-                    raise CannotAuthenticate(str(error))
-                LOG.warning('SSLError: ' + str(error))
-                self.ca_cert = False
-                retry += 1
-                if retry > self.request_retries:
-                    raise
-                continue
-            break
+    def exists(self, pfn):
+        """ Checks if the requested file is known by the referred RSE.
 
-        if retry == 2:
-            LOG.error('cannot get auth_token')
-            return False
+            :param pfn: Physical file name
 
-        if result and result.status_code != codes.ok:   # pylint: disable-msg=E1101
-            exc_cls, exc_msg = self._get_exception(headers=result.headers,
-                                                   status_code=result.status_code,
-                                                   data=result.content)
-            raise exc_cls(exc_msg)
+            :returns: True if the file exists, False if it doesn't
 
-        self.auth_token = result.headers['x-rucio-auth-token']
-        LOG.debug('got new token \'%s\'' % self.auth_token)
-        return True
+            :raise  ServiceUnavailable, RSEAccessDenied
+        """
+        path = self.path2pfn(pfn)
+        try:
+            result = self.session.request('HEAD', path, verify=False, timeout=self.timeout, cert=self.cert)
+            if result.status_code == 200:
+                return True
+            elif result.status_code in [401, ]:
+                raise exception.RSEAccessDenied()
+            elif result.status_code in [404, ]:
+                return False
+            else:
+                # catchall exception
+                raise exception.RucioException(result.status_code, result.text)
+        except requests.exceptions.ConnectionError as error:
+            raise exception.ServiceUnavailable(error)
+
+    def get(self, pfn, dest='.', transfer_timeout=None):
+        """ Provides access to files stored inside connected the RSE.
+
+            :param pfn: Physical file name of requested file
+            :param dest: Name and path of the files when stored at the client
+            :param transfer_timeout: Transfer timeout (in seconds) - dummy
 
-    def __get_token_gss(self):
+            :raises DestinationNotAccessible, ServiceUnavailable, SourceNotFound, RSEAccessDenied
         """
-        Sends a request to get an auth token from the server and stores it as a class attribute. Uses Kerberos authentication.
+        path = self.path2pfn(pfn)
+        chunksize = 1024
+        try:
+            result = self.session.get(path, verify=False, stream=True, timeout=self.timeout, cert=self.cert)
+            if result and result.status_code in [200, ]:
+                length = None
+                if 'content-length' in result.headers:
+                    length = int(result.headers['content-length'])
+                with open(dest, 'wb') as file_out:
+                    nchunk = 0
+                    if not length:
+                        print('Malformed HTTP response (missing content-length header).')
+                    for chunk in result.iter_content(chunksize):
+                        file_out.write(chunk)
+                        if length:
+                            nchunk += 1
+            elif result.status_code in [404, ]:
+                raise exception.SourceNotFound()
+            elif result.status_code in [401, 403]:
+                raise exception.RSEAccessDenied()
+            else:
+                # catchall exception
+                raise exception.RucioException(result.status_code, result.text)
+        except requests.exceptions.ConnectionError as error:
+            raise exception.ServiceUnavailable(error)
+        except requests.exceptions.ReadTimeout as error:
+            raise exception.ServiceUnavailable(error)
+
+    def put(self, source, target, source_dir=None, transfer_timeout=None, progressbar=False):
+        """ Allows to store files inside the referred RSE.
+
+            :param source: Physical file name
+            :param target: Name of the file on the storage system e.g. with prefixed scope
+            :param source_dir Path where the to be transferred files are stored in the local file system
+            :param transfer_timeout Transfer timeout (in seconds) - dummy
+
+            :raises DestinationNotAccessible, ServiceUnavailable, SourceNotFound, RSEAccessDenied
+        """
+        path = self.path2pfn(target)
+        full_name = source_dir + '/' + source if source_dir else source
+        directories = path.split('/')
+        # Try the upload without testing the existence of the destination directory
+        try:
+            if not os.path.exists(full_name):
+                raise exception.SourceNotFound()
+            it = UploadInChunks(full_name, 10000000, progressbar)
+            result = self.session.put(path, data=IterableToFileAdapter(it), verify=False, allow_redirects=True, timeout=self.timeout, cert=self.cert)
+            if result.status_code in [200, 201]:
+                return
+            if result.status_code in [409, ]:
+                raise exception.FileReplicaAlreadyExists()
+            else:
+                # Create the directories before issuing the PUT
+                for directory_level in reversed(list(range(1, 4))):
+                    upper_directory = "/".join(directories[:-directory_level])
+                    self.mkdir(upper_directory)
+                try:
+                    if not os.path.exists(full_name):
+                        raise exception.SourceNotFound()
+                    it = UploadInChunks(full_name, 10000000, progressbar)
+                    result = self.session.put(path, data=IterableToFileAdapter(it), verify=False, allow_redirects=True, timeout=self.timeout, cert=self.cert)
+                    if result.status_code in [200, 201]:
+                        return
+                    if result.status_code in [409, ]:
+                        raise exception.FileReplicaAlreadyExists()
+                    elif result.status_code in [401, ]:
+                        raise exception.RSEAccessDenied()
+                    else:
+                        # catchall exception
+                        raise exception.RucioException(result.status_code, result.text)
+                except requests.exceptions.ConnectionError as error:
+                    raise exception.ServiceUnavailable(error)
+                except IOError as error:
+                    raise exception.SourceNotFound(error)
+        except requests.exceptions.ConnectionError as error:
+            raise exception.ServiceUnavailable(error)
+        except requests.exceptions.ReadTimeout as error:
+            raise exception.ServiceUnavailable(error)
+        except IOError as error:
+            raise exception.SourceNotFound(error)
+
+    def rename(self, pfn, new_pfn):
+        """ Allows to rename a file stored inside the connected RSE.
+
+            :param pfn:      Current physical file name
+            :param new_pfn  New physical file name
+
+            :raises DestinationNotAccessible, ServiceUnavailable, SourceNotFound, RSEAccessDenied
+        """
+        path = self.path2pfn(pfn)
+        new_path = self.path2pfn(new_pfn)
+        directories = new_path.split('/')
+
+        headers = {'Destination': new_path}
+        # Try the rename without testing the existence of the destination directory
+        try:
+            result = self.session.request('MOVE', path, verify=False, headers=headers, timeout=self.timeout, cert=self.cert)
+            if result.status_code == 201:
+                return
+            elif result.status_code in [404, ]:
+                raise exception.SourceNotFound()
+            else:
+                # Create the directories before issuing the MOVE
+                for directory_level in reversed(list(range(1, 4))):
+                    upper_directory = "/".join(directories[:-directory_level])
+                    self.mkdir(upper_directory)
+                try:
+                    result = self.session.request('MOVE', path, verify=False, headers=headers, timeout=self.timeout, cert=self.cert)
+                    if result.status_code == 201:
+                        return
+                    elif result.status_code in [404, ]:
+                        raise exception.SourceNotFound()
+                    elif result.status_code in [401, ]:
+                        raise exception.RSEAccessDenied()
+                    else:
+                        # catchall exception
+                        raise exception.RucioException(result.status_code, result.text)
+                except requests.exceptions.ConnectionError as error:
+                    raise exception.ServiceUnavailable(error)
+        except requests.exceptions.ConnectionError as error:
+            raise exception.ServiceUnavailable(error)
+        except requests.exceptions.ReadTimeout as error:
+            raise exception.ServiceUnavailable(error)
+
+    def delete(self, pfn):
+        """ Deletes a file from the connected RSE.
+
+            :param pfn: Physical file name
 
-        :returns: True if the token was successfully received. False otherwise.
+            :raises ServiceUnavailable, SourceNotFound, RSEAccessDenied, ResourceTemporaryUnavailable
         """
+        path = self.path2pfn(pfn)
+        try:
+            result = self.session.delete(path, verify=False, timeout=self.timeout, cert=self.cert)
+            if result.status_code in [204, ]:
+                return
+            elif result.status_code in [404, ]:
+                raise exception.SourceNotFound()
+            elif result.status_code in [401, 403]:
+                raise exception.RSEAccessDenied()
+            elif result.status_code in [500, 503]:
+                raise exception.ResourceTemporaryUnavailable()
+            else:
+                # catchall exception
+                raise exception.RucioException(result.status_code, result.text)
+        except requests.exceptions.ConnectionError as error:
+            raise exception.ServiceUnavailable(error)
+        except requests.exceptions.ReadTimeout as error:
+            raise exception.ServiceUnavailable(error)
 
-        headers = {'X-Rucio-Account': self.account}
-        url = build_url(self.auth_host, path='auth/gss')
+    def mkdir(self, directory):
+        """ Internal method to create directories
 
-        retry = 0
-        while retry <= self.AUTH_RETRIES:
-            try:
-                result = self.session.get(url, headers=headers,
-                                          verify=self.ca_cert, auth=HTTPKerberosAuth())
-            except SSLError as error:
-                LOG.warning('SSLError: ' + str(error))
-                self.ca_cert = False
-                retry += 1
-                if retry > self.request_retries:
-                    raise
-                continue
-            break
+            :param directory: Name of the directory that needs to be created
 
-        if retry == 2:
-            LOG.error('cannot get auth_token')
-            return False
-
-        if result.status_code != codes.ok:   # pylint: disable-msg=E1101
-            exc_cls, exc_msg = self._get_exception(headers=result.headers,
-                                                   status_code=result.status_code,
-                                                   data=result.content)
-            raise exc_cls(exc_msg)
-
-        self.auth_token = result.headers['x-rucio-auth-token']
-        LOG.debug('got new token \'%s\'' % self.auth_token)
-        return True
-
-    def __get_token(self):
-        """
-        Calls the corresponding method to receive an auth token depending on the auth type. To be used if a 401 - Unauthorized error is received.
-        """
-
-        retry = 0
-        LOG.debug('get a new token')
-        while retry <= self.AUTH_RETRIES:
-            if self.auth_type == 'userpass':
-                if not self.__get_token_userpass():
-                    raise CannotAuthenticate('userpass authentication failed')
-            elif self.auth_type == 'x509' or self.auth_type == 'x509_proxy':
-                if not self.__get_token_x509():
-                    raise CannotAuthenticate('x509 authentication failed')
-            elif self.auth_type == 'gss':
-                if not self.__get_token_gss():
-                    raise CannotAuthenticate('kerberos authentication failed')
+            :raises DestinationNotAccessible, ServiceUnavailable, SourceNotFound, RSEAccessDenied
+        """
+        path = self.path2pfn(directory)
+        try:
+            result = self.session.request('MKCOL', path, verify=False, timeout=self.timeout, cert=self.cert)
+            if result.status_code in [201, 405]:  # Success or directory already exists
+                return
+            elif result.status_code in [404, ]:
+                raise exception.SourceNotFound()
+            elif result.status_code in [401, ]:
+                raise exception.RSEAccessDenied()
             else:
-                raise CannotAuthenticate('auth type \'%s\' not supported' % self.auth_type)
+                # catchall exception
+                raise exception.RucioException(result.status_code, result.text)
+        except requests.exceptions.ConnectionError as error:
+            raise exception.ServiceUnavailable(error)
+        except requests.exceptions.ReadTimeout as error:
+            raise exception.ServiceUnavailable(error)
+
+    def ls(self, filename):
+        """ Internal method to list files/directories
+
+            :param filename: Name of the directory that needs to be created
+
+            :raises DestinationNotAccessible, ServiceUnavailable, SourceNotFound, RSEAccessDenied
+        """
+        path = self.path2pfn(filename)
+        headers = {'Depth': '1'}
+        self.exists(filename)
+        try:
+            result = self.session.request('PROPFIND', path, verify=False, headers=headers, timeout=self.timeout, cert=self.cert)
+            if result.status_code in [404, ]:
+                raise exception.SourceNotFound()
+            elif result.status_code in [401, ]:
+                raise exception.RSEAccessDenied()
 
-            if self.auth_token is not None:
-                self.__write_token()
-                self.headers['X-Rucio-Auth-Token'] = self.auth_token
-                break
+            try:
+                propfind = _PropfindResponse.parse(result.text)
+            except ValueError:
+                raise exception.ServiceUnavailable("Couldn't parse WebDAV response.")
 
-            retry += 1
+            list_files = [self.server + file.href for file in propfind.files if file.href is not None]
 
-        if self.auth_token is None:
-            raise CannotAuthenticate('cannot get an auth token from server')
+            try:
+                list_files.remove(filename + '/')
+            except ValueError:
+                pass
+            try:
+                list_files.remove(filename)
+            except ValueError:
+                pass
 
-    def __read_token(self):
+            return list_files
+        except requests.exceptions.ConnectionError as error:
+            raise exception.ServiceUnavailable(error)
+        except requests.exceptions.ReadTimeout as error:
+            raise exception.ServiceUnavailable(error)
+
+    def stat(self, path):
         """
-        Checks if a local token file exists and reads the token from it.
+            Returns the stats of a file.
+
+            :param path: path to file
+
+            :raises ServiceUnavailable: if some generic error occured in the library.
+            :raises SourceNotFound: if the source file was not found on the referred storage.
+            :raises RSEAccessDenied: in case of permission issue.
 
-        :return: True if a token could be read. False if no file exists.
+            :returns: a dict with two keys, filesize and adler32 of the file provided in path.
         """
-        if not path.exists(self.token_file):
-            return False
+        headers = {'Depth': '1'}
+        dict_ = {}
+        try:
+            result = self.session.request('PROPFIND', path, verify=False, headers=headers, timeout=self.timeout, cert=self.cert)
+            if result.status_code in [404, ]:
+                raise exception.SourceNotFound()
+            elif result.status_code in [401, ]:
+                raise exception.RSEAccessDenied()
+            if result.status_code in [400, ]:
+                raise exception.InvalidRequest()
+        except requests.exceptions.ConnectionError as error:
+            raise exception.ServiceUnavailable(error)
+        except requests.exceptions.ReadTimeout as error:
+            raise exception.ServiceUnavailable(error)
+
+        path_parts = self.parse_pfns(path)[path]
+        local_path = os.path.join(path_parts['prefix'], path_parts['path'][1:], path_parts['name'])
 
         try:
-            token_file_handler = open(self.token_file, 'r')
-            self.auth_token = token_file_handler.readline()
-            self.headers['X-Rucio-Auth-Token'] = self.auth_token
-        except IOError as (errno, strerror):  # NOQA
-            print("I/O error({0}): {1}".format(errno, strerror))
-        except Exception:
-            raise
+            propfind = _PropfindResponse.parse(result.text)
+        except ValueError:
+            raise exception.ServiceUnavailable("Couldn't parse WebDAV response.")
 
-        LOG.debug('read token \'%s\' from file' % self.auth_token)
-        return True
+        for file in propfind.files:
+            if file.href != str(local_path):
+                continue
 
-    def __write_token(self):
-        """
-        Write the current auth_token to the local token file.
+            if file.size is None:
+                continue
+
+            dict_['filesize'] = file.size
+            break
+        else:
+            raise exception.ServiceUnavailable("WebDAV response didn't include content length for requested path.")
+
+        return dict_
+
+    def get_space_usage(self):
         """
+        Get RSE space usage information.
 
-        token_path = self.TOKEN_PATH_PREFIX + self.account
-        self.token_file = token_path + '/' + self.TOKEN_PREFIX + self.account
+        :returns: a list with dict containing 'totalsize' and 'unusedsize'
 
-        # check if rucio temp directory is there. If not create it with permissions only for the current user
-        if not path.isdir(token_path):
-            try:
-                LOG.debug('rucio token folder \'%s\' not found. Create it.' % token_path)
-                makedirs(token_path, 0700)
-            except Exception:
-                raise
+        :raises ServiceUnavailable: if some generic error occured in the library.
+        """
+        endpoint_basepath = self.path2pfn('')
+        headers = {'Depth': '0'}
 
-        # if the file exists check if the stored token is valid. If not request a new one and overwrite the file. Otherwise use the one from the file
         try:
-            file_d, file_n = mkstemp(dir=token_path)
-            with fdopen(file_d, "w") as f_token:
-                f_token.write(self.auth_token)
-            move(file_n, self.token_file)
-        except IOError as (errno, strerror):  # NOQA
-            print("I/O error({0}): {1}".format(errno, strerror))
-        except Exception:
-            raise
-
-    def __authenticate(self):
-        """
-        Main method for authentication. It first tries to read a locally saved token. If not available it requests a new one.
-        """
-
-        if self.auth_type == 'userpass':
-            if self.creds['username'] is None or self.creds['password'] is None:
-                raise NoAuthInformation('No username or password passed')
-        elif self.auth_type == 'x509':
-            if self.creds['client_cert'] is None:
-                raise NoAuthInformation('The path to the client certificate is required')
-        elif self.auth_type == 'x509_proxy':
-            if self.creds['client_proxy'] is None:
-                raise NoAuthInformation('The client proxy has to be defined')
-        elif self.auth_type == 'gss':
-            pass
-        else:
-            raise CannotAuthenticate('auth type \'%s\' not supported' % self.auth_type)
-
-        if not self.__read_token():
-            self.__get_token()
+            root = ET.fromstring(self.session.request('PROPFIND', endpoint_basepath, verify=False, headers=headers, cert=self.session.cert).text)
+            usedsize = root[0][1][0].find('{DAV:}quota-used-bytes').text
+            try:
+                unusedsize = root[0][1][0].find('{DAV:}quota-available-bytes').text
+            except Exception:
+                print('No free space given, return -999')
+                unusedsize = -999
+            totalsize = int(usedsize) + int(unusedsize)
+            return totalsize, unusedsize
+        except Exception as error:
+            raise exception.ServiceUnavailable(error)
```

### Comparing `rucio-clients-1.9.6/lib/rucio/client/client.py` & `rucio-clients-32.0.0rc1/lib/rucio/client/client.py`

 * *Files 24% similar despite different names*

```diff
@@ -1,47 +1,77 @@
-# Copyright European Organization for Nuclear Research (CERN)
+# -*- coding: utf-8 -*-
+# Copyright European Organization for Nuclear Research (CERN) since 2012
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
-# You may not use this file except in compliance with the License.
-# You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
 #
-# Authors:
-# - Thomas Beermann, <thomas.beermann@cern.ch>, 2012
-# - Vincent Garonne,  <vincent.garonne@cern.ch> , 2011-2013
-# - Ralph Vigne, <ralph.vigne@cern.ch>, 2015
-# - Cedric Serfon, <cedric.serfon@cern.ch>, 2015-2016
+#    http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 
 """
 Client class for callers of the Rucio system
 """
 
 from rucio.client.accountclient import AccountClient
 from rucio.client.accountlimitclient import AccountLimitClient
+from rucio.client.configclient import ConfigClient
+from rucio.client.credentialclient import CredentialClient
 from rucio.client.didclient import DIDClient
+from rucio.client.diracclient import DiracClient
+from rucio.client.exportclient import ExportClient
+from rucio.client.importclient import ImportClient
+from rucio.client.lifetimeclient import LifetimeClient
 from rucio.client.lockclient import LockClient
 from rucio.client.metaclient import MetaClient
 from rucio.client.pingclient import PingClient
 from rucio.client.replicaclient import ReplicaClient
+from rucio.client.requestclient import RequestClient
 from rucio.client.rseclient import RSEClient
 from rucio.client.ruleclient import RuleClient
 from rucio.client.scopeclient import ScopeClient
 from rucio.client.subscriptionclient import SubscriptionClient
-from rucio.client.configclient import ConfigClient
+from rucio.client.touchclient import TouchClient
 
 
-class Client(AccountClient, AccountLimitClient, MetaClient, PingClient, ReplicaClient, RSEClient, ScopeClient, DIDClient, RuleClient, SubscriptionClient, LockClient, ConfigClient):
+class Client(AccountClient,
+             AccountLimitClient,
+             MetaClient,
+             PingClient,
+             ReplicaClient,
+             RequestClient,
+             RSEClient,
+             ScopeClient,
+             DIDClient,
+             RuleClient,
+             SubscriptionClient,
+             LockClient,
+             ConfigClient,
+             TouchClient,
+             ImportClient,
+             ExportClient,
+             CredentialClient,
+             DiracClient,
+             LifetimeClient):
 
     """Main client class for accessing Rucio resources. Handles the authentication."""
 
-    def __init__(self, rucio_host=None, auth_host=None, account=None, ca_cert=None, auth_type=None, creds=None, timeout=None, user_agent='rucio-clients'):
+    def __init__(self, **args):
         """
         Constructor for the Rucio main client class.
 
         :param rucio_host: the host of the rucio system.
         :param auth_host: the host of the rucio authentication server.
         :param account: the rucio account that should be used to interact with the rucio system.
         :param ca_cert: the certificate to verify the server.
         :param auth_type: the type of authentication to use (e.g. userpass, x509 ...)
         :param creds: credentials needed for authentication.
         :param timeout: Float describes the timeout of the request (in seconds).
+        :param vo: The vo that the client will interact with.
+        :param logger: Logger instance to use (optional)
         """
-        super(Client, self).__init__(rucio_host=rucio_host, auth_host=auth_host, account=account, ca_cert=ca_cert, auth_type=auth_type, creds=creds, timeout=timeout, user_agent=user_agent)
+        super(Client, self).__init__(**args)
```

### Comparing `rucio-clients-1.9.6/lib/rucio/client/didclient.py` & `rucio-clients-32.0.0rc1/lib/rucio/client/didclient.py`

 * *Files 24% similar despite different names*

```diff
@@ -1,183 +1,199 @@
-'''
-  Copyright European Organization for Nuclear Research (CERN)
-
-  Licensed under the Apache License, Version 2.0 (the "License");
-  You may not use this file except in compliance with the License.
-  You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0
-
-  Authors:
-  - Vincent Garonne, <vincent.garonne@cern.ch>, 2012-2016
-  - Mario Lassnig, <mario.lassnig@cern.ch>, 2012-2013
-  - Thomas Beermann, <thomas.beermann@cern.ch> 2013
-  - Yun-Pin Sun, <yun-pin.sun@cern.ch>, 2013
-  - Cedric Serfon, <cedric.serfon@cern.ch>, 2014-2015
-  - Ralph Vigne, <ralph.vigne@cern.ch>, 2015
-  - Martin Barisits, <martin.barisits@cern.ch>, 2014-2015
-'''
+# -*- coding: utf-8 -*-
+# Copyright European Organization for Nuclear Research (CERN) since 2012
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#    http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 
+from datetime import datetime
 from json import dumps
+from urllib.parse import quote_plus
+
 from requests.status_codes import codes
 
 from rucio.client.baseclient import BaseClient
 from rucio.client.baseclient import choice
+from rucio.common.exception import DeprecationError
 from rucio.common.utils import build_url, render_json, render_json_list, date_to_str
 
 
 class DIDClient(BaseClient):
 
     """DataIdentifier client class for working with data identifiers"""
 
     DIDS_BASEURL = 'dids'
+    ARCHIVES_BASEURL = 'archives'
 
-    def __init__(self, rucio_host=None, auth_host=None, account=None, ca_cert=None, auth_type=None, creds=None, timeout=None, user_agent='rucio-clients'):
-        super(DIDClient, self).__init__(rucio_host, auth_host, account, ca_cert, auth_type, creds, timeout, user_agent)
-
-    def list_dids(self, scope, filters, type='collection', long=False):
+    def list_dids(self, scope, filters, did_type='collection', long=False, recursive=False):
         """
         List all data identifiers in a scope which match a given pattern.
 
         :param scope: The scope name.
-        :param filters: A dictionary of key/value pairs like {'name': 'file_name','rse-expression': 'tier0'}.
-        :param type: The type of the did: 'all'(container, dataset or file)|'collection'(dataset or container)|'dataset'|'container'|'file'
+        :param filters: A nested dictionary of key/value pairs like [{'key1': 'value1', 'key2.lte': 'value2'}, {'key3.gte, 'value3'}].
+                        Keypairs in the same dictionary are AND'ed together, dictionaries are OR'ed together. Keys should be suffixed
+                        like <key>.<operation>, e.g. key1 >= value1 is equivalent to {'key1.gte': value}, where <operation> belongs to one
+                        of the set {'lte', 'gte', 'gt', 'lt', 'ne' or ''}. Equivalence doesn't require an operator.
+        :param did_type: The type of the did: 'all'(container, dataset or file)|'collection'(dataset or container)|'dataset'|'container'|'file'
         :param long: Long format option to display more information for each DID.
+        :param recursive: Recursively list DIDs content.
         """
-        path = '/'.join([self.DIDS_BASEURL, scope, 'dids', 'search'])
-        payload = {}
-        if long:
-            payload['long'] = 1
+        path = '/'.join([self.DIDS_BASEURL, quote_plus(scope), 'dids', 'search'])
 
-        for k, v in filters.items():
-            if k in ('created_before', 'created_after'):
-                payload[k] = date_to_str(v)
-            else:
-                payload[k] = v
-        payload['type'] = type
+        # stringify dates.
+        if isinstance(filters, dict):   # backwards compatability for filters as single {}
+            filters = [filters]
+        for or_group in filters:
+            for key, value in or_group.items():
+                if isinstance(value, datetime):
+                    or_group[key] = date_to_str(value)
+
+        payload = {
+            'type': did_type,
+            'filters': filters,
+            'long': long,
+            'recursive': recursive
+        }
 
         url = build_url(choice(self.list_hosts), path=path, params=payload)
 
-        r = self._send_request(url, type='GET')
+        r = self._send_request(url, type_='GET')
+
         if r.status_code == codes.ok:
             dids = self._load_json_data(r)
             return dids
         else:
             exc_cls, exc_msg = self._get_exception(headers=r.headers, status_code=r.status_code, data=r.content)
             raise exc_cls(exc_msg)
 
-    def add_did(self, scope, name, type, statuses=None, meta=None, rules=None, lifetime=None, dids=None, rse=None):
+    def list_dids_extended(self, scope, filters, did_type='collection', long=False, recursive=False):
+        """
+        List all data identifiers in a scope which match a given pattern (DEPRECATED)
+        """
+        raise DeprecationError("Command or function has been deprecated. Please use list_dids instead.")
+
+    def add_did(self, scope, name, did_type, statuses=None, meta=None, rules=None, lifetime=None, dids=None, rse=None):
         """
         Add data identifier for a dataset or container.
 
         :param scope: The scope name.
         :param name: The data identifier name.
-        :paran type: The data identifier type (file|dataset|container).
+        :param did_type: The data identifier type (file|dataset|container).
         :param statuses: Dictionary with statuses, e.g.g {'monotonic':True}.
-        :meta: Meta-data associated with the data identifier is represented using key/value pairs in a dictionary.
-        :rules: Replication rules associated with the data identifier. A list of dictionaries, e.g., [{'copies': 2, 'rse_expression': 'TIERS1'}, ].
+        :param meta: Meta-data associated with the data identifier is represented using key/value pairs in a dictionary.
+        :param rules: Replication rules associated with the data identifier. A list of dictionaries, e.g., [{'copies': 2, 'rse_expression': 'TIERS1'}, ].
         :param lifetime: DID's lifetime (in seconds).
         :param dids: The content.
         :param rse: The RSE name when registering replicas.
         """
-        path = '/'.join([self.DIDS_BASEURL, scope, name])
+        path = '/'.join([self.DIDS_BASEURL, quote_plus(scope), quote_plus(name)])
         url = build_url(choice(self.list_hosts), path=path)
         # Build json
-        data = {'type': type}
+        data = {'type': did_type}
         if statuses:
             data['statuses'] = statuses
         if meta:
             data['meta'] = meta
         if rules:
             data['rules'] = rules
         if lifetime:
             data['lifetime'] = lifetime
         if dids:
             data['dids'] = dids
         if rse:
             data['rse'] = rse
-        r = self._send_request(url, type='POST', data=render_json(**data))
+        r = self._send_request(url, type_='POST', data=render_json(**data))
         if r.status_code == codes.created:
             return True
         else:
             exc_cls, exc_msg = self._get_exception(headers=r.headers, status_code=r.status_code, data=r.content)
             raise exc_cls(exc_msg)
 
     def add_dids(self, dids):
         """
         Bulk add datasets/containers.
         """
         path = '/'.join([self.DIDS_BASEURL])
         url = build_url(choice(self.list_hosts), path=path)
-        r = self._send_request(url, type='POST', data=render_json_list(dids))
+        r = self._send_request(url, type_='POST', data=render_json_list(dids))
         if r.status_code == codes.created:
             return True
         else:
             exc_cls, exc_msg = self._get_exception(headers=r.headers, status_code=r.status_code, data=r.content)
             raise exc_cls(exc_msg)
 
     def add_dataset(self, scope, name, statuses=None, meta=None, rules=None, lifetime=None, files=None, rse=None):
         """
         Add data identifier for a dataset.
 
         :param scope: The scope name.
         :param name: The data identifier name.
         :param statuses: Dictionary with statuses, e.g.g {'monotonic':True}.
-        :meta: Meta-data associated with the data identifier is represented using key/value pairs in a dictionary.
-        :rules: Replication rules associated with the data identifier. A list of dictionaries, e.g., [{'copies': 2, 'rse_expression': 'TIERS1'}, ].
+        :param meta: Meta-data associated with the data identifier is represented using key/value pairs in a dictionary.
+        :param rules: Replication rules associated with the data identifier. A list of dictionaries, e.g., [{'copies': 2, 'rse_expression': 'TIERS1'}, ].
         :param lifetime: DID's lifetime (in seconds).
         :param files: The content.
         :param rse: The RSE name when registering replicas.
         """
-        return self.add_did(scope=scope, name=name, type='DATASET',
+        return self.add_did(scope=scope, name=name, did_type='DATASET',
                             statuses=statuses, meta=meta, rules=rules,
                             lifetime=lifetime, dids=files, rse=rse)
 
     def add_datasets(self, dsns):
         """
         Bulk add datasets.
 
         :param dsns: A list of datasets.
         """
-        return self.add_dids(dids=[dict(dsn.items() + [('type', 'DATASET')]) for dsn in dsns])
+        return self.add_dids(dids=[dict(list(dsn.items()) + [('type', 'DATASET')]) for dsn in dsns])
 
     def add_container(self, scope, name, statuses=None, meta=None, rules=None, lifetime=None):
         """
         Add data identifier for a container.
 
         :param scope: The scope name.
         :param name: The data identifier name.
         :param statuses: Dictionary with statuses, e.g.g {'monotonic':True}.
-        :meta: Meta-data associated with the data identifier is represented using key/value pairs in a dictionary.
-        :rules: Replication rules associated with the data identifier. A list of dictionaries, e.g., [{'copies': 2, 'rse_expression': 'TIERS1'}, ].
+        :param meta: Meta-data associated with the data identifier is represented using key/value pairs in a dictionary.
+        :param rules: Replication rules associated with the data identifier. A list of dictionaries, e.g., [{'copies': 2, 'rse_expression': 'TIERS1'}, ].
         :param lifetime: DID's lifetime (in seconds).
         """
-        return self.add_did(scope=scope, name=name, type='CONTAINER', statuses=statuses, meta=meta, rules=rules)
+        return self.add_did(scope=scope, name=name, did_type='CONTAINER', statuses=statuses, meta=meta, rules=rules, lifetime=lifetime)
 
     def add_containers(self, cnts):
         """
         Bulk add containers.
 
         :param cnts: A list of containers.
         """
-        return self.add_dids(dids=[dict(cnts.items() + [('type', 'CONTAINER')]) for cnt in cnts])
+        return self.add_dids(dids=[dict(list(cnts.items()) + [('type', 'CONTAINER')]) for cnt in cnts])
 
     def attach_dids(self, scope, name, dids, rse=None):
         """
         Attach data identifier.
 
         :param scope: The scope name.
         :param name: The data identifier name.
         :param dids: The content.
         :param rse: The RSE name when registering replicas.
         """
-        path = '/'.join([self.DIDS_BASEURL, scope, name, 'dids'])
+        path = '/'.join([self.DIDS_BASEURL, quote_plus(scope), quote_plus(name), 'dids'])
         url = build_url(choice(self.list_hosts), path=path)
         data = {'dids': dids}
         if rse:
             data['rse'] = rse
-        r = self._send_request(url, type='POST', data=render_json(**data))
+        r = self._send_request(url, type_='POST', data=render_json(**data))
         if r.status_code == codes.created:
             return True
         else:
             exc_cls, exc_msg = self._get_exception(headers=r.headers, status_code=r.status_code, data=r.content)
             raise exc_cls(exc_msg)
 
     def detach_dids(self, scope, name, dids):
@@ -185,37 +201,37 @@
         Detach data identifier
 
         :param scope: The scope name.
         :param name: The data identifier name.
         :param dids: The content.
         """
 
-        path = '/'.join([self.DIDS_BASEURL, scope, name, 'dids'])
+        path = '/'.join([self.DIDS_BASEURL, quote_plus(scope), quote_plus(name), 'dids'])
         url = build_url(choice(self.list_hosts), path=path)
         data = {'dids': dids}
-        r = self._send_request(url, type='DEL', data=render_json(**data))
+        r = self._send_request(url, type_='DEL', data=render_json(**data))
         if r.status_code == codes.ok:
             return True
         exc_cls, exc_msg = self._get_exception(headers=r.headers, status_code=r.status_code, data=r.content)
         raise exc_cls(exc_msg)
 
     def attach_dids_to_dids(self, attachments, ignore_duplicate=False):
         """
         Add dids to dids.
 
         :param attachments: The attachments.
             attachments is: [attachment, attachment, ...]
             attachment is: {'scope': scope, 'name': name, 'dids': dids}
             dids is: [{'scope': scope, 'name': name}, ...]
-            :param ignore_duplicate: If True, ignore duplicate entries.
+        :param ignore_duplicate: If True, ignore duplicate entries.
         """
         path = '/'.join([self.DIDS_BASEURL, 'attachments'])
         url = build_url(choice(self.list_hosts), path=path)
         data = {'ignore_duplicate': ignore_duplicate, 'attachments': attachments}
-        r = self._send_request(url, type='POST', data=dumps(data))
+        r = self._send_request(url, type_='POST', data=dumps(data))
         if r.status_code in (codes.ok, codes.no_content, codes.created):
             return True
 
         exc_cls, exc_msg = self._get_exception(headers=r.headers, status_code=r.status_code, data=r.content)
         raise exc_cls(exc_msg)
 
     def add_files_to_datasets(self, attachments, ignore_duplicate=False):
@@ -260,24 +276,23 @@
         :param scope: The scope name.
         :param name: The dataset name.
         :param files: The content.
         :param rse: The RSE name when registering replicas.
         """
         return self.attach_dids(scope=scope, name=name, dids=files, rse=rse)
 
-    def add_files_to_archive(self, scope, name, files, rse=None):
+    def add_files_to_archive(self, scope, name, files):
         """
         Add files to archive.
 
         :param scope: The scope name.
         :param name: The dataset name.
         :param files: The content.
-        :param rse: The RSE name when registering replicas.
         """
-        pass
+        return self.attach_dids(scope=scope, name=name, dids=files)
 
     def add_datasets_to_container(self, scope, name, dsns):
         """
         Add datasets to container.
 
         :param scope: The scope name.
         :param name: The dataset name.
@@ -287,45 +302,45 @@
 
     def add_containers_to_container(self, scope, name, cnts):
         """
         Add containers to container.
 
         :param scope: The scope name.
         :param name: The dataset name.
-        :param dsns: The content.
+        :param cnts: The content.
         """
         return self.attach_dids(scope=scope, name=name, dids=cnts)
 
     def list_content(self, scope, name):
         """
         List data identifier contents.
 
         :param scope: The scope name.
         :param name: The data identifier name.
         """
 
-        path = '/'.join([self.DIDS_BASEURL, scope, name, 'dids'])
+        path = '/'.join([self.DIDS_BASEURL, quote_plus(scope), quote_plus(name), 'dids'])
         url = build_url(choice(self.list_hosts), path=path)
-        r = self._send_request(url, type='GET')
+        r = self._send_request(url, type_='GET')
         if r.status_code == codes.ok:
             return self._load_json_data(r)
         exc_cls, exc_msg = self._get_exception(headers=r.headers, status_code=r.status_code, data=r.content)
         raise exc_cls(exc_msg)
 
     def list_content_history(self, scope, name):
         """
         List data identifier contents history.
 
         :param scope: The scope name.
         :param name: The data identifier name.
         """
 
-        path = '/'.join([self.DIDS_BASEURL, scope, name, 'dids', 'history'])
+        path = '/'.join([self.DIDS_BASEURL, quote_plus(scope), quote_plus(name), 'dids', 'history'])
         url = build_url(choice(self.list_hosts), path=path)
-        r = self._send_request(url, type='GET')
+        r = self._send_request(url, type_='GET')
         if r.status_code == codes.ok:
             return self._load_json_data(r)
         exc_cls, exc_msg = self._get_exception(headers=r.headers, status_code=r.status_code, data=r.content)
         raise exc_cls(exc_msg)
 
     def list_files(self, scope, name, long=None):
         """
@@ -333,92 +348,155 @@
 
         :param scope: The scope name.
         :param name: The data identifier name.
         :param long: A boolean to choose if GUID is returned or not.
         """
 
         payload = {}
-        path = '/'.join([self.DIDS_BASEURL, scope, name, 'files'])
+        path = '/'.join([self.DIDS_BASEURL, quote_plus(scope), quote_plus(name), 'files'])
         if long:
             payload['long'] = True
         url = build_url(choice(self.list_hosts), path=path, params=payload)
 
-        r = self._send_request(url, type='GET')
+        r = self._send_request(url, type_='GET')
         if r.status_code == codes.ok:
             return self._load_json_data(r)
         else:
             exc_cls, exc_msg = self._get_exception(headers=r.headers, status_code=r.status_code, data=r.content)
             raise exc_cls(exc_msg)
 
-    def get_did(self, scope, name):
+    def get_did(self, scope, name, dynamic=False, dynamic_depth=None):
         """
         Retrieve a single data identifier.
 
         :param scope: The scope name.
         :param name: The data identifier name.
+        :param dynamic_depth: The DID type as string ('FILE'/'DATASET') at which to stop the dynamic
+        length/bytes calculation. If not set, the size will not be computed dynamically.
+        :param dynamic: (Deprecated) same as dynamic_depth = 'FILE'
         """
 
-        path = '/'.join([self.DIDS_BASEURL, scope, name])
-        url = build_url(choice(self.list_hosts), path=path)
-        r = self._send_request(url, type='GET')
+        path = '/'.join([self.DIDS_BASEURL, quote_plus(scope), quote_plus(name)])
+        params = {}
+        if dynamic_depth:
+            params['dynamic_depth'] = dynamic_depth
+        elif dynamic:
+            params['dynamic_depth'] = 'FILE'
+        url = build_url(choice(self.list_hosts), path=path, params=params)
+        r = self._send_request(url, type_='GET')
         if r.status_code == codes.ok:
-            return self._load_json_data(r).next()
+            return next(self._load_json_data(r))
         else:
             exc_cls, exc_msg = self._get_exception(headers=r.headers, status_code=r.status_code, data=r.content)
             raise exc_cls(exc_msg)
 
-    def get_metadata(self, scope, name):
+    def get_metadata(self, scope, name, plugin='DID_COLUMN'):
         """
         Get data identifier metadata
 
         :param scope: The scope name.
         :param name: The data identifier name.
+        :param plugin: Backend Metadata plugin the Rucio server should use to query data.
         """
-        path = '/'.join([self.DIDS_BASEURL, scope, name, 'meta'])
+        path = '/'.join([self.DIDS_BASEURL, quote_plus(scope), quote_plus(name), 'meta'])
         url = build_url(choice(self.list_hosts), path=path)
-        r = self._send_request(url, type='GET')
+        payload = {}
+        payload['plugin'] = plugin
+        r = self._send_request(url, type_='GET', params=payload)
         if r.status_code == codes.ok:
             meta = self._load_json_data(r)
-            return meta.next()
+            return next(meta)
         else:
             exc_cls, exc_msg = self._get_exception(headers=r.headers, status_code=r.status_code, data=r.content)
             raise exc_cls(exc_msg)
 
+    def get_metadata_bulk(self, dids, inherit=False):
+        """
+        Bulk get data identifier metadata
+        :param inherit:            A boolean. If set to true, the metadata of the parent are concatenated.
+        :param dids:               A list of dids.
+        """
+        data = {'dids': dids, 'inherit': inherit}
+        path = '/'.join([self.DIDS_BASEURL, 'bulkmeta'])
+        url = build_url(choice(self.list_hosts), path=path)
+        r = self._send_request(url, type_='POST', data=dumps(data))
+        if r.status_code == codes.ok:
+            return self._load_json_data(r)
+        exc_cls, exc_msg = self._get_exception(headers=r.headers, status_code=r.status_code, data=r.content)
+        raise exc_cls(exc_msg)
+
     def set_metadata(self, scope, name, key, value, recursive=False):
         """
         Set data identifier metadata
 
         :param scope: The scope name.
         :param name: The data identifier name.
         :param key: the key.
         :param value: the value.
         :param recursive: Option to propagate the metadata change to content.
         """
-        path = '/'.join([self.DIDS_BASEURL, scope, name, 'meta', key])
+        path = '/'.join([self.DIDS_BASEURL, quote_plus(scope), quote_plus(name), 'meta', key])
         url = build_url(choice(self.list_hosts), path=path)
         data = dumps({'value': value, 'recursive': recursive})
-        r = self._send_request(url, type='POST', data=data)
+        r = self._send_request(url, type_='POST', data=data)
+        if r.status_code == codes.created:
+            return True
+        else:
+            exc_cls, exc_msg = self._get_exception(headers=r.headers, status_code=r.status_code, data=r.content)
+            raise exc_cls(exc_msg)
+
+    def set_metadata_bulk(self, scope, name, meta, recursive=False):
+        """
+        Set data identifier metadata in bulk.
+
+        :param scope: The scope name.
+        :param name: The data identifier name.
+        :param meta: the metadata key-values.
+        :type meta: dict
+        :param recursive: Option to propagate the metadata change to content.
+        """
+        path = '/'.join([self.DIDS_BASEURL, quote_plus(scope), quote_plus(name), 'meta'])
+        url = build_url(choice(self.list_hosts), path=path)
+        data = dumps({'meta': meta, 'recursive': recursive})
+        r = self._send_request(url, type_='POST', data=data)
+        if r.status_code == codes.created:
+            return True
+        else:
+            exc_cls, exc_msg = self._get_exception(headers=r.headers, status_code=r.status_code, data=r.content)
+            raise exc_cls(exc_msg)
+
+    def set_dids_metadata_bulk(self, dids, recursive=False):
+        """
+        Set metadata to a list of data identifiers.
+
+        :param dids: A list of dids including metadata, i.e. [['scope': scope1, 'name': name1, 'meta': {key1: value1, key2: value2}] .
+        :param recursive: Option to propagate the metadata update to content.
+        """
+        path = '/'.join([self.DIDS_BASEURL, 'bulkdidsmeta'])
+        url = build_url(choice(self.list_hosts), path=path)
+        data = dumps({'dids': dids, 'recursive': recursive})
+        r = self._send_request(url, type_='POST', data=data)
         if r.status_code == codes.created:
             return True
         else:
             exc_cls, exc_msg = self._get_exception(headers=r.headers, status_code=r.status_code, data=r.content)
             raise exc_cls(exc_msg)
 
     def set_status(self, scope, name, **kwargs):
         """
         Set data identifier status
 
         :param scope: The scope name.
         :param name: The data identifier name.
         :param kwargs:  Keyword arguments of the form status_name=value.
         """
-        path = '/'.join([self.DIDS_BASEURL, scope, name, 'status'])
+        path = '/'.join([self.DIDS_BASEURL, quote_plus(scope), quote_plus(name), 'status'])
         url = build_url(choice(self.list_hosts), path=path)
         data = dumps(kwargs)
-        r = self._send_request(url, type='PUT', data=data)
+        r = self._send_request(url, type_='PUT', data=data)
         if r.status_code in (codes.ok, codes.no_content, codes.created):
             return True
 
         exc_cls, exc_msg = self._get_exception(headers=r.headers, status_code=r.status_code, data=r.content)
         raise exc_cls(exc_msg)
 
     def close(self, scope, name):
@@ -434,68 +512,69 @@
         """
         Delete data identifier metadata
 
         :param scope: The scope name.
         :param name: The data identifier.
         :param key: the key.
         """
-        path = '/'.join([self.DIDS_BASEURL, scope, name, 'meta', key])
-        url = build_url(choice(self.list_hosts), path=path)
-        r = self._send_request(url, type='DEL')
+        path = '/'.join([self.DIDS_BASEURL, quote_plus(scope), quote_plus(name), 'meta'])
+        url = build_url(choice(self.list_hosts), path=path, params={'key': key})
 
+        r = self._send_request(url, type_='DEL')
         if r.status_code == codes.ok:
             return True
         else:
             exc_cls, exc_msg = self._get_exception(headers=r.headers, status_code=r.status_code, data=r.content)
             raise exc_cls(exc_msg)
 
     def list_did_rules(self, scope, name):
         """
         List the associated rules of a data identifier.
 
         :param scope: The scope name.
         :param name: The data identifier name.
         """
 
-        path = '/'.join([self.DIDS_BASEURL, scope, name, 'rules'])
+        path = '/'.join([self.DIDS_BASEURL, quote_plus(scope), quote_plus(name), 'rules'])
         url = build_url(choice(self.list_hosts), path=path)
-        r = self._send_request(url, type='GET')
+        r = self._send_request(url, type_='GET')
         if r.status_code == codes.ok:
             return self._load_json_data(r)
         else:
             exc_cls, exc_msg = self._get_exception(headers=r.headers, status_code=r.status_code, data=r.content)
             raise exc_cls(exc_msg)
 
     def list_associated_rules_for_file(self, scope, name):
         """
         List the associated rules a file is affected from..
 
         :param scope: The scope name.
         :param name:  The file name.
         """
 
-        path = '/'.join([self.DIDS_BASEURL, scope, name, 'associated_rules'])
+        path = '/'.join([self.DIDS_BASEURL, quote_plus(scope), quote_plus(name), 'associated_rules'])
         url = build_url(choice(self.list_hosts), path=path)
-        r = self._send_request(url, type='GET')
+        r = self._send_request(url, type_='GET')
         if r.status_code == codes.ok:
             return self._load_json_data(r)
         else:
             exc_cls, exc_msg = self._get_exception(headers=r.headers, status_code=r.status_code, data=r.content)
             raise exc_cls(exc_msg)
 
     def get_dataset_by_guid(self, guid):
         """
         Get the parent datasets for a given GUID.
-       :param guid: The GUID.
+        :param guid: The GUID.
 
         :returns: A did
         """
+
         path = '/'.join([self.DIDS_BASEURL, guid, 'guid'])
         url = build_url(choice(self.list_hosts), path=path)
-        r = self._send_request(url, type='GET')
+        r = self._send_request(url, type_='GET')
         if r.status_code == codes.ok:
             return self._load_json_data(r)
         else:
             exc_cls, exc_msg = self._get_exception(headers=r.headers, status_code=r.status_code, data=r.content)
             raise exc_cls(exc_msg)
 
     def scope_list(self, scope, name=None, recursive=False):
@@ -504,40 +583,40 @@
 
         :param scope: The scope name.
         :param name: The data identifier name.
         :param recursive: boolean, True or False.
         """
 
         payload = {}
-        path = '/'.join([self.DIDS_BASEURL, scope, ''])
+        path = '/'.join([self.DIDS_BASEURL, quote_plus(scope), ''])
         if name:
             payload['name'] = name
         if recursive:
             payload['recursive'] = True
         url = build_url(choice(self.list_hosts), path=path, params=payload)
 
-        r = self._send_request(url, type='GET')
+        r = self._send_request(url, type_='GET')
         if r.status_code == codes.ok:
             return self._load_json_data(r)
         else:
             exc_cls, exc_msg = self._get_exception(headers=r.headers, status_code=r.status_code, data=r.content)
             raise exc_cls(exc_msg)
 
     def list_parent_dids(self, scope, name):
         """
         List parent dataset/containers of a did.
 
         :param scope: The scope.
         :param name:  The name.
         """
 
-        path = '/'.join([self.DIDS_BASEURL, scope, name, 'parents'])
+        path = '/'.join([self.DIDS_BASEURL, quote_plus(scope), quote_plus(name), 'parents'])
         url = build_url(choice(self.list_hosts), path=path)
 
-        r = self._send_request(url, type='GET')
+        r = self._send_request(url, type_='GET')
         if r.status_code == codes.ok:
             return self._load_json_data(r)
         else:
             exc_cls, exc_msg = self._get_exception(headers=r.headers, status_code=r.status_code, data=r.content)
             raise exc_cls(exc_msg)
 
     def create_did_sample(self, input_scope, input_name, output_scope, output_name, nbfiles):
@@ -547,43 +626,66 @@
         :param input_scope: The scope of the input DID.
         :param input_name: The name of the input DID.
         :param output_scope: The scope of the output dataset.
         :param output_name: The name of the output dataset.
         :param account: The account.
         :param nbfiles: The number of files to register in the output dataset.
         """
-        path = '/'.join([self.DIDS_BASEURL, input_scope, input_name, output_scope, output_name, str(nbfiles), 'sample'])
+        path = '/'.join([self.DIDS_BASEURL, 'sample'])
+        data = dumps({
+            'input_scope': input_scope,
+            'input_name': input_name,
+            'output_scope': output_scope,
+            'output_name': output_name,
+            'nbfiles': str(nbfiles)
+        })
         url = build_url(choice(self.list_hosts), path=path)
-        r = self._send_request(url, type='POST', data=dumps({}))
+        r = self._send_request(url, type_='POST', data=data)
         if r.status_code == codes.created:
             return True
         else:
             exc_cls, exc_msg = self._get_exception(headers=r.headers, status_code=r.status_code, data=r.content)
             raise exc_cls(exc_msg)
 
     def resurrect(self, dids):
         """
         Resurrect a list of dids.
 
         :param dids:  A list of dids [{'scope': scope, 'name': name}, ...]
         """
         path = '/'.join([self.DIDS_BASEURL, 'resurrect'])
         url = build_url(choice(self.list_hosts), path=path)
-        r = self._send_request(url, type='POST', data=dumps(dids))
+        r = self._send_request(url, type_='POST', data=dumps(dids))
         if r.status_code == codes.created:
             return True
         else:
             exc_cls, exc_msg = self._get_exception(headers=r.headers, status_code=r.status_code, data=r.content)
             raise exc_cls(exc_msg)
 
     def add_temporary_dids(self, dids):
         """
         Bulk add temporary data identifiers.
 
         :param dids: A list of dids.
         """
         url = build_url(choice(self.list_hosts), path='tmp_dids')
-        r = self._send_request(url, type='POST', data=dumps(dids))
+        r = self._send_request(url, type_='POST', data=dumps(dids))
         if r.status_code == codes.created:
             return True
         exc_cls, exc_msg = self._get_exception(headers=r.headers, status_code=r.status_code, data=r.content)
         raise exc_cls(exc_msg)
+
+    def list_archive_content(self, scope, name):
+        """
+        List archive contents.
+
+        :param scope: The scope name.
+        :param name: The data identifier name.
+        """
+        path = '/'.join([self.ARCHIVES_BASEURL, quote_plus(scope), quote_plus(name), 'files'])
+        url = build_url(choice(self.list_hosts), path=path)
+
+        r = self._send_request(url, type_='GET')
+        if r.status_code == codes.ok:
+            return self._load_json_data(r)
+        exc_cls, exc_msg = self._get_exception(headers=r.headers, status_code=r.status_code, data=r.content)
+        raise exc_cls(exc_msg)
```

### Comparing `rucio-clients-1.9.6/lib/rucio/client/fileclient.py` & `rucio-clients-32.0.0rc1/lib/rucio/client/pingclient.py`

 * *Files 24% similar despite different names*

```diff
@@ -1,49 +1,45 @@
-# Copyright European Organization for Nuclear Research (CERN)
+# -*- coding: utf-8 -*-
+# Copyright European Organization for Nuclear Research (CERN) since 2012
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
-# You may not use this file except in compliance with the License.
-# You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
 #
-# Authors:
-# - Vincent Garonne, <vincent.garonne@cern.ch>, 2012-2015
-# - Cedric Serfon, <cedric.serfon@cern.ch>, 2014
-# - Ralph Vigne, <ralph.vigne@cern.ch>, 2015
+#    http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 
 from json import loads
+
 from requests.status_codes import codes
 
 from rucio.client.baseclient import BaseClient
-from rucio.client.baseclient import choice
 from rucio.common.utils import build_url
 
 
-class FileClient(BaseClient):
-    """Dataset client class for working with dataset"""
-
-    BASEURL = 'files'
+class PingClient(BaseClient):
 
-    def __init__(self, rucio_host=None, auth_host=None, account=None, ca_cert=None, auth_type=None, creds=None, timeout=None, user_agent='rucio-clients'):
-        """ Constructor """
-        super(FileClient, self).__init__(rucio_host, auth_host, account, ca_cert, auth_type, creds, timeout, user_agent)
+    """Ping client class"""
 
-    def list_file_replicas(self, scope, lfn):
+    def ping(self):
         """
-        List file replicas.
+        Sends a ping request to the rucio server.
 
-        :param scope: the scope.
-        :param lfn: the lfn.
-
-        :return: List of replicas.
+        :return: Dictonnary with server information
         """
-        path = '/'.join([self.BASEURL, scope, lfn, 'rses'])
-        url = build_url(choice(self.list_hosts), path=path)
 
-        r = self._send_request(url, type='GET')
+        headers = None
+        path = 'ping'
+        url = build_url(self.host, path=path)
+        r = self._send_request(url, headers=headers, type_='GET')
 
         if r.status_code == codes.ok:
-            rses = loads(r.text)
-            return rses
-        else:
-            print r.status_code
-            exc_cls, exc_msg = self._get_exception(headers=r.headers, status_code=r.status_code, data=r.content)
-            raise exc_cls(exc_msg)
+            server_info = loads(r.text)
+            return server_info
+
+        exc_cls, exc_msg = self._get_exception(headers=r.headers, status_code=r.status_code, data=r.content)
+        raise exc_cls(exc_msg)
```

### Comparing `rucio-clients-1.9.6/lib/rucio/client/metaclient.py` & `rucio-clients-32.0.0rc1/lib/rucio/client/metaclient.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,55 +1,58 @@
-# Copyright European Organization for Nuclear Research (CERN)
+# -*- coding: utf-8 -*-
+# Copyright European Organization for Nuclear Research (CERN) since 2012
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
-# You may not use this file except in compliance with the License.
-# You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
 #
-# Authors:
-# - Vincent Garonne, <vincent.garonne@cern.ch>, 2012-2015
-# - Cedric Serfon, <cedric.serfon@cern.ch>, 2014
-# - Ralph Vigne, <ralph.vigne@cern.ch>, 2015
+#    http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 
 from json import dumps, loads
+from urllib.parse import quote_plus
+
 from requests.status_codes import codes
 
 from rucio.client.baseclient import BaseClient
 from rucio.client.baseclient import choice
 from rucio.common.utils import build_url
 
 
 class MetaClient(BaseClient):
 
     """Meta client class for working with data identifier attributes"""
 
     META_BASEURL = 'meta'
 
-    def __init__(self, rucio_host=None, auth_host=None, account=None, ca_cert=None, auth_type=None, creds=None, timeout=None, user_agent='rucio-clients'):
-        super(MetaClient, self).__init__(rucio_host, auth_host, account, ca_cert, auth_type, creds, timeout, user_agent)
-
     def add_key(self, key, key_type, value_type=None, value_regexp=None):
         """
         Sends the request to add a new key.
 
         :param key: the name for the new key.
         :param key_type: the type of the key: all(container, dataset, file), collection(dataset or container), file, derived(compute from file for collection).
         :param value_type: the type of the value, if defined.
         :param value_regexp: the regular expression that values should match, if defined.
 
         :return: True if key was created successfully.
         :raises Duplicate: if key already exists.
         """
 
-        path = '/'.join([self.META_BASEURL, key])
+        path = '/'.join([self.META_BASEURL, quote_plus(key)])
         url = build_url(choice(self.list_hosts), path=path)
         data = dumps({'value_type': value_type and str(value_type),
                       'value_regexp': value_regexp,
                       'key_type': key_type})
 
-        r = self._send_request(url, type='POST', data=data)
+        r = self._send_request(url, type_='POST', data=data)
 
         if r.status_code == codes.created:
             return True
         else:
             exc_cls, exc_msg = self._get_exception(headers=r.headers, status_code=r.status_code, data=r.content)
             raise exc_cls(exc_msg)
 
@@ -71,15 +74,15 @@
 
     def list_values(self, key):
         """
         Sends the request to list all values for a key.
 
         :return: a list containing the names of all values for a key.
         """
-        path = self.META_BASEURL + '/' + key + '/'
+        path = '/'.join([self.META_BASEURL, quote_plus(key)]) + '/'
         url = build_url(choice(self.list_hosts), path=path)
         r = self._send_request(url)
         if r.status_code == codes.ok:
             values = loads(r.text)
             return values
         else:
             exc_cls, exc_msg = self._get_exception(headers=r.headers, status_code=r.status_code, data=r.content)
@@ -92,18 +95,18 @@
         :param key: the name for key.
         :param value: the value.
 
         :return: True if value was created successfully.
         :raises Duplicate: if valid already exists.
         """
 
-        path = self.META_BASEURL + '/' + key + '/'
+        path = '/'.join([self.META_BASEURL, quote_plus(key)]) + '/'
         data = dumps({'value': value})
         url = build_url(choice(self.list_hosts), path=path)
-        r = self._send_request(url, type='POST', data=data)
+        r = self._send_request(url, type_='POST', data=data)
         if r.status_code == codes.created:
             return True
         else:
             exc_cls, exc_msg = self._get_exception(headers=r.headers, status_code=r.status_code, data=r.content)
             raise exc_cls(exc_msg)
 
     def del_value(self, key, value):
@@ -119,16 +122,16 @@
         """
         Delete an allowed key.
 
         :param key: the name for key.
         """
         pass
 
-    def update_key(self, key, type=None, regepx=None):
+    def update_key(self, key, type_=None, regexp=None):
         """
         Update a key.
 
         :param key: the name for key.
-        :param type: the type of the value, if defined.
+        :param type_: the type of the value, if defined.
         :param regexp: the regular expression that values should match, if defined.
         """
         pass
```

### Comparing `rucio-clients-1.9.6/lib/rucio/client/pingclient.py` & `rucio-clients-32.0.0rc1/lib/rucio/client/importclient.py`

 * *Files 27% similar despite different names*

```diff
@@ -1,39 +1,42 @@
-# Copyright European Organization for Nuclear Research (CERN)
+# -*- coding: utf-8 -*-
+# Copyright European Organization for Nuclear Research (CERN) since 2012
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
-# You may not use this file except in compliance with the License.
-# You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
 #
-# Authors:
-# - Vincent Garonne, <vincent.garonne@cern.ch>, 2012-2015
-# - Thomas Beermann, <thomas.beermann@cern.ch>, 2012
-# - Ralph Vigne, <ralph.vigne@cern.ch>, 2015
+#    http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 
-from json import loads
 from requests.status_codes import codes
 
 from rucio.client.baseclient import BaseClient
-from rucio.common.utils import build_url
-
+from rucio.client.baseclient import choice
+from rucio.common.utils import build_url, render_json
 
-class PingClient(BaseClient):
 
-    """Ping client class"""
+class ImportClient(BaseClient):
+    """RSE client class for importing data into Rucio"""
 
-    def __init__(self, rucio_host=None, auth_host=None, account=None, ca_cert=None, auth_type=None, creds=None, timeout=None, user_agent='rucio-clients'):
-        super(PingClient, self).__init__(rucio_host, auth_host, account, ca_cert, auth_type, creds, timeout, user_agent)
+    IMPORT_BASEURL = 'import'
 
-    def ping(self):
+    def import_data(self, data):
         """
-        Sends a ping request to the rucio server.
+        Imports data into Rucio.
 
-        :return: Dictonnary with server information
+        :param data: a dict containing data to be imported into Rucio.
         """
+        path = '/'.join([self.IMPORT_BASEURL])
+        url = build_url(choice(self.list_hosts), path=path)
 
-        headers = None
-        path = 'ping'
-        url = build_url(self.host, path=path)
-        r = self._send_request(url, headers=headers, type='GET')
-        if r.status_code == codes.ok:
-            server_info = loads(r.text)
-            return server_info
+        r = self._send_request(url, type_='POST', data=render_json(**data))
+        if r.status_code == codes.created:
+            return r.text
+        else:
+            exc_cls, exc_msg = self._get_exception(headers=r.headers, status_code=r.status_code, data=r.content)
+            raise exc_cls(exc_msg)
```

### Comparing `rucio-clients-1.9.6/lib/rucio/client/replicaclient.py` & `rucio-clients-32.0.0rc1/lib/rucio/rse/protocols/globus.py`

 * *Files 26% similar despite different names*

```diff
@@ -1,239 +1,243 @@
-'''
-  Copyright European Organization for Nuclear Research (CERN)
+# -*- coding: utf-8 -*-
+# Copyright European Organization for Nuclear Research (CERN) since 2012
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#    http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 
-  Licensed under the Apache License, Version 2.0 (the "License");
-  You may not use this file except in compliance with the License.
-  You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0
+import logging
+from urllib.parse import urlparse
 
-  Authors:
-  - Vincent Garonne, <vincent.garonne@cern.ch>, 2013-2016
-  - Cedric Serfon, <cedric.serfon@cern.ch>, 2014-2015
-  - Ralph Vigne, <ralph.vigne@cern.ch>, 2015
-'''
+from rucio.common import exception
+from rucio.common.extra import import_extras
+from rucio.core.rse import get_rse_attribute
+from rucio.rse.protocols.protocol import RSEProtocol
+from rucio.transfertool.globus_library import get_transfer_client, send_delete_task, send_bulk_delete_task
 
-from json import dumps, loads
-from requests.status_codes import codes
+EXTRA_MODULES = import_extras(['globus_sdk'])
 
-from rucio.client.baseclient import BaseClient
-from rucio.client.baseclient import choice
-from rucio.common.utils import build_url, render_json
+if EXTRA_MODULES['globus_sdk']:
+    from globus_sdk import TransferAPIError  # pylint: disable=import-error
 
 
-class ReplicaClient(BaseClient):
-    """Replica client class for working with replicas"""
+class GlobusRSEProtocol(RSEProtocol):
+    """ Implementing access to RSEs using the Globus service as a Rucio RSE protocol. """
 
-    REPLICAS_BASEURL = 'replicas'
+    def __init__(self, protocol_attr, rse_settings, logger=logging.log):
+        """ Initializes the object with information about the referred RSE.
 
-    def __init__(self, rucio_host=None, auth_host=None, account=None, ca_cert=None, auth_type=None, creds=None, timeout=None, user_agent='rucio-clients'):
-        super(ReplicaClient, self).__init__(rucio_host, auth_host, account, ca_cert, auth_type, creds, timeout, user_agent)
-
-    def declare_bad_file_replicas(self, pfns, reason):
+            :param props: Properties of the requested protocol
         """
-        Declare a list of bad replicas.
+        super(GlobusRSEProtocol, self).__init__(protocol_attr, rse_settings, logger=logger)
+        self.globus_endpoint_id = get_rse_attribute(self.rse.get('id'), 'globus_endpoint_id')
+        self.logger = logger
 
-        :param pfns: The list of PFNs.
-        :param reason: The reason of the loss.
+    def lfns2pfns(self, lfns):
         """
-        data = {'reason': reason, 'pfns': pfns}
-        url = build_url(self.host, path='/'.join([self.REPLICAS_BASEURL, 'bad']))
-        headers = {}
-        r = self._send_request(url, headers=headers, type='POST', data=dumps(data))
-        if r.status_code == codes.created:
-            return loads(r.text)
-        exc_cls, exc_msg = self._get_exception(headers=r.headers, status_code=r.status_code, data=r.content)
-        raise exc_cls(exc_msg)
+            Retruns a fully qualified PFN for the file referred by path.
 
-    def declare_suspicious_file_replicas(self, pfns, reason):
-        """
-        Declare a list of bad replicas.
+            :param path: The path to the file.
 
-        :param pfns: The list of PFNs.
-        :param reason: The reason of the loss.
+            :returns: Fully qualified PFN.
         """
-        data = {'reason': reason, 'pfns': pfns}
-        url = build_url(self.host, path='/'.join([self.REPLICAS_BASEURL, 'suspicious']))
-        headers = {}
-        r = self._send_request(url, headers=headers, type='POST', data=dumps(data))
-        if r.status_code == codes.created:
-            return loads(r.text)
-        exc_cls, exc_msg = self._get_exception(headers=r.headers, status_code=r.status_code, data=r.content)
-        raise exc_cls(exc_msg)
+        pfns = {}
+        prefix = self.attributes['prefix']
 
-    def get_did_from_pfns(self, pfns, rse=None):
-        """
-        Get the DIDs associated to a PFN on one given RSE
+        if not prefix.startswith('/'):
+            prefix = ''.join(['/', prefix])
+        if not prefix.endswith('/'):
+            prefix = ''.join([prefix, '/'])
+
+        lfns = [lfns] if isinstance(lfns, dict) else lfns
+        for lfn in lfns:
+            scope, name = lfn['scope'], lfn['name']
+
+            if 'path' in lfn and lfn['path'] is not None:
+                pfns['%s:%s' % (scope, name)] = ''.join([prefix, lfn['path'] if not lfn['path'].startswith('/') else lfn['path'][1:]])
+            else:
+                pfns['%s:%s' % (scope, name)] = ''.join([prefix, self._get_path(scope=scope, name=name)])
+        return pfns
+
+    def _get_path(self, scope, name):
+        """ Transforms the logical file name into a PFN.
+            Suitable for sites implementing the RUCIO naming convention.
+            This implementation is only invoked if the RSE is deterministic.
 
-        :param pfns: The list of PFNs.
-        :param rse: The RSE name.
-        :returns: A list of dictionaries {pfn: {'scope': scope, 'name': name}}
+            :param scope: scope
+            :param name: filename
+
+            :returns: RSE specific URI of the physical file
         """
-        data = {'rse': rse, 'pfns': pfns}
-        url = build_url(self.host, path='/'.join([self.REPLICAS_BASEURL, 'dids']))
-        headers = {}
-        r = self._send_request(url, headers=headers, type='POST', data=dumps(data))
-        if r.status_code == codes.ok:
-            return self._load_json_data(r)
-        exc_cls, exc_msg = self._get_exception(headers=r.headers, status_code=r.status_code, data=r.content)
-        raise exc_cls(exc_msg)
+        return self.translator.path(scope, name)
 
-    def list_replicas(self, dids, schemes=None, unavailable=False,
-                      all_states=False, metalink=None, rse_expression=None):
+    def parse_pfns(self, pfns):
         """
-        List file replicas for a list of data identifiers (DIDs).
+            Splits the given PFN into the parts known by the protocol. It is also checked if the provided protocol supportes the given PFNs.
+
+            :param pfns: a list of a fully qualified PFNs
+
+            :returns: dic with PFN as key and a dict with path and name as value
 
-        :param dids: The list of data identifiers (DIDs) like :
-        [{'scope': <scope1>, 'name': <name1>}, {'scope': <scope2>, 'name': <name2>}, ...]
-        :param schemes: A list of schemes to filter the replicas. (e.g. file, http, ...)
-        :param unavailable: Also include unavailable replicas in the list.
-        :param metalink: ``None`` (default) retrieves as JSON,
-                         ``3`` retrieves as metalink+xml,
-                         ``4`` retrieves as metalink4+xml
-        :param rse_expression: The RSE expression to restrict replicas on a set of RSEs.
+            :raises RSEFileNameNotSupported: if the provided PFN doesn't match with the protocol settings
         """
-        data = {'dids': dids}
+        ret = dict()
+        pfns = [pfns] if isinstance(pfns, str) else pfns
 
-        if schemes:
-            data['schemes'] = schemes
-        if unavailable:
-            data['unavailable'] = True
-        data['all_states'] = all_states
+        for pfn in pfns:
+            parsed = urlparse(pfn)
+            scheme = parsed.scheme
+            hostname = parsed.netloc.partition(':')[0]
+            port = int(parsed.netloc.partition(':')[2]) if parsed.netloc.partition(':')[2] != '' else 0
+            while '//' in parsed.path:
+                parsed = parsed._replace(path=parsed.path.replace('//', '/'))
+            path = parsed.path
 
-        if rse_expression:
-            data['rse_expression'] = rse_expression
+            # Protect against 'lazy' defined prefixes for RSEs in the repository
+            if not self.attributes['prefix'].startswith('/'):
+                self.attributes['prefix'] = '/' + self.attributes['prefix']
+            if not self.attributes['prefix'].endswith('/'):
+                self.attributes['prefix'] += '/'
 
-        url = build_url(choice(self.list_hosts),
-                        path='/'.join([self.REPLICAS_BASEURL, 'list']))
+            if self.attributes['hostname'] != hostname:
+                if self.attributes['hostname'] != 'localhost':  # In the database empty hostnames are replaced with localhost but for some URIs (e.g. file) a hostname is not included
+                    raise exception.RSEFileNameNotSupported('Invalid hostname: provided \'%s\', expected \'%s\'' % (hostname, self.attributes['hostname']))
 
-        headers = {}
-        if metalink is not None:
-            if metalink == 3:
-                headers['Accept'] = 'application/metalink+xml'
-            elif metalink == 4:
-                headers['Accept'] = 'application/metalink4+xml'
+            if self.attributes['port'] != port:
+                raise exception.RSEFileNameNotSupported('Invalid port: provided \'%s\', expected \'%s\'' % (port, self.attributes['port']))
 
-        # pass json dict in querystring
-        r = self._send_request(url, headers=headers, type='POST', data=dumps(data))
-        if r.status_code == codes.ok:
-            if not metalink:
-                return self._load_json_data(r)
-            return r.text
-        exc_cls, exc_msg = self._get_exception(headers=r.headers, status_code=r.status_code, data=r.content)
-        raise exc_cls(exc_msg)
+            if not path.startswith(self.attributes['prefix']):
+                raise exception.RSEFileNameNotSupported('Invalid prefix: provided \'%s\', expected \'%s\'' % ('/'.join(path.split('/')[0:len(self.attributes['prefix'].split('/')) - 1]),
+                                                                                                              self.attributes['prefix']))  # len(...)-1 due to the leading '/
 
-    def add_replica(self, rse, scope, name, bytes, adler32, pfn=None, md5=None, meta={}):
+            # Spliting parsed.path into prefix, path, filename
+            prefix = self.attributes['prefix']
+            path = path.partition(self.attributes['prefix'])[2]
+            name = path.split('/')[-1]
+            path = '/'.join(path.split('/')[:-1])
+            if not path.startswith('/'):
+                path = '/' + path
+            if path != '/' and not path.endswith('/'):
+                path = path + '/'
+            ret[pfn] = {'path': path, 'name': name, 'scheme': scheme, 'prefix': prefix, 'port': port, 'hostname': hostname, }
+
+        return ret
+
+    def exists(self, path):
         """
-        Add file replicas to a RSE.
+            Checks if the requested file is known by the referred RSE.
 
-        :param rse: the RSE name.
-        :param scope: The scope of the file.
-        :param name: The name of the file.
-        :param bytes: The size in bytes.
-        :param adler32: adler32 checksum.
-        :param pfn: PFN of the file for non deterministic RSE.
-        :param md5: md5 checksum.
-        :param meta: Metadata attributes.
+            :param path: Physical file name
 
-        :return: True if files were created successfully.
+            :returns: True if the file exists, False if it doesn't
 
-        """
-        dict = {'scope': scope, 'name': name, 'bytes': bytes, 'meta': meta, 'adler32': adler32}
-        if md5:
-            dict['md5'] = md5
-        if pfn:
-            dict['pfn'] = pfn
-        return self.add_replicas(rse=rse, files=[dict])
+            :raises SourceNotFound: if the source file was not found on the referred storage.
 
-    def add_replicas(self, rse, files, ignore_availability=True):
         """
-        Bulk add file replicas to a RSE.
 
-        :param rse: the RSE name.
-        :param files: The list of files. This is a list of DIDs like :
-        [{'scope': <scope1>, 'name': <name1>}, {'scope': <scope2>, 'name': <name2>}, ...]
-        :param ignore_availability: Ignore the RSE blacklisting.
+        filepath = '/'.join(path.split('/')[0:-1]) + '/'
+        filename = path.split('/')[-1]
 
-        :return: True if files were created successfully.
-        """
-        url = build_url(choice(self.list_hosts), path=self.REPLICAS_BASEURL)
-        data = {'rse': rse, 'files': files, 'ignore_availability': ignore_availability}
-        r = self._send_request(url, type='POST', data=render_json(**data))
-        if r.status_code == codes.created:
-            return True
-        exc_cls, exc_msg = self._get_exception(headers=r.headers, status_code=r.status_code, data=r.content)
-        raise exc_cls(exc_msg)
+        transfer_client = get_transfer_client()
+        exists = False
+
+        if self.globus_endpoint_id:
+            try:
+                resp = transfer_client.operation_ls(endpoint_id=self.globus_endpoint_id, path=filepath)
+                exists = len([r for r in resp if r['name'] == filename]) > 0
+            except TransferAPIError as err:
+                print(err)
+        else:
+            print('No rse attribute found for globus endpoint id.')
+
+        return exists
 
-    def delete_replicas(self, rse, files, ignore_availability=True):
+    def list(self, path):
         """
-        Bulk delete file replicas from a RSE.
 
-        :param rse: the RSE name.
-        :param files: The list of files. This is a list of DIDs like :
-        [{'scope': <scope1>, 'name': <name1>}, {'scope': <scope2>, 'name': <name2>}, ...]
-        :param ignore_availability: Ignore the RSE blacklisting.
+            Checks if the requested path is known by the referred RSE and returns a list of items
+
+            :param path: Physical file name
+
+            :returns: List of items
 
-        :return: True if files have been deleted successfully.
         """
-        url = build_url(choice(self.list_hosts), path=self.REPLICAS_BASEURL)
-        data = {'rse': rse, 'files': files, 'ignore_availability': ignore_availability}
-        r = self._send_request(url, type='DEL', data=render_json(**data))
-        if r.status_code == codes.ok:
-            return True
-        exc_cls, exc_msg = self._get_exception(headers=r.headers, status_code=r.status_code, data=r.content)
-        raise exc_cls(exc_msg)
 
-    def update_replicas_states(self, rse, files):
+        transfer_client = get_transfer_client()
+        items = []
+
+        if self.globus_endpoint_id:
+            try:
+                resp = transfer_client.operation_ls(endpoint_id=self.globus_endpoint_id, path=path)
+                items = resp['DATA']
+            except TransferAPIError as err:
+                print(err)
+        else:
+            print('No rse attribute found for globus endpoint id.')
+
+        return items
+
+    def delete(self, path):
         """
-        Bulk update the file replicas states from a RSE.
+            Deletes a file from the connected RSE.
 
-        :param rse: the RSE name.
-        :param files: The list of files. This is a list of DIDs like :
-        [{'scope': <scope1>, 'name': <name1>}, {'scope': <scope2>, 'name': <name2>}, ...]
+            :param path: path to the to be deleted file
 
-        :return: True if files have been deleted successfully.
+            :raises ServiceUnavailable: if some generic error occured in the library.
+            :raises SourceNotFound: if the source file was not found on the referred storage.
         """
-        url = build_url(choice(self.list_hosts), path=self.REPLICAS_BASEURL)
-        data = {'rse': rse, 'files': files}
-        r = self._send_request(url, type='PUT', data=render_json(**data))
-        if r.status_code == codes.ok:
-            return True
-        exc_cls, exc_msg = self._get_exception(headers=r.headers, status_code=r.status_code, data=r.content)
-        raise exc_cls(exc_msg)
+        if self.globus_endpoint_id:
+            try:
+                delete_response = send_delete_task(endpoint_id=self.globus_endpoint_id, path=path, logger=self.logger)
+            except TransferAPIError as err:
+                self.logger(logging.WARNING, str(err))
+                raise exception.RucioException(err)
+        else:
+            raise exception.RucioException('No rse attribute found for globus endpoint id.')
+
+        if delete_response['code'] != 'Accepted':
+            self.logger(logging.DEBUG, 'delete_response: %s' % delete_response)
+            raise exception.RucioException('delete_task not accepted by Globus')
 
-    def list_dataset_replicas(self, scope, name, deep=False):
+    def bulk_delete(self, pfns):
         """
-        :param scope: The scope of the dataset.
-        :param name: The name of the dataset.
-        :param deep: Lookup at the file level.
+            Submits an async task to bulk delete files on globus endpoint.
 
-        :returns: A list of dict dataset replicas
+            :param pfns: list of pfns to delete
+
+            :raises TransferAPIError: if unexpected response from the service.
         """
-        payload = {}
-        if deep:
-            payload = {'deep': True}
+        if self.globus_endpoint_id:
+            try:
+                bulk_delete_response = send_bulk_delete_task(endpoint_id=self.globus_endpoint_id, pfns=pfns, logger=self.logger)
+            except TransferAPIError as err:
+                raise exception.RucioException(err)
+        else:
+            raise exception.RucioException('No rse attribute found for globus endpoint id.')
 
-        url = build_url(self.host,
-                        path='/'.join([self.REPLICAS_BASEURL, scope, name, 'datasets']),
-                        params=payload)
-        r = self._send_request(url, type='GET')
-        if r.status_code == codes.ok:
-            return self._load_json_data(r)
-        exc_cls, exc_msg = self._get_exception(headers=r.headers, status_code=r.status_code, data=r.content)
-        raise exc_cls(exc_msg)
+        if bulk_delete_response['code'] != 'Accepted':
+            self.logger(logging.DEBUG, 'delete_response: %s' % bulk_delete_response)
+            raise exception.RucioException('delete_task not accepted by Globus')
 
-    def list_datasets_per_rse(self, rse, filters=None, limit=None):
+    def connect(self):
         """
-        List datasets at a RSE.
+            Establishes the actual connection to the referred RSE.
 
-        :param rse: the rse name.
-        :param filters: dictionary of attributes by which the results should be filtered.
-        :param limit: limit number.
+            reaper2 daemon requires implementation of protocol.connect
+        """
+        pass
 
-        :returns: A list of dict dataset replicas
+    def close(self):
         """
-        url = build_url(self.host, path='/'.join([self.REPLICAS_BASEURL, 'rse', rse]))
-        r = self._send_request(url, type='GET')
-        if r.status_code == codes.ok:
-            return self._load_json_data(r)
+            Closes the connection to RSE.
 
-        exc_cls, exc_msg = self._get_exception(headers=r.headers, status_code=r.status_code, data=r.content)
-        raise exc_cls(exc_msg)
+            reaper2 daemon requires implementation of protocol.close
+        """
+        pass
```

### Comparing `rucio-clients-1.9.6/lib/rucio/client/rseclient.py` & `rucio-clients-32.0.0rc1/lib/rucio/client/replicaclient.py`

 * *Files 20% similar despite different names*

```diff
@@ -1,467 +1,444 @@
-# Copyright European Organization for Nuclear Research (CERN)
+# -*- coding: utf-8 -*-
+# Copyright European Organization for Nuclear Research (CERN) since 2012
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
-# You may not use this file except in compliance with the License.
-# You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
 #
-# Authors:
-# - Thomas Beermann, <thomas.beermann@cern.ch>, 2012
-# - Vincent Garonne, <vincent.garonne@cern.ch>, 2012-2015
-# - Mario Lassnig, <mario.lassnig@cern.ch>, 2013
-# - Ralph Vigne, <ralph.vigne@cern.ch>, 2013-2015
-# - Martin Barisits, <martin.barisits@cern.ch>, 2013
-# - Cedric Serfon, <cedric.serfon@cern.ch>, 2014
+#    http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 
+from datetime import datetime
 from json import dumps, loads
+from urllib.parse import quote_plus
+
 from requests.status_codes import codes
-from urllib import quote
 
 from rucio.client.baseclient import BaseClient
 from rucio.client.baseclient import choice
-from rucio.common.utils import build_url
+from rucio.common.utils import build_url, render_json, chunks
 
 
-class RSEClient(BaseClient):
-    """RSE client class for working with rucio RSEs"""
+class ReplicaClient(BaseClient):
+    """Replica client class for working with replicas"""
 
-    RSE_BASEURL = 'rses'
+    REPLICAS_BASEURL = 'replicas'
+    REPLICAS_CHUNK_SIZE = 1000
 
-    def __init__(self, rucio_host=None, auth_host=None, account=None, ca_cert=None, auth_type=None, creds=None, timeout=None, user_agent='rucio-clients'):
-        super(RSEClient, self).__init__(rucio_host, auth_host, account, ca_cert, auth_type, creds, timeout, user_agent)
+    def quarantine_replicas(self, replicas, rse=None, rse_id=None):
+        """
+        Add quaratined replicas for RSE.
 
-    def get_rse(self, rse):
+        :param replicas: List of replica infos: {'scope': <scope> (optional), 'name': <name> (optional), 'path':<path> (required)}.
+        :param rse: RSE name.
+        :param rse_id: RSE id. Either RSE name or RSE id must be specified, but not both
         """
-        Returns details about the referred RSE.
 
-        :param rse: Name of the referred RSE
+        if (rse is None) == (rse_id is None):
+            raise ValueError("Either RSE name or RSE id must be specified, but not both")
 
-        :returns: A dict containing all attributes of the referred RSE
+        url = build_url(self.host, path='/'.join([self.REPLICAS_BASEURL, 'quarantine']))
+        headers = {}
+        for chunk in chunks(replicas, self.REPLICAS_CHUNK_SIZE):
+            data = {'rse': rse, 'rse_id': rse_id, 'replicas': chunk}
+            r = self._send_request(url, headers=headers, type_='POST', data=dumps(data))
+            if r.status_code != codes.ok:
+                exc_cls, exc_msg = self._get_exception(headers=r.headers, status_code=r.status_code, data=r.content)
+                raise exc_cls(exc_msg)
 
-        :raises RSENotFound: if the referred RSE was not found in the database
+    def declare_bad_file_replicas(self, replicas, reason, force=False):
         """
-        path = '/'.join([self.RSE_BASEURL, rse])
-        url = build_url(choice(self.list_hosts), path=path)
+        Declare a list of bad replicas.
 
-        r = self._send_request(url, type='GET')
-        if r.status_code == codes.ok:
-            rse = loads(r.text)
-            return rse
-        else:
-            exc_cls, exc_msg = self._get_exception(headers=r.headers, status_code=r.status_code, data=r.content)
-            raise exc_cls(exc_msg)
-
-    def add_rse(self, rse, **kwargs):
-        """
-        Sends the request to create a new RSE.
-
-        :param rse: the name of the rse.
-        :param deterministic: Boolean to know if the pfn is generated deterministically.
-        :param volatile: Boolean for RSE cache.
-        :param city: City for the RSE.
-        :param region_code: The region code for the RSE.
-        :param country_name: The country.
-        :param continent: The continent.
-        :param time_zone: Timezone.
-        :param staging_area: Staging area.
-        :param ISP: Internet service provider.
-
-        :return: True if location was created successfully else False.
-        :raises Duplicate: if rse already exists.
-        """
-        path = 'rses/' + rse
-        url = build_url(choice(self.list_hosts), path=path)
-        r = self._send_request(url, type='POST', data=dumps(kwargs))
+        :param replicas: Either a list of PFNs (string) or a list of dicts {'scope': <scope>, 'name': <name>, 'rse_id': <rse_id> or 'rse': <rse_name>}
+        :param reason: The reason of the loss.
+        :param force: boolean, tell the serrver to ignore existing replica status in the bad_replicas table. Default: False
+        :returns: Dictionary {"rse_name": ["did: error",...]} - list of strings for DIDs failed to declare, by RSE
+        """
+
+        out = {}    # {rse: ["did: error text",...]}
+        url = build_url(self.host, path='/'.join([self.REPLICAS_BASEURL, 'bad']))
+        headers = {}
+        for chunk in chunks(replicas, self.REPLICAS_CHUNK_SIZE):
+            data = {'reason': reason, 'replicas': chunk, 'force': force}
+            r = self._send_request(url, headers=headers, type_='POST', data=dumps(data))
+            if r.status_code not in (codes.created, codes.ok):
+                exc_cls, exc_msg = self._get_exception(headers=r.headers, status_code=r.status_code, data=r.content)
+                raise exc_cls(exc_msg)
+            chunk_result = loads(r.text)
+            if chunk_result:
+                for rse, lst in chunk_result.items():
+                    out.setdefault(rse, []).extend(lst)
+        return out
+
+    def declare_bad_did_replicas(self, rse, dids, reason):
+        """
+        Declare a list of bad replicas.
+
+        :param rse: The RSE where the bad replicas reside
+        :param dids: The DIDs of the bad replicas
+        :param reason: The reason of the loss.
+        """
+        data = {'reason': reason, 'rse': rse, 'dids': dids}
+        url = build_url(self.host, path='/'.join([self.REPLICAS_BASEURL, 'bad/dids']))
+        headers = {}
+        r = self._send_request(url, headers=headers, type_='POST', data=dumps(data))
         if r.status_code == codes.created:
-            return True
+            return loads(r.text)
         exc_cls, exc_msg = self._get_exception(headers=r.headers, status_code=r.status_code, data=r.content)
         raise exc_cls(exc_msg)
 
-    def update_rse(self, rse, parameters):
+    def declare_suspicious_file_replicas(self, pfns, reason):
         """
-        Update RSE properties like availability or name.
+        Declare a list of bad replicas.
 
-        :param rse: the name of the new rse.
-        :param  parameters: A dictionnary with property (name, read, write, delete as keys).
+        :param pfns: The list of PFNs.
+        :param reason: The reason of the loss.
         """
-        path = 'rses/' + rse
-        url = build_url(choice(self.list_hosts), path=path)
-        r = self._send_request(url, type='PUT', data=dumps(parameters))
+        data = {'reason': reason, 'pfns': pfns}
+        url = build_url(self.host, path='/'.join([self.REPLICAS_BASEURL, 'suspicious']))
+        headers = {}
+        r = self._send_request(url, headers=headers, type_='POST', data=dumps(data))
         if r.status_code == codes.created:
-            return True
+            return loads(r.text)
         exc_cls, exc_msg = self._get_exception(headers=r.headers, status_code=r.status_code, data=r.content)
         raise exc_cls(exc_msg)
 
-    def delete_rse(self, rse):
+    def get_did_from_pfns(self, pfns, rse=None):
         """
-        Sends the request to delete a rse.
+        Get the DIDs associated to a PFN on one given RSE
 
-        :param rse: the name of the rse.
-        :return: True if location was created successfully else False.
+        :param pfns: The list of PFNs.
+        :param rse: The RSE name.
+        :returns: A list of dictionaries {pfn: {'scope': scope, 'name': name}}
         """
-        path = 'rses/' + rse
-        url = build_url(choice(self.list_hosts), path=path)
-        r = self._send_request(url, type='DEL')
+        data = {'rse': rse, 'pfns': pfns}
+        url = build_url(self.host, path='/'.join([self.REPLICAS_BASEURL, 'dids']))
+        headers = {}
+        r = self._send_request(url, headers=headers, type_='POST', data=dumps(data))
         if r.status_code == codes.ok:
-            return True
-        else:
-            exc_cls, exc_msg = self._get_exception(headers=r.headers, status_code=r.status_code, data=r.content)
-            raise exc_cls(exc_msg)
+            return self._load_json_data(r)
+        exc_cls, exc_msg = self._get_exception(headers=r.headers, status_code=r.status_code, data=r.content)
+        raise exc_cls(exc_msg)
 
-    def list_rses(self, rse_expression=None):
-        """
-        Sends the request to list all rucio locations(RSEs).
+    def list_replicas(self, dids, schemes=None, ignore_availability=True,
+                      all_states=False, metalink=False, rse_expression=None,
+                      client_location=None, sort=None, domain=None,
+                      signature_lifetime=None, nrandom=None,
+                      resolve_archives=True, resolve_parents=False,
+                      updated_after=None):
+        """
+        List file replicas for a list of data identifiers (DIDs).
+
+        :param dids: The list of data identifiers (DIDs) like :
+            [{'scope': <scope1>, 'name': <name1>}, {'scope': <scope2>, 'name': <name2>}, ...]
+        :param schemes: A list of schemes to filter the replicas. (e.g. file, http, ...)
+        :param ignore_availability: Also include replicas from blocked RSEs into the list
+        :param metalink: ``False`` (default) retrieves as JSON,
+                         ``True`` retrieves as metalink4+xml.
+        :param rse_expression: The RSE expression to restrict replicas on a set of RSEs.
+        :param client_location: Client location dictionary for PFN modification {'ip', 'fqdn', 'site', 'latitude', 'longitude'}
+        :param sort: Sort the replicas: ``geoip`` - based on src/dst IP topographical distance
+                                        ``closeness`` - based on src/dst closeness
+                                        ``dynamic`` - Rucio Dynamic Smart Sort (tm)
+        :param domain: Define the domain. None is fallback to 'wan', otherwise 'wan, 'lan', or 'all'
+        :param signature_lifetime: If supported, in seconds, restrict the lifetime of the signed PFN.
+        :param nrandom: pick N random replicas. If the initial number of replicas is smaller than N, returns all replicas.
+        :param resolve_archives: When set to True, find archives which contain the replicas.
+        :param resolve_parents: When set to True, find all parent datasets which contain the replicas.
+        :param updated_after: epoch timestamp or datetime object (UTC time), only return replicas updated after this time
+
+        :returns: A list of dictionaries with replica information.
+
+        """
+        data = {'dids': dids,
+                'domain': domain}
+
+        if schemes:
+            data['schemes'] = schemes
+        if ignore_availability is not None:
+            data['ignore_availability'] = ignore_availability
+        data['all_states'] = all_states
 
-        :rse_expression: RSE Expression to use as filter.
-        :return:         a list containing the names of all rucio locations.
-        """
         if rse_expression:
-            path = ['rses', "?expression=" + quote(rse_expression)]
-            path = '/'.join(path)
-        else:
-            path = 'rses/'
-        url = build_url(choice(self.list_hosts), path=path)
-        r = self._send_request(url, type='GET')
-        if r.status_code == codes.ok:
-            return self._load_json_data(r)
-        else:
-            exc_cls, exc_msg = self._get_exception(headers=r.headers, status_code=r.status_code, data=r.content)
-            raise exc_cls(exc_msg)
+            data['rse_expression'] = rse_expression
 
-    def add_rse_attribute(self, rse, key, value):
-        """
-        Sends the request to add a RSE attribute.
+        if client_location:
+            data['client_location'] = client_location
 
-        :param rse: the name of the rse.
-        :param key: the attribute key.
-        :param value: the attribute value.
+        if sort:
+            data['sort'] = sort
 
-        :return: True if RSE attribute was created successfully else False.
-        :raises Duplicate: if RSE attribute already exists.
-        """
-        path = '/'.join([self.RSE_BASEURL, rse, 'attr', key])
-        url = build_url(choice(self.list_hosts), path=path)
-        data = dumps({'value': value})
+        if updated_after:
+            if isinstance(updated_after, datetime):
+                # encode in UTC string with format '%Y-%m-%dT%H:%M:%S'  e.g. '2020-03-02T12:01:38'
+                data['updated_after'] = updated_after.strftime('%Y-%m-%dT%H:%M:%S')
+            else:
+                data['updated_after'] = updated_after
 
-        r = self._send_request(url, type='POST', data=data)
-        if r.status_code == codes.created:
-            return True
-        else:
-            exc_cls, exc_msg = self._get_exception(headers=r.headers, status_code=r.status_code, data=r.content)
-            raise exc_cls(exc_msg)
+        if signature_lifetime:
+            data['signature_lifetime'] = signature_lifetime
 
-    def delete_rse_attribute(self, rse, key):
-        """
-        Sends the request to delete a RSE attribute.
+        if nrandom:
+            data['nrandom'] = nrandom
 
-        :param rse: the RSE name.
-        :param key: the attribute key.
+        data['resolve_archives'] = resolve_archives
 
-        :return: True if RSE attribute was deleted successfully else False.
-        """
-        path = '/'.join([self.RSE_BASEURL, rse, 'attr', key])
-        url = build_url(choice(self.list_hosts), path=path)
+        data['resolve_parents'] = resolve_parents
+
+        url = build_url(choice(self.list_hosts),
+                        path='/'.join([self.REPLICAS_BASEURL, 'list']))
 
-        r = self._send_request(url, type='DEL')
+        headers = {}
+        if metalink:
+            headers['Accept'] = 'application/metalink4+xml'
+
+        # pass json dict in querystring
+        r = self._send_request(url, headers=headers, type_='POST', data=dumps(data), stream=True)
         if r.status_code == codes.ok:
-            return True
-        else:
-            exc_cls, exc_msg = self._get_exception(headers=r.headers, status_code=r.status_code, data=r.content)
-            raise exc_cls(exc_msg)
+            if not metalink:
+                return self._load_json_data(r)
+            return r.text
+        exc_cls, exc_msg = self._get_exception(headers=r.headers, status_code=r.status_code, data=r.content)
+        raise exc_cls(exc_msg)
 
-    def list_rse_attributes(self, rse):
+    def list_suspicious_replicas(self, rse_expression=None, younger_than=None, nattempts=None):
         """
-        Sends the request to get RSE attributes.
+        List file replicas tagged as suspicious.
 
-        :param rse: The RSE name.
+        :param rse_expression: The RSE expression to restrict replicas on a set of RSEs.
+        :param younger_than: Datetime object to select the replicas which were declared since younger_than date. Default value = 10 days ago.
+        :param nattempts: The minimum number of replica appearances in the bad_replica DB table from younger_than date. Default value = 0.
+        :param state: State of the replica, either 'BAD' or 'SUSPICIOUS'. No value returns replicas with either state.
 
-        :return: True if RSE attribute was created successfully else False.
         """
-        path = '/'.join([self.RSE_BASEURL, rse, 'attr/'])
-        url = build_url(choice(self.list_hosts), path=path)
-        r = self._send_request(url, type='GET')
+        params = {}
+        if rse_expression:
+            params['rse_expression'] = rse_expression
+
+        if younger_than:
+            params['younger_than'] = younger_than
+
+        if nattempts:
+            params['nattempts'] = nattempts
+
+        url = build_url(choice(self.list_hosts),
+                        path='/'.join([self.REPLICAS_BASEURL, 'suspicious']))
+        r = self._send_request(url, type_='GET', params=params)
         if r.status_code == codes.ok:
-            attributes = loads(r.text)
-            return attributes
-        else:
-            exc_cls, exc_msg = self._get_exception(headers=r.headers, status_code=r.status_code, data=r.content)
-            raise exc_cls(exc_msg)
-
-    def add_protocol(self, rse, params):
-        """
-        Sends the request to create a new protocol for the given RSE.
-
-        :param rse: the name of the  rse.
-        :param scheme: identifier of this protocol
-        :param params: Attributes of the protocol. Supported are:
-            hostname:       hostname for this protocol (default = localhost)
-            port:           port for this protocol (default = 0)
-            prefix:         string used as a prfeix for this protocol when generating
-                            the PFN (default = None)
-            impl:           qualified name of the implementation class for this
-                            protocol (mandatory)
-            read:           integer representing the priority of this procotol for
-                            read operations (default = -1)
-            write:          integer representing the priority of this procotol for
-                            write operations (default = -1)
-            delete:         integer representing the priority of this procotol for
-                            delet operations (default = -1)
-            extended_attributes:  miscellaneous protocol specific information e.g. spacetoken
-                            for SRM (default = None)
-
-
-        :return: True if protocol was created successfully else False.
-
-        :raises Duplicate: if protocol with same hostname, port and protocol identifier
-                            already exists for the given RSE.
-        :raises RSENotFound: if the RSE doesn't exist.
-        :raises KeyNotFound: if params is missing manadtory attributes to create the
-                             protocol.
-        :raises AccessDenied: if not authorized.
-        """
-        scheme = params['scheme']
-        path = '/'.join([self.RSE_BASEURL, rse, 'protocols', scheme])
-        url = build_url(choice(self.list_hosts), path=path)
-        r = self._send_request(url, type='POST', data=dumps(params))
-        if r.status_code == codes.created:
-            return True
-        else:
-            exc_cls, exc_msg = self._get_exception(headers=r.headers, status_code=r.status_code, data=r.content)
-            raise exc_cls(exc_msg)
+            return self._load_json_data(r)
+        exc_cls, exc_msg = self._get_exception(headers=r.headers, status_code=r.status_code, data=r.content)
+        raise exc_cls(exc_msg)
 
-    def get_protocols(self, rse, protocol_domain='ALL', operation=None, default=False, scheme=None):
+    def add_replica(self, rse, scope, name, bytes_, adler32, pfn=None, md5=None, meta={}):
         """
-        Returns protocol information. Parameter comibantions are:
-        (operation OR default) XOR protocol.
+        Add file replicas to a RSE.
 
         :param rse: the RSE name.
-        :param protocol_domain: The scope of the protocol. Supported are 'LAN', 'WAN', and 'ALL' (as default).
-        :param operation: The name of the requested operation (read, write, or delete).
-                          If None, all operations are queried.
-        :param default: Indicates if only the default operations should be returned.
-        :param scheme: The identifier of the requested protocol.
+        :param scope: The scope of the file.
+        :param name: The name of the file.
+        :param bytes_: The size in bytes.
+        :param adler32: adler32 checksum.
+        :param pfn: PFN of the file for non deterministic RSE.
+        :param md5: md5 checksum.
+        :param meta: Metadata attributes.
+
+        :return: True if files were created successfully.
+
+        """
+        dict_ = {'scope': scope, 'name': name, 'bytes': bytes_, 'meta': meta, 'adler32': adler32}
+        if md5:
+            dict_['md5'] = md5
+        if pfn:
+            dict_['pfn'] = pfn
+        return self.add_replicas(rse=rse, files=[dict_])
 
-        :returns: A list with details about each matching protocol.
-
-        :raises RSENotFound: if the RSE doesn't exist.
-        :raises RSEProtocolNotSupported: if no matching protocol entry could be found.
-        :raises RSEOperationNotSupported: if no matching protocol entry for the requested
-                                          operation could be found.
+    def add_replicas(self, rse, files, ignore_availability=True):
         """
+        Bulk add file replicas to a RSE.
 
-        path = None
-        params = {}
-        if scheme:
-            path = '/'.join([self.RSE_BASEURL, rse, 'protocols', scheme])
-        else:
-            path = '/'.join([self.RSE_BASEURL, rse, 'protocols'])
-            if operation:
-                params['operation'] = operation
-            if default:
-                params['default'] = default
-        params['protocol_domain'] = protocol_domain
-        url = build_url(choice(self.list_hosts), path=path, params=params)
+        :param rse: the RSE name.
+        :param files: The list of files. This is a list of DIDs like :
+            [{'scope': <scope1>, 'name': <name1>}, {'scope': <scope2>, 'name': <name2>}, ...]
+        :param ignore_availability: Ignore the RSE blocklsit.
 
-        r = self._send_request(url, type='GET')
-        if r.status_code == codes.ok:
-            protocols = loads(r.text)
-            return protocols
-        else:
-            exc_cls, exc_msg = self._get_exception(headers=r.headers, status_code=r.status_code, data=r.content)
-            raise exc_cls(exc_msg)
+        :return: True if files were created successfully.
 
-    def delete_protocols(self, rse, scheme, hostname=None, port=None):
         """
-        Deletes matching protocols from RSE. Protocols using the same identifier can be
-        distinguished by hostname and port.
+        url = build_url(choice(self.list_hosts), path=self.REPLICAS_BASEURL)
+        data = {'rse': rse, 'files': files, 'ignore_availability': ignore_availability}
+        r = self._send_request(url, type_='POST', data=render_json(**data))
+        if r.status_code == codes.created:
+            return True
+        exc_cls, exc_msg = self._get_exception(headers=r.headers, status_code=r.status_code, data=r.content)
+        raise exc_cls(exc_msg)
+
+    def delete_replicas(self, rse, files, ignore_availability=True):
+        """
+        Bulk delete file replicas from a RSE.
 
         :param rse: the RSE name.
-        :param scheme: identifier of the protocol.
-        :param hostname: hostname of the protocol.
-        :param port: port of the protocol.
-
-        :returns: True if success.
-
-        :raises RSEProtocolNotSupported: if no matching protocol entry could be found.
-        :raises RSENotFound: if the RSE doesn't exist.
-        :raises AccessDenied: if not authorized.
-        """
-        path = [self.RSE_BASEURL, rse, 'protocols', scheme]
-        if hostname:
-            path.append(hostname)
-            if port:
-                path.append(str(port))
-
-        path = '/'.join(path)
-        url = build_url(choice(self.list_hosts), path=path)
-        r = self._send_request(url, type='DEL')
+        :param files: The list of files. This is a list of DIDs like :
+            [{'scope': <scope1>, 'name': <name1>}, {'scope': <scope2>, 'name': <name2>}, ...]
+        :param ignore_availability: Ignore the RSE blocklist.
+
+        :return: True if files have been deleted successfully.
+
+        """
+        url = build_url(choice(self.list_hosts), path=self.REPLICAS_BASEURL)
+        data = {'rse': rse, 'files': files, 'ignore_availability': ignore_availability}
+        r = self._send_request(url, type_='DEL', data=render_json(**data))
         if r.status_code == codes.ok:
             return True
-        else:
-            exc_cls, exc_msg = self._get_exception(headers=r.headers, status_code=r.status_code, data=r.content)
-            raise exc_cls(exc_msg)
+        exc_cls, exc_msg = self._get_exception(headers=r.headers, status_code=r.status_code, data=r.content)
+        raise exc_cls(exc_msg)
 
-    def update_protocols(self, rse, scheme, data, hostname=None, port=None):
+    def update_replicas_states(self, rse, files):
         """
-        Updates matching protocols from RSE. Protocol using the same identifier can be
-        distinguished by hostname and port.
+        Bulk update the file replicas states from a RSE.
 
         :param rse: the RSE name.
-        :param scheme: identifier of the protocol.
-        :param data: A dict providing the new values of the protocol attibutes.
-                     Keys must match column names in database.
-        :param hostname: hostname of the protocol.
-        :param port: port of the protocol.
-
-        :returns: True if success.
-
-        :raises RSEProtocolNotSupported: if no matching protocol entry could be found.
-        :raises RSENotFound: if the RSE doesn't exist.
-        :raises KeyNotFound: if invalid data was provided for update.
-        :raises AccessDenied: if not authorized.
-        """
-        path = [self.RSE_BASEURL, rse, 'protocols', scheme]
-        if hostname:
-            path.append(hostname)
-            if port:
-                path.append(str(port))
-
-        path = '/'.join(path)
-        url = build_url(choice(self.list_hosts), path=path)
-        r = self._send_request(url, type='PUT', data=dumps(data))
+        :param files: The list of files. This is a list of DIDs like :
+            [{'scope': <scope1>, 'name': <name1>, 'state': <state1>}, {'scope': <scope2>, 'name': <name2>, 'state': <state2>}, ...],
+            where a state value can be either of:
+              'A' (AVAILABLE)
+              'U' (UNAVAILABLE)
+              'C' (COPYING)
+              'B' (BEING_DELETED)
+              'D' (BAD)
+              'T' (TEMPORARY_UNAVAILABLE)
+        :return: True if replica states have been updated successfully, otherwise an exception is raised.
+
+        """
+        url = build_url(choice(self.list_hosts), path=self.REPLICAS_BASEURL)
+        data = {'rse': rse, 'files': files}
+        r = self._send_request(url, type_='PUT', data=render_json(**data))
         if r.status_code == codes.ok:
             return True
-        else:
-            exc_cls, exc_msg = self._get_exception(headers=r.headers, status_code=r.status_code, data=r.content)
-            raise exc_cls(exc_msg)
+        exc_cls, exc_msg = self._get_exception(headers=r.headers, status_code=r.status_code, data=r.content)
+        raise exc_cls(exc_msg)
 
-    def swap_protocols(self, rse, domain, operation, scheme_a, scheme_b):
+    def list_dataset_replicas(self, scope, name, deep=False):
         """
-        Swaps the priorities of the provided operation.
+        List dataset replicas for a did (scope:name).
 
-        :param rse: the RSE name.
-        :param domain: the domain in which priorities should be swapped i.e. wan or lan.
-        :param operation: the operation that should be swapped i.e. read, write, or delete.
-        :param scheme_a: the scheme of one of the two protocols to be swapped, e.g. srm.
-        :param scheme_b: the scheme of the other of the two protocols to be swapped, e.g. http.
-
-        :returns: True if success.
-
-        :raises RSEProtocolNotSupported: if no matching protocol entry could be found.
-        :raises RSENotFound: if the RSE doesn't exist.
-        :raises KeyNotFound: if invalid data was provided for update.
-        :raises AccessDenied: if not authorized.
-        """
-        protocol_a = protocol_b = None
-        protocols = self.get_protocols(rse, domain, operation, False, scheme_a)['protocols']
-        for p in protocols:
-            if p['scheme'] == scheme_a:
-                protocol_a = p
-            if p['scheme'] == scheme_b:
-                protocol_b = p
-        if (protocol_a or protocol_b) is None:
-            return False
-        priority_a = protocol_a['domains'][domain][operation]
-        priority_b = protocol_b['domains'][domain][operation]
-        self.update_protocols(rse, protocol_a['scheme'], {'domains': {domain: {operation: priority_b}}}, protocol_a['hostname'], protocol_a['port'])
-        self.update_protocols(rse, protocol_b['scheme'], {'domains': {domain: {operation: priority_a}}}, protocol_b['hostname'], protocol_b['port'])
-        return True
+        :param scope: The scope of the dataset.
+        :param name: The name of the dataset.
+        :param deep: Lookup at the file level.
+
+        :returns: A list of dict dataset replicas.
 
-    def set_rse_usage(self, rse, source, used, free):
         """
-        Set RSE usage information.
+        payload = {}
+        if deep:
+            payload = {'deep': True}
 
-        :param rse: the RSE name.
-        :param source: the information source, e.g. srm.
-        :param used: the used space in bytes.
-        :param free: the free in bytes.
-
-        :returns: True if successful, otherwise false.
-        """
-        path = [self.RSE_BASEURL, rse, 'usage']
-        path = '/'.join(path)
-        url = build_url(choice(self.list_hosts), path=path)
-        data = {'source': source, 'used': used, 'free': free}
-        r = self._send_request(url, type='PUT', data=dumps(data))
+        url = build_url(self.host,
+                        path='/'.join([self.REPLICAS_BASEURL, quote_plus(scope), quote_plus(name), 'datasets']),
+                        params=payload)
+        r = self._send_request(url, type_='GET')
         if r.status_code == codes.ok:
-            return True
-        else:
-            exc_cls, exc_msg = self._get_exception(headers=r.headers, status_code=r.status_code, data=r.content)
-            raise exc_cls(exc_msg)
+            return self._load_json_data(r)
+        exc_cls, exc_msg = self._get_exception(headers=r.headers, status_code=r.status_code, data=r.content)
+        raise exc_cls(exc_msg)
 
-    def get_rse_usage(self, rse, filters=None):
+    def list_dataset_replicas_bulk(self, dids):
         """
-        Get RSE usage information.
+        List dataset replicas for a did (scope:name).
 
-        :param rse: the RSE name.
-        :param filters: dictionary of attributes by which the results should be filtered
+        :param dids: The list of DIDs of the datasets.
 
-        :returns: True if successful, otherwise false.
+        :returns: A list of dict dataset replicas.
         """
-        path = [self.RSE_BASEURL, rse, 'usage']
-        path = '/'.join(path)
-        url = build_url(choice(self.list_hosts), path=path)
-        r = self._send_request(url, type='GET', params=filters)
+        payload = {'dids': list(dids)}
+        url = build_url(self.host, path='/'.join([self.REPLICAS_BASEURL, 'datasets_bulk']))
+        r = self._send_request(url, type_='POST', data=dumps(payload))
         if r.status_code == codes.ok:
             return self._load_json_data(r)
-        else:
-            exc_cls, exc_msg = self._get_exception(headers=r.headers, status_code=r.status_code, data=r.content)
-            raise exc_cls(exc_msg)
+        exc_cls, exc_msg = self._get_exception(headers=r.headers, status_code=r.status_code, data=r.content)
+        raise exc_cls(exc_msg)
 
-    def list_rse_usage_history(self, rse, filters=None):
+    def list_dataset_replicas_vp(self, scope, name, deep=False):
         """
-        List RSE usage history information.
+        List dataset replicas for a DID (scope:name) using the
+        Virtual Placement service.
 
-        :param rse: The RSE name.
-        :param filters: dictionary of attributes by which the results should be filtered.
+        NOTICE: This is an RnD function and might change or go away at any time.
 
-        :returns:  list of dictionnaries.
+        :param scope: The scope of the dataset.
+        :param name: The name of the dataset.
+        :param deep: Lookup at the file level.
+
+        :returns: If VP exists a list of dicts of sites
         """
-        path = [self.RSE_BASEURL, rse, 'usage', 'history']
-        path = '/'.join(path)
-        url = build_url(choice(self.list_hosts), path=path)
-        r = self._send_request(url, type='GET', params=filters)
+        payload = {}
+        if deep:
+            payload = {'deep': True}
+
+        url = build_url(self.host,
+                        path='/'.join([self.REPLICAS_BASEURL, quote_plus(scope), quote_plus(name), 'datasets_vp']),
+                        params=payload)
+        r = self._send_request(url, type_='GET')
         if r.status_code == codes.ok:
             return self._load_json_data(r)
-        else:
-            exc_cls, exc_msg = self._get_exception(headers=r.headers, status_code=r.status_code, data=r.content)
-            raise exc_cls(exc_msg)
+        exc_cls, exc_msg = self._get_exception(headers=r.headers, status_code=r.status_code, data=r.content)
+        raise exc_cls(exc_msg)
 
-    def set_rse_limits(self, rse, name, value):
+    def list_datasets_per_rse(self, rse, filters=None, limit=None):
         """
-        Set RSE limit information.
+        List datasets at a RSE.
 
-        :param rse: The RSE name.
-        :param name: The name of the limit.
-        :param value: The feature value. Set to -1 to remove the limit.
+        :param rse: the rse name.
+        :param filters: dictionary of attributes by which the results should be filtered.
+        :param limit: limit number.
 
-        :returns: True if successful, otherwise false.
-        """
-        path = [self.RSE_BASEURL, rse, 'limits']
-        path = '/'.join(path)
-        url = build_url(choice(self.list_hosts), path=path)
-        r = self._send_request(url, type='PUT', data=dumps({'name': name, 'value': value}))
+        :returns: A list of dict dataset replicas.
 
+        """
+        url = build_url(self.host, path='/'.join([self.REPLICAS_BASEURL, 'rse', rse]))
+        r = self._send_request(url, type_='GET')
         if r.status_code == codes.ok:
-            return True
+            return self._load_json_data(r)
 
         exc_cls, exc_msg = self._get_exception(headers=r.headers, status_code=r.status_code, data=r.content)
         raise exc_cls(exc_msg)
 
-    def get_rse_limits(self, rse):
+    def add_bad_pfns(self, pfns, reason, state, expires_at):
         """
-        Get RSE limits.
+        Declare a list of bad replicas.
 
-        :param rse: The RSE name.
+        :param pfns: The list of PFNs.
+        :param reason: The reason of the loss.
+        :param state: The state of the replica. Either BAD, SUSPICIOUS, TEMPORARY_UNAVAILABLE
+        :param expires_at: Specify a timeout for the TEMPORARY_UNAVAILABLE replicas. None for BAD files.
+
+        :return: True if PFNs were created successfully.
 
-        :returns: True if successful, otherwise false.
         """
+        data = {'reason': reason, 'pfns': pfns, 'state': state, 'expires_at': expires_at}
+        url = build_url(self.host, path='/'.join([self.REPLICAS_BASEURL, 'bad/pfns']))
+        headers = {}
+        r = self._send_request(url, headers=headers, type_='POST', data=dumps(data))
+        if r.status_code == codes.created:
+            return True
+        exc_cls, exc_msg = self._get_exception(headers=r.headers, status_code=r.status_code, data=r.content)
+        raise exc_cls(exc_msg)
 
-        path = [self.RSE_BASEURL, rse, 'limits']
-        path = '/'.join(path)
-        url = build_url(choice(self.list_hosts), path=path)
-        r = self._send_request(url, type='GET')
-        if r.status_code == codes.ok:
-            return self._load_json_data(r).next()
-        else:
-            exc_cls, exc_msg = self._get_exception(headers=r.headers, status_code=r.status_code, data=r.content)
-            raise exc_cls(exc_msg)
+    def set_tombstone(self, replicas):
+        """
+        Set a tombstone on a list of replicas.
+
+        :param replicas: list of replicas.
+        """
+        url = build_url(self.host, path='/'.join([self.REPLICAS_BASEURL, 'tombstone']))
+        data = {'replicas': replicas}
+        r = self._send_request(url, type_='POST', data=render_json(**data))
+        if r.status_code == codes.created:
+            return True
+        exc_cls, exc_msg = self._get_exception(headers=r.headers, status_code=r.status_code, data=r.content)
+        raise exc_cls(exc_msg)
```

### Comparing `rucio-clients-1.9.6/lib/rucio/client/ruleclient.py` & `rucio-clients-32.0.0rc1/lib/rucio/client/ruleclient.py`

 * *Files 21% similar despite different names*

```diff
@@ -1,204 +1,287 @@
-# Copyright European Organization for Nuclear Research (CERN)
+# -*- coding: utf-8 -*-
+# Copyright European Organization for Nuclear Research (CERN) since 2012
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
-# You may not use this file except in compliance with the License.
-# You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
 #
-# Authors:
-# - Vincent Garonne, <vincent.garonne@cern.ch>, 2012-2015
-# - Martin Barisits, <martin.barisits@cern.ch>, 2013-2016
-# - Cedric Serfon, <cedric.serfon@cern.ch>, 2014
-# - Ralph Vigne, <ralph.vigne@cern.ch>, 2015
+#    http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 
 from json import dumps, loads
+from typing import Any, Optional, Union
+from urllib.parse import quote_plus
+
 from requests.status_codes import codes
 
 from rucio.client.baseclient import BaseClient
 from rucio.client.baseclient import choice
 from rucio.common.utils import build_url
 
 
 class RuleClient(BaseClient):
 
     """RuleClient class for working with replication rules"""
 
     RULE_BASEURL = 'rules'
 
-    def __init__(self, rucio_host=None, auth_host=None, account=None, ca_cert=None, auth_type=None, creds=None, timeout=None, dq2_wrapper=False):
-        super(RuleClient, self).__init__(rucio_host, auth_host, account, ca_cert, auth_type, creds, timeout, dq2_wrapper)
-
-    def add_replication_rule(self, dids, copies, rse_expression, weight=None, lifetime=None, grouping='DATASET', account=None,
-                             locked=False, source_replica_expression=None, activity=None, notify='N', purge_replicas=False,
-                             ignore_availability=False, comment=None, ask_approval=False, asynchronous=False, priority=3):
+    def add_replication_rule(
+        self,
+        dids: list[str],
+        copies: int,
+        rse_expression: str,
+        priority: int = 3,
+        lifetime: Optional[int] = None,
+        grouping: str = 'DATASET',
+        notify: str = 'N',
+        source_replica_expression: Optional[str] = None,
+        activity: Optional[str] = None,
+        account: Optional[str] = None,
+        meta: Optional[str] = None,
+        ignore_availability: bool = False,
+        purge_replicas: bool = False,
+        ask_approval: bool = False,
+        asynchronous: bool = False,
+        locked: bool = False,
+        delay_injection=None,
+        comment=None,
+        weight=None,
+    ):
         """
         :param dids:                       The data identifier set.
         :param copies:                     The number of replicas.
         :param rse_expression:             Boolean string expression to give the list of RSEs.
-        :param weight:                     If the weighting option of the replication rule is used, the choice of RSEs takes their weight into account.
+        :param priority:                   Priority of the transfers.
         :param lifetime:                   The lifetime of the replication rules (in seconds).
         :param grouping:                   ALL -  All files will be replicated to the same RSE.
                                            DATASET - All files in the same dataset will be replicated to the same RSE.
                                            NONE - Files will be completely spread over all allowed RSEs without any grouping considerations at all.
-        :param account:                    The account owning the rule.
-        :param locked:                     If the rule is locked, it cannot be deleted.
+        :param notify:                     Notification setting for the rule (Y, N, C).
         :param source_replica_expression:  RSE Expression for RSEs to be considered for source replicas.
         :param activity:                   Transfer Activity to be passed to FTS.
-        :param notify:                     Notification setting for the rule (Y, N, C).
-        :param purge_replicas:             When the rule gets deleted purge the associated replicas immediately.
+        :param account:                    The account owning the rule.
+        :param meta:                       Metadata, as dictionary.
         :param ignore_availability:        Option to ignore the availability of RSEs.
+        :param purge_replicas:             When the rule gets deleted purge the associated replicas immediately.
         :param ask_approval:               Ask for approval of this replication rule.
         :param asynchronous:               Create rule asynchronously by judge-injector.
-        :param priority:                   Priority of the transfers.
+        :param locked:                     If the rule is locked, it cannot be deleted.
+        :param delay_injection:
         :param comment:                    Comment about the rule.
+        :param weight:                     If the weighting option of the replication rule is used, the choice of RSEs takes their weight into account.
         """
         path = self.RULE_BASEURL + '/'
         url = build_url(choice(self.list_hosts), path=path)
         # TODO remove the subscription_id from the client; It will only be used by the core;
         data = dumps({'dids': dids, 'copies': copies, 'rse_expression': rse_expression,
                       'weight': weight, 'lifetime': lifetime, 'grouping': grouping,
                       'account': account, 'locked': locked, 'source_replica_expression': source_replica_expression,
                       'activity': activity, 'notify': notify, 'purge_replicas': purge_replicas,
                       'ignore_availability': ignore_availability, 'comment': comment, 'ask_approval': ask_approval,
-                      'asynchronous': asynchronous, 'priority': priority})
-        r = self._send_request(url, type='POST', data=data)
+                      'asynchronous': asynchronous, 'delay_injection': delay_injection, 'priority': priority, 'meta': meta})
+        r = self._send_request(url, type_='POST', data=data)
         if r.status_code == codes.created:
             return loads(r.text)
-        else:
-            exc_cls, exc_msg = self._get_exception(headers=r.headers, status_code=r.status_code, data=r.content)
-            raise exc_cls(exc_msg)
+        exc_cls, exc_msg = self._get_exception(headers=r.headers, status_code=r.status_code, data=r.content)
+        raise exc_cls(exc_msg)
 
-    def delete_replication_rule(self, rule_id, purge_replicas=None):
+    def delete_replication_rule(
+        self, rule_id: str, purge_replicas: Optional[bool] = None
+    ):
         """
         Deletes a replication rule and all associated locks.
 
         :param rule_id:         The id of the rule to be deleted
         :param purge_replicas:  Immediately delete the replicas.
         :raises:                RuleNotFound, AccessDenied
         """
 
         path = self.RULE_BASEURL + '/' + rule_id
         url = build_url(choice(self.list_hosts), path=path)
 
         data = dumps({'purge_replicas': purge_replicas})
 
-        r = self._send_request(url, type='DEL', data=data)
+        r = self._send_request(url, type_='DEL', data=data)
 
         if r.status_code == codes.ok:
             return True
-        else:
-            exc_cls, exc_msg = self._get_exception(headers=r.headers, status_code=r.status_code, data=r.content)
-            raise exc_cls(exc_msg)
+        exc_cls, exc_msg = self._get_exception(headers=r.headers, status_code=r.status_code, data=r.content)
+        raise exc_cls(exc_msg)
 
-    def get_replication_rule(self, rule_id):
+    def get_replication_rule(self, rule_id: str):
         """
         Get a replication rule.
 
         :param rule_id:  The id of the rule to be retrieved.
         :raises:         RuleNotFound
         """
         path = self.RULE_BASEURL + '/' + rule_id
         url = build_url(choice(self.list_hosts), path=path)
-        r = self._send_request(url, type='GET')
+        r = self._send_request(url, type_='GET')
         if r.status_code == codes.ok:
-            return self._load_json_data(r).next()
+            return next(self._load_json_data(r))
         else:
             exc_cls, exc_msg = self._get_exception(headers=r.headers, status_code=r.status_code, data=r.content)
             raise exc_cls(exc_msg)
 
-    def update_replication_rule(self, rule_id, options):
+    def update_replication_rule(self, rule_id: str, options: dict[str, Any]):
         """
         :param rule_id:   The id of the rule to be retrieved.
         :param options:   Options dictionary.
         :raises:          RuleNotFound
         """
         path = self.RULE_BASEURL + '/' + rule_id
         url = build_url(choice(self.list_hosts), path=path)
         data = dumps({'options': options})
-        r = self._send_request(url, type='PUT', data=data)
+        r = self._send_request(url, type_='PUT', data=data)
         if r.status_code == codes.ok:
             return True
-        else:
-            exc_cls, exc_msg = self._get_exception(headers=r.headers, status_code=r.status_code, data=r.content)
-            raise exc_cls(exc_msg)
+        exc_cls, exc_msg = self._get_exception(headers=r.headers, status_code=r.status_code, data=r.content)
+        raise exc_cls(exc_msg)
 
-    def reduce_replication_rule(self, rule_id, copies, exclude_expression=None):
+    def reduce_replication_rule(
+        self, rule_id: str, copies: int, exclude_expression=None
+    ):
         """
         :param rule_id:             Rule to be reduced.
         :param copies:              Number of copies of the new rule.
         :param exclude_expression:  RSE Expression of RSEs to exclude.
         :raises:                    RuleReplaceFailed, RuleNotFound
         """
 
         path = self.RULE_BASEURL + '/' + rule_id + '/reduce'
         url = build_url(choice(self.list_hosts), path=path)
         data = dumps({'copies': copies, 'exclude_expression': exclude_expression})
-        r = self._send_request(url, type='POST', data=data)
+        r = self._send_request(url, type_='POST', data=data)
         if r.status_code == codes.ok:
             return loads(r.text)
-        else:
-            exc_cls, exc_msg = self._get_exception(headers=r.headers, status_code=r.status_code, data=r.content)
-            raise exc_cls(exc_msg)
+        exc_cls, exc_msg = self._get_exception(headers=r.headers, status_code=r.status_code, data=r.content)
+        raise exc_cls(exc_msg)
+
+    def move_replication_rule(
+        self, rule_id: str, rse_expression: str, override
+    ):
+        """
+        Move a replication rule to another RSE and, once done, delete the original one.
+
+        :param rule_id:                    Rule to be moved.
+        :param rse_expression:             RSE expression of the new rule.
+        :param override:                   Configurations to update for the new rule.
+        :raises:                           RuleNotFound, RuleReplaceFailed
+        """
 
-    def approve_replication_rule(self, rule_id):
+        path = self.RULE_BASEURL + '/' + rule_id + '/move'
+        url = build_url(choice(self.list_hosts), path=path)
+        data = dumps({
+            'rule_id': rule_id,
+            'rse_expression': rse_expression,
+            'override': override,
+        })
+        r = self._send_request(url, type_='POST', data=data)
+        if r.status_code == codes.created:
+            return loads(r.text)
+        exc_cls, exc_msg = self._get_exception(headers=r.headers, status_code=r.status_code, data=r.content)
+        raise exc_cls(exc_msg)
+
+    def approve_replication_rule(self, rule_id: str):
         """
         :param rule_id:             Rule to be approved.
         :raises:                    RuleNotFound
         """
 
         path = self.RULE_BASEURL + '/' + rule_id
         url = build_url(choice(self.list_hosts), path=path)
         data = dumps({'options': {'approve': True}})
-        r = self._send_request(url, type='PUT', data=data)
+        r = self._send_request(url, type_='PUT', data=data)
         if r.status_code == codes.ok:
             return True
-        else:
-            exc_cls, exc_msg = self._get_exception(headers=r.headers, status_code=r.status_code, data=r.content)
-            raise exc_cls(exc_msg)
+        exc_cls, exc_msg = self._get_exception(headers=r.headers, status_code=r.status_code, data=r.content)
+        raise exc_cls(exc_msg)
 
-    def deny_replication_rule(self, rule_id):
+    def deny_replication_rule(self, rule_id: str, reason: Optional[str] = None):
         """
         :param rule_id:             Rule to be denied.
+        :param reason:              Reason for denying the rule.
         :raises:                    RuleNotFound
         """
 
         path = self.RULE_BASEURL + '/' + rule_id
         url = build_url(choice(self.list_hosts), path=path)
-        data = dumps({'options': {'approve': False}})
-        r = self._send_request(url, type='PUT', data=data)
+        options: dict[str, Union[bool, str]] = {'approve': False}
+        if reason:
+            options['comment'] = reason
+        data = dumps({'options': options})
+        r = self._send_request(url, type_='PUT', data=data)
         if r.status_code == codes.ok:
             return True
-        else:
-            exc_cls, exc_msg = self._get_exception(headers=r.headers, status_code=r.status_code, data=r.content)
-            raise exc_cls(exc_msg)
+        exc_cls, exc_msg = self._get_exception(headers=r.headers, status_code=r.status_code, data=r.content)
+        raise exc_cls(exc_msg)
 
-    def list_replication_rule_full_history(self, scope, name):
+    def list_replication_rule_full_history(
+            self, scope: Union[str, bytes], name: Union[str, bytes]
+    ):
         """
         List the rule history of a DID.
 
         :param scope: The scope of the DID.
         :param name: The name of the DID.
         """
-        path = self.RULE_BASEURL + '/' + scope + '/' + name + '/history'
+        path = '/'.join([self.RULE_BASEURL, quote_plus(scope), quote_plus(name), 'history'])
         url = build_url(choice(self.list_hosts), path=path)
-        r = self._send_request(url, type='GET')
+        r = self._send_request(url, type_='GET')
         if r.status_code == codes.ok:
             return self._load_json_data(r)
-        else:
-            exc_cls, exc_msg = self._get_exception(r.headers, r.status_code)
-            raise exc_cls(exc_msg)
+        exc_cls, exc_msg = self._get_exception(r.headers, r.status_code)
+        raise exc_cls(exc_msg)
 
-    def examine_replication_rule(self, rule_id):
+    def examine_replication_rule(self, rule_id: str):
         """
         Examine a replication rule for errors during transfer.
 
         :param rule_id:             Rule to be denied.
         :raises:                    RuleNotFound
         """
         path = self.RULE_BASEURL + '/' + rule_id + '/analysis'
         url = build_url(choice(self.list_hosts), path=path)
-        r = self._send_request(url, type='GET')
+        r = self._send_request(url, type_='GET')
         if r.status_code == codes.ok:
-            return self._load_json_data(r).next()
+            return next(self._load_json_data(r))
+        exc_cls, exc_msg = self._get_exception(r.headers, r.status_code)
+        raise exc_cls(exc_msg)
+
+    def list_replica_locks(self, rule_id: str):
+        """
+        List details of all replica locks for a rule.
+
+        :param rule_id:             Rule to be denied.
+        :raises:                    RuleNotFound
+        """
+        path = self.RULE_BASEURL + '/' + rule_id + '/locks'
+        url = build_url(choice(self.list_hosts), path=path)
+        r = self._send_request(url, type_='GET')
+        if r.status_code == codes.ok:
+            return self._load_json_data(r)
+        exc_cls, exc_msg = self._get_exception(r.headers, r.status_code)
+        raise exc_cls(exc_msg)
+
+    def list_replication_rules(self, filters=None):
+        """
+        List all replication rules which match a filter
+        :param filters: dictionary of attributes by which the rules should be filtered
+
+        :returns: True if successful, otherwise false.
+        """
+        filters = filters or {}
+        path = self.RULE_BASEURL + '/'
+        url = build_url(choice(self.list_hosts), path=path)
+        r = self._send_request(url, type_='GET', params=filters)
+        if r.status_code == codes.ok:
+            return self._load_json_data(r)
         else:
-            exc_cls, exc_msg = self._get_exception(r.headers, r.status_code)
+            exc_cls, exc_msg = self._get_exception(headers=r.headers, status_code=r.status_code, data=r.content)
             raise exc_cls(exc_msg)
```

### Comparing `rucio-clients-1.9.6/lib/rucio/client/subscriptionclient.py` & `rucio-clients-32.0.0rc1/lib/rucio/client/subscriptionclient.py`

 * *Files 15% similar despite different names*

```diff
@@ -1,46 +1,48 @@
-# Copyright European Organization for Nuclear Research (CERN)
+# -*- coding: utf-8 -*-
+# Copyright European Organization for Nuclear Research (CERN) since 2012
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
-# You may not use this file except in compliance with the License.
-# You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
 #
-# Authors:
-# - Cedric Serfon, <cedric.serfon@cern.ch>, 2013-2014
-# - Ralph Vigne, <ralph.vigne@cern.ch>, 2015
-# - Vincent Garonne, <vincent.garonne@cern.ch>, 2015
+#    http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 
 from json import dumps
+
 from requests.status_codes import codes
 
 from rucio.client.baseclient import BaseClient
 from rucio.client.baseclient import choice
 from rucio.common.utils import build_url
 
 
 class SubscriptionClient(BaseClient):
 
     """SubscriptionClient class for working with subscriptions"""
 
     SUB_BASEURL = 'subscriptions'
 
-    def __init__(self, rucio_host=None, auth_host=None, account=None, ca_cert=None, auth_type=None, creds=None, timeout=None, user_agent='rucio-clients'):
-        super(SubscriptionClient, self).__init__(rucio_host, auth_host, account, ca_cert, auth_type, creds, timeout, user_agent)
-
-    def add_subscription(self, name, account, filter, replication_rules, comments, lifetime, retroactive, dry_run, priority=3):
+    def add_subscription(self, name, account, filter_, replication_rules, comments, lifetime, retroactive, dry_run, priority=3):
         """
         Adds a new subscription which will be verified against every new added file and dataset
 
         :param name: Name of the subscription
         :type:  String
         :param account: Account identifier
         :type account:  String
-        :param filter: Dictionary of attributes by which the input data should be filtered
+        :param filter_: Dictionary of attributes by which the input data should be filtered
                        **Example**: ``{'dsn': 'data11_hi*.express_express.*,data11_hi*physics_MinBiasOverlay*', 'account': 'tzero'}``
-        :type filter:  Dict
+        :type filter_:  Dict
         :param replication_rules: Replication rules to be set : Dictionary with keys copies, rse_expression, weight, rse_expression
         :type replication_rules:  Dict
         :param comments: Comments for the subscription
         :type comments:  String
         :param lifetime: Subscription's lifetime (days); False if subscription has no lifetime
         :type lifetime:  Integer or False
         :param retroactive: Flag to know if the subscription should be applied on previous data
@@ -48,25 +50,27 @@
         :param dry_run: Just print the subscriptions actions without actually executing them (Useful if retroactive flag is set)
         :type dry_run:  Boolean
         :param priority: The priority of the subscription (3 by default)
         :type priority: Integer
         """
         path = self.SUB_BASEURL + '/' + account + '/' + name
         url = build_url(choice(self.list_hosts), path=path)
-        if filter and type(filter) != dict:
+        if retroactive:
+            raise NotImplementedError('Retroactive mode is not implemented')
+        if filter_ and not isinstance(filter_, dict):
             raise TypeError('filter should be a dict')
-        if replication_rules and type(replication_rules) != list:
+        if replication_rules and not isinstance(replication_rules, list):
             raise TypeError('replication_rules should be a list')
-        data = dumps({'filter': filter, 'replication_rules': replication_rules, 'comments': comments,
-                      'lifetime': lifetime, 'retroactive': retroactive, 'dry_run': dry_run, 'priority': priority})
-        r = self._send_request(url, type='POST', data=data)
-        if r.status_code == codes.created:
-            return r.text
+        data = dumps({'options': {'filter': filter_, 'replication_rules': replication_rules, 'comments': comments,
+                                  'lifetime': lifetime, 'retroactive': retroactive, 'dry_run': dry_run, 'priority': priority}})
+        result = self._send_request(url, type_='POST', data=data)
+        if result.status_code == codes.created:   # pylint: disable=no-member
+            return result.text
         else:
-            exc_cls, exc_msg = self._get_exception(headers=r.headers, status_code=r.status_code, data=r.content)
+            exc_cls, exc_msg = self._get_exception(headers=result.headers, status_code=result.status_code, data=result.content)
             raise exc_cls(exc_msg)
 
     def list_subscriptions(self, name=None, account=None):
         """
         Returns a dictionary with the subscription information :
         Examples: ``{'status': 'INACTIVE/ACTIVE/BROKEN', 'last_modified_date': ...}``
 
@@ -79,35 +83,37 @@
         :raises: exception.NotFound if subscription is not found
         """
         path = self.SUB_BASEURL
         if account:
             path += '/%s' % (account)
             if name:
                 path += '/%s' % (name)
+        elif name:
+            path += '/Name/%s' % (name)
         else:
             path += '/'
         url = build_url(choice(self.list_hosts), path=path)
-        r = self._send_request(url, type='GET')
-        if r.status_code == codes.ok:
-            return self._load_json_data(r)
+        result = self._send_request(url, type_='GET')
+        if result.status_code == codes.ok:   # pylint: disable=no-member
+            return self._load_json_data(result)
         else:
-            exc_cls, exc_msg = self._get_exception(headers=r.headers, status_code=r.status_code, data=r.content)
+            exc_cls, exc_msg = self._get_exception(headers=result.headers, status_code=result.status_code, data=result.content)
             raise exc_cls(exc_msg)
 
-    def update_subscription(self, name, account=None, filter=None, replication_rules=None, comments=None, lifetime=None, retroactive=None, dry_run=None, priority=None):
+    def update_subscription(self, name, account=None, filter_=None, replication_rules=None, comments=None, lifetime=None, retroactive=None, dry_run=None, priority=None):
         """
         Updates a subscription
 
         :param name: Name of the subscription
         :type:  String
         :param account: Account identifier
         :type account:  String
-        :param filter: Dictionary of attributes by which the input data should be filtered
+        :param filter_: Dictionary of attributes by which the input data should be filtered
                        **Example**: ``{'dsn': 'data11_hi*.express_express.*,data11_hi*physics_MinBiasOverlay*', 'account': 'tzero'}``
-        :type filter:  Dict
+        :type filter_:  Dict
         :param replication_rules: Replication rules to be set : Dictionary with keys copies, rse_expression, weight, rse_expression
         :type replication_rules:  Dict
         :param comments: Comments for the subscription
         :type comments:  String
         :param lifetime: Subscription's lifetime (days); False if subscription has no lifetime
         :type lifetime:  Integer or False
         :param retroactive: Flag to know if the subscription should be applied on previous data
@@ -116,38 +122,40 @@
         :type dry_run:  Boolean
         :param priority: The priority of the subscription
         :type priority: Integer
         :raises: exception.NotFound if subscription is not found
         """
         if not account:
             account = self.account
+        if retroactive:
+            raise NotImplementedError('Retroactive mode is not implemented')
         path = self.SUB_BASEURL + '/' + account + '/' + name
         url = build_url(choice(self.list_hosts), path=path)
-        if filter and type(filter) != dict:
+        if filter_ and not isinstance(filter_, dict):
             raise TypeError('filter should be a dict')
-        if replication_rules and type(replication_rules) != list:
+        if replication_rules and not isinstance(replication_rules, list):
             raise TypeError('replication_rules should be a list')
-        data = dumps({'filter': filter, 'replication_rules': replication_rules, 'comments': comments,
-                      'lifetime': lifetime, 'retroactive': retroactive, 'dry_run': dry_run, 'priority': priority})
-        r = self._send_request(url, type='PUT', data=data)
-        if r.status_code == codes.created:
+        data = dumps({'options': {'filter': filter_, 'replication_rules': replication_rules, 'comments': comments,
+                                  'lifetime': lifetime, 'retroactive': retroactive, 'dry_run': dry_run, 'priority': priority}})
+        result = self._send_request(url, type_='PUT', data=data)
+        if result.status_code == codes.created:   # pylint: disable=no-member
             return True
         else:
-            exc_cls, exc_msg = self._get_exception(headers=r.headers, status_code=r.status_code, data=r.content)
+            exc_cls, exc_msg = self._get_exception(headers=result.headers, status_code=result.status_code, data=result.content)
             raise exc_cls(exc_msg)
 
     def list_subscription_rules(self, account, name):
         """
         List the associated rules of a subscription.
 
         :param account: Account of the subscription.
         :param name:    Name of the subscription.
         """
 
         path = '/'.join([self.SUB_BASEURL, account, name, 'Rules'])
         url = build_url(choice(self.list_hosts), path=path)
-        r = self._send_request(url, type='GET')
-        if r.status_code == codes.ok:
-            return self._load_json_data(r)
+        result = self._send_request(url, type_='GET')
+        if result.status_code == codes.ok:   # pylint: disable=no-member
+            return self._load_json_data(result)
         else:
-            exc_cls, exc_msg = self._get_exception(headers=r.headers, status_code=r.status_code, data=r.content)
+            exc_cls, exc_msg = self._get_exception(headers=result.headers, status_code=result.status_code, data=result.content)
             raise exc_cls(exc_msg)
```

### Comparing `rucio-clients-1.9.6/lib/rucio/common/exception.py` & `rucio-clients-32.0.0rc1/lib/rucio/common/exception.py`

 * *Files 26% similar despite different names*

```diff
@@ -1,46 +1,46 @@
-# Copyright European Organization for Nuclear Research (CERN)
+# -*- coding: utf-8 -*-
+# Copyright European Organization for Nuclear Research (CERN) since 2012
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
-# You may not use this file except in compliance with the License.
-# You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
 #
-# Authors:
-# - Thomas Beermann, <thomas.beermann@cern.ch> , 2012
-# - Angelos Molfetas, <angelos.molfetas@cern,ch>, 2012
-# - Mario Lassnig, <mario.lassnig@cern.ch>, 2012, 2014-2015
-# - Vincent Garonne, <vincent.garonne@cern.ch>, 2011-2013
-# - Ralph Vigne, <ralph.vigne@cern.ch>, 2012-2013
-# - Martin Barisits, <martin.barisits@cern.ch>, 2012-2016
-# - Cedric Serfon, <cedric.serfon@cern.ch>, 2013-2015
-# - Wen Guan, <wen.guan@cern.ch>, 2014
-
+#    http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 
-"""Exceptions used with Rucio.
+"""
+    Exceptions used with Rucio.
 
-The base exception class is :class:`. RucioException`.
-Exceptions which are raised are all subclasses of it.
+    The base exception class is :class:`. RucioException`.
+    Exceptions which are raised are all subclasses of it.
 
 """
 
 from rucio.common.constraints import AUTHORIZED_VALUE_TYPES
 
 
 class RucioException(Exception):
     """
     To correctly use this class, inherit from it and define
     a 'message' property. That message will get printf'd
     with the keyword arguments provided to the constructor.
     """
 
     def __init__(self, *args, **kwargs):
-        super(RucioException, self).__init__(args, kwargs)
+        super(RucioException, self).__init__(*args, **kwargs)
         self._message = "An unknown exception occurred."
         self.args = args
         self.kwargs = kwargs
+        self.error_code = 1
         self._error_string = None
 
     def __str__(self):
         try:
             self._error_string = self._message % self.kwargs
         except Exception:
             # at least get the core message out if something happened
@@ -51,631 +51,1042 @@
             # of the exception message
             # Convert all arguments into their string representations...
             args = ["%s" % arg for arg in self.args if arg]
             self._error_string = (self._error_string + "\nDetails: %s" % '\n'.join(args))
         return self._error_string.strip()
 
 
-# Please insert new exceptions in alphabetic order
+# Please insert new exceptions sorted by error_code, not alphabetically.
 
 class AccessDenied(RucioException):
     """
     AccessDenied
     """
     def __init__(self, *args, **kwargs):
-        super(AccessDenied, self).__init__(args, kwargs)
+        super(AccessDenied, self).__init__(*args, **kwargs)
         self._message = "Access to the requested resource denied."
+        self.error_code = 2
 
 
 class AccountNotFound(RucioException):
     """
     AccountNotFound
     """
     def __init__(self, *args, **kwargs):
-        super(AccountNotFound, self).__init__(args, kwargs)
+        super(AccountNotFound, self).__init__(*args, **kwargs)
         self._message = "Account does not exist."
+        self.error_code = 3
 
 
 class CannotAuthenticate(RucioException):
     """
     CannotAuthenticate
     """
     def __init__(self, *args, **kwargs):
-        super(CannotAuthenticate, self).__init__(args, kwargs)
+        super(CannotAuthenticate, self).__init__(*args, **kwargs)
         self._message = "Cannot authenticate."
+        self.error_code = 4
 
 
 class ClientParameterMismatch(RucioException):
     """
     RucioException
     """
     def __init__(self, *args, **kwargs):
-        super(ClientParameterMismatch, self).__init__(args, kwargs)
+        super(ClientParameterMismatch, self).__init__(*args, **kwargs)
         self._message = "Client parameters don\'t match."
+        self.error_code = 5
 
 
 class ClientProtocolNotSupported(RucioException):
     """
     RucioException
     """
     def __init__(self, *args, **kwargs):
-        super(ClientProtocolNotSupported, self).__init__(args, kwargs)
+        super(ClientProtocolNotSupported, self).__init__(*args, **kwargs)
         self._message = "Client protocol not supported."
+        self.error_code = 6
 
 
 class ConfigNotFound(RucioException):
     """
     RucioException
     """
     def __init__(self, *args, **kwargs):
-        super(ConfigNotFound, self).__init__(args, kwargs)
+        super(ConfigNotFound, self).__init__(*args, **kwargs)
         self._message = "Configuration not found."
+        self.error_code = 7
 
 
 class ConfigurationError(RucioException):
     """
     RucioException
     """
     def __init__(self, *args, **kwargs):
-        super(ConfigurationError, self).__init__(args, kwargs)
+        super(ConfigurationError, self).__init__(*args, **kwargs)
         self._message = "Error during configuration."
+        self.error_code = 8
 
 
 class CounterNotFound(RucioException):
     """
     RucioException
     """
     def __init__(self, *args, **kwargs):
-        super(CounterNotFound, self).__init__(args, kwargs)
+        super(CounterNotFound, self).__init__(*args, **kwargs)
         self._message = "The requested counter does not exist."
+        self.error_code = 9
 
 
 class DatabaseException(RucioException):
     """
     RucioException
     """
     def __init__(self, *args, **kwargs):
-        super(DatabaseException, self).__init__(args, kwargs)
+        super(DatabaseException, self).__init__(*args, **kwargs)
         self._message = "Database exception."
+        self.error_code = 10
 
 
 class DataIdentifierAlreadyExists(RucioException):
     """
     RucioException
     """
     def __init__(self, *args, **kwargs):
-        super(DataIdentifierAlreadyExists, self).__init__(args, kwargs)
+        super(DataIdentifierAlreadyExists, self).__init__(*args, **kwargs)
         self._message = "Data Identifier Already Exists."
+        self.error_code = 11
 
 
 class DataIdentifierNotFound(RucioException):
     """
     RucioException
     """
     def __init__(self, *args, **kwargs):
-        super(DataIdentifierNotFound, self).__init__(args, kwargs)
+        super(DataIdentifierNotFound, self).__init__(*args, **kwargs)
         self._message = "Data identifier not found."
+        self.error_code = 12
 
 
 class DestinationNotAccessible(RucioException):
     """
     RucioException
     """
     def __init__(self, *args, **kwargs):
-        super(DestinationNotAccessible, self).__init__(args, kwargs)
+        super(DestinationNotAccessible, self).__init__(*args, **kwargs)
         self._message = "Access to local destination denied."
+        self.error_code = 13
 
 
 class Duplicate(RucioException):
     """
     RucioException
     """
     def __init__(self, *args, **kwargs):
-        super(Duplicate, self).__init__(args, kwargs)
+        super(Duplicate, self).__init__(*args, **kwargs)
         self._message = "An object with the same identifier already exists."
+        self.error_code = 14
 
 
 class DuplicateContent(RucioException):
     """
     RucioException
     """
     def __init__(self, *args, **kwargs):
-        super(DuplicateContent, self).__init__(args, kwargs)
+        super(DuplicateContent, self).__init__(*args, **kwargs)
         self._message = "Data identifier already added to the destination content."
+        self.error_code = 15
 
 
 class DuplicateRule(RucioException):
     """
     RucioException
     """
     def __init__(self, *args, **kwargs):
-        super(DuplicateRule, self).__init__(args, kwargs)
+        super(DuplicateRule, self).__init__(*args, **kwargs)
         self._message = "A duplicate rule for this account, did, rse_expression, copies already exists."
+        self.error_code = 16
 
 
 class ErrorLoadingCredentials(RucioException):
     """
     RucioException
     """
     def __init__(self, *args, **kwargs):
-        super(ErrorLoadingCredentials, self).__init__(args, kwargs)
+        super(ErrorLoadingCredentials, self).__init__(*args, **kwargs)
         self._message = "Unable to to load user credentials."
+        self.error_code = 17
 
 
 class FileAlreadyExists(RucioException):
     """
     RucioException
     """
     def __init__(self, *args, **kwargs):
-        super(FileAlreadyExists, self).__init__(args, kwargs)
+        super(FileAlreadyExists, self).__init__(*args, **kwargs)
         self._message = "The file already exists."
+        self.error_code = 18
 
 
 class FileConsistencyMismatch(RucioException):
     """
     RucioException
     """
     def __init__(self, *args, **kwargs):
-        super(FileConsistencyMismatch, self).__init__(args, kwargs)
+        super(FileConsistencyMismatch, self).__init__(*args, **kwargs)
         self._message = "Error related to file consistency."
+        self.error_code = 19
 
 
 class FileReplicaAlreadyExists(RucioException):
     """
     RucioException
     """
     def __init__(self, *args, **kwargs):
-        super(FileReplicaAlreadyExists, self).__init__(args, kwargs)
+        super(FileReplicaAlreadyExists, self).__init__(*args, **kwargs)
         self._message = "File name in specified scope already exists"
+        self.error_code = 20
 
 
 class ReplicaNotFound(RucioException):
     """
     RucioException
     """
     def __init__(self, *args, **kwargs):
-        super(ReplicaNotFound, self).__init__(args, kwargs)
+        super(ReplicaNotFound, self).__init__(*args, **kwargs)
         self._message = "Replica not found"
+        self.error_code = 21
 
 
 class ReplicaUnAvailable(RucioException):
     """
     RucioException
     """
     def __init__(self, *args, **kwargs):
-        super(ReplicaUnAvailable, self).__init__(args, kwargs)
+        super(ReplicaUnAvailable, self).__init__(*args, **kwargs)
         self._message = "Replica unavailable"
+        self.error_code = 22
 
 
 class FullStorage(RucioException):
     """
     RucioException
     """
     def __init__(self, *args, **kwargs):
-        super(FullStorage, self).__init__(args, kwargs)
+        super(FullStorage, self).__init__(*args, **kwargs)
         self._message = "The Referenced storage is out of disk space."
+        self.error_code = 23
 
 
 class IdentityError(RucioException):
     """
     RucioException
     """
     def __init__(self, *args, **kwargs):
-        super(IdentityError, self).__init__(args, kwargs)
+        super(IdentityError, self).__init__(*args, **kwargs)
         self._message = "Identity error."
+        self.error_code = 24
 
 
 class IdentityNotFound(RucioException):
     def __init__(self, *args, **kwargs):
-        super(IdentityNotFound, self).__init__(args, kwargs)
+        super(IdentityNotFound, self).__init__(*args, **kwargs)
         self._message = "This identity does not exist."
+        self.error_code = 25
 
 
 class InputValidationError(RucioException):
     """
     RucioException
     """
     def __init__(self, *args, **kwargs):
-        super(InputValidationError, self).__init__(args, kwargs)
+        super(InputValidationError, self).__init__(*args, **kwargs)
         self._message = "There is an error with one of the input parameters."
+        self.error_code = 26
 
 
 class InsufficientAccountLimit(RucioException):
     """
     RucioException
     """
     def __init__(self, *args, **kwargs):
-        super(InsufficientAccountLimit, self).__init__(args, kwargs)
+        super(InsufficientAccountLimit, self).__init__(*args, **kwargs)
         self._message = "There is not enough quota left to fulfil the operation."
+        self.error_code = 27
 
 
 class InsufficientTargetRSEs(RucioException):
     """
     RucioException
     """
     def __init__(self, *args, **kwargs):
-        super(InsufficientTargetRSEs, self).__init__(args, kwargs)
+        super(InsufficientTargetRSEs, self).__init__(*args, **kwargs)
         self._message = "There are not enough target RSEs to fulfil the request at this time."
+        self.error_code = 28
 
 
 class InvalidMetadata(RucioException):
     """
     RucioException
     """
     def __init__(self, *args, **kwargs):
-        super(InvalidMetadata, self).__init__(args, kwargs)
+        super(InvalidMetadata, self).__init__(*args, **kwargs)
         self._message = "Provided metadata is considered invalid."
+        self.error_code = 29
 
 
 class InvalidObject(RucioException):
     """
     RucioException
     """
     def __init__(self, *args, **kwargs):
-        super(InvalidObject, self).__init__(args, kwargs)
+        super(InvalidObject, self).__init__(*args, **kwargs)
         self._message = "Provided object does not match schema."
+        self.error_code = 30
 
 
 class InvalidReplicationRule(RucioException):
     """
     RucioException
     """
     def __init__(self, *args, **kwargs):
-        super(InvalidReplicationRule, self).__init__(args, kwargs)
+        super(InvalidReplicationRule, self).__init__(*args, **kwargs)
         self._message = "Provided replication rule is considered invalid."
+        self.error_code = 31
 
 
 class InvalidRSEExpression(RucioException):
     """
     RucioException
     """
     def __init__(self, *args, **kwargs):
-        super(InvalidRSEExpression, self).__init__(args, kwargs)
+        super(InvalidRSEExpression, self).__init__(*args, **kwargs)
         self._message = "Provided RSE expression is considered invalid."
+        self.error_code = 32
 
 
 class InvalidRuleWeight(RucioException):
     """
     RucioException
     """
     def __init__(self, *args, **kwargs):
-        super(InvalidRuleWeight, self).__init__(args, kwargs)
+        super(InvalidRuleWeight, self).__init__(*args, **kwargs)
         self._message = "An invalid weight value/type is used for an RSE."
+        self.error_code = 33
 
 
 class InvalidType(RucioException):
     """
     RucioException
     """
     def __init__(self, *args, **kwargs):
-        super(InvalidType, self).__init__(args, kwargs)
+        super(InvalidType, self).__init__(*args, **kwargs)
         self._message = "Provided type is considered invalid."
+        self.error_code = 34
 
 
 class InvalidValueForKey(RucioException):
     """
     RucioException
     """
     def __init__(self, *args, **kwargs):
-        super(InvalidValueForKey, self).__init__(args, kwargs)
+        super(InvalidValueForKey, self).__init__(*args, **kwargs)
         self._message = "Invalid value for the key."
+        self.error_code = 35
 
 
 class InvalidRequest(RucioException):
     """
     RucioException
     """
     def __init__(self, *args, **kwargs):
-        super(InvalidRequest, self).__init__(args, kwargs)
+        super(InvalidRequest, self).__init__(*args, **kwargs)
         self._message = "Request is considered invalid."
+        self.error_code = 36
 
 
 class InvalidPath(RucioException):
     """
     RucioException
     """
     def __init__(self, *args, **kwargs):
-        super(InvalidPath, self).__init__(args, kwargs)
+        super(InvalidPath, self).__init__(*args, **kwargs)
         self._message = "The path provided is invalid."
+        self.error_code = 37
 
 
 class KeyNotFound(RucioException):
     """
     RucioException
     """
     def __init__(self, *args, **kwargs):
-        super(KeyNotFound, self).__init__(args, kwargs)
+        super(KeyNotFound, self).__init__(*args, **kwargs)
         self._message = "Key does not exist."
+        self.error_code = 38
+
+
+class LifetimeExceptionDuplicate(RucioException):
+    """
+    RucioException
+    """
+    def __init__(self, *args, **kwargs):
+        super(LifetimeExceptionDuplicate, self).__init__(*args, **kwargs)
+        self._message = "An exception already exists."
+        self.error_code = 39
+
+
+class LifetimeExceptionNotFound(RucioException):
+    """
+    RucioException
+    """
+    def __init__(self, *args, **kwargs):
+        super(LifetimeExceptionNotFound, self).__init__(*args, **kwargs)
+        self._message = "Exception does not exist."
+        self.error_code = 40
 
 
 class ManualRuleApprovalBlocked(RucioException):
     """
     RucioException
     """
     def __init__(self, *args, **kwargs):
-        super(ManualRuleApprovalBlocked, self).__init__(args, kwargs)
+        super(ManualRuleApprovalBlocked, self).__init__(*args, **kwargs)
         self._message = "Manual rule approval is blocked on this RSE."
+        self.error_code = 41
 
 
 class MissingClientParameter(RucioException):
     """
     RucioException
     """
     def __init__(self, *args, **kwargs):
-        super(MissingClientParameter, self).__init__(args, kwargs)
+        super(MissingClientParameter, self).__init__(*args, **kwargs)
         self._message = "Client parameters are missing."
+        self.error_code = 42
 
 
 class MissingDependency(RucioException):
     """
     RucioException
     """
     def __init__(self, *args, **kwargs):
-        super(MissingDependency, self).__init__(args, kwargs)
+        super(MissingDependency, self).__init__(*args, **kwargs)
         self._message = "One dependency is missing."
+        self.error_code = 43
 
 
 class MissingSourceReplica(RucioException):
     """
     RucioException
     """
     def __init__(self, *args, **kwargs):
-        super(MissingSourceReplica, self).__init__(args, kwargs)
+        super(MissingSourceReplica, self).__init__(*args, **kwargs)
         self._message = "Source replicas are missing to fulfil the request at this moment."
+        self.error_code = 44
 
 
 class NameTypeError(RucioException):
     """
     RucioException
     """
     def __init__(self, *args, **kwargs):
-        super(NameTypeError, self).__init__(args, kwargs)
+        super(NameTypeError, self).__init__(*args, **kwargs)
         self._message = "Name is of the wrong type"
+        self.error_code = 45
 
 
 class NoAuthInformation(RucioException):
     """
     RucioException
     """
     def __init__(self, *args, **kwargs):
-        super(NoAuthInformation, self).__init__(args, kwargs)
+        super(NoAuthInformation, self).__init__(*args, **kwargs)
         self._message = "No authentication information passed."
+        self.error_code = 46
+
+
+class NoFilesDownloaded(RucioException):
+    """
+    RucioException
+    """
+    def __init__(self, *args, **kwargs):
+        super(NoFilesDownloaded, self).__init__(*args, **kwargs)
+        self._message = "None of the requested files have been downloaded."
+        self.error_code = 75
+
+
+class NotAllFilesDownloaded(RucioException):
+    """
+    RucioException
+    """
+    def __init__(self, *args, **kwargs):
+        super(NotAllFilesDownloaded, self).__init__(*args, **kwargs)
+        self._message = "Not all of the requested files have been downloaded."
+        self.error_code = 76
 
 
 class ReplicationRuleCreationTemporaryFailed(RucioException):
     """
     RucioException
     """
     def __init__(self, *args, **kwargs):
-        super(ReplicationRuleCreationTemporaryFailed, self).__init__(args, kwargs)
+        super(ReplicationRuleCreationTemporaryFailed, self).__init__(*args, **kwargs)
         self._message = "The creation of the replication rule failed at this time. Please try again later."
+        self.error_code = 47
 
 
 class RequestNotFound(RucioException):
     def __init__(self, *args, **kwargs):
-        super(RequestNotFound, self).__init__(args, kwargs)
+        super(RequestNotFound, self).__init__(*args, **kwargs)
         self._message = "A request for this DID and RSE does not exist."
+        self.error_code = 48
 
 
 class RSEAccessDenied(RucioException):
     """
     RucioException
     """
     def __init__(self, *args, **kwargs):
-        super(RSEAccessDenied, self).__init__(args, kwargs)
+        super(RSEAccessDenied, self).__init__(*args, **kwargs)
         self._message = "Referenced RSE not reachable."
+        self.error_code = 49
 
 
-class RSEBlacklisted(RucioException):
+class RSEWriteBlocked(RucioException):
     """
     RucioException
     """
     def __init__(self, *args, **kwargs):
-        super(RSEBlacklisted, self).__init__(args, kwargs)
-        self._message = "RSE excluded due to write blacklisting."
+        super(RSEWriteBlocked, self).__init__(*args, **kwargs)
+        self._message = "RSE excluded; not available for writing."
+        self.error_code = 50
 
 
 class RSENotConnected(RucioException):
     """
     RucioException
     """
     def __init__(self, *args, **kwargs):
-        super(RSENotConnected, self).__init__(args, kwargs)
+        super(RSENotConnected, self).__init__(*args, **kwargs)
         self._message = "Connection to RSE not established."
+        self.error_code = 51
 
 
 class RSENotFound(RucioException):
     """
     RucioException
     """
     def __init__(self, *args, **kwargs):
-        super(RSENotFound, self).__init__(args, kwargs)
+        super(RSENotFound, self).__init__(*args, **kwargs)
         self._message = "RSE does not exist."
+        self.error_code = 52
 
 
 class RSEProtocolNotSupported(RucioException):
     """
     RucioException
     """
     def __init__(self, *args, **kwargs):
-        super(RSEProtocolNotSupported, self).__init__(args, kwargs)
+        super(RSEProtocolNotSupported, self).__init__(*args, **kwargs)
         self._message = "RSE does not support requested protocol."
+        self.error_code = 53
 
 
 class RSEProtocolPriorityError(RucioException):
     """
     RucioException
     """
     def __init__(self, *args, **kwargs):
-        super(RSEProtocolPriorityError, self).__init__(args, kwargs)
+        super(RSEProtocolPriorityError, self).__init__(*args, **kwargs)
         self._message = "RSE does not support provided protocol priority for protocol."
+        self.error_code = 54
 
 
 class RSEProtocolDomainNotSupported(RucioException):
     """
     RucioException
     """
     def __init__(self, *args, **kwargs):
-        super(RSEProtocolDomainNotSupported, self).__init__(args, kwargs)
+        super(RSEProtocolDomainNotSupported, self).__init__(*args, **kwargs)
         self._message = "RSE does not support requested protocol scope."
+        self.error_code = 55
 
 
 class RSEOperationNotSupported(RucioException):
     """
     RucioException
     """
     def __init__(self, *args, **kwargs):
-        super(RSEOperationNotSupported, self).__init__(args, kwargs)
+        super(RSEOperationNotSupported, self).__init__(*args, **kwargs)
         self._message = "RSE does not support requested operation."
+        self.error_code = 56
 
 
 class RSEFileNameNotSupported(RucioException):
     """
     RucioException
     """
     def __init__(self, *args, **kwargs):
-        super(RSEFileNameNotSupported, self).__init__(args, kwargs)
+        super(RSEFileNameNotSupported, self).__init__(*args, **kwargs)
         self._message = "RSE does not support provided filename."
+        self.error_code = 57
 
 
 class RSEOverQuota(RucioException):
     """
     RucioException
     """
     def __init__(self, *args, **kwargs):
-        super(RSEOverQuota, self).__init__(args, kwargs)
+        super(RSEOverQuota, self).__init__(*args, **kwargs)
         self._message = "Quota of Referenced RSE is exceeded."
+        self.error_code = 58
 
 
 class ResourceTemporaryUnavailable(RucioException):
     """
     RucioException
     """
     def __init__(self, *args, **kwargs):
-        super(ResourceTemporaryUnavailable, self).__init__(args, kwargs)
+        super(ResourceTemporaryUnavailable, self).__init__(*args, **kwargs)
         self._message = "The resource is temporary not available."
+        self.error_code = 59
 
 
 class RuleNotFound(RucioException):
     """
     RucioException
     """
     def __init__(self, *args, **kwargs):
-        super(RuleNotFound, self).__init__(args, kwargs)
+        super(RuleNotFound, self).__init__(*args, **kwargs)
         self._message = "No replication rule found."
+        self.error_code = 60
 
 
 class RuleReplaceFailed(RucioException):
     """
     RucioException
     """
     def __init__(self, *args, **kwargs):
-        super(RuleReplaceFailed, self).__init__(args, kwargs)
+        super(RuleReplaceFailed, self).__init__(*args, **kwargs)
         self._message = "The replace operation for the rule failed."
+        self.error_code = 61
 
 
 class ScratchDiskLifetimeConflict(RucioException):
     """
     RucioException
     """
     def __init__(self, *args, **kwargs):
-        super(ScratchDiskLifetimeConflict, self).__init__(args, kwargs)
+        super(ScratchDiskLifetimeConflict, self).__init__(*args, **kwargs)
         self._message = "The requested replication rule exceeds the maximum SCRATCHDISK lifetime of 15 days."
+        self.error_code = 62
 
 
 class ServiceUnavailable(RucioException):
     """
     RucioException
     """
     def __init__(self, *args, **kwargs):
-        super(ServiceUnavailable, self).__init__(args, kwargs)
+        super(ServiceUnavailable, self).__init__(*args, **kwargs)
         self._message = "The requested service is not available at the moment."
+        self.error_code = 63
 
 
 class ScopeAccessDenied(RucioException):
     """
     RucioException
     """
     def __init__(self, *args, **kwargs):
-        super(ScopeAccessDenied, self).__init__(args, kwargs)
+        super(ScopeAccessDenied, self).__init__(*args, **kwargs)
         self._message = "Access to Referenced scope denied."
+        self.error_code = 64
 
 
 class ScopeNotFound(RucioException):
     """
     RucioException
     """
     def __init__(self, *args, **kwargs):
-        super(ScopeNotFound, self).__init__(args, kwargs)
+        super(ScopeNotFound, self).__init__(*args, **kwargs)
         self._message = "Scope does not exist."
+        self.error_code = 65
 
 
 class SourceAccessDenied(RucioException):
     """
     RucioException
     """
     def __init__(self, *args, **kwargs):
-        super(SourceAccessDenied, self).__init__(args, kwargs)
+        super(SourceAccessDenied, self).__init__(*args, **kwargs)
         self._message = "Access to local source file denied."
+        self.error_code = 66
 
 
 class SourceNotFound(RucioException):
     """
     RucioException
     """
     def __init__(self, *args, **kwargs):
-        super(SourceNotFound, self).__init__(args, kwargs)
+        super(SourceNotFound, self).__init__(*args, **kwargs)
         self._message = "Source file not found."
+        self.error_code = 67
 
 
 class StagingAreaRuleRequiresLifetime(RucioException):
     """
     RucioException
     """
     def __init__(self, *args, **kwargs):
-        super(StagingAreaRuleRequiresLifetime, self).__init__(args, kwargs)
+        super(StagingAreaRuleRequiresLifetime, self).__init__(*args, **kwargs)
         self._message = "A rule involving a staging area requires a lifetime!"
+        self.error_code = 68
 
 
 class SubscriptionDuplicate(RucioException):
     """
     RucioException
     """
     def __init__(self, *args, **kwargs):
-        super(SubscriptionDuplicate, self).__init__(args, kwargs)
+        super(SubscriptionDuplicate, self).__init__(*args, **kwargs)
         self._message = "A subscription with the same identifier already exists."
+        self.error_code = 69
 
 
 class SubscriptionNotFound(RucioException):
     """
     RucioException
     """
     def __init__(self, *args, **kwargs):
-        super(SubscriptionNotFound, self).__init__(args, kwargs)
+        super(SubscriptionNotFound, self).__init__(*args, **kwargs)
         self._message = "Subscription not found."
+        self.error_code = 70
+
+
+class UnsupportedDIDType(RucioException):
+    """
+    RucioException
+    """
+    def __init__(self, *args, **kwargs):
+        super(UnsupportedDIDType, self).__init__(*args, **kwargs)
+        self._message = "Unsupported DID type for this operation. Only DATASET or FILE is allowed."
+        self.error_code = 71
 
 
 class UnsupportedOperation(RucioException):
     """
     RucioException
     """
     def __init__(self, *args, **kwargs):
-        super(UnsupportedOperation, self).__init__(args, kwargs)
+        super(UnsupportedOperation, self).__init__(*args, **kwargs)
         self._message = "The resource doesn't support the requested operation."
+        self.error_code = 72
 
 
 class UnsupportedStatus(RucioException):
     """
     RucioException
     """
     def __init__(self, *args, **kwargs):
-        super(UnsupportedStatus, self).__init__(args, kwargs)
+        super(UnsupportedStatus, self).__init__(*args, **kwargs)
         self._message = "Unsupported data identifier status."
+        self.error_code = 73
 
 
 class UnsupportedValueType(RucioException):
     """
     RucioException
     """
     def __init__(self, *args, **kwargs):
-        super(UnsupportedValueType, self).__init__(args, kwargs)
+        super(UnsupportedValueType, self).__init__(*args, **kwargs)
         self._message = "Unsupported type for the value. List of supported types: %s." % str(AUTHORIZED_VALUE_TYPES)
+        self.error_code = 74
+
+
+class MissingModuleException(RucioException):
+    """
+    RucioException
+    """
+    def __init__(self, *args, **kwargs):
+        super(MissingModuleException, self).__init__(*args, **kwargs)
+        self._message = "The module is not installed."
+        self.error_code = 77
+
+
+class ServerConnectionException(RucioException):
+    """
+    RucioException
+    """
+    def __init__(self, *args, **kwargs):
+        super(ServerConnectionException, self).__init__(*args, **kwargs)
+        self._message = "Cannot connect to the Rucio server."
+        self.error_code = 78
+
+
+class NoFilesUploaded(RucioException):
+    """
+    RucioException
+    """
+    def __init__(self, *args, **kwargs):
+        super(NoFilesUploaded, self).__init__(*args, **kwargs)
+        self._message = "None of the given files have been uploaded."
+        self.error_code = 79
+
+
+class NotAllFilesUploaded(RucioException):
+    """
+    RucioException
+    """
+    def __init__(self, *args, **kwargs):
+        super(NotAllFilesUploaded, self).__init__(*args, **kwargs)
+        self._message = "Not all of the given files have been uploaded."
+        self.error_code = 80
+
+
+class RSEChecksumUnavailable(RucioException):
+    """
+    Cannot retrieve checksum from RSE
+    """
+    def __init__(self, *args, **kwargs):
+        super(RSEChecksumUnavailable, self).__init__(*args, **kwargs)
+        self._message = "RSE checksum unavailable."
+        self.error_code = 81
+
+
+class UndefinedPolicy(RucioException):
+    """
+    Cannot find a defined policy in the Rucio config
+    """
+    def __init__(self, *args, **kwargs):
+        super(UndefinedPolicy, self).__init__(*args, **kwargs)
+        self._message = "No policy is defined."
+        self.error_code = 82
+
+
+class TransferToolTimeout(RucioException):
+    """
+    Timeout from the transfer tool
+    """
+    def __init__(self, *args, **kwargs):
+        super(TransferToolTimeout, self).__init__(*args, **kwargs)
+        self._message = "Timeout from the transfer tool."
+        self.error_code = 83
+
+
+class TransferToolWrongAnswer(RucioException):
+    """
+    Wrong answer returned by the transfer tool
+    """
+    def __init__(self, *args, **kwargs):
+        super(TransferToolWrongAnswer, self).__init__(*args, **kwargs)
+        self._message = "Wrong answer returned by the transfer tool."
+        self.error_code = 84
+
+
+class RSEAttributeNotFound(RucioException):
+    """
+    RSE attribute not found.
+    """
+    def __init__(self, *args, **kwargs):
+        super(RSEAttributeNotFound, self).__init__(*args, **kwargs)
+        self._message = "RSE attribute not found."
+        self.error_code = 85
+
+
+class UnsupportedKeyType(RucioException):
+    """
+    RucioException
+    """
+    def __init__(self, *args, **kwargs):
+        super(UnsupportedKeyType, self).__init__(*args, **kwargs)
+        self._message = "Unsupported type for the key."
+        self.error_code = 86
+
+
+class MetalinkJsonParsingError(RucioException):
+    """
+    Failed to parse input with metalink and json
+    """
+    def __init__(self, data, metalink_err, json_err, *args, **kwargs):
+        super(MetalinkJsonParsingError, self).__init__(*args, **kwargs)
+        self._message = 'Failed parsing of %s. MetalinkError: %s. JsonError: %s' % (data, metalink_err, json_err)
+        self.error_code = 87
+
+
+class ReplicaIsLocked(RucioException):
+    """
+    Replica has one or more locks.
+    """
+    def __init__(self, *args, **kwargs):
+        super(ReplicaIsLocked, self).__init__(*args, **kwargs)
+        self._message = 'Replica is locked'
+        self.error_code = 88
+
+
+class UnsupportedRequestedContentType(RucioException):
+    """
+    The requested content type is not supported by the API endpoint.
+    """
+    def __init__(self, *args, **kwargs):
+        super(UnsupportedRequestedContentType, self).__init__(*args, **kwargs)
+        self._message = 'The requested content type is not supported.'
+        self.error_code = 89
+
+
+class DuplicateFileTransferSubmission(RucioException):
+    """
+    A transfer for the same file is already submitted to the Transfer Tool.
+    """
+    def __init__(self, *args, **kwargs):
+        super(DuplicateFileTransferSubmission, self).__init__(*args, **kwargs)
+        self._message = 'One or more files are already submitted to the transfer tool'
+        self.error_code = 90
+
+
+class DIDError(RucioException):
+    """
+    An operation related to DID type went wrong
+    """
+    def __init__(self, *args, **kwargs):
+        super(DIDError, self).__init__(*args, **kwargs)
+        self._message = 'Error using DID type'
+        self.error_code = 91
+
+
+class NoDistance(RucioException):
+    """
+    No distance can be found between 2 RSEs
+    """
+    def __init__(self, *args, **kwargs):
+        super(NoDistance, self).__init__(*args, **kwargs)
+        self._message = 'Cannot found a distance between 2 RSEs'
+        self.error_code = 92
+
+
+class PolicyPackageNotFound(RucioException):
+    """
+    The policy package specified in the config file cannot be loaded.
+    """
+    def __init__(self, *args, **kwargs):
+        super(PolicyPackageNotFound, self).__init__(*args, **kwargs)
+        self._message = 'The specified policy package cannot be loaded'
+        self.error_code = 93
+
+
+class CannotAuthorize(RucioException):
+    """
+    Failed to authorize an operation.
+    """
+    def __init__(self, *args, **kwargs):
+        super(CannotAuthorize, self).__init__(*args, **kwargs)
+        self._message = 'Can not authorize operation.'
+        self.error_code = 94
+
+
+class SubscriptionWrongParameter(RucioException):
+    """
+    RucioException
+    """
+    def __init__(self, *args, **kwargs):
+        super(SubscriptionWrongParameter, self).__init__(*args, **kwargs)
+        self._message = "Subscription wrong parameters"
+        self.error_code = 95
+
+
+class VONotFound(RucioException):
+    """
+    Requested VO does not exist.
+    """
+    def __init__(self, *args, **kwargs):
+        super(VONotFound, self).__init__(*args, **kwargs)
+        self._message = 'The requested VO does not exist'
+        self.error_code = 96
+
+
+class UnsupportedAccountName(RucioException):
+    """
+    Requested account name is not supported for users.
+    """
+    def __init__(self, *args, **kwargs):
+        super(UnsupportedAccountName, self).__init__(*args, **kwargs)
+        self._message = 'The requested account name cannot be used'
+        self.error_code = 97
+
+
+class DuplicateCriteriaInDIDFilter(RucioException):
+    """
+    Duplicate criteria found in DID filter.
+    """
+    def __init__(self, *args, **kwargs):
+        super(DuplicateCriteriaInDIDFilter, self).__init__(*args, **kwargs)
+        self._message = 'Duplicate criteria for key/operator in filter expression: {}'.format(args[0])
+        self.error_code = 98
+
+
+class DIDFilterSyntaxError(RucioException):
+    """
+    DID filter is not parsable.
+    """
+    def __init__(self, *args, **kwargs):
+        super(DIDFilterSyntaxError, self).__init__(*args, **kwargs)
+        self._message = 'Syntax error in filter expression.'
+        self.error_code = 99
+
+
+class InvalidAlgorithmName(RucioException):
+    """
+    The given algorithm name is not valid for the VO.
+    """
+    def __init__(self, algorithm, vo, *args, **kwargs):
+        super(InvalidAlgorithmName, self).__init__(*args, **kwargs)
+        self.message = 'Algorithm name %s is not valid for VO %s' % (algorithm, vo)
+        self.error_code = 100
+
+
+class FilterEngineGenericError(RucioException):
+    """
+    Generic Filter Engine error.
+    """
+    def __init__(self, *args, **kwargs):
+        super(FilterEngineGenericError, self).__init__(*args, **kwargs)
+        self._message = 'Generic filter engine error.'
+        self.error_code = 101
+
+
+class MetadataSchemaMismatchError(RucioException):
+    """
+    External table does not match expected table schema.
+    """
+    def __init__(self, *args, **kwargs):
+        super(MetadataSchemaMismatchError, self).__init__(*args, **kwargs)
+        self._message = 'The external table does not match the expected table schema.'
+        self.error_code = 102
+
+
+class PolicyPackageVersionError(RucioException):
+    """
+    Policy package is not compatible with this version of Rucio.
+    """
+    def __init__(self, package, *args, **kwargs):
+        super(PolicyPackageVersionError, self).__init__(*args, **kwargs)
+        self._message = 'Policy package %s is not compatible with this Rucio version' % package
+        self.error_code = 103
+
+
+class InvalidSourceReplicaExpression(RucioException):
+    """
+    Source Replica Expression Considered Invalid
+    """
+
+    def __init__(self, *args, **kwargs):
+        super(InvalidSourceReplicaExpression, self).__init__(*args, **kwargs)
+        self._message = 'Provided Source Replica expression is considered invalid.'
+        self.error_code = 104
+
+
+class DeprecationError(RucioException):
+    """
+    Function has been deprecated.
+    """
+    def __init__(self, *args, **kwargs):
+        super(DeprecationError, self).__init__(*args, **kwargs)
+        self._message = 'Command or function has been deprecated.'
+        self.error_code = 105
```

### Comparing `rucio-clients-1.9.6/lib/rucio/common/schema.py` & `rucio-clients-32.0.0rc1/lib/rucio/common/schema/icecube.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,70 +1,84 @@
-'''
-  Copyright European Organization for Nuclear Research (CERN)
-
-  Licensed under the Apache License, Version 2.0 (the "License");
-  You may not use this file except in compliance with the License.
-  You may obtain a copy of the License at
-  http://www.apache.org/licenses/LICENSE-2.0
-
-  Authors:
-  - Vincent Garonne, <vincent.garonne@cern.ch>, 2013-2016
-  - Cedric Serfon, <cedric.serfon@cern.ch>, 2014-2015
-  - Joaquin Bogado, <joaquin.bogado@cern.ch>, 2015
-  - Mario Lassnig, <mario.lassnig@cern.ch>, 2015
-  - Martin Barisits, <martin.barisits@cern.ch>, 2016
-'''
+# -*- coding: utf-8 -*-
+# Copyright European Organization for Nuclear Research (CERN) since 2012
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#    http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 
 from jsonschema import validate, ValidationError
 
 from rucio.common.exception import InvalidObject
 
+ACCOUNT_LENGTH = 25
 
 ACCOUNT = {"description": "Account name",
            "type": "string",
-           "pattern": "^[a-z0-9-_]{1,30}$"}
+           "pattern": "^[a-z0-9-_]{1,%s}$" % ACCOUNT_LENGTH}
+
+ACCOUNTS = {"description": "Array of accounts",
+            "type": "array",
+            "items": ACCOUNT,
+            "minItems": 0,
+            "maxItems": 1000}
+
 
 ACCOUNT_TYPE = {"description": "Account type",
                 "type": "string",
                 "enum": ["USER", "GROUP", "SERVICE"]}
 
 ACTIVITY = {"description": "Activity name",
             "type": "string",
             "enum": ["Data Brokering", "Data Consolidation", "Data rebalancing",
                      "Debug", "Express", "Functional Test", "Group Subscriptions",
                      "Production Input", "Production Output",
-                     "Analysis Input", "Staging",
+                     "Analysis Input", "Analysis Output", "Staging",
                      "T0 Export", "T0 Tape", "Upload/Download (Job)",
                      "Upload/Download (User)", "User Subscriptions"]}
 
+SCOPE_LENGTH = 25
+
 SCOPE = {"description": "Scope name",
          "type": "string",
-         "pattern": "^[a-zA-Z'_'-.0-9]{1,30}$"}
+         "pattern": "^[a-zA-Z_\\-.0-9]{1,%s}$" % SCOPE_LENGTH}
 
 R_SCOPE = {"description": "Scope name",
            "type": "string",
            "pattern": "\\w"}
 
-NAME = {"description": "Data Identifier name",
+
+NAME_LENGTH = 500
+
+NAME = {"description": "IceCube Data Identifier name",
         "type": "string",
-        "pattern": "^[A-Za-z0-9][A-Za-z0-9\\.\\-\\_]{1,255}$"}
+        "pattern": r"^\/[A-Za-z0-9][A-Za-z0-9\\.\\-\\_\/\#]{1,%s}$" % NAME_LENGTH}
 
-R_NAME = {"description": "Data Identifier name",
-          "type": "string",
-          "pattern": "\\w"}
+# read name
+R_NAME = NAME
 
 LOCKED = {"description": "Rule locked status",
           "type": ["boolean", "null"]}
 
 ASK_APPROVAL = {"description": "Rule approval request",
                 "type": ["boolean", "null"]}
 
 ASYNCHRONOUS = {"description": "Asynchronous rule creation",
                 "type": ["boolean", "null"]}
 
+DELAY_INJECTION = {"description": "Time (in seconds) to wait before starting applying the rule. Implies asynchronous rule creation.",
+                   "type": ["integer", "null"]}
+
 PURGE_REPLICAS = {"description": "Rule purge replica status",
                   "type": "boolean"}
 
 IGNORE_AVAILABILITY = {"description": "Rule ignore availability status",
                        "type": "boolean"}
 
 RSE = {"description": "RSE name",
@@ -93,20 +107,24 @@
 
 GROUPING = {"description": "Rule grouping",
             "type": ["string", "null"],
             "enum": ["DATASET", "NONE", "ALL", None]}
 
 NOTIFY = {"description": "Rule notification setting",
           "type": ["string", "null"],
-          "enum": ["Y", "C", "N", None]}
+          "enum": ["Y", "C", "N", "P", None]}
 
 COMMENT = {"description": "Rule comment",
            "type": ["string", "null"],
            "maxLength": 250}
 
+METADATA = {"description": "Rule wfms metadata",
+            "type": ["string", "null"],
+            "maxLength": 3999}
+
 BYTES = {"description": "Size in bytes",
          "type": "integer"}
 
 ADLER32 = {"description": "adler32",
            "type": "string",
            "pattern": "^[a-fA-F\\d]{8}$"}
 
@@ -161,16 +179,18 @@
                        "activity": ACTIVITY,
                        "notify": NOTIFY,
                        "purge_replicas": PURGE_REPLICAS,
                        "ignore_availability": IGNORE_AVAILABILITY,
                        "comment": COMMENT,
                        "ask_approval": ASK_APPROVAL,
                        "asynchronous": ASYNCHRONOUS,
+                       "delay_injection": DELAY_INJECTION,
                        "priority": PRIORITY,
-                       'split_container': SPLIT_CONTAINER},
+                       'split_container': SPLIT_CONTAINER,
+                       'meta': METADATA},
         "required": ["dids", "copies", "rse_expression"],
         "additionalProperties": False}
 
 RULES = {"description": "Array of replication rules",
          "type": "array",
          "items": RULE,
          "minItems": 1,
@@ -207,18 +227,16 @@
                       "adler32": ADLER32,
                       "md5": MD5,
                       "state": REPLICA_STATE,
                       "pfn": PFN},
        "required": ["scope", "name"],
        "additionalProperties": False}
 
-DID_FILTERS = {"description": "Filters dictionary to list DIDs",
-               "type": "object",
-               "properties": {"created_before": DATE,
-                              "created_afted": DATE},
+DID_FILTERS = {"description": "Array to filter DIDs by metadata",
+               "type": "array",
                "additionalProperties": True}
 
 R_DID = {"description": "Data Identifier(DID)",
          "type": "object",
          "properties": {"scope": R_SCOPE,
                         "name": R_NAME,
                         "type": DID_TYPE,
@@ -267,15 +285,15 @@
                                       "stream_name": {"type": "array"},
                                       "project": {"type": "array"},
                                       "scope": {"type": "array"},
                                       "pattern": {"type": "string"},
                                       "excluded_pattern": {"type": "string"},
                                       "group": {"type": "string"},
                                       "provenance": {"type": "string"},
-                                      "account": {"type": "string", "pattern": "^[a-z0-9-]{1,30}$"},
+                                      "account": ACCOUNTS,
                                       "grouping": {"type": "string"},
                                       "split_rule": {"type": "boolean"}}}
 
 ADD_REPLICA_FILE = {"description": "add replica file",
                     "type": "object",
                     "properties": {"scope": SCOPE,
                                    "name": NAME,
@@ -317,15 +335,38 @@
                          "required": ['files', 'rse', 'operation']}
 
 MESSAGE_OPERATION = {"type": "object",
                      "properties": {'operation': {"enum": ["add_replicas", "delete_replicas"]}}}
 
 ACCOUNT_ATTRIBUTE = {"description": "Account attribute",
                      "type": "string",
-                     "pattern": r'^[a-z0-9-_]{1,30}$'}
+                     "pattern": r'^[a-zA-Z0-9-_\\/\\.]{1,30}$'}
+
+SCOPE_NAME_REGEXP = '/([^/]*)(?=/)(.*)'
+
+DISTANCE = {"description": "RSE distance",
+            "type": "object",
+            "properties": {
+                "src_rse_id": {"type": "string"},
+                "dest_rse_id": {"type": "string"},
+                "ranking": {"type": "integer"}
+            },
+            "required": ["src_rse_id", "dest_rse_id", "ranking"],
+            "additionalProperties": True}
+
+IMPORT = {"description": "import data into rucio.",
+          "type": "object",
+          "properties": {
+              "rses": {
+                  "type": "object"
+              },
+              "distances": {
+                  "type": "object"
+              }
+          }}
 
 SCHEMAS = {'account': ACCOUNT,
            'account_type': ACCOUNT_TYPE,
            'activity': ACTIVITY,
            'name': NAME,
            'r_name': R_NAME,
            'rse': RSE,
@@ -341,15 +382,16 @@
            'collection': COLLECTION,
            'collections': COLLECTIONS,
            'attachment': ATTACHMENT,
            'attachments': ATTACHMENTS,
            'subscription_filter': SUBSCRIPTION_FILTER,
            'cache_add_replicas': CACHE_ADD_REPLICAS,
            'cache_delete_replicas': CACHE_DELETE_REPLICAS,
-           'account_attribute': ACCOUNT_ATTRIBUTE}
+           'account_attribute': ACCOUNT_ATTRIBUTE,
+           'import': IMPORT}
 
 
 def validate_schema(name, obj):
     """
     Validate object against json schema
 
     :param name: The json schema name.
```

### Comparing `rucio-clients-1.9.6/lib/rucio/rse/protocols/cache.py` & `rucio-clients-32.0.0rc1/lib/rucio/rse/protocols/cache.py`

 * *Files 24% similar despite different names*

```diff
@@ -1,32 +1,34 @@
-# Copyright European Organization for Nuclear Research (CERN)
+# -*- coding: utf-8 -*-
+# Copyright European Organization for Nuclear Research (CERN) since 2012
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
-# You may not use this file except in compliance with the License.
+# you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
-# http://www.apache.org/licenses/LICENSE-2.0
 #
-# Authors:
-# - Vincent Garonne, <vincent.garonne@cern.ch>, 2016
-
-
-from exceptions import NotImplementedError
+#    http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 
 from rucio.rse.protocols import protocol
 
 
 class Default(protocol.RSEProtocol):
     """ Implementing access to RSEs using the local filesystem."""
 
-    def __init__(self, protocol_attr, rse_settings):
+    def __init__(self, protocol_attr, rse_settings, logger=None):
         """ Initializes the object with information about the referred RSE.
 
-            :param props Properties derived from the RSE Repository
+            :param props: Properties derived from the RSE Repository
         """
-        super(Default, self).__init__(protocol_attr, rse_settings)
+        super(Default, self).__init__(protocol_attr, rse_settings, logger=logger)
         self.attributes.pop('determinism_type', None)
         self.files = []
 
     def _get_path(self, scope, name):
         """ Transforms the physical file name into the local URI in the referred RSE.
             Suitable for sites implementoing the RUCIO naming convention.
 
@@ -47,26 +49,26 @@
 
         """
         return ''.join([self.rse['scheme'], '://%s' % self.rse['hostname'], path])
 
     def exists(self, pfn):
         """ Checks if the requested file is known by the referred RSE.
 
-            :param pfn Physical file name
+            :param pfn: Physical file name
 
             :returns: True if the file exists, False if it doesn't
 
             :raise  ServiceUnavailable
         """
         raise NotImplementedError
 
     def connect(self):
         """ Establishes the actual connection to the referred RSE.
 
-            :param credentials Provide all necessary information to establish a connection
+            :param credentials: Provide all necessary information to establish a connection
                 to the referred storage system. Some is loaded from the repository inside the
                 RSE class and some must be provided specific for the SFTP protocol like
                 username, password, private_key, private_key_pass, port.
                 For details about possible additional parameters and details about their usage
                 see the pysftp.Connection() documentation.
                 NOTE: the host parametrer is overwritten with the value provided by the repository
 
@@ -74,46 +76,48 @@
         """
         raise NotImplementedError
 
     def close(self):
         """ Closes the connection to RSE."""
         raise NotImplementedError
 
-    def get(self, pfn, dest):
+    def get(self, pfn, dest, transfer_timeout=None):
         """ Provides access to files stored inside connected the RSE.
 
-            :param pfn Physical file name of requested file
-            :param dest Name and path of the files when stored at the client
+            :param pfn: Physical file name of requested file
+            :param dest: Name and path of the files when stored at the client
+            :param transfer_timeout Transfer timeout (in seconds)
 
             :raises DestinationNotAccessible, ServiceUnavailable, SourceNotFound
          """
         raise NotImplementedError
 
-    def put(self, source, target, source_dir=None):
+    def put(self, source, target, source_dir=None, transfer_timeout=None):
         """ Allows to store files inside the referred RSE.
 
-            :param source Physical file name
-            :param target Name of the file on the storage system e.g. with prefixed scope
+            :param source: Physical file name
+            :param target: Name of the file on the storage system e.g. with prefixed scope
             :param source_dir Path where the to be transferred files are stored in the local file system
+            :param transfer_timeout Transfer timeout (in seconds)
 
             :raises DestinationNotAccessible, ServiceUnavailable, SourceNotFound
         """
         raise NotImplementedError
 
     def delete(self, pfn):
         """ Deletes a file from the connected RSE.
 
-            :param pfn Physical file name
+            :param pfn: Physical file name
 
             :raises ServiceUnavailable, SourceNotFound
         """
         raise NotImplementedError
 
     def rename(self, pfn, new_pfn):
         """ Allows to rename a file stored inside the connected RSE.
 
-            :param pfn      Current physical file name
+            :param pfn:      Current physical file name
             :param new_pfn  New physical file name
 
             :raises DestinationNotAccessible, ServiceUnavailable, SourceNotFound
         """
         raise NotImplementedError
```

### Comparing `rucio-clients-1.9.6/lib/rucio/rse/protocols/gfal.py` & `rucio-clients-32.0.0rc1/lib/rucio/rse/protocols/srm.py`

 * *Files 17% similar despite different names*

```diff
@@ -1,146 +1,162 @@
-# Copyright European Organization for Nuclear Research (CERN)
+# -*- coding: utf-8 -*-
+# Copyright European Organization for Nuclear Research (CERN) since 2012
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
-# You may not use this file except in compliance with the License.
+# you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
-# http://www.apache.org/licenses/LICENSE-2.0
 #
-# Authors:
-# - Wen Guan, <wguan@cern.ch>, 2014-2016
-# - Cedric Serfon, <cedric.serfon@cern.ch>, 2014-2016
-# - Vincent Garonne, <vincent.garonne@cern.ch>, 2016
-# - Mario Lassnig, <mario.lassnig@cern.ch>, 2016
+#    http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 
-import errno
-import json
 import os
 import re
-import urlparse
+import urllib.parse as urlparse
+from subprocess import getstatusoutput
 
-from rucio.common import exception, config
+from rucio.common import exception
+from rucio.common.utils import execute
 from rucio.rse.protocols import protocol
 
-try:
-    import gfal2
-except:
-    if not config.config_has_section('database'):
-        raise exception.MissingDependency('Missing dependency : gfal2')
-
 
 class Default(protocol.RSEProtocol):
-    """ Implementing access to RSEs using the srm protocol."""
+    """ Implementing access to RSEs using the SRM protocol. """
 
     def lfns2pfns(self, lfns):
         """
         Returns a fully qualified PFN for the file referred by path.
 
         :param path: The path to the file.
-
         :returns: Fully qualified PFN.
         """
 
         pfns = {}
         prefix = self.attributes['prefix']
-        if self.attributes['extended_attributes'] is not None and 'web_service_path' in self.attributes['extended_attributes'].keys():
+        if self.attributes['extended_attributes'] is not None and\
+           'web_service_path' in list(self.attributes['extended_attributes'].keys()):
             web_service_path = self.attributes['extended_attributes']['web_service_path']
         else:
             web_service_path = ''
 
         if not prefix.startswith('/'):
             prefix = ''.join(['/', prefix])
         if not prefix.endswith('/'):
             prefix = ''.join([prefix, '/'])
 
         hostname = self.attributes['hostname']
         if '://' in hostname:
             hostname = hostname.split("://")[1]
 
         lfns = [lfns] if type(lfns) == dict else lfns
-        if self.attributes['port'] == 0:
+        if not self.attributes['port']:
             for lfn in lfns:
-                scope, name = lfn['scope'], lfn['name']
-                path = lfn['path'] if 'path' in lfn and lfn['path'] else self._get_path(scope=scope, name=name)
-                if self.attributes['scheme'] != 'root' and path.startswith('/'):  # do not modify path if it is root
+                scope, name, path = lfn['scope'], lfn['name'], lfn.get('path')
+                if not path:
+                    path = self._get_path(scope=scope, name=name)
+                if path.startswith('/'):
                     path = path[1:]
-                pfns['%s:%s' % (scope, name)] = ''.join([self.attributes['scheme'], '://', hostname, web_service_path, prefix, path])
+                pfns['%s:%s' % (scope, name)] = ''.join([self.attributes['scheme'], '://',
+                                                         hostname, web_service_path, prefix, path])
         else:
             for lfn in lfns:
-                scope, name = lfn['scope'], lfn['name']
-                path = lfn['path'] if 'path' in lfn and lfn['path'] else self._get_path(scope=scope, name=name)
-                if self.attributes['scheme'] != 'root' and path.startswith('/'):  # do not modify path if it is root
+                scope, name, path = lfn['scope'], lfn['name'], lfn.get('path')
+                if not path:
+                    path = self._get_path(scope=scope, name=name)
+                if path.startswith('/'):
                     path = path[1:]
-                pfns['%s:%s' % (scope, name)] = ''.join([self.attributes['scheme'], '://', hostname, ':', str(self.attributes['port']), web_service_path, prefix, path])
+                pfns['%s:%s' % (scope, name)] = ''.join([self.attributes['scheme'], '://',
+                                                         hostname, ':', str(self.attributes['port']),
+                                                         web_service_path, prefix, path])
 
         return pfns
 
     def parse_pfns(self, pfns):
         """
         Splits the given PFN into the parts known by the protocol. During parsing the PFN is also checked for
         validity on the given RSE with the given protocol.
 
         :param pfn: a fully qualified PFN
-
         :returns: a dict containing all known parts of the PFN for the protocol e.g. scheme, path, filename
-
         :raises RSEFileNameNotSupported: if the provided PFN doesn't match with the protocol settings
         """
 
         ret = dict()
-        pfns = [pfns] if ((type(pfns) == str) or (type(pfns) == unicode)) else pfns
+        pfns = [pfns] if isinstance(pfns, str) else pfns
         for pfn in pfns:
             parsed = urlparse.urlparse(pfn)
-            if parsed.path.startswith('/srm/managerv2') or parsed.path.startswith('/srm/managerv1') or parsed.path.startswith('/srm/v2/server'):
+            if parsed.path.startswith('/srm/managerv2') or\
+               parsed.path.startswith('/srm/managerv1') or\
+               parsed.path.startswith('/srm/v2/server'):
                 scheme, hostname, port, service_path, path = re.findall(r"([^:]+)://([^:/]+):?(\d+)?([^:]+=)?([^:]+)", pfn)[0]
             else:
                 scheme = parsed.scheme
                 hostname = parsed.netloc.partition(':')[0]
                 port = parsed.netloc.partition(':')[2]
                 path = parsed.path
                 service_path = ''
 
-            if self.attributes['hostname'] != hostname and self.attributes['hostname'] != scheme + "://" + hostname:
-                raise exception.RSEFileNameNotSupported('Invalid hostname: provided \'%s\', expected \'%s\'' % (hostname, self.attributes['hostname']))
+            # force type conversion
+            try:
+                port = int(port)
+            except:
+                port = ''
+
+            if self.attributes['hostname'] != hostname and\
+               self.attributes['hostname'] != scheme + "://" + hostname:
+                raise exception.RSEFileNameNotSupported('Invalid hostname: provided \'%s\', expected \'%s\'' % (hostname,
+                                                                                                                self.attributes['hostname']))
 
             if port != '' and str(self.attributes['port']) != str(port):
-                raise exception.RSEFileNameNotSupported('Invalid port: provided \'%s\', expected \'%s\'' % (port, self.attributes['port']))
+                raise exception.RSEFileNameNotSupported('Invalid port: provided \'%s\', expected \'%s\'' % (port,
+                                                                                                            self.attributes['port']))
             elif port == '':
                 port = self.attributes['port']
 
             if not path.startswith(self.attributes['prefix']):
                 raise exception.RSEFileNameNotSupported('Invalid prefix: provided \'%s\', expected \'%s\'' % ('/'.join(path.split('/')[0:len(self.attributes['prefix'].split('/')) - 1]),
                                                                                                               self.attributes['prefix']))  # len(...)-1 due to the leading '/
+
             # Spliting path into prefix, path, filename
             prefix = self.attributes['prefix']
             path = path.partition(self.attributes['prefix'])[2]
             name = path.split('/')[-1]
-            path = path.partition(name)[0]
-            if not path.startswith('/'):
-                path = '/' + path
-            ret[pfn] = {'scheme': scheme, 'port': port, 'hostname': hostname, 'path': path, 'name': name, 'prefix': prefix, 'web_service_path': service_path}
+            path = '/' + '/'.join(path.split('/')[:-1]) if not self.rse['staging_area'] else None
+
+            if path != '/' and path[:-1] != '/':
+                path += '/'
+
+            ret[pfn] = {'scheme': scheme, 'port': port, 'hostname': hostname,
+                        'path': path, 'name': name, 'prefix': prefix,
+                        'web_service_path': service_path}
 
         return ret
 
     def path2pfn(self, path):
         """
         Returns a fully qualified PFN for the file referred by path.
 
         :param path: The path to the file.
-
         :returns: Fully qualified PFN.
         """
 
-        if '://' in path:
+        if path.startswith("srm://"):
             return path
 
         hostname = self.attributes['hostname']
         if '://' in hostname:
             hostname = hostname.split("://")[1]
 
-        if 'extended_attributes' in self.attributes.keys() and self.attributes['extended_attributes'] is not None and 'web_service_path' in self.attributes['extended_attributes'].keys():
+        if 'extended_attributes' in list(self.attributes.keys()) and\
+           self.attributes['extended_attributes'] is not None and\
+           'web_service_path' in list(self.attributes['extended_attributes'].keys()):
             web_service_path = self.attributes['extended_attributes']['web_service_path']
         else:
             web_service_path = ''
 
         if not path.startswith('srm'):
             if self.attributes['port'] > 0:
                 return ''.join([self.attributes['scheme'], '://', hostname, ':', str(self.attributes['port']), web_service_path, path])
@@ -148,308 +164,176 @@
                 return ''.join([self.attributes['scheme'], '://', hostname, web_service_path, path])
         else:
             return path
 
     def connect(self):
         """
         Establishes the actual connection to the referred RSE.
+        As a quick and dirty impelementation we just use this method to check if the lcg tools are available.
         If we decide to use gfal, init should be done here.
 
-        :raises RSEAccessDenied
+        :raises RSEAccessDenied: Cannot connect.
         """
 
-        self.__ctx = gfal2.creat_context()
-        self.__ctx.set_opt_string_list("SRM PLUGIN", "TURL_PROTOCOLS", ["gsiftp", "rfio", "gsidcap", "dcap", "kdcap"])
+        status, lcglscommand = getstatusoutput('which lcg-ls')
+        if status:
+            raise exception.RSEAccessDenied('Cannot find lcg tools')
+        endpoint_basepath = self.path2pfn(self.attributes['prefix'])
+        status, result = getstatusoutput('%s -vv $LCGVO -b --srm-timeout 60 -D srmv2 -l %s' % (lcglscommand, endpoint_basepath))
+        if status:
+            if result == '':
+                raise exception.RSEAccessDenied('Endpoint not reachable. lcg-ls failed with status code %s but no further details.' % (str(status)))
+            else:
+                raise exception.RSEAccessDenied('Endpoint not reachable : %s' % str(result))
 
-    def get(self, path, dest):
+    def get(self, path, dest, transfer_timeout=None):
         """
         Provides access to files stored inside connected the RSE.
 
         :param path: Physical file name of requested file
         :param dest: Name and path of the files when stored at the client
+        :param transfer_timeout: Transfer timeout (in seconds)
 
         :raises DestinationNotAccessible: if the destination storage was not accessible.
         :raises ServiceUnavailable: if some generic error occured in the library.
         :raises SourceNotFound: if the source file was not found on the referred storage.
         """
 
-        dest = os.path.abspath(dest)
-        if ':' not in dest:
-            dest = "file://" + dest
+        timeout_option = ''
+        if transfer_timeout:
+            timeout_option = '--sendreceive-timeout %s' % transfer_timeout
 
         try:
-            status = self.__gfal2_copy(path, dest)
+            cmd = 'lcg-cp $LCGVO -v -b --srm-timeout 3600 %s -D srmv2 %s file:%s' % (timeout_option, path, dest)
+            status, out, err = execute(cmd)
             if status:
-                raise exception.RucioException()
-        except exception.DestinationNotAccessible as error:
-            raise exception.DestinationNotAccessible(str(error))
+                if self.__parse_srm_error__("SRM_INVALID_PATH", out, err):
+                    raise exception.SourceNotFound(err)
+                raise exception.RucioException(err)
         except exception.SourceNotFound as error:
             raise exception.SourceNotFound(str(error))
         except Exception as error:
             raise exception.ServiceUnavailable(error)
 
-    def put(self, source, target, source_dir):
+    def put(self, source, target, source_dir, transfer_timeout=None):
         """
         Allows to store files inside the referred RSE.
 
         :param source: path to the source file on the client file system
         :param target: path to the destination file on the storage
         :param source_dir: Path where the to be transferred files are stored in the local file system
+        :param transfer_timeout: Transfer timeout (in seconds)
 
         :raises DestinationNotAccessible: if the destination storage was not accessible.
         :raises ServiceUnavailable: if some generic error occured in the library.
         :raises SourceNotFound: if the source file was not found on the referred storage.
         """
 
         source_url = '%s/%s' % (source_dir, source) if source_dir else source
 
-        source_url = os.path.abspath(source_url)
         if not os.path.exists(source_url):
             raise exception.SourceNotFound()
-        if ':' not in source_url:
-            source_url = "file://" + source_url
 
-        space_token = None
-        if self.attributes['extended_attributes'] is not None and 'space_token' in self.attributes['extended_attributes'].keys():
-            space_token = self.attributes['extended_attributes']['space_token']
+        space_token = ''
+        if self.attributes['extended_attributes'] is not None and 'space_token' in list(self.attributes['extended_attributes'].keys()):
+            space_token = '--dst %s' % self.attributes['extended_attributes']['space_token']
+
+        timeout_option = ''
+        if transfer_timeout:
+            timeout_option = '--sendreceive-timeout %s' % transfer_timeout
 
         try:
-            status = self.__gfal2_copy(str(source_url), str(target), None, space_token)
+            cmd = 'lcg-cp $LCGVO -v -b --srm-timeout 3600 %s -D srmv2 %s file:%s %s' % (timeout_option, space_token, source_url, target)
+            status, out, err = execute(cmd)
             if status:
-                raise exception.RucioException()
-        except exception.DestinationNotAccessible as error:
-            raise exception.DestinationNotAccessible(str(error))
-        except exception.SourceNotFound as error:
-            raise exception.DestinationNotAccessible(str(error))
+                raise exception.RucioException(err)
         except Exception as error:
             raise exception.ServiceUnavailable(error)
 
     def delete(self, path):
         """
         Deletes a file from the connected RSE.
 
         :param path: path to the to be deleted file
-
         :raises ServiceUnavailable: if some generic error occured in the library.
         :raises SourceNotFound: if the source file was not found on the referred storage.
         """
 
-        pfns = [path] if ((type(path) == str) or (type(path) == unicode)) else path
+        pfns = [path] if isinstance(path, str) else path
 
         try:
-            status = self.__gfal2_rm(pfns)
-            if status:
-                raise exception.RucioException()
+            pfn_chunks = [pfns[i:i + 20] for i in range(0, len(pfns), 20)]
+            for pfn_chunk in pfn_chunks:
+                cmd = 'lcg-del $LCGVO -v -b -l --srm-timeout 600 -D srmv2'
+                for pfn in pfn_chunk:
+                    cmd += ' ' + pfn
+                status, out, err = execute(cmd)
+                if status:
+                    if self.__parse_srm_error__("SRM_INVALID_PATH", out, err):
+                        raise exception.SourceNotFound(err)
+                    raise exception.RucioException(err)
         except exception.SourceNotFound as error:
             raise exception.SourceNotFound(str(error))
         except Exception as error:
             raise exception.ServiceUnavailable(error)
 
     def rename(self, path, new_path):
         """
         Allows to rename a file stored inside the connected RSE.
 
         :param path: path to the current file on the storage
         :param new_path: path to the new file on the storage
-
         :raises DestinationNotAccessible: if the destination storage was not accessible.
         :raises ServiceUnavailable: if some generic error occured in the library.
         :raises SourceNotFound: if the source file was not found on the referred storage.
         """
 
+        space_token = ''
+        if self.attributes['extended_attributes'] is not None and 'space_token' in list(self.attributes['extended_attributes'].keys()):
+            space_token = '--dst %s' % self.attributes['extended_attributes']['space_token']
+
         try:
-            status = self.__gfal2_rename(path, new_path)
+            cmd = 'lcg-cp $LCGVO -v -b --srm-timeout 3600 -D srmv2 %s %s %s' % (space_token, path, new_path)
+            status, out, err = execute(cmd)
             if status:
-                raise exception.RucioException()
-        except exception.DestinationNotAccessible as error:
-            raise exception.DestinationNotAccessible(str(error))
-        except exception.SourceNotFound as error:
-            raise exception.SourceNotFound(str(error))
+                raise exception.RucioException(err)
+
+            cmd = 'lcg-del $LCGVO -v -b -l --srm-timeout 600 -D srmv2 %s' % (path)
+            status, out, err = execute(cmd)
+            if status:
+                raise exception.RucioException(err)
         except Exception as error:
             raise exception.ServiceUnavailable(error)
 
     def exists(self, path):
         """
         Checks if the requested file is known by the referred RSE.
 
         :param path: Physical file name
-
         :returns: True if the file exists, False if it doesn't
-
         :raises SourceNotFound: if the source file was not found on the referred storage.
         """
 
         try:
-            status = self.__gfal2_exist(path)
+            cmd = 'lcg-ls $LCGVO -v -b --srm-timeout 60 -D srmv2  %s' % (path)
+            status, out, err = execute(cmd)
             if status:
                 return False
             return True
-        except exception.SourceNotFound as error:
-            return False
         except Exception as error:
             raise exception.ServiceUnavailable(error)
 
+    def __parse_srm_error__(self, err_code, out, err):
+        """Parse the error message to return error code."""
+        if out is not None and len(out) > 0:
+            if out.count(err_code) > 0:
+                return True
+        if err is not None and len(err) > 0:
+            if err.count(err_code) > 0:
+                return True
+        return False
+
     def close(self):
         """
         Closes the connection to RSE.
         """
-
-        del self.__ctx
-        self.__ctx = None
-
-    def __gfal2_copy(self, src, dest, src_spacetoken=None, dest_spacetoken=None):
-        """
-        Uses gfal2 to copy file from src to dest.
-
-        :param src: Physical source file name
-        :param src_spacetoken: The source file's space token
-        :param dest: Physical destination file name
-        :param dest_spacetoken: The destination file's space token
-
-        :returns: 0 if copied successfully, other than 0 if failed
-
-        :raises SourceNotFound: if source file cannot be found.
-        :raises RucioException: if it failed to copy the file.
-        """
-
-        ctx = self.__ctx
-        params = ctx.transfer_parameters()
-        if src_spacetoken:
-            params.src_spacetoken = str(src_spacetoken)
-        if dest_spacetoken:
-            params.dst_spacetoken = str(dest_spacetoken)
-        params.timeout = 3600
-
-        dir_name = os.path.dirname(dest)
-        # This function will be removed soon. gfal2 will create parent dir automatically.
-        try:
-            ctx.mkdir_rec(str(dir_name), 0775)
-        except:
-            pass
-
-        try:
-            ret = ctx.filecopy(params, str(src), str(dest))
-            return ret
-        except gfal2.GError as error:
-            if error.code == errno.ENOENT or 'No such file' in error.message:
-                raise exception.SourceNotFound(error)
-            raise exception.RucioException(error)
-
-    def __gfal2_rm(self, paths):
-        """
-        Uses gfal2 to remove the file.
-
-        :param path: Physical file name
-
-        :returns: 0 if removed successfully, other than 0 if failed
-
-        :raises SourceNotFound: if the source file was not found.
-        :raises RucioException: if it failed to remove the file.
-        """
-
-        ctx = self.__ctx
-
-        try:
-            for path in paths:
-                ret = ctx.unlink(str(path))
-                if ret:
-                    return ret
-            return ret
-        except gfal2.GError as error:
-            if error.code == errno.ENOENT or 'No such file' in error.message:
-                raise exception.SourceNotFound(error)
-            raise exception.RucioException(error)
-
-    def __gfal2_exist(self, path):
-        """
-        Uses gfal2 to check whether the file exists.
-
-        :param path: Physical file name
-
-        :returns: 0 if it exists, -1 if it doesn't
-
-        :raises RucioException: if the error is not source not found.
-        """
-
-        ctx = self.__ctx
-
-        try:
-            if ctx.stat(str(path)):
-                return 0
-            return -1
-        except gfal2.GError as error:
-            if error.code == errno.ENOENT or 'No such file' in error.message:
-                return -1
-            raise exception.RucioException(error)
-
-    def __gfal2_rename(self, path, new_path):
-        """
-        Uses gfal2 to rename a file.
-
-        :param path: path to the current file on the storage
-        :param new_path: path to the new file on the storage
-
-        :returns: 0 if it exists, -1 if it doesn't
-
-        :raises RucioException: if failed.
-        """
-
-        ctx = self.__ctx
-
-        try:
-            dir_name = os.path.dirname(new_path)
-            # This function will be removed soon. gfal2 will create parent dir automatically.
-            try:
-                ctx.mkdir_rec(str(dir_name), 0775)
-            except Exception:
-                pass
-            ret = ctx.rename(str(path), str(new_path))
-            return ret
-        except gfal2.GError as error:
-            if error.code == errno.ENOENT or 'No such file' in error.message:
-                raise exception.SourceNotFound(error)
-            raise exception.RucioException(error)
-
-    def get_space_usage(self):
-        """
-        Get RSE space usage information.
-
-        :returns: a list with dict containing 'totalsize' and 'unusedsize'
-
-        :raises ServiceUnavailable: if some generic error occured in the library.
-        """
-        endpoint_basepath = self.path2pfn(self.attributes['prefix'])
-        space_token = None
-        if self.attributes['extended_attributes'] is not None and 'space_token' in self.attributes['extended_attributes'].keys():
-            space_token = self.attributes['extended_attributes']['space_token']
-
-        if space_token is None or space_token == "":
-            raise exception.RucioException("Space token is not defined for protocol: %s" % (self.attributes['scheme']))
-
-        try:
-            totalsize, unusedsize = self.__gfal2_get_space_usage(endpoint_basepath, space_token)
-            return totalsize, unusedsize
-        except Exception as error:
-            raise exception.ServiceUnavailable(error)
-
-    def __gfal2_get_space_usage(self, path, space_token):
-        """
-        Uses gfal2 to get space usage info with space token.
-
-        :param path: the endpoint path
-        :param space_token: a string space token. E.g. "ATLASDATADISK"
-
-        :returns: a list with dict containing 'totalsize' and 'unusedsize'
-
-        :raises ServiceUnavailable: if failed.
-        """
-
-        ctx = self.__ctx
-
-        try:
-            ret_usage = ctx.getxattr(str(path), str("spacetoken.description?" + space_token))
-            usage = json.loads(ret_usage)
-            totalsize = usage[0]["totalsize"]
-            unusedsize = usage[0]["unusedsize"]
-            return totalsize, unusedsize
-        except gfal2.GError as error:
-            raise Exception(str(error))
+        pass
```

### Comparing `rucio-clients-1.9.6/lib/rucio/rse/protocols/gsiftp.py` & `rucio-clients-32.0.0rc1/lib/rucio/rse/protocols/mock.py`

 * *Files 27% similar despite different names*

```diff
@@ -1,127 +1,124 @@
-# Copyright European Organization for Nuclear Research (CERN)
+# -*- coding: utf-8 -*-
+# Copyright European Organization for Nuclear Research (CERN) since 2012
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
-# You may not use this file except in compliance with the License.
+# you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
-# http://www.apache.org/licenses/LICENSE-2.0
 #
-# Authors:
-# - Vincent Garonne, <vincent.garonne@cern.ch>, 2014
-# - Wen Guan, <wen.guan@cern.ch>, 2015
-# - Tomas Javurek, <Tomas.Javurek@cern.ch>, 2016
-# - Cedric Serfon, <cedric.serfon@cern.ch>, 2016
-
-import json
-import os
-import requests
+#    http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 
 from rucio.common import exception
 from rucio.rse.protocols import protocol
 
 
 class Default(protocol.RSEProtocol):
-    """ Implementing access to RSEs using gsiftp."""
+    """ Implementing access to RSEs using the local filesystem."""
 
-    def __init__(self, protocol_attr, rse_settings):
+    def __init__(self, protocol_attr, rse_settings, logger=None):
         """ Initializes the object with information about the referred RSE.
 
-            :param props Properties derived from the RSE Repository
+            :param props: Properties derived from the RSE Repository
         """
-        super(Default, self).__init__(protocol_attr, rse_settings)
+        super(Default, self).__init__(protocol_attr, rse_settings, logger=logger)
+        self.attributes.pop('determinism_type', None)
+        self.files = []
 
-    def connect(self):
+    def path2pfn(self, path):
         """
-        Establishes the actual connection to the referred RSE.
-        If we decide to use gfal, init should be done here.
+            Retruns a fully qualified PFN for the file referred by path.
+
+            :param path: The path to the file.
+
+            :returns: Fully qualified PFN.
 
-        :raises RSEAccessDenied
+        """
+        return ''.join([self.rse['scheme'], '://%s' % self.rse['hostname'], path])
+
+    def exists(self, pfn):
+        """ Checks if the requested file is known by the referred RSE.
+
+            :param pfn: Physical file name
+
+            :returns: True if the file exists, False if it doesn't
+
+            :raise  ServiceUnavailable
+        """
+        return pfn in self.files
+
+    def connect(self):
+        """ Establishes the actual connection to the referred RSE.
+
+            :param credentials: Provide all necessary information to establish a connection
+                to the referred storage system. Some is loaded from the repository inside the
+                RSE class and some must be provided specific for the SFTP protocol like
+                username, password, private_key, private_key_pass, port.
+                For details about possible additional parameters and details about their usage
+                see the pysftp.Connection() documentation.
+                NOTE: the host parametrer is overwritten with the value provided by the repository
+
+            :raise RSEAccessDenied
         """
         pass
 
     def close(self):
+        """ Closes the connection to RSE."""
+        pass
+
+    def get(self, pfn, dest, transfer_timeout=None):
+        """ Provides access to files stored inside connected the RSE.
+
+            :param pfn: Physical file name of requested file
+            :param dest: Name and path of the files when stored at the client
+            :param transfer_timeout Transfer timeout (in seconds) - dummy
+
+            :raises DestinationNotAccessible, ServiceUnavailable, SourceNotFound
+         """
+        if pfn not in self.files:
+            raise exception.SourceNotFound(pfn)
+
+    def put(self, source, target, source_dir=None, transfer_timeout=None):
+        """ Allows to store files inside the referred RSE.
+
+            :param source: Physical file name
+            :param target: Name of the file on the storage system e.g. with prefixed scope
+            :param source_dir Path where the to be transferred files are stored in the local file system
+            :param transfer_timeout Transfer timeout (in seconds) - dummy
+
+            :raises DestinationNotAccessible, ServiceUnavailable, SourceNotFound
         """
-        Closes the connection to RSE.
+        self.files.append(target)
+
+    def delete(self, pfn):
+        """ Deletes a file from the connected RSE.
+
+            :param pfn: Physical file name
+
+            :raises ServiceUnavailable, SourceNotFound
         """
         pass
 
-    def get_space_usage(self):
+    def bulk_delete(self, pfns):
         """
-        Get RSE space usage information.
+            Submits an async task to bulk delete files.
 
-        :returns: a list with dict containing 'totalsize' and 'unusedsize'
+            :param pfns: list of pfns to delete
 
-        :raises ServiceUnavailable: if some generic error occured in the library.
+            :raises TransferAPIError: if unexpected response from the service.
         """
-        # original
-        rse_name = self.rse['rse']
-        endpoint_path = ''.join([self.attributes['scheme'], '://', self.attributes['hostname'], ':', str(self.attributes['port']), '/atlas/dq2/site-size'])
-        dest = '/tmp/rucio-gsiftp-site-size_' + rse_name
-
-        space_usage_url = ''
-        # url of space usage json, woud be nicer to have it in rse_settings
-        agis = requests.get('http://atlas-agis-api.cern.ch/request/ddmendpoint/query/list/?json').json()
-        agis_token = ''
-        for res in agis:
-            if rse_name == res['name']:
-                agis_token = res['token']
-                space_usage_url = res['space_usage_url']
-
-        import gfal2
-        try:
-            if os.path.exists(dest):
-                os.remove(dest)
-            ctx = gfal2.creat_context()
-            ctx.set_opt_string_list("SRM PLUGIN", "TURL_PROTOCOLS", ["gsiftp", "rfio", "gsidcap", "dcap", "kdcap"])
-            params = ctx.transfer_parameters()
-            params.timeout = 3600
-            ret = ctx.filecopy(params, str(space_usage_url), str('file://' + dest))
-
-            if ret == 0:
-                data_file = open(dest)
-                data = json.load(data_file)
-                data_file.close()
-                if agis_token not in data.keys():
-                    print 'ERROR: space usage json has different token as key'
-                else:
-                    totalsize = data[agis_token]['total_space']
-                    used = data[agis_token]['used_space']
-                    unusedsize = totalsize - used
-                    return totalsize, unusedsize
-        except Exception as e:
-            print e
-            raise exception.ServiceUnavailable(e)
-
-        space_token = None
-        if self.attributes['extended_attributes'] is not None:
-            space_token = self.attributes['extended_attributes'].get('space_token')
-        xattr_name = 'spacetoken'
-        if space_token:
-            xattr_name = 'spacetoken?%s' % space_token
-
-        try:
-            if os.path.exists(dest):
-                os.remove(dest)
-            ctx = gfal2.creat_context()
-
-            # See if the remote server supports the the SITE USAGE command
-            try:
-                data = json.loads(ctx.getxattr(str(endpoint_path), xattr_name))
-                totalsize = data['totalsize']
-                unusedsize = data['unusedsize']
-                return totalsize, unusedsize
-            except gfal2.GError:
-                pass
-            params = ctx.transfer_parameters()
-            params.timeout = 60
-            ret = ctx.filecopy(params, str(endpoint_path), str('file://' + dest))
-            if ret == 0:
-                data_file = open(dest)
-                data = json.load(data_file)
-                data_file.close()
-                totalsize = data['sizes']['total']
-                availablesize = data['sizes']['available']
-                # unusedsize = totalsize - availablesize # tjavurek responsible for correction
-            return totalsize, availablesize
-        except Exception as e:
-            print e
-            raise exception.ServiceUnavailable(e)
+        pass
+
+    def rename(self, pfn, new_pfn):
+        """ Allows to rename a file stored inside the connected RSE.
+
+            :param pfn:      Current physical file name
+            :param new_pfn  New physical file name
+
+            :raises DestinationNotAccessible, ServiceUnavailable, SourceNotFound
+        """
+        pass
```

### Comparing `rucio-clients-1.9.6/lib/rucio/rse/protocols/http_cache.py` & `rucio-clients-32.0.0rc1/lib/rucio/rse/protocols/http_cache.py`

 * *Files 17% similar despite different names*

```diff
@@ -1,34 +1,35 @@
-'''
-  Copyright European Organization for Nuclear Research (CERN)
-
-  Licensed under the Apache License, Version 2.0 (the "License");
-  You may not use this file except in compliance with the License.
-  You may obtain a copy of the License at
-  http://www.apache.org/licenses/LICENSE-2.0
-
-  Authors:
-  - Vincent Garonne, <vincent.garonne@cern.ch>, 2016
-'''
-
-from exceptions import NotImplementedError
+# -*- coding: utf-8 -*-
+# Copyright European Organization for Nuclear Research (CERN) since 2012
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#    http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 
 from rucio.rse.protocols import ngarc
 
 
 class Default(ngarc.Default):
 
     """ Implementing access to RSEs using the ngarc protocol."""
 
-    def __init__(self, protocol_attr, rse_settings):
+    def __init__(self, protocol_attr, rse_settings, logger=None):
         """ Initializes the object with information about the referred RSE.
 
-            :param props Properties derived from the RSE Repository
+            :param props: Properties derived from the RSE Repository
         """
-        super(Default, self).__init__(protocol_attr, rse_settings)
+        super(Default, self).__init__(protocol_attr, rse_settings, logger=logger)
         self.attributes.pop('determinism_type', None)
         self.files = []
 
     def _get_path(self, scope, name):
         """ Transforms the physical file name into the local URI in the referred RSE.
             Suitable for sites implementoing the RUCIO naming convention.
 
@@ -46,36 +47,37 @@
             :param path: The path to the file.
 
             :returns: Fully qualified PFN.
 
         """
         return ''.join([self.attributes['scheme'], '://%s' % self.attributes['hostname'], path])
 
-    def put(self, source, target, source_dir=None):
+    def put(self, source, target, source_dir=None, transfer_timeout=None):
         """ Allows to store files inside the referred RSE.
 
-            :param source Physical file name
-            :param target Name of the file on the storage system e.g. with prefixed scope
+            :param source: Physical file name
+            :param target: Name of the file on the storage system e.g. with prefixed scope
             :param source_dir Path where the to be transferred files are stored in the local file system
+            :param transfer_timeout Transfer timeout (in seconds)
 
             :raises DestinationNotAccessible, ServiceUnavailable, SourceNotFound
         """
         raise NotImplementedError
 
     def delete(self, pfn):
         """ Deletes a file from the connected RSE.
 
-            :param pfn Physical file name
+            :param pfn: Physical file name
 
             :raises ServiceUnavailable, SourceNotFound
         """
         raise NotImplementedError
 
     def rename(self, pfn, new_pfn):
         """ Allows to rename a file stored inside the connected RSE.
 
-            :param pfn      Current physical file name
+            :param pfn:      Current physical file name
             :param new_pfn  New physical file name
 
             :raises DestinationNotAccessible, ServiceUnavailable, SourceNotFound
         """
         raise NotImplementedError
```

### Comparing `rucio-clients-1.9.6/lib/rucio/rse/protocols/mock.py` & `rucio-clients-32.0.0rc1/lib/rucio/rse/protocols/dummy.py`

 * *Files 26% similar despite different names*

```diff
@@ -1,31 +1,34 @@
-# Copyright European Organization for Nuclear Research (CERN)
+# -*- coding: utf-8 -*-
+# Copyright European Organization for Nuclear Research (CERN) since 2012
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
-# You may not use this file except in compliance with the License.
+# you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
-# http://www.apache.org/licenses/LICENSE-2.0
 #
-# Authors:
-# - Vincent Garonne, <vincent.garonne@cern.ch>, 2013
-
+#    http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 
-from rucio.common import exception
 from rucio.rse.protocols import protocol
 
 
 class Default(protocol.RSEProtocol):
     """ Implementing access to RSEs using the local filesystem."""
 
-    def __init__(self, protocol_attr, rse_settings):
+    def __init__(self, protocol_attr, rse_settings, logger=None):
         """ Initializes the object with information about the referred RSE.
 
-            :param props Properties derived from the RSE Repository
+            :param props: Properties derived from the RSE Repository
         """
-        super(Default, self).__init__(protocol_attr, rse_settings)
+        super(Default, self).__init__(protocol_attr, rse_settings, logger=logger)
         self.attributes.pop('determinism_type', None)
         self.files = []
 
     def path2pfn(self, path):
         """
             Retruns a fully qualified PFN for the file referred by path.
 
@@ -35,74 +38,75 @@
 
         """
         return ''.join([self.rse['scheme'], '://%s' % self.rse['hostname'], path])
 
     def exists(self, pfn):
         """ Checks if the requested file is known by the referred RSE.
 
-            :param pfn Physical file name
+            :param pfn: Physical file name
 
             :returns: True if the file exists, False if it doesn't
 
             :raise  ServiceUnavailable
         """
-        return pfn in self.files
+        raise NotImplementedError
 
     def connect(self):
         """ Establishes the actual connection to the referred RSE.
 
-            :param credentials Provide all necessary information to establish a connection
+            :param credentials: Provide all necessary information to establish a connection
                 to the referred storage system. Some is loaded from the repository inside the
                 RSE class and some must be provided specific for the SFTP protocol like
                 username, password, private_key, private_key_pass, port.
                 For details about possible additional parameters and details about their usage
                 see the pysftp.Connection() documentation.
                 NOTE: the host parametrer is overwritten with the value provided by the repository
 
             :raise RSEAccessDenied
         """
-        pass
+        raise NotImplementedError
 
     def close(self):
         """ Closes the connection to RSE."""
-        pass
+        raise NotImplementedError
 
-    def get(self, pfn, dest):
+    def get(self, pfn, dest, transfer_timeout=None):
         """ Provides access to files stored inside connected the RSE.
 
-            :param pfn Physical file name of requested file
-            :param dest Name and path of the files when stored at the client
+            :param pfn: Physical file name of requested file
+            :param dest: Name and path of the files when stored at the client
+            :param transfer_timeout Transfer timeout (in seconds)
 
             :raises DestinationNotAccessible, ServiceUnavailable, SourceNotFound
          """
-        if pfn not in self.files:
-            raise exception.SourceNotFound(pfn)
+        raise NotImplementedError
 
-    def put(self, source, target, source_dir=None):
+    def put(self, source, target, source_dir=None, transfer_timeout=None):
         """ Allows to store files inside the referred RSE.
 
-            :param source Physical file name
-            :param target Name of the file on the storage system e.g. with prefixed scope
+            :param source: Physical file name
+            :param target: Name of the file on the storage system e.g. with prefixed scope
             :param source_dir Path where the to be transferred files are stored in the local file system
+            :param transfer_timeout Transfer timeout (in seconds)
 
             :raises DestinationNotAccessible, ServiceUnavailable, SourceNotFound
         """
-        self.files.append(target)
+        raise NotImplementedError
 
     def delete(self, pfn):
         """ Deletes a file from the connected RSE.
 
-            :param pfn Physical file name
+            :param pfn: Physical file name
 
             :raises ServiceUnavailable, SourceNotFound
         """
-        pass
+        raise NotImplementedError
 
     def rename(self, pfn, new_pfn):
         """ Allows to rename a file stored inside the connected RSE.
 
-            :param pfn      Current physical file name
+            :param pfn:      Current physical file name
             :param new_pfn  New physical file name
 
             :raises DestinationNotAccessible, ServiceUnavailable, SourceNotFound
         """
-        pass
+        raise NotImplementedError
```

### Comparing `rucio-clients-1.9.6/lib/rucio/rse/protocols/ngarc.py` & `rucio-clients-32.0.0rc1/lib/rucio/rse/protocols/ngarc.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,26 +1,30 @@
-# Copyright European Organization for Nuclear Research (CERN)
+# -*- coding: utf-8 -*-
+# Copyright European Organization for Nuclear Research (CERN) since 2012
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
-# You may not use this file except in compliance with the License.
+# you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
-# http://www.apache.org/licenses/LICENSE-2.0
 #
-# Authors:
-# - David Cameron <david.cameron@cern.ch>, 2014
-
-
-from rucio.rse.protocols import protocol
-from rucio.common.exception import FileAlreadyExists, ServiceUnavailable, SourceNotFound
+#    http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 
 import errno
 import os
 
+from rucio.common.exception import FileAlreadyExists, ServiceUnavailable, SourceNotFound
+from rucio.rse.protocols import protocol
+
 try:
-    import arc
+    import arc  # pylint: disable=import-error
 except:
     pass
 
 
 class DataPoint:
     '''
     Wrapper around arc.datapoint_from_url() which does not clean up DataPoints
@@ -35,31 +39,31 @@
     def __del__(self):
         arc.DataPoint.__swig_destroy__(self.h)
 
 
 class Default(protocol.RSEProtocol):
     """ Implementing access to RSEs using ARC client."""
 
-    def __init__(self, protocol_attr, rse_settings):
+    def __init__(self, protocol_attr, rse_settings, logger=None):
         """
         Set up UserConfig object.
         """
-        super(Default, self).__init__(protocol_attr, rse_settings)
+        super(Default, self).__init__(protocol_attr, rse_settings, logger=logger)
 
         # Arc logging to stdout, uncomment for debugging. Should use root
         # logger level eventually.
 #         root_logger = arc.Logger_getRootLogger()
 #         stream = arc.LogStream(sys.stdout)
 #         root_logger.addDestination(stream)
 #         # Set threshold to VERBOSE or DEBUG for more information
 #         root_logger.setThreshold(arc.DEBUG)
 
         self.cfg = arc.UserConfig()
         try:
-            self.cfg.ProxyPath(str(os.getenv['X509_USER_PROXY']))
+            self.cfg.ProxyPath(os.environ['X509_USER_PROXY'])
         except:
             pass
 
     def path2pfn(self, path):
         """
             Retruns a fully qualified PFN for the file referred by path.
 
@@ -69,15 +73,15 @@
 
         """
         return ''.join([self.rse['scheme'], '://%s' % self.rse['hostname'], path])
 
     def exists(self, pfn):
         """ Checks if the requested file is known by the referred RSE.
 
-            :param pfn Physical file name
+            :param pfn: Physical file name
 
             :returns: True if the file exists, False if it doesn't
 
             :raise  ServiceUnavailable
         """
         dp = DataPoint(str(pfn), self.cfg)
         fileinfo = arc.FileInfo()
@@ -97,15 +101,15 @@
         """
         pass
 
     def close(self):
         """ Closes the connection to RSE."""
         pass
 
-    def __arc_copy(self, src, dest, space_token=None):
+    def __arc_copy(self, src, dest, space_token=None, transfer_timeout=None):
 
         # TODO set proxy path
 
         # Convert the arguments to DataPoint objects
         source = DataPoint(str(src), self.cfg)
         if source.h is None:
             raise ServiceUnavailable("Can't handle source %s" % src)
@@ -129,49 +133,51 @@
         if not status:
             if status.GetErrno() == errno.ENOENT:
                 raise SourceNotFound()
             if status.GetErrno() == errno.EEXIST:
                 raise FileAlreadyExists()
             raise ServiceUnavailable(str(status))
 
-    def get(self, pfn, dest):
+    def get(self, pfn, dest, transfer_timeout=None):
         """ Provides access to files stored inside connected the RSE.
 
-            :param pfn Physical file name of requested file
-            :param dest Name and path of the files when stored at the client
+            :param pfn: Physical file name of requested file
+            :param dest: Name and path of the files when stored at the client
+            :param transfer_timeout Transfer timeout (in seconds) - dummy
 
             :raises DestinationNotAccessible, ServiceUnavailable, SourceNotFound
         """
-        self.__arc_copy(pfn, dest)
+        self.__arc_copy(pfn, dest, transfer_timeout=transfer_timeout)
 
-    def put(self, source, target, source_dir=None):
+    def put(self, source, target, source_dir=None, transfer_timeout=None):
         """ Allows to store files inside the referred RSE.
 
-            :param source Physical file name
-            :param target Name of the file on the storage system e.g. with prefixed scope
+            :param source: Physical file name
+            :param target: Name of the file on the storage system e.g. with prefixed scope
             :param source_dir Path where the to be transferred files are stored in the local file system
+            :param transfer_timeout Transfer timeout (in seconds) - dummy
 
             :raises DestinationNotAccessible, ServiceUnavailable, SourceNotFound
         """
 
         if source_dir:
             sf = source_dir + '/' + source
         else:
             sf = source
 
         space_token = None
-        if self.attributes['extended_attributes'] is not None and 'space_token' in self.attributes['extended_attributes'].keys():
+        if self.attributes['extended_attributes'] is not None and 'space_token' in list(self.attributes['extended_attributes'].keys()):
             space_token = self.attributes['extended_attributes']['space_token']
 
-        self.__arc_copy(sf, target, space_token)
+        self.__arc_copy(sf, target, space_token, transfer_timeout=transfer_timeout)
 
     def delete(self, pfn):
         """ Deletes a file from the connected RSE.
 
-            :param pfn Physical file name
+            :param pfn: Physical file name
 
             :raises ServiceUnavailable, SourceNotFound
         """
         dp = DataPoint(str(pfn), self.cfg)
         if dp.h is None:
             raise ServiceUnavailable("Can't handle pfn %s" % pfn)
 
@@ -180,15 +186,15 @@
             if status.GetErrno() == errno.ENOENT:
                 raise SourceNotFound()
             raise ServiceUnavailable(str(status))
 
     def rename(self, pfn, new_pfn):
         """ Allows to rename a file stored inside the connected RSE.
 
-            :param pfn      Current physical file name
+            :param pfn:      Current physical file name
             :param new_pfn  New physical file name
 
             :raises DestinationNotAccessible, ServiceUnavailable, SourceNotFound
         """
         dp = DataPoint(str(pfn), self.cfg)
         if dp.h is None:
             raise ServiceUnavailable("Can't handle pfn %s" % pfn)
```

### Comparing `rucio-clients-1.9.6/lib/rucio/rse/protocols/protocol.py` & `rucio-clients-32.0.0rc1/lib/rucio/rse/protocols/xrootd.py`

 * *Files 25% similar despite different names*

```diff
@@ -1,283 +1,295 @@
-# Copyright European Organization for Nuclear Research (CERN)
+# -*- coding: utf-8 -*-
+# Copyright European Organization for Nuclear Research (CERN) since 2012
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
-# You may not use this file except in compliance with the License.
+# you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
-# http://www.apache.org/licenses/LICENSE-2.0
 #
-# Authors:
-# - Ralph Vigne, <ralph.vigne@cern.ch>, 2012-2014
-# - Cedric Serfon, <cedric.serfon@cern.ch>, 2013-2016
-# - Wen Guan, <wen.guan@cern.ch>, 2014
-# - Cheng-Hsi Chao, <cheng-hsi.chao@cern.ch>, 2014
-
-import hashlib
+#    http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 
-from exceptions import NotImplementedError
-from urlparse import urlparse
+import logging
+import os
 
 from rucio.common import exception
-from rucio.rse import rsemanager
-
-if rsemanager.CLIENT_MODE:
-    from rucio.client.replicaclient import ReplicaClient
+from rucio.common.utils import execute, PREFERRED_CHECKSUM
+from rucio.rse.protocols import protocol
 
-if rsemanager.SERVER_MODE:
-    from rucio.core import replica
 
+class Default(protocol.RSEProtocol):
+    """ Implementing access to RSEs using the XRootD protocol using GSI authentication."""
 
-class RSEProtocol(object):
-    """ This class is virtual and acts as a base to inherit new protocols from. It further provides some common functionality which applies for the amjority of the protocols."""
-
-    def __init__(self, protocol_attr, rse_settings):
+    def __init__(self, protocol_attr, rse_settings, logger=logging.log):
         """ Initializes the object with information about the referred RSE.
 
-            :param props: Properties of the reuested protocol
+            :param props: Properties derived from the RSE Repository
         """
-        self.attributes = protocol_attr
-        self.renaming = True
-        self.rse = rse_settings
-        if not self.rse['deterministic']:
-            if rsemanager.CLIENT_MODE:
-                setattr(self, 'lfns2pfns', self.__lfns2pfns_client)
-            if rsemanager.SERVER_MODE:
-                setattr(self, '_get_path', self._get_path_nondeterministic_server)
-        else:
-            self.attributes['determinism_type'] = 'default'
+        super(Default, self).__init__(protocol_attr, rse_settings, logger=logger)
 
-    def lfns2pfns(self, lfns):
+        self.scheme = self.attributes['scheme']
+        self.hostname = self.attributes['hostname']
+        self.port = str(self.attributes['port'])
+        self.logger = logger
+
+    def path2pfn(self, path):
         """
-            Retruns a fully qualified PFN for the file referred by path.
+            Returns a fully qualified PFN for the file referred by path.
 
             :param path: The path to the file.
 
             :returns: Fully qualified PFN.
-        """
-        pfns = {}
-        prefix = self.attributes['prefix']
 
-        if not prefix.startswith('/'):
-            prefix = ''.join(['/', prefix])
-        if not prefix.endswith('/'):
-            prefix = ''.join([prefix, '/'])
-
-        lfns = [lfns] if type(lfns) == dict else lfns
-        for lfn in lfns:
-            scope, name = lfn['scope'], lfn['name']
-            if 'path' in lfn and lfn['path'] is not None:
-                pfns['%s:%s' % (scope, name)] = ''.join([self.attributes['scheme'],
-                                                         '://',
-                                                         self.attributes['hostname'],
-                                                         ':',
-                                                         str(self.attributes['port']),
-                                                         prefix,
-                                                         lfn['path'] if not lfn['path'].startswith('/') else lfn['path'][1:]
-                                                         ])
+        """
+        self.logger(logging.DEBUG, 'xrootd.path2pfn: path: {}'.format(path))
+        if not path.startswith('xroot') and not path.startswith('root'):
+            if path.startswith('/'):
+                return '%s://%s:%s/%s' % (self.scheme, self.hostname, self.port, path)
             else:
-                pfns['%s:%s' % (scope, name)] = ''.join([self.attributes['scheme'],
-                                                         '://',
-                                                         self.attributes['hostname'],
-                                                         ':',
-                                                         str(self.attributes['port']),
-                                                         prefix,
-                                                         self._get_path(scope=scope, name=name)
-                                                         ])
-        return pfns
+                return '%s://%s:%s//%s' % (self.scheme, self.hostname, self.port, path)
+        else:
+            return path
 
-    def __lfns2pfns_client(self, lfns):
-        """ Provides the path of a replica for non-deterministic sites. Will be assigned to get path by the __init__ method if neccessary.
+    def exists(self, pfn):
+        """ Checks if the requested file is known by the referred RSE.
 
-            :param scope: list of DIDs
+            :param pfn: Physical file name
 
-            :returns: dict with scope:name as keys and PFN as value (in case of errors the Rucio exception si assigned to the key)
-        """
-        client = ReplicaClient()
-        pfns = {}
+            :returns: True if the file exists, False if it doesn't
 
-        lfns = [lfns] if type(lfns) == dict else lfns
-        for lfn in lfns:
-            scope = lfn['scope']
-            name = lfn['name']
-            replicas = [r for r in client.list_replicas([{'scope': scope, 'name': name}, ], schemes=[self.attributes['scheme'], ])]  # schemes is used to narrow down the response message.
-            if len(replicas) > 1:
-                pfns['%s:%s' % (scope, name)] = exception.RSEOperationNotSupported('This operation can only be performed for files.')
-            if not len(replicas):
-                pfns['%s:%s' % (scope, name)] = exception.RSEOperationNotSupported('File not found.')
-            pfns['%s:%s' % (scope, name)] = replicas[0]['rses'][self.rse['rse']][0] if (self.rse['rse'] in replicas[0]['rses'].keys()) else exception.RSEOperationNotSupported('Replica not found on given RSE.')
-        return pfns
+            :raise  ServiceUnavailable
+        """
+        self.logger(logging.DEBUG, 'xrootd.exists: pfn: {}'.format(pfn))
+        try:
+            path = self.pfn2path(pfn)
+            cmd = 'XrdSecPROTOCOL=gsi xrdfs %s:%s stat %s' % (self.hostname, self.port, path)
+            self.logger(logging.DEBUG, 'xrootd.exists: cmd: {}'.format(cmd))
+            status, out, err = execute(cmd)
+            if status != 0:
+                return False
+        except Exception as e:
+            raise exception.ServiceUnavailable(e)
 
-    def _get_path(self, scope, name):
-        """ Transforms the logical file name into a PFN.
-            Suitable for sites implementing the RUCIO naming convention.
-
-            :param lfn: filename
-            :param scope: scope
-
-            :returns: RSE specific URI of the physical file
-        """
-        hstr = hashlib.md5('%s:%s' % (scope, name)).hexdigest()
-        if scope.startswith('user') or scope.startswith('group'):
-            scope = scope.replace('.', '/')
-        return '%s/%s/%s/%s' % (scope, hstr[0:2], hstr[2:4], name)
-
-    def _get_path_nondeterministic_server(self, scope, name):
-        """ Provides the path of a replica for non-deterministic sites. Will be assigned to get path by the __init__ method if neccessary. """
-        r = replica.get_replica(rse=self.rse['rse'], scope=scope, name=name, rse_id=self.rse['id'])
-        if 'path' in r and r['path'] is not None:
-            path = r['path']
-        elif 'state' in r and (r['state'] is None or r['state'] == 'UNAVAILABLE'):
-            raise exception.ReplicaUnAvailable('Missing path information and state is UNAVAILABLE for replica %s:%s on non-deterministic storage named %s' % (scope, name, self.rse['rse']))
-        else:
-            raise exception.ReplicaNotFound('Missing path information for replica %s:%s on non-deterministic storage named %s' % (scope, name, self.rse['rse']))
-        if path.startswith('/'):
-            path = path[1:]
-        if path.endswith('/'):
-            path = path[:-1]
-        return path
+        return True
 
-    def parse_pfns(self, pfns):
+    def stat(self, path):
         """
-            Splits the given PFN into the parts known by the protocol. It is also checked if the provided protocol supportes the given PFNs.
+        Returns the stats of a file.
 
-            :param pfns: a list of a fully qualified PFNs
+        :param path: path to file
 
-            :returns: dic with PFN as key and a dict with path and name as value
+        :raises ServiceUnavailable: if some generic error occured in the library.
 
-            :raises RSEFileNameNotSupported: if the provided PFN doesn't match with the protocol settings
+        :returns: a dict with two keys, filesize and an element of GLOBALLY_SUPPORTED_CHECKSUMS.
         """
-        ret = dict()
-        pfns = [pfns] if ((type(pfns) == str) or (type(pfns) == unicode)) else pfns
-
-        for pfn in pfns:
-            parsed = urlparse(pfn)
-            scheme = parsed.scheme
-            hostname = parsed.netloc.partition(':')[0]
-            port = int(parsed.netloc.partition(':')[2]) if parsed.netloc.partition(':')[2] != '' else 0
-            while '//' in parsed.path:
-                parsed = parsed._replace(path=parsed.path.replace('//', '/'))
-            path = parsed.path
-
-            # Protect against 'lazy' defined prefixes for RSEs in the repository
-            if not self.attributes['prefix'].startswith('/'):
-                self.attributes['prefix'] = '/' + self.attributes['prefix']
-            if not self.attributes['prefix'].endswith('/'):
-                self.attributes['prefix'] += '/'
+        self.logger(logging.DEBUG, 'xrootd.stat: path: {}'.format(path))
+        ret = {}
+        chsum = None
+        if path.startswith('root:'):
+            path = self.pfn2path(path)
+
+        try:
+            # xrdfs stat for getting filesize
+            cmd = 'XrdSecPROTOCOL=gsi xrdfs %s:%s stat %s' % (self.hostname, self.port, path)
+            self.logger(logging.DEBUG, 'xrootd.stat: filesize cmd: {}'.format(cmd))
+            status_stat, out, err = execute(cmd)
+            if status_stat == 0:
+                for line in out.split('\n'):
+                    if line and ':' in line:
+                        k, v = line.split(':', maxsplit=1)
+                        if k.strip().lower() == 'size':
+                            ret['filesize'] = v.strip()
+                            break
+
+            # xrdfs query checksum for getting checksum
+            cmd = 'XrdSecPROTOCOL=gsi xrdfs %s:%s query checksum %s' % (self.hostname, self.port, path)
+            self.logger(logging.DEBUG, 'xrootd.stat: checksum cmd: {}'.format(cmd))
+            status_query, out, err = execute(cmd)
+            if status_query == 0:
+                chsum, value = out.strip('\n').split()
+                ret[chsum] = value
+
+        except Exception as e:
+            raise exception.ServiceUnavailable(e)
+
+        if 'filesize' not in ret:
+            raise exception.ServiceUnavailable('Filesize could not be retrieved.')
+        if PREFERRED_CHECKSUM != chsum or not chsum:
+            msg = '{} does not match with {}'.format(chsum, PREFERRED_CHECKSUM)
+            raise exception.RSEChecksumUnavailable(msg)
 
-            if self.attributes['hostname'] != hostname:
-                if self.attributes['hostname'] != 'localhost':  # In the database empty hostnames are replaced with localhost but for some URIs (e.g. file) a hostname is not included
-                    raise exception.RSEFileNameNotSupported('Invalid hostname: provided \'%s\', expected \'%s\'' % (hostname, self.attributes['hostname']))
+        return ret
 
-            if self.attributes['port'] != port:
-                raise exception.RSEFileNameNotSupported('Invalid port: provided \'%s\', expected \'%s\'' % (port, self.attributes['port']))
+    def pfn2path(self, pfn):
+        """
+        Returns the path of a file given the pfn, i.e. scheme and hostname are subtracted from the pfn.
 
-            if not path.startswith(self.attributes['prefix']):
-                raise exception.RSEFileNameNotSupported('Invalid prefix: provided \'%s\', expected \'%s\'' % ('/'.join(path.split('/')[0:len(self.attributes['prefix'].split('/')) - 1]),
-                                                                                                              self.attributes['prefix']))  # len(...)-1 due to the leading '/
+        :param path: pfn of a file
 
-            # Spliting parsed.path into prefix, path, filename
+        :returns: path.
+        """
+        self.logger(logging.DEBUG, 'xrootd.pfn2path: pfn: {}'.format(pfn))
+        if pfn.startswith('//'):
+            return pfn
+        elif pfn.startswith('/'):
+            return '/' + pfn
+        else:
             prefix = self.attributes['prefix']
-            path = path.partition(self.attributes['prefix'])[2]
-            name = path.split('/')[-1]
-            path = path.partition(name)[0]
-            if not path.startswith('/'):
-                path = '/' + path
-            ret[pfn] = {'path': path, 'name': name, 'scheme': scheme, 'prefix': prefix, 'port': port, 'hostname': hostname, }
+            path = pfn.partition(self.attributes['prefix'])[2]
+            path = prefix + path
+            return path
 
-        return ret
-
-    def exists(self, path):
+    def lfns2pfns(self, lfns):
         """
-            Checks if the requested file is known by the referred RSE.
+        Returns a fully qualified PFN for the file referred by path.
 
-            :param path: Physical file name
+        :param path: The path to the file.
 
-            :returns: True if the file exists, False if it doesn't
-
-            :raises SourceNotFound: if the source file was not found on the referred storage.
+        :returns: Fully qualified PFN.
         """
-        raise NotImplementedError
+        self.logger(logging.DEBUG, 'xrootd.lfns2pfns: lfns: {}'.format(lfns))
+        pfns = {}
+        prefix = self.attributes['prefix']
+
+        if not prefix.startswith('/'):
+            prefix = ''.join(['/', prefix])
+        if not prefix.endswith('/'):
+            prefix = ''.join([prefix, '/'])
+
+        lfns = [lfns] if type(lfns) == dict else lfns
+        for lfn in lfns:
+            scope, name = lfn['scope'], lfn['name']
+            if 'path' in lfn and lfn['path'] is not None:
+                pfns['%s:%s' % (scope, name)] = ''.join([self.attributes['scheme'], '://', self.attributes['hostname'], ':', str(self.attributes['port']), prefix, lfn['path']])
+            else:
+                pfns['%s:%s' % (scope, name)] = ''.join([self.attributes['scheme'], '://', self.attributes['hostname'], ':', str(self.attributes['port']), prefix, self._get_path(scope=scope, name=name)])
+        return pfns
 
     def connect(self):
-        """
-            Establishes the actual connection to the referred RSE.
+        """ Establishes the actual connection to the referred RSE.
 
-            :raises RSEAccessDenied: if no connection could be established.
-        """
-        raise NotImplementedError
+            :param credentials: Provides information to establish a connection
+                to the referred storage system. For S3 connections these are
+                access_key, secretkey, host_base, host_bucket, progress_meter
+                and skip_existing.
+
+            :raises RSEAccessDenied
+        """
+        self.logger(logging.DEBUG, 'xrootd.connect: port: {}, hostname {}'.format(self.port, self.hostname))
+        try:
+            # The query stats call is not implemented on some xroot doors.
+            # Workaround: fail, if server does not reply within 10 seconds for static config query
+            cmd = 'XrdSecPROTOCOL=gsi XRD_REQUESTTIMEOUT=10 xrdfs %s:%s query config %s:%s' % (self.hostname, self.port, self.hostname, self.port)
+            self.logger(logging.DEBUG, 'xrootd.connect: cmd: {}'.format(cmd))
+            status, out, err = execute(cmd)
+            if status != 0:
+                raise exception.RSEAccessDenied(err)
+        except Exception as e:
+            raise exception.RSEAccessDenied(e)
 
     def close(self):
         """ Closes the connection to RSE."""
-        raise NotImplementedError
+        pass
 
-    def get(self, path, dest):
-        """
-            Provides access to files stored inside connected the RSE.
+    def get(self, pfn, dest, transfer_timeout=None):
+        """ Provides access to files stored inside connected the RSE.
 
-            :param path: Physical file name of requested file
+            :param pfn: Physical file name of requested file
             :param dest: Name and path of the files when stored at the client
+            :param transfer_timeout: Transfer timeout (in seconds) - dummy
 
-            :raises DestinationNotAccessible: if the destination storage was not accessible.
-            :raises ServiceUnavailable: if some generic error occured in the library.
-            :raises SourceNotFound: if the source file was not found on the referred storage.
-         """
-        raise NotImplementedError
+            :raises DestinationNotAccessible, ServiceUnavailable, SourceNotFound
+        """
+        self.logger(logging.DEBUG, 'xrootd.get: pfn: {}'.format(pfn))
+        try:
+            cmd = 'XrdSecPROTOCOL=gsi xrdcp -f %s %s' % (pfn, dest)
+            self.logger(logging.DEBUG, 'xrootd.get: cmd: {}'.format(cmd))
+            status, out, err = execute(cmd)
+            if status == 54:
+                raise exception.SourceNotFound()
+            elif status != 0:
+                raise exception.RucioException(err)
+        except Exception as e:
+            raise exception.ServiceUnavailable(e)
 
-    def put(self, source, target, source_dir):
+    def put(self, filename, target, source_dir, transfer_timeout=None):
         """
             Allows to store files inside the referred RSE.
 
             :param source: path to the source file on the client file system
             :param target: path to the destination file on the storage
             :param source_dir: Path where the to be transferred files are stored in the local file system
+            :param transfer_timeout: Transfer timeout (in seconds) - dummy
 
             :raises DestinationNotAccessible: if the destination storage was not accessible.
             :raises ServiceUnavailable: if some generic error occured in the library.
             :raises SourceNotFound: if the source file was not found on the referred storage.
         """
-        raise NotImplementedError
+        self.logger(logging.DEBUG, 'xrootd.put: filename: {} target: {}'.format(filename, target))
+        source_dir = source_dir or '.'
+        source_url = '%s/%s' % (source_dir, filename)
+        self.logger(logging.DEBUG, 'xrootd put: source url: {}'.format(source_url))
+        path = self.path2pfn(target)
+        if not os.path.exists(source_url):
+            raise exception.SourceNotFound()
+        try:
+            cmd = 'XrdSecPROTOCOL=gsi xrdcp -f %s %s' % (source_url, path)
+            self.logger(logging.DEBUG, 'xrootd.put: cmd: {}'.format(cmd))
+            status, out, err = execute(cmd)
+            if status != 0:
+                raise exception.RucioException(err)
+        except Exception as e:
+            raise exception.ServiceUnavailable(e)
 
-    def delete(self, path):
+    def delete(self, pfn):
         """
             Deletes a file from the connected RSE.
 
-            :param path: path to the to be deleted file
+            :param pfn: Physical file name
 
             :raises ServiceUnavailable: if some generic error occured in the library.
             :raises SourceNotFound: if the source file was not found on the referred storage.
         """
-        raise NotImplementedError
+        self.logger(logging.DEBUG, 'xrootd.delete: pfn: {}'.format(pfn))
+        if not self.exists(pfn):
+            raise exception.SourceNotFound()
+        try:
+            path = self.pfn2path(pfn)
+            cmd = 'XrdSecPROTOCOL=gsi xrdfs %s:%s rm %s' % (self.hostname, self.port, path)
+            self.logger(logging.DEBUG, 'xrootd.delete: cmd: {}'.format(cmd))
+            status, out, err = execute(cmd)
+            if status != 0:
+                raise exception.RucioException(err)
+        except Exception as e:
+            raise exception.ServiceUnavailable(e)
 
-    def rename(self, path, new_path):
+    def rename(self, pfn, new_pfn):
         """ Allows to rename a file stored inside the connected RSE.
 
-            :param path: path to the current file on the storage
-            :param new_path: path to the new file on the storage
-
+            :param pfn:      Current physical file name
+            :param new_pfn  New physical file name
             :raises DestinationNotAccessible: if the destination storage was not accessible.
             :raises ServiceUnavailable: if some generic error occured in the library.
             :raises SourceNotFound: if the source file was not found on the referred storage.
         """
-        raise NotImplementedError
-
-    def get_space_usage(self):
-        """
-            Get RSE space usage information.
-
-            :returns: a list with dict containing 'totalsize' and 'unusedsize'
-
-            :raises ServiceUnavailable: if some generic error occured in the library.
-        """
-        raise NotImplementedError
-
-    def stat(self, path):
-        """
-            Returns the stats of a file.
-
-            :param path: path to file
-
-            :raises ServiceUnavailable: if some generic error occured in the library.
-            :raises SourceNotFound: if the source file was not found on the referred storage.
-
-            :returns: a dict with two keys, filesize and adler32 of the file provided in path.
-        """
-        raise NotImplementedError
+        self.logger(logging.DEBUG, 'xrootd.rename: pfn: {}'.format(pfn))
+        if not self.exists(pfn):
+            raise exception.SourceNotFound()
+        try:
+            path = self.pfn2path(pfn)
+            new_path = self.pfn2path(new_pfn)
+            new_dir = new_path[:new_path.rindex('/') + 1]
+            cmd = 'XrdSecPROTOCOL=gsi xrdfs %s:%s mkdir -p %s' % (self.hostname, self.port, new_dir)
+            self.logger(logging.DEBUG, 'xrootd.stat: mkdir cmd: {}'.format(cmd))
+            status, out, err = execute(cmd)
+            cmd = 'XrdSecPROTOCOL=gsi xrdfs %s:%s mv %s %s' % (self.hostname, self.port, path, new_path)
+            self.logger(logging.DEBUG, 'xrootd.stat: rename cmd: {}'.format(cmd))
+            status, out, err = execute(cmd)
+            if status != 0:
+                raise exception.RucioException(err)
+        except Exception as e:
+            raise exception.ServiceUnavailable(e)
```

### Comparing `rucio-clients-1.9.6/lib/rucio/rse/protocols/rfio.py` & `rucio-clients-32.0.0rc1/lib/rucio/rse/protocols/rfio.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,34 +1,43 @@
-# Copyright European Organization for Nuclear Research (CERN)
+# -*- coding: utf-8 -*-
+# Copyright European Organization for Nuclear Research (CERN) since 2012
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
-# You may not use this file except in compliance with the License.
+# you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
-# http://www.apache.org/licenses/LICENSE-2.0
 #
-# Authors:
-# - Vincent Garonne, <vincent.garonne@cern.ch>, 2013
+#    http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+"""
+RFIO protocol
+"""
 
 import os
-
 from os.path import dirname
-from urlparse import urlparse
+from urllib.parse import urlparse
 
-from rucio.common.utils import execute
 from rucio.common import exception
+from rucio.common.utils import execute
 from rucio.rse.protocols import protocol
 
 
 class Default(protocol.RSEProtocol):
+    """ Implementing access to RSEs using the RFIO protocol. """
 
     def connect(self, credentials):
         """
             Establishes the actual connection to the referred RSE.
 
-            :param: credentials needed to establish a connection with the stroage.
+            :param credentials: needed to establish a connection with the stroage.
 
             :raises RSEAccessDenied: if no connection could be established.
         """
         extended_attributes = self.rse['protocol']['extended_attributes']
         if 'STAGE_SVCCLASS' in extended_attributes:
             os.environ['STAGE_SVCCLASS'] = extended_attributes['STAGE_SVCCLASS']
 
@@ -58,34 +67,36 @@
         return status == 0
 
     def close(self):
         """ Closes the connection to RSE."""
         if 'STAGE_SVCCLASS' in os.environ:
             del os.environ['STAGE_SVCCLASS']
 
-    def put(self, source, target, source_dir):
+    def put(self, source, target, source_dir, transfer_timeout=None):
         """
             Allows to store files inside the referred RSE.
 
             :param source: path to the source file on the client file system
             :param target: path to the destination file on the storage
             :param source_dir: Path where the to be transferred files are stored in the local file system
+            :param transfer_timeout: Transfer timeout (in seconds) - dummy
 
             :raises DestinationNotAccessible: if the destination storage was not accessible.
             :raises ServiceUnavailable: if some generic error occured in the library.
             :raises SourceNotFound: if the source file was not found on the referred storage.
         """
         if not self.exists(dirname(target)):
             self.mkdir(dirname(target))
 
         cmd = 'rfcp %(source)s %(path)s' % locals()
         status, out, err = execute(cmd)
         return status == 0
 
     def mkdir(self, directory):
+        """ Create new directory. """
         cmd = 'rfmkdir -p %(path)s' % locals()
         status, out, err = execute(cmd)
         return status == 0
 
     def split_pfn(self, pfn):
         """
             Splits the given PFN into the parts known by the protocol. During parsing the PFN is also checked for
```

### Comparing `rucio-clients-1.9.6/lib/rucio/rse/protocols/s3.py` & `rucio-clients-32.0.0rc1/lib/rucio/rse/protocols/posix.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,237 +1,251 @@
-# Copyright European Organization for Nuclear Research (CERN)
+# -*- coding: utf-8 -*-
+# Copyright European Organization for Nuclear Research (CERN) since 2012
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
-# You may not use this file except in compliance with the License.
+# you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
-# http://www.apache.org/licenses/LICENSE-2.0
 #
-# Authors:
-# - Ralph Vigne, <ralph.vigne@cern.ch>, 2012-2014
-
-
+#    http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+import logging
+import os
+import os.path
+import shutil
 from subprocess import call
-from urlparse import urlparse
-# IMPORTANT: If the order of the S3 imports is changed, they fail!
-from S3.Exceptions import S3Error, InvalidFileError
-from S3.S3 import S3
-from S3.Config import Config
-from S3.S3Uri import S3Uri
 
 from rucio.common import exception
+from rucio.common.utils import adler32
 from rucio.rse.protocols import protocol
 
 
 class Default(protocol.RSEProtocol):
-    """ Implementing access to RSEs using the S3 protocol."""
-
-    def __init__(self, protocol_attr, rse_settings):
-        super(Default, self).__init__(protocol_attr, rse_settings)
-        if 'determinism_type' in self.attributes:
-            self.attributes['determinism_type'] = 's3'
-
-    def _get_path(self, scope, name):
-        """ Transforms the physical file name into the local URI in the referred RSE.
-            Suitable for sites implementoing the RUCIO naming convention.
-
-            :param name: filename
-            :param scope: scope
-
-            :returns: RSE specific URI of the physical file
-        """
-        # On S3 the default naming convention is not supproted
-        # It is therefore changed to bucket being either user, group, ... followed by the
-        # scope as prefix and the lfn as actual file name
-        # IMPORTANT: The prefix defined in the RSE properties are ignored due to system constraints
-        bucket = scope.split('.')[0].upper()
-        scope = scope.split('.')[1]
-        return '%s/%s/%s' % (bucket, scope, name)
-
-    def lfns2pfns(self, lfns):
-        """ Retruns a fully qualified PFN for the file referred by path.
-
-            :param path: The path to the file.
-
-            :returns: Fully qualified PFN.
-        """
-        pfns = {}
-        lfns = [lfns] if type(lfns) == dict else lfns
-        for lfn in lfns:
-            scope, name = lfn['scope'], lfn['name']
-            pfns['%s:%s' % (scope, name)] = ''.join([self.attributes['scheme'], '://', self._get_path(scope=scope, name=name)])
-        return pfns
+    """ Implementing access to RSEs using the local filesystem."""
 
     def exists(self, pfn):
         """
             Checks if the requested file is known by the referred RSE.
 
-            :param path: Physical file name
+            :param pfn: Physical file name
 
             :returns: True if the file exists, False if it doesn't
 
             :raises SourceNotFound: if the source file was not found on the referred storage.
         """
+        status = ''
         try:
-            self.__s3.object_info(S3Uri(pfn))
-            return True
-        except S3Error as e:
-            if e.status == 404:
-                return False
-            else:
-                raise exception.ServiceUnavailable(e)
+            status = os.path.exists(self.pfn2path(pfn))
+        except Exception as e:
+            raise exception.ServiceUnavailable(e)
+        return status
 
     def connect(self):
         """
             Establishes the actual connection to the referred RSE.
 
-            :param: credentials needed to establish a connection with the stroage.
+            :param credentials: needed to establish a connection with the stroage.
 
             :raises RSEAccessDenied: if no connection could be established.
         """
-        try:
-            cfg = Config()
-            for k in self.rse['credentials']:
-                cfg.update_option(k.encode('utf-8'), self.rse['credentials'][k].encode('utf-8'))
-            self.__s3 = S3(cfg)
-        except Exception as e:
-            raise exception.RSEAccessDenied(e)
+        pass
 
     def close(self):
         """ Closes the connection to RSE."""
         pass
 
-    def get(self, pfn, dest):
-        """
-            Provides access to files stored inside connected the RSE.
+    def get(self, pfn, dest, transfer_timeout=None):
+        """ Provides access to files stored inside connected the RSE.
 
-            :param path: Physical file name of requested file
+            :param pfn: Physical file name of requested file
             :param dest: Name and path of the files when stored at the client
+            :param transfer_timeout Transfer timeout (in seconds) - dummy
 
             :raises DestinationNotAccessible: if the destination storage was not accessible.
             :raises ServiceUnavailable: if some generic error occured in the library.
             :raises SourceNotFound: if the source file was not found on the referred storage.
          """
-        tf = None
         try:
-            tf = open(dest, 'wb')
-            self.__s3.object_get(S3Uri(pfn), tf)
-            tf.close()
-        except S3Error as e:
-            tf.close()
-            call(['rm', dest])  # Must be changed if resume will be supported
-            if e.status in [404, 403]:
-                raise exception.SourceNotFound(e)
-            else:
-                raise exception.ServiceUnavailable(e)
+            shutil.copy(self.pfn2path(pfn), dest)
         except IOError as e:
+            try:  # To check if the error happend local or remote
+                with open(dest, 'wb'):
+                    pass
+                call(['rm', '-rf', dest])
+            except IOError as e:
+                if e.errno == 2:
+                    raise exception.DestinationNotAccessible(e)
+                else:
+                    raise exception.ServiceUnavailable(e)
             if e.errno == 2:
-                raise exception.DestinationNotAccessible(e)
+                raise exception.SourceNotFound(e)
             else:
                 raise exception.ServiceUnavailable(e)
 
-    def put(self, source, target, source_dir=None):
+    def put(self, source, target, source_dir=None, transfer_timeout=None):
         """
             Allows to store files inside the referred RSE.
 
             :param source: path to the source file on the client file system
             :param target: path to the destination file on the storage
             :param source_dir: Path where the to be transferred files are stored in the local file system
+            :param transfer_timeout Transfer timeout (in seconds) - dummy
 
             :raises DestinationNotAccessible: if the destination storage was not accessible.
             :raises ServiceUnavailable: if some generic error occured in the library.
             :raises SourceNotFound: if the source file was not found on the referred storage.
         """
-        full_name = source_dir + '/' + source if source_dir else source
-        try:
-            self.__s3.object_put(full_name, S3Uri(target))
-        except S3Error as e:
-            if e.info['Code'] in ['NoSuchBucket', "AccessDenied"]:
-                raise exception.DestinationNotAccessible(e)
+        target = self.pfn2path(target)
+
+        if source_dir:
+            sf = source_dir + '/' + source
+        else:
+            sf = source
+        try:
+            dirs = os.path.dirname(target)
+            if not os.path.exists(dirs):
+                os.makedirs(dirs)
+            shutil.copy(sf, target)
+        except IOError as e:
+            if e.errno == 2:
+                raise exception.SourceNotFound(e)
+            elif not self.exists(self.rse['prefix']):
+                path = ''
+                for p in self.rse['prefix'].split('/'):
+                    path += p + '/'
+                    os.mkdir(path)
+                shutil.copy(sf, self.pfn2path(target))
             else:
-                raise exception.ServiceUnavailable(e)
-        except InvalidFileError as error:
-            raise exception.SourceNotFound(error)
+                raise exception.DestinationNotAccessible(e)
 
     def delete(self, pfn):
-        """
-            Deletes a file from the connected RSE.
+        """ Deletes a file from the connected RSE.
 
-            :param path: path to the to be deleted file
+            :param pfn: pfn to the to be deleted file
 
             :raises ServiceUnavailable: if some generic error occured in the library.
             :raises SourceNotFound: if the source file was not found on the referred storage.
         """
         try:
-            self.__s3.object_delete(S3Uri(pfn))
-        except S3Error as e:
-            if e.status in [404, 403]:
+            os.remove(self.pfn2path(pfn))
+        except OSError as e:
+            if e.errno == 2:
                 raise exception.SourceNotFound(e)
-            else:
-                raise exception.ServiceUnavailable(e)
 
     def rename(self, pfn, new_pfn):
         """ Allows to rename a file stored inside the connected RSE.
 
             :param path: path to the current file on the storage
             :param new_path: path to the new file on the storage
 
             :raises DestinationNotAccessible: if the destination storage was not accessible.
             :raises ServiceUnavailable: if some generic error occured in the library.
             :raises SourceNotFound: if the source file was not found on the referred storage.
         """
+        path = self.pfn2path(pfn)
+        new_path = self.pfn2path(new_pfn)
         try:
-            self.__s3.object_move(S3Uri(pfn), S3Uri(new_pfn))
-        except S3Error as e:
-            if e.status in [404, 403]:
-                if self.exists(pfn):
+            if not os.path.exists(os.path.dirname(new_path)):
+                os.makedirs(os.path.dirname(new_path))
+            os.rename(path, new_path)
+        except IOError as e:
+            if e.errno == 2:
+                if self.exists(self.pfn2path(path)):
                     raise exception.SourceNotFound(e)
                 else:
                     raise exception.DestinationNotAccessible(e)
             else:
                 raise exception.ServiceUnavailable(e)
 
-    def parse_pfns(self, pfns):
-        """
-            Splits the given PFN into the parts known by the protocol. During parsing the PFN is also checked for
-            validity on the given RSE with the given protocol.
+    def lfns2pfns(self, lfns):
+        """ Returns fully qualified PFNs for the file referred by each lfn in
+            the lfns list.
 
-            :param pfn: a fully qualified PFN
+            :param lfns: List of lfns. If lfn['path'] is present it is used as
+                   the path to the file, otherwise the path is constructed
+                   deterministically.
 
-            :returns: a dict containing all known parts of the PFN for the protocol e.g. scheme, path, filename
+            :returns: Fully qualified PFNs.
+        """
+        pfns = {}
+        prefix = self.attributes['prefix']
 
-            :raises RSEFileNameNotSupported: if the provided PFN doesn't match with the protocol settings
-        """
-        # s3 URI: s3://[Bucket]/[path]/[name]; Bucket/path = scope/user
-        ret = dict()
-        pfns = [pfns] if ((type(pfns) == str) or (type(pfns) == unicode)) else pfns
-
-        for pfn in pfns:
-            parsed = urlparse(pfn)
-            scheme = parsed.scheme
-            hostname = None
-            port = 0
-            path = ''.join([parsed.netloc, parsed.path])
-
-            # Spliting parsed.path into prefix, path, filename
-            prefix = self.attributes['prefix']
-            path = path.partition(self.attributes['prefix'])[2]
-            name = path.split('/')[-1]
-            path = path.partition(name)[0]
-            if not path.startswith('/'):
-                path = '/' + path
-            ret[pfn] = {'path': path, 'name': name, 'scheme': scheme, 'prefix': prefix, 'port': port, 'hostname': hostname, }
+        if not prefix.startswith('/'):
+            prefix = ''.join(['/', prefix])
+        if not prefix.endswith('/'):
+            prefix = ''.join([prefix, '/'])
 
-        return ret
+        lfns = [lfns] if isinstance(lfns, dict) else lfns
+        for lfn in lfns:
+            scope, name = str(lfn['scope']), lfn['name']
+            if lfn.get('path'):
+                pfns['%s:%s' % (scope, name)] = ''.join([self.attributes['scheme'],
+                                                         '://',
+                                                         self.attributes['hostname'],
+                                                         prefix,
+                                                         lfn['path'] if not lfn['path'].startswith('/') else lfn['path'][1:]
+                                                         ])
+            else:
+                pfns['%s:%s' % (scope, name)] = ''.join([self.attributes['scheme'],
+                                                         '://',
+                                                         self.attributes['hostname'],
+                                                         prefix,
+                                                         self._get_path(scope=scope, name=name)
+                                                         ])
+        return pfns
 
     def pfn2path(self, pfn):
-        tmp = self.parse_pfn(pfn)
-        return '/'.join([tmp['path'], tmp['name']])
+        tmp = list(self.parse_pfns(pfn).values())[0]
+        return '/'.join([tmp['prefix'], tmp['path'], tmp['name']])
 
     def stat(self, pfn):
-        """ Determines the file size in bytes  of the provided file.
+        """ Determines the file size in bytes and checksum (adler32) of the provided file.
 
             :param pfn: The PFN the file.
 
-            :returns: a dict containing the key filesize.
+            :returns: a dict containing the keys filesize and adler32.
         """
-        info = self.__s3.object_info(S3Uri(pfn))
-        return {'filesize': int(info['headers']['content-length'])}
+        path = self.pfn2path(pfn)
+        return {'filesize': os.stat(path)[os.path.stat.ST_SIZE], 'adler32': adler32(path)}
+
+
+class Symlink(Default):
+    """ Implementing access to RSEs using the local filesystem, creating a symlink on a get """
+
+    def get(self, pfn, dest, transfer_timeout=None):
+        """ Provides access to files stored inside connected the RSE.
+            A download/get will create a symlink on the local file system pointing to the
+            underlying file. Other operations act directly on the remote file.
+            :param pfn: Physical file name of requested file
+            :param dest: Name and path of the files when stored at the client
+            :param transfer_timeout Transfer timeout (in seconds) - dummy
+            :raises DestinationNotAccessible: if the destination storage was not accessible.
+            :raises ServiceUnavailable: if some generic error occured in the library.
+            :raises SourceNotFound: if the source file was not found on the referred storage.
+         """
+        path = self.pfn2path(pfn)
+        os.symlink(path, dest)
+        self.logger(logging.DEBUG,
+                    'Symlink {} created for {} from {}'
+                    .format(dest, path, pfn))
+        if not os.lstat(dest):
+            # problem in creating the symlink
+            self.logger(logging.ERROR, 'Symlink {} could not be created'.format(dest))
+            raise exception.DestinationNotAccessible()
+        if not os.path.exists(dest):
+            # could not find the file following the symlink
+            self.logger(logging.ERROR, 'Symlink {} appears to be a broken link to {}'
+                        .format(dest, path))
+            if os.lstat(dest) and os.path.islink(dest):
+                os.unlink(dest)
+            raise exception.SourceNotFound()
+
+    def pfn2path(self, pfn):
+        # obtain path and sanitise from multiple slashes, etc
+        path = os.path.normpath(super().pfn2path(pfn))
+        self.logger(logging.DEBUG, 'Extracted path: {} from: {}'.format(path, pfn))
+        return path
```

### Comparing `rucio-clients-1.9.6/lib/rucio/rse/protocols/signeds3.py` & `rucio-clients-32.0.0rc1/lib/rucio/rse/protocols/rclone.py`

 * *Files 24% similar despite different names*

```diff
@@ -1,378 +1,365 @@
-# Copyright European Organization for Nuclear Research (CERN)
+# -*- coding: utf-8 -*-
+# Copyright European Organization for Nuclear Research (CERN) since 2012
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
-# You may not use this file except in compliance with the License.
+# you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
-# http://www.apache.org/licenses/LICENSE-2.0
 #
-# Authors:
-# - Wen Guan, <wen.guan@cern.ch>, 2016
+#    http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 
+import json
+import logging
 import os
-import requests
-import urlparse
-
-from progressbar import ProgressBar
-from sys import stdout
 
-from rucio.client.objectstoreclient import ObjectStoreClient
 from rucio.common import exception
+from rucio.common.config import get_config_dirs
+from rucio.common.utils import execute, PREFERRED_CHECKSUM
 from rucio.rse.protocols import protocol
 
 
-class UploadInChunks(object):
-    '''
-    Class to upload by chunks.
-    '''
-
-    def __init__(self, filename, chunksize, progressbar=False):
-        self.__totalsize = os.path.getsize(filename)
-        self.__readsofar = 0
-        self.__filename = filename
-        self.__chunksize = chunksize
-        self.__progressbar = progressbar
-
-    def __iter__(self):
-        try:
-            with open(self.__filename, 'rb') as file_in:
-                while True:
-                    data = file_in.read(self.__chunksize)
-                    if not data:
-                        if self.__progressbar:
-                            stdout.write("\n")
-                        break
-                    self.__readsofar += len(data)
-                    if self.__progressbar:
-                        percent = self.__readsofar * 100 / self.__totalsize
-                        stdout.write("\r{percent:3.0f}%".format(percent=percent))
-                    yield data
-        except OSError as error:
-            raise exception.SourceNotFound(error)
-
-    def __len__(self):
-        return self.__totalsize
-
-
-class IterableToFileAdapter(object):
-    '''
-    Class IterableToFileAdapter
-    '''
-    def __init__(self, iterable):
-        self.iterator = iter(iterable)
-        self.length = len(iterable)
-
-    def read(self, size=-1):   # TBD: add buffer for `len(data) > size` case
-        return next(self.iterator, b'')
-
-    def __len__(self):
-        return self.length
+def load_conf_file(file_name):
+    config_dir = next(filter(lambda d: os.path.exists(os.path.join(d, file_name)), get_config_dirs()))
+    with open(os.path.join(config_dir, file_name)) as f:
+        return json.load(f)
 
 
 class Default(protocol.RSEProtocol):
-    """ Implementing access to RSEs using the S3 protocol."""
+    """ Implementing access to RSEs using the rclone protocol."""
 
-    def __init__(self, protocol_attr, rse_settings):
-        super(Default, self).__init__(protocol_attr, rse_settings)
-        self.session = requests.session()
-        self.timeout = 300
-        self.cert = None
+    def __init__(self, protocol_attr, rse_settings, logger=logging.log):
+        """ Initializes the object with information about the referred RSE.
 
-    def _get_path(self, scope, name):
-        """ Transforms the physical file name into the local URI in the referred RSE.
-            Suitable for sites implementoing the RUCIO naming convention.
+            :param props: Properties derived from the RSE Repository
+        """
+        super(Default, self).__init__(protocol_attr, rse_settings, logger=logger)
+        if len(rse_settings['protocols']) == 1:
+            raise exception.RucioException('rclone initialization requires at least one other protocol defined on the RSE. (from ssh, sftp, posix, webdav)')
+        self.scheme = self.attributes['scheme']
+        setuprclone = False
+        for protocols in reversed(rse_settings['protocols']):
+            if protocol_attr['impl'] == protocols['impl']:
+                continue
+            else:
+                setuprclone = self.setuphostname(protocols)
+                if setuprclone:
+                    break
+
+        if not setuprclone:
+            raise exception.RucioException('rclone could not be initialized.')
+        self.logger = logger
+
+    def setuphostname(self, protocols):
+        """ Initializes the rclone object with information about protocols in the referred RSE.
+
+            :param protocols: Protocols in the RSE
+        """
+        if protocols['scheme'] in ['scp', 'rsync', 'sftp']:
+            self.hostname = 'ssh_rclone_rse'
+            self.host = protocols['hostname']
+            self.port = str(protocols['port'])
+            if protocols['extended_attributes'] is not None and 'user' in list(protocols['extended_attributes'].keys()):
+                self.user = protocols['extended_attributes']['user']
+            else:
+                self.user = None
+            try:
+                data = load_conf_file('rclone-init.cfg')
+                key_file = data[self.host + '_ssh']['key_file']
+            except KeyError:
+                self.logger(logging.ERROR, 'rclone.init: rclone-init.cfg:- Field value missing for "{}_ssh: key_file"'.format(self.host))
+                return False
+            try:
+                if self.user:
+                    cmd = 'rclone config create {0} sftp host {1} user {2} port {3} key_file {4}'.format(self.hostname, self.host, self.user, str(self.port), key_file)
+                    self.logger(logging.DEBUG, 'rclone.init: cmd: {}'.format(cmd))
+                    status, out, err = execute(cmd)
+                    if status:
+                        return False
+                else:
+                    cmd = 'rclone config create {0} sftp host {1} port {2} key_file {3}'.format(self.hostname, self.host, str(self.port), key_file)
+                    self.logger(logging.DEBUG, 'rclone.init: cmd: {}'.format(cmd))
+                    status, out, err = execute(cmd)
+                    if status:
+                        return False
+            except Exception as e:
+                raise exception.ServiceUnavailable(e)
+
+        elif protocols['scheme'] == 'file':
+            self.hostname = '%s_rclone_rse' % (protocols['scheme'])
+            self.host = 'localhost'
+            try:
+                cmd = 'rclone config create {0} local'.format(self.hostname)
+                self.logger(logging.DEBUG, 'rclone.init: cmd: {}'.format(cmd))
+                status, out, err = execute(cmd)
+                if status:
+                    return False
+            except Exception as e:
+                raise exception.ServiceUnavailable(e)
+
+        elif protocols['scheme'] in ['davs', 'https']:
+            self.hostname = '%s_rclone_rse' % (protocols['scheme'])
+            self.host = protocols['hostname']
+            url = '%s://%s:%s%s' % (protocols['scheme'], protocols['hostname'], str(protocols['port']), protocols['prefix'])
+            try:
+                data = load_conf_file('rclone-init.cfg')
+                bearer_token = data[self.host + '_webdav']['bearer_token']
+            except KeyError:
+                self.logger(logging.ERROR, 'rclone.init: rclone-init.cfg:- Field value missing for "{}_webdav: bearer_token"'.format(self.host))
+                return False
+            try:
+                cmd = 'rclone config create {0} webdav url {1} vendor other bearer_token {2}'.format(self.hostname, url, bearer_token)
+                self.logger(logging.DEBUG, 'rclone.init: cmd: {}'.format(cmd))
+                status, out, err = execute(cmd)
+                if status:
+                    return False
+            except Exception as e:
+                raise exception.ServiceUnavailable(e)
 
-            :param name: filename
-            :param scope: scope
+        else:
+            self.logger(logging.DEBUG, 'rclone.init: {} protocol impl not supported by rucio rclone'.format(protocols['impl']))
+            return False
 
-            :returns: RSE specific URI of the physical file
-        """
-        return '%s:%s' % (scope, name)
+        return True
 
     def path2pfn(self, path):
         """
             Returns a fully qualified PFN for the file referred by path.
 
             :param path: The path to the file.
 
             :returns: Fully qualified PFN.
 
         """
-        if path.startswith("s3:") or path.startswith("http"):
+        self.logger(logging.DEBUG, 'rclone.path2pfn: path: {}'.format(path))
+        if not path.startswith('rclone://'):
+            return '%s://%s/%s' % (self.scheme, self.host, path)
+        else:
             return path
 
-        prefix = self.attributes['prefix']
-
-        if not prefix.startswith('/'):
-            prefix = ''.join(['/', prefix])
-        if not prefix.endswith('/'):
-            prefix = ''.join([prefix, '/'])
-
-        return ''.join([self.attributes['scheme'], '://', self.attributes['hostname'], ':', str(self.attributes['port']), prefix, path])
+    def exists(self, pfn):
+        """ Checks if the requested file is known by the referred RSE.
 
-    def lfns2pfns(self, lfns, operation='read'):
-        """
-            Retruns a fully qualified PFN for the file referred by path.
+            :param pfn: Physical file name
 
-            :param path: The path to the file.
+            :returns: True if the file exists, False if it doesn't
 
-            :returns: Fully qualified PFN.
+            :raise  ServiceUnavailable
         """
-        pfns = {}
-        lfns = [lfns] if type(lfns) == dict else lfns
-        for lfn in lfns:
-            scope, name = lfn['scope'], lfn['name']
-            if 'path' in lfn and lfn['path'] and self.rse['deterministic']:
-                path = lfn['path']
-            elif 'prefix' in lfn and lfn['prefix'] is not None:
-                path = os.path.join(lfn['prefix'], scope + '/' + name)
-            else:
-                path = self._get_path(scope=scope, name=name)
+        self.logger(logging.DEBUG, 'rclone.exists: pfn: {}'.format(pfn))
+        try:
+            path = self.pfn2path(pfn)
+            cmd = 'rclone lsf %s:%s' % (self.hostname, path)
+            self.logger(logging.DEBUG, 'rclone.exists: cmd: {}'.format(cmd))
+            status, out, err = execute(cmd)
+            if status:
+                return False
+        except Exception as e:
+            raise exception.ServiceUnavailable(e)
 
-            pfns['%s:%s' % (scope, name)] = self.path2pfn(path)
-        return pfns
+        return True
 
-    def parse_pfns(self, pfns):
+    def stat(self, path):
         """
-            Splits the given PFN into the parts known by the protocol. It is also checked if the provided protocol supportes the given PFNs.
+        Returns the stats of a file.
 
-            :param pfns: a list of a fully qualified PFNs
+        :param path: path to file
 
-            :returns: dic with PFN as key and a dict with path and name as value
+        :raises ServiceUnavailable: if some generic error occured in the library.
 
-            :raises RSEFileNameNotSupported: if the provided PFN doesn't match with the protocol settings
+        :returns: a dict with two keys, filesize and an element of GLOBALLY_SUPPORTED_CHECKSUMS.
         """
-        ret = dict()
-        pfns = [pfns] if ((type(pfns) == str) or (type(pfns) == unicode)) else pfns
+        self.logger(logging.DEBUG, 'rclone.stat: path: {}'.format(path))
+        ret = {}
+        chsum = None
+        if path.startswith('rclone://'):
+            path = self.pfn2path(path)
 
-        for pfn in pfns:
+        try:
+            # rclone stat for getting filesize
+            cmd = 'rclone size {0}:{1}'.format(self.hostname, path)
+            self.logger(logging.DEBUG, 'rclone.stat: filesize cmd: {}'.format(cmd))
+            status_stat, out, err = execute(cmd)
+            if status_stat == 0:
+                fsize = (out.split('\n')[1]).split(' ')[4][1:]
+                ret['filesize'] = fsize
+
+            # rclone query checksum for getting md5 checksum
+            cmd = 'rclone md5sum %s:%s' % (self.hostname, path)
+            self.logger(logging.DEBUG, 'rclone.stat: checksum cmd: {}'.format(cmd))
+            status_query, out, err = execute(cmd)
+
+            if status_query == 0:
+                chsum = 'md5'
+                val = out.strip(' ').split()
+                ret[chsum] = val[0]
 
-            parsed = urlparse.urlparse(pfn)
-            scheme = parsed.scheme
-            hostname = parsed.netloc.partition(':')[0]
-            port = int(parsed.netloc.partition(':')[2]) if parsed.netloc.partition(':')[2] != '' else 0
-            while '//' in parsed.path:
-                parsed = parsed._replace(path=parsed.path.replace('//', '/'))
-            path = parsed.path
+        except Exception as e:
+            raise exception.ServiceUnavailable(e)
 
-            # Protect against 'lazy' defined prefixes for RSEs in the repository
-            if not self.attributes['prefix'].startswith('/'):
-                self.attributes['prefix'] = '/' + self.attributes['prefix']
-            if not self.attributes['prefix'].endswith('/'):
-                self.attributes['prefix'] += '/'
+        if 'filesize' not in ret:
+            raise exception.ServiceUnavailable('Filesize could not be retrieved.')
+        if PREFERRED_CHECKSUM != chsum or not chsum:
+            msg = '{} does not match with {}'.format(chsum, PREFERRED_CHECKSUM)
+            raise exception.RSEChecksumUnavailable(msg)
 
-            if self.attributes['hostname'] != hostname:
-                if self.attributes['hostname'] != 'localhost':  # In the database empty hostnames are replaced with localhost but for some URIs (e.g. file) a hostname is not included
-                    raise exception.RSEFileNameNotSupported('Invalid hostname: provided \'%s\', expected \'%s\'' % (hostname, self.attributes['hostname']))
+        return ret
 
-            if self.attributes['port'] != port:
-                raise exception.RSEFileNameNotSupported('Invalid port: provided \'%s\', expected \'%s\'' % (port, self.attributes['port']))
+    def pfn2path(self, pfn):
+        """
+        Returns the path of a file given the pfn, i.e. scheme, user and hostname are subtracted from the pfn.
 
-            if not path.startswith(self.attributes['prefix']):
-                raise exception.RSEFileNameNotSupported('Invalid prefix: provided \'%s\', expected \'%s\'' % ('/'.join(path.split('/')[0:len(self.attributes['prefix'].split('/')) - 1]),
-                                                                                                              self.attributes['prefix']))  # len(...)-1 due to the leading '/
+        :param path: pfn of a file
 
-            # Spliting parsed.path into prefix, path, filename
+        :returns: path.
+        """
+        path = pfn
+        if pfn.startswith('rclone://'):
+            self.logger(logging.DEBUG, 'rclone.pfn2path: pfn: {}'.format(pfn))
             prefix = self.attributes['prefix']
-            path = path.partition(self.attributes['prefix'])[2]
-            name = path.split('/')[-1]
-            path = path.partition(name)[0]
-            if not path.startswith('/'):
-                path = '/' + path
-            ret[pfn] = {'path': path, 'name': name, 'scheme': scheme, 'prefix': prefix, 'port': port, 'hostname': hostname, }
+            path = pfn.partition(self.attributes['prefix'])[2]
+            path = prefix + path
+        return path
 
-        return ret
+    def lfns2pfns(self, lfns):
+        """
+        Returns a fully qualified PFN for the file referred by path.
 
-    def _connect(self):
-        url = self.path2pfn('')
-        client = ObjectStoreClient()
-        return client.connect(self.rse['rse'], url)
-
-    def _get_signed_urls(self, urls, operation='read'):
-        client = ObjectStoreClient()
-        return client.get_signed_urls(urls, rse=self.rse['rse'], operation=operation)
-
-    def _get_signed_url(self, url, operation='read'):
-        client = ObjectStoreClient()
-        return client.get_signed_url(url, rse=self.rse['rse'], operation=operation)
-
-    def _get_metadata(self, urls):
-        client = ObjectStoreClient()
-        return client.get_metadata(urls, rse=self.rse['rse'])
-
-    def _rename(self, url, new_url):
-        client = ObjectStoreClient()
-        return client.rename(url, new_url, rse=self.rse['rse'])
+        :param path: The path to the file.
 
-    def connect(self):
+        :returns: Fully qualified PFN.
         """
-            Establishes the actual connection to the referred RSE.
+        self.logger(logging.DEBUG, 'rclone.lfns2pfns: lfns: {}'.format(lfns))
+        pfns = {}
+        prefix = self.attributes['prefix']
 
-            :param: credentials needed to establish a connection with the stroage.
+        if not prefix.startswith('/'):
+            prefix = ''.join(['/', prefix])
+        if not prefix.endswith('/'):
+            prefix = ''.join([prefix, '/'])
 
-            :raises RSEAccessDenied: if no connection could be established.
+        lfns = [lfns] if type(lfns) == dict else lfns
+        for lfn in lfns:
+            scope, name = lfn['scope'], lfn['name']
+            if 'path' in lfn and lfn['path'] is not None:
+                pfns['%s:%s' % (scope, name)] = ''.join([self.attributes['scheme'], '://', self.host, ':', prefix, lfn['path']])
+            else:
+                pfns['%s:%s' % (scope, name)] = ''.join([self.attributes['scheme'], '://', self.host, ':', prefix, self._get_path(scope=scope, name=name)])
+        return pfns
+
+    def connect(self):
+        """ Establishes the actual connection to the referred RSE.
+
+            :raises RSEAccessDenied
         """
-        self._connect()
+        self.logger(logging.DEBUG, 'rclone.connect: hostname {}'.format(self.hostname))
+        try:
+            cmd = 'rclone lsd %s:' % (self.hostname)
+            status, out, err = execute(cmd)
+            if status:
+                raise exception.RSEAccessDenied(err)
+        except Exception as e:
+            raise exception.RSEAccessDenied(e)
 
     def close(self):
         """ Closes the connection to RSE."""
         pass
 
-    def get(self, path, dest):
-        """
-            Provides access to files stored inside connected the RSE.
+    def get(self, pfn, dest, transfer_timeout=None):
+        """ Provides access to files stored inside connected the RSE.
 
-            :param path: Physical file name of requested file
+            :param pfn: Physical file name of requested file
             :param dest: Name and path of the files when stored at the client
+            :param transfer_timeout: Transfer timeout (in seconds) - dummy
 
-            :raises DestinationNotAccessible: if the destination storage was not accessible.
-            :raises ServiceUnavailable: if some generic error occured in the library.
-            :raises SourceNotFound: if the source file was not found on the referred storage.
-         """
-        path = self._get_signed_url(path, 'read')
-        print path
-        if isinstance(path, Exception):
-            raise path
-
-        chunksize = 1024
+            :raises DestinationNotAccessible, ServiceUnavailable, SourceNotFound
+        """
+        self.logger(logging.DEBUG, 'rclone.get: pfn: {}'.format(pfn))
         try:
-            result = self.session.get(path, verify=False, stream=True, timeout=self.timeout)
-            if result and result.status_code in [200, ]:
-                length = None
-                if 'content-length' in result.headers:
-                    length = int(result.headers['content-length'])
-                    totnchunk = int(length / chunksize) + 1
-                with open(dest, 'wb') as f:
-                    nchunk = 0
-                    try:
-                        if length:
-                            pbar = ProgressBar(maxval=totnchunk).start()
-                        else:
-                            print 'Malformed HTTP response (missing content-length header). Cannot show progress bar.'
-                        for chunk in result.iter_content(chunksize):
-                            f.write(chunk)
-                            if length:
-                                nchunk += 1
-                                pbar.update(nchunk)
-                    finally:
-                        if length:
-                            pbar.finish()
-
-            elif result.status_code in [404, ]:
-                raise exception.SourceNotFound()
-            elif result.status_code in [401, 403]:
-                raise exception.RSEAccessDenied()
-            else:
-                # catchall exception
-                raise exception.RucioException(result.status_code, result.text)
-        except requests.exceptions.ConnectionError as error:
-            raise exception.ServiceUnavailable(error)
+            path = self.pfn2path(pfn)
+            cmd = 'rclone copyto %s:%s %s' % (self.hostname, path, dest)
+            self.logger(logging.DEBUG, 'rclone.get: cmd: {}'.format(cmd))
+            status, out, err = execute(cmd)
+            if status:
+                raise exception.RucioException(err)
+        except Exception as e:
+            raise exception.ServiceUnavailable(e)
 
-    def put(self, source, target, source_dir=None):
+    def put(self, filename, target, source_dir, transfer_timeout=None):
         """
             Allows to store files inside the referred RSE.
 
             :param source: path to the source file on the client file system
             :param target: path to the destination file on the storage
             :param source_dir: Path where the to be transferred files are stored in the local file system
+            :param transfer_timeout: Transfer timeout (in seconds) - dummy
 
             :raises DestinationNotAccessible: if the destination storage was not accessible.
             :raises ServiceUnavailable: if some generic error occured in the library.
             :raises SourceNotFound: if the source file was not found on the referred storage.
         """
-        full_name = source_dir + '/' + source if source_dir else source
-        path = self._get_signed_url(target, operation='write')
-        full_name = source_dir + '/' + source if source_dir else source
+        self.logger(logging.DEBUG, 'rclone.put: filename: {} target: {}'.format(filename, target))
+        source_dir = source_dir or '.'
+        source_url = '%s/%s' % (source_dir, filename)
+        self.logger(logging.DEBUG, 'rclone.put: source url: {}'.format(source_url))
+
+        path = self.pfn2path(target)
+        if not os.path.exists(source_url):
+            raise exception.SourceNotFound()
         try:
-            if not os.path.exists(full_name):
-                raise exception.SourceNotFound()
-            it = UploadInChunks(full_name, 10000000, progressbar=False)
-            result = self.session.put(path, data=IterableToFileAdapter(it), verify=False, allow_redirects=True, timeout=self.timeout, cert=self.cert)
-            if result.status_code in [200, 201]:
-                return
-            if result.status_code in [409, ]:
-                raise exception.FileReplicaAlreadyExists()
-            else:
-                try:
-                    if not os.path.exists(full_name):
-                        raise exception.SourceNotFound()
-                    it = UploadInChunks(full_name, 10000000, progressbar=False)
-                    result = self.session.put(path, data=IterableToFileAdapter(it), verify=False, allow_redirects=True, timeout=self.timeout, cert=self.cert)
-                    if result.status_code in [200, 201]:
-                        return
-                    if result.status_code in [409, ]:
-                        raise exception.FileReplicaAlreadyExists()
-                    elif result.status_code in [401, ]:
-                        raise exception.RSEAccessDenied()
-                    else:
-                        # catchall exception
-                        raise exception.RucioException(result.status_code, result.text)
-                except requests.exceptions.ConnectionError as error:
-                    raise exception.ServiceUnavailable(error)
-                except IOError as error:
-                    raise exception.SourceNotFound(error)
-        except requests.exceptions.ConnectionError as error:
-            raise exception.ServiceUnavailable(error)
-        except IOError as error:
-            raise exception.SourceNotFound(error)
-
-    def stat(self, pfn):
-        """ Determines the file size in bytes  of the provided file.
-
-            :param pfn: The PFN the file.
-
-            :returns: a dict containing the key filesize.
-        """
-        try:
-            metadata = self._get_metadata([pfn])
-            if pfn in metadata and metadata[pfn]:
-                if isinstance(metadata[pfn], Exception):
-                    raise metadata[pfn]
-                else:
-                    return metadata[pfn]
-            else:
-                raise exception.RucioException('Failed to check file state: %s' % metadata)
-        except exception.SourceNotFound as e:
-            raise exception.SourceNotFound(e)
+            cmd = 'rclone copyto %s %s:%s' % (source_url, self.hostname, path)
+            self.logger(logging.DEBUG, 'rclone.put: cmd: {}'.format(cmd))
+            status, out, err = execute(cmd)
+            if status:
+                raise exception.RucioException(err)
         except Exception as e:
             raise exception.ServiceUnavailable(e)
 
-    def exists(self, pfn):
+    def delete(self, pfn):
         """
-            Checks if the requested file is known by the referred RSE.
+            Deletes a file from the connected RSE.
 
-            :param path: Physical file name
-
-            :returns: True if the file exists, False if it doesn't
+            :param pfn: Physical file name
 
+            :raises ServiceUnavailable: if some generic error occured in the library.
             :raises SourceNotFound: if the source file was not found on the referred storage.
         """
+        self.logger(logging.DEBUG, 'rclone.delete: pfn: {}'.format(pfn))
+        if not self.exists(pfn):
+            raise exception.SourceNotFound()
         try:
-            metadata = self._get_metadata([pfn])
-            if pfn in metadata and metadata[pfn]:
-                if isinstance(metadata[pfn], Exception):
-                    raise metadata[pfn]
-                else:
-                    return True
-            else:
-                raise exception.RucioException('Failed to check file %s state: %s' % (pfn, metadata))
-        except exception.SourceNotFound:
-            return False
+            path = self.pfn2path(pfn)
+            cmd = 'rclone delete %s:%s' % (self.hostname, path)
+            self.logger(logging.DEBUG, 'rclone.delete: cmd: {}'.format(cmd))
+            status, out, err = execute(cmd)
+            if status != 0:
+                raise exception.RucioException(err)
         except Exception as e:
-            raise exception.RucioException(e)
+            raise exception.ServiceUnavailable(e)
 
     def rename(self, pfn, new_pfn):
         """ Allows to rename a file stored inside the connected RSE.
 
-            :param path: path to the current file on the storage
-            :param new_path: path to the new file on the storage
-
+            :param pfn:      Current physical file name
+            :param new_pfn  New physical file name
             :raises DestinationNotAccessible: if the destination storage was not accessible.
             :raises ServiceUnavailable: if some generic error occured in the library.
             :raises SourceNotFound: if the source file was not found on the referred storage.
         """
+        self.logger(logging.DEBUG, 'rclone.rename: pfn: {}'.format(pfn))
+        if not self.exists(pfn):
+            raise exception.SourceNotFound()
         try:
-            self._rename(pfn, new_pfn)
-        except exception.SourceNotFound as e:
-            raise exception.SourceNotFound(e)
+            path = self.pfn2path(pfn)
+            new_path = self.pfn2path(new_pfn)
+            cmd = 'rclone moveto %s:%s %s:%s' % (self.hostname, path, self.hostname, new_path)
+            self.logger(logging.DEBUG, 'rclone.stat: rename cmd: {}'.format(cmd))
+            status, out, err = execute(cmd)
+            if status != 0:
+                raise exception.RucioException(err)
         except Exception as e:
             raise exception.ServiceUnavailable(e)
```

### Comparing `rucio-clients-1.9.6/lib/rucio/rse/protocols/srm.py` & `rucio-clients-32.0.0rc1/lib/rucio/rse/protocols/ssh.py`

 * *Files 26% similar despite different names*

```diff
@@ -1,315 +1,414 @@
-# Copyright European Organization for Nuclear Research (CERN)
+# -*- coding: utf-8 -*-
+# Copyright European Organization for Nuclear Research (CERN) since 2012
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
-# You may not use this file except in compliance with the License.
+# you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
-# http://www.apache.org/licenses/LICENSE-2.0
 #
-# Authors:
-# - Vincent Garonne, <vincent.garonne@cern.ch>, 2012-2013
-# - Cedric Serfon, <cedric.serfon@cern.ch>, 2013-2015
-# - Wen Guan, <wguan@cern.ch>, 2014
-# - Mario Lassnig, <mario.lassnig@cern.ch>, 2014
+#    http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 
-import commands
+import logging
 import os
 import re
-import urlparse
 
 from rucio.common import exception
-from rucio.common.utils import execute
+from rucio.common.utils import execute, PREFERRED_CHECKSUM
 from rucio.rse.protocols import protocol
 
 
 class Default(protocol.RSEProtocol):
-    """ Implementing access to RSEs using the SRM protocol. """
+    """ Implementing access to RSEs using the SSH protocol."""
 
-    def lfns2pfns(self, lfns):
-        """
-        Returns a fully qualified PFN for the file referred by path.
+    def __init__(self, protocol_attr, rse_settings, logger=logging.log):
+        """ Initializes the object with information about the referred RSE.
 
-        :param path: The path to the file.
-        :returns: Fully qualified PFN.
+            :param props: Properties derived from the RSE Repository
         """
+        super(Default, self).__init__(protocol_attr, rse_settings, logger=logger)
 
-        pfns = {}
-        prefix = self.attributes['prefix']
+        self.scheme = self.attributes['scheme']
+        self.hostname = self.attributes['hostname']
+        self.port = str(self.attributes['port'])
+        self.path = None
         if self.attributes['extended_attributes'] is not None and\
-           'web_service_path' in self.attributes['extended_attributes'].keys():
-            web_service_path = self.attributes['extended_attributes']['web_service_path']
+           'user' in list(self.attributes['extended_attributes'].keys()):
+            self.sshuser = self.attributes['extended_attributes']['user'] + '@'
         else:
-            web_service_path = ''
+            self.sshuser = ''
+        self.logger = logger
 
-        if not prefix.startswith('/'):
-            prefix = ''.join(['/', prefix])
-        if not prefix.endswith('/'):
-            prefix = ''.join([prefix, '/'])
+    def path2pfn(self, path):
+        """
+            Returns a fully qualified PFN for the file referred by path.
 
-        hostname = self.attributes['hostname']
-        if '://' in hostname:
-            hostname = hostname.split("://")[1]
+            :param path: The path to the file.
 
-        lfns = [lfns] if type(lfns) == dict else lfns
-        if not self.attributes['port']:
-            for lfn in lfns:
-                scope, name, path = lfn['scope'], lfn['name'], lfn.get('path')
-                if not path:
-                    path = self._get_path(scope=scope, name=name)
-                if path.startswith('/'):
-                    path = path[1:]
-                pfns['%s:%s' % (scope, name)] = ''.join([self.attributes['scheme'], '://',
-                                                         hostname, web_service_path, prefix, path])
+            :returns: Fully qualified PFN.
+
+        """
+        self.logger(logging.DEBUG, 'ssh.path2pfn: path: {}'.format(path))
+        if not path.startswith(str(self.scheme) + '://'):
+            return '%s://%s%s:%s/%s' % (self.scheme, self.sshuser, self.hostname, self.port, path)
         else:
-            for lfn in lfns:
-                scope, name, path = lfn['scope'], lfn['name'], lfn.get('path')
-                if not path:
-                    path = self._get_path(scope=scope, name=name)
-                if path.startswith('/'):
-                    path = path[1:]
-                pfns['%s:%s' % (scope, name)] = ''.join([self.attributes['scheme'], '://',
-                                                         hostname, ':', str(self.attributes['port']),
-                                                         web_service_path, prefix, path])
+            return path
 
-        return pfns
+    def exists(self, pfn):
+        """ Checks if the requested file is known by the referred RSE.
 
-    def parse_pfns(self, pfns):
+            :param pfn: Physical file name
+
+            :returns: True if the file exists, False if it doesn't
+
+            :raise  ServiceUnavailable
         """
-        Splits the given PFN into the parts known by the protocol. During parsing the PFN is also checked for
-        validity on the given RSE with the given protocol.
+        self.logger(logging.DEBUG, 'ssh.exists: pfn: {}'.format(pfn))
+        try:
+            path = self.pfn2path(pfn)
+            cmd = 'ssh -p %s %s%s find %s' % (self.port, self.sshuser, self.hostname, path)
+            self.logger(logging.DEBUG, 'ssh.exists: cmd: {}'.format(cmd))
+            status, out, err = execute(cmd)
+            if status:
+                return False
+        except Exception as e:
+            raise exception.ServiceUnavailable(e)
 
-        :param pfn: a fully qualified PFN
-        :returns: a dict containing all known parts of the PFN for the protocol e.g. scheme, path, filename
-        :raises RSEFileNameNotSupported: if the provided PFN doesn't match with the protocol settings
-        """
-
-        ret = dict()
-        pfns = [pfns] if ((type(pfns) == str) or (type(pfns) == unicode)) else pfns
-        for pfn in pfns:
-            parsed = urlparse.urlparse(pfn)
-            if parsed.path.startswith('/srm/managerv2') or\
-               parsed.path.startswith('/srm/managerv1') or\
-               parsed.path.startswith('/srm/v2/server'):
-                scheme, hostname, port, service_path, path = re.findall(r"([^:]+)://([^:/]+):?(\d+)?([^:]+=)?([^:]+)", pfn)[0]
-            else:
-                scheme = parsed.scheme
-                hostname = parsed.netloc.partition(':')[0]
-                port = parsed.netloc.partition(':')[2]
-                path = parsed.path
-                service_path = ''
-
-            if self.attributes['hostname'] != hostname and\
-               self.attributes['hostname'] != scheme + "://" + hostname:
-                raise exception.RSEFileNameNotSupported('Invalid hostname: provided \'%s\', expected \'%s\'' % (hostname,
-                                                                                                                self.attributes['hostname']))
-
-            if port != '' and str(self.attributes['port']) != str(port):
-                raise exception.RSEFileNameNotSupported('Invalid port: provided \'%s\', expected \'%s\'' % (port,
-                                                                                                            self.attributes['port']))
-            elif port == '':
-                port = self.attributes['port']
-
-            if not path.startswith(self.attributes['prefix']):
-                raise exception.RSEFileNameNotSupported('Invalid prefix: provided \'%s\', expected \'%s\'' % ('/'.join(path.split('/')[0:len(self.attributes['prefix'].split('/')) - 1]),
-                                                                                                              self.attributes['prefix']))  # len(...)-1 due to the leading '/
+        return True
 
-            # Spliting path into prefix, path, filename
-            prefix = self.attributes['prefix']
-            path = path.partition(self.attributes['prefix'])[2]
-            name = path.split('/')[-1]
-            path = '/' + path.partition(name)[0] if not self.rse['staging_area'] else None
-            ret[pfn] = {'scheme': scheme, 'port': port, 'hostname': hostname,
-                        'path': path, 'name': name, 'prefix': prefix,
-                        'web_service_path': service_path}
+    def stat(self, path):
+        """
+        Returns the stats of a file.
+
+        :param path: path to file
+
+        :raises ServiceUnavailable: if some generic error occured in the library.
+
+        :returns: a dict with two keys, filesize and an element of GLOBALLY_SUPPORTED_CHECKSUMS.
+        """
+        self.logger(logging.DEBUG, 'ssh.stat: path: {}'.format(path))
+        ret = {}
+        chsum = None
+        path = self.pfn2path(path)
+
+        try:
+            # ssh stat for getting filesize
+            cmd = 'ssh -p {0} {1}{2} stat --printf="%s" {3}'.format(self.port, self.sshuser, self.hostname, path)
+            self.logger(logging.DEBUG, 'ssh.stat: filesize cmd: {}'.format(cmd))
+            status_stat, out, err = execute(cmd)
+            if status_stat == 0:
+                ret['filesize'] = out
+
+            # ssh query checksum for getting md5 checksum
+            cmd = 'ssh -p %s %s%s md5sum %s' % (self.port, self.sshuser, self.hostname, path)
+            self.logger(logging.DEBUG, 'ssh.stat: checksum cmd: {}'.format(cmd))
+            status_query, out, err = execute(cmd)
+
+            if status_query == 0:
+                chsum = 'md5'
+                val = out.strip(' ').split()
+                ret[chsum] = val[0]
+
+        except Exception as e:
+            raise exception.ServiceUnavailable(e)
+
+        if 'filesize' not in ret:
+            raise exception.ServiceUnavailable('Filesize could not be retrieved.')
+        if PREFERRED_CHECKSUM != chsum or not chsum:
+            msg = '{} does not match with {}'.format(chsum, PREFERRED_CHECKSUM)
+            raise exception.RSEChecksumUnavailable(msg)
 
         return ret
 
-    def path2pfn(self, path):
+    def pfn2path(self, pfn):
+        """
+        Returns the path of a file given the pfn, i.e. scheme, user and hostname are subtracted from the pfn.
+
+        :param path: pfn of a file
+
+        :returns: path.
+        """
+        path = pfn
+        if pfn.startswith(str(self.scheme) + '://'):
+            self.logger(logging.DEBUG, 'ssh.pfn2path: pfn: {}'.format(pfn))
+            prefix = self.attributes['prefix']
+            path = pfn.partition(self.attributes['prefix'])[2]
+            path = prefix + path
+        return path
+
+    def lfns2pfns(self, lfns):
         """
         Returns a fully qualified PFN for the file referred by path.
 
         :param path: The path to the file.
+
         :returns: Fully qualified PFN.
         """
+        self.logger(logging.DEBUG, 'ssh.lfns2pfns: lfns: {}'.format(lfns))
+        pfns = {}
+        prefix = self.attributes['prefix']
 
-        if path.startswith("srm://"):
-            return path
-
-        hostname = self.attributes['hostname']
-        if '://' in hostname:
-            hostname = hostname.split("://")[1]
-
-        if 'extended_attributes' in self.attributes.keys() and\
-           self.attributes['extended_attributes'] is not None and\
-           'web_service_path' in self.attributes['extended_attributes'].keys():
-            web_service_path = self.attributes['extended_attributes']['web_service_path']
-        else:
-            web_service_path = ''
+        if not prefix.startswith('/'):
+            prefix = ''.join(['/', prefix])
+        if not prefix.endswith('/'):
+            prefix = ''.join([prefix, '/'])
 
-        if not path.startswith('srm'):
-            if self.attributes['port'] > 0:
-                return ''.join([self.attributes['scheme'], '://', hostname, ':', str(self.attributes['port']), web_service_path, path])
+        lfns = [lfns] if type(lfns) == dict else lfns
+        for lfn in lfns:
+            scope, name = lfn['scope'], lfn['name']
+            if 'path' in lfn and lfn['path'] is not None:
+                pfns['%s:%s' % (scope, name)] = ''.join([self.attributes['scheme'], '://', self.sshuser, self.hostname, ':', self.port, prefix, lfn['path']])
             else:
-                return ''.join([self.attributes['scheme'], '://', hostname, web_service_path, path])
-        else:
-            return path
+                pfns['%s:%s' % (scope, name)] = ''.join([self.attributes['scheme'], '://', self.sshuser, self.hostname, ':', self.port, prefix, self._get_path(scope=scope, name=name)])
+        return pfns
 
     def connect(self):
+        """ Establishes the actual connection to the referred RSE.
+
+            :raises RSEAccessDenied
         """
-        Establishes the actual connection to the referred RSE.
-        As a quick and dirty impelementation we just use this method to check if the lcg tools are available.
-        If we decide to use gfal, init should be done here.
-
-        :raises RSEAccessDenied: Cannot connect.
-        """
+        self.logger(logging.DEBUG, 'ssh.connect: port: {}, hostname {}, ssh-user {}'.format(self.port, self.hostname, self.sshuser))
+        try:
+            cmd = 'ssh -p %s %s%s echo ok 2>&1' % (self.port, self.sshuser, self.hostname)
+            status, out, err = execute(cmd)
+            checker = re.search(r'ok', out)
+            if not checker:
+                raise exception.RSEAccessDenied(err)
+        except Exception as e:
+            raise exception.RSEAccessDenied(e)
 
-        status, lcglscommand = commands.getstatusoutput('which lcg-ls')
-        if status:
-            raise exception.RSEAccessDenied('Cannot find lcg tools')
-        endpoint_basepath = self.path2pfn(self.attributes['prefix'])
-        status, result = commands.getstatusoutput('%s -vv $LCGVO -b --srm-timeout 60 -D srmv2 -l %s' % (lcglscommand, endpoint_basepath))
-        if status:
-            if result == '':
-                raise exception.RSEAccessDenied('Endpoint not reachable. lcg-ls failed with status code %s but no further details.' % (str(status)))
-            else:
-                raise exception.RSEAccessDenied('Endpoint not reachable : %s' % str(result))
+    def close(self):
+        """ Closes the connection to RSE."""
+        pass
 
-    def get(self, path, dest):
-        """
-        Provides access to files stored inside connected the RSE.
+    def get(self, pfn, dest, transfer_timeout=None):
+        """ Provides access to files stored inside connected the RSE.
 
-        :param path: Physical file name of requested file
-        :param dest: Name and path of the files when stored at the client
-        :raises DestinationNotAccessible: if the destination storage was not accessible.
-        :raises ServiceUnavailable: if some generic error occured in the library.
-        :raises SourceNotFound: if the source file was not found on the referred storage.
-        """
+            :param pfn: Physical file name of requested file
+            :param dest: Name and path of the files when stored at the client
+            :param transfer_timeout: Transfer timeout (in seconds) - dummy
 
+            :raises DestinationNotAccessible, ServiceUnavailable, SourceNotFound
+        """
+        self.logger(logging.DEBUG, 'ssh.get: pfn: {}'.format(pfn))
         try:
-            cmd = 'lcg-cp $LCGVO -v -b --srm-timeout 3600 -D srmv2 %s file:%s' % (path, dest)
+            path = self.pfn2path(pfn)
+            destdir = os.path.dirname(dest)
+            cmd = 'mkdir -p %s' % (destdir)
+            self.logger(logging.DEBUG, 'ssh.get: cmd: {}'.format(cmd))
+            status, out, err = execute(cmd)
+            cmd = 'scp %s%s:%s %s' % (self.sshuser, self.hostname, path, dest)
+            self.logger(logging.DEBUG, 'ssh.get: cmd: {}'.format(cmd))
             status, out, err = execute(cmd)
             if status:
-                if self.__parse_srm_error__("SRM_INVALID_PATH", out, err):
-                    raise exception.SourceNotFound(err)
                 raise exception.RucioException(err)
-        except exception.SourceNotFound as error:
-            raise exception.SourceNotFound(str(error))
-        except Exception as error:
-            raise exception.ServiceUnavailable(error)
-
-    def put(self, source, target, source_dir):
-        """
-        Allows to store files inside the referred RSE.
-
-        :param source: path to the source file on the client file system
-        :param target: path to the destination file on the storage
-        :param source_dir: Path where the to be transferred files are stored in the local file system
-        :raises DestinationNotAccessible: if the destination storage was not accessible.
-        :raises ServiceUnavailable: if some generic error occured in the library.
-        :raises SourceNotFound: if the source file was not found on the referred storage.
+        except Exception as e:
+            raise exception.ServiceUnavailable(e)
+
+    def put(self, filename, target, source_dir, transfer_timeout=None):
         """
+            Allows to store files inside the referred RSE.
 
-        source_url = '%s/%s' % (source_dir, source) if source_dir else source
+            :param source: path to the source file on the client file system
+            :param target: path to the destination file on the storage
+            :param source_dir: Path where the to be transferred files are stored in the local file system
+            :param transfer_timeout: Transfer timeout (in seconds) - dummy
 
+            :raises DestinationNotAccessible: if the destination storage was not accessible.
+            :raises ServiceUnavailable: if some generic error occured in the library.
+            :raises SourceNotFound: if the source file was not found on the referred storage.
+        """
+        self.logger(logging.DEBUG, 'ssh.put: filename: {} target: {}'.format(filename, target))
+        source_dir = source_dir or '.'
+        source_url = '%s/%s' % (source_dir, filename)
+        self.logger(logging.DEBUG, 'ssh.put: source url: {}'.format(source_url))
+
+        path = self.pfn2path(target)
+        pathdir = os.path.dirname(path)
         if not os.path.exists(source_url):
             raise exception.SourceNotFound()
-
-        space_token = ''
-        if self.attributes['extended_attributes'] is not None and 'space_token' in self.attributes['extended_attributes'].keys():
-            space_token = '--dst %s' % self.attributes['extended_attributes']['space_token']
-
         try:
-            cmd = 'lcg-cp $LCGVO -v -b --srm-timeout 3600 -D srmv2 %s file:%s %s' % (space_token, source_url, target)
+            cmd = 'ssh %s%s "mkdir -p %s" && scp %s %s%s:%s' % (self.sshuser, self.hostname, pathdir, source_url, self.sshuser, self.hostname, path)
+            self.logger(logging.DEBUG, 'ssh.put: cmd: {}'.format(cmd))
             status, out, err = execute(cmd)
             if status:
                 raise exception.RucioException(err)
-        except Exception as error:
-            raise exception.ServiceUnavailable(error)
+        except Exception as e:
+            raise exception.ServiceUnavailable(e)
 
-    def delete(self, path):
+    def delete(self, pfn):
         """
-        Deletes a file from the connected RSE.
+            Deletes a file from the connected RSE.
 
-        :param path: path to the to be deleted file
-        :raises ServiceUnavailable: if some generic error occured in the library.
-        :raises SourceNotFound: if the source file was not found on the referred storage.
+            :param pfn: Physical file name
+
+            :raises ServiceUnavailable: if some generic error occured in the library.
+            :raises SourceNotFound: if the source file was not found on the referred storage.
         """
+        self.logger(logging.DEBUG, 'ssh.delete: pfn: {}'.format(pfn))
+        if not self.exists(pfn):
+            raise exception.SourceNotFound()
+        try:
+            path = self.pfn2path(pfn)
+            cmd = 'ssh -p %s %s%s rm %s' % (self.port, self.sshuser, self.hostname, path)
+            self.logger(logging.DEBUG, 'ssh.delete: cmd: {}'.format(cmd))
+            status, out, err = execute(cmd)
+            if status != 0:
+                raise exception.RucioException(err)
+        except Exception as e:
+            raise exception.ServiceUnavailable(e)
 
-        pfns = [path] if ((type(path) == str) or (type(path) == unicode)) else path
+    def rename(self, pfn, new_pfn):
+        """ Allows to rename a file stored inside the connected RSE.
 
+            :param pfn:      Current physical file name
+            :param new_pfn  New physical file name
+            :raises DestinationNotAccessible: if the destination storage was not accessible.
+            :raises ServiceUnavailable: if some generic error occured in the library.
+            :raises SourceNotFound: if the source file was not found on the referred storage.
+        """
+        self.logger(logging.DEBUG, 'ssh.rename: pfn: {}'.format(pfn))
+        if not self.exists(pfn):
+            raise exception.SourceNotFound()
         try:
-            pfn_chunks = [pfns[i:i + 20] for i in range(0, len(pfns), 20)]
-            for pfn_chunk in pfn_chunks:
-                cmd = 'lcg-del $LCGVO -v -b -l --srm-timeout 600 -D srmv2'
-                for pfn in pfn_chunk:
-                    cmd += ' ' + pfn
-                status, out, err = execute(cmd)
-                if status:
-                    if self.__parse_srm_error__("SRM_INVALID_PATH", out, err):
-                        raise exception.SourceNotFound(err)
-                    raise exception.RucioException(err)
-        except exception.SourceNotFound as error:
-            raise exception.SourceNotFound(str(error))
-        except Exception as error:
-            raise exception.ServiceUnavailable(error)
-
-    def rename(self, path, new_path):
-        """
-        Allows to rename a file stored inside the connected RSE.
-
-        :param path: path to the current file on the storage
-        :param new_path: path to the new file on the storage
-        :raises DestinationNotAccessible: if the destination storage was not accessible.
+            path = self.pfn2path(pfn)
+            new_path = self.pfn2path(new_pfn)
+            new_dir = new_path[:new_path.rindex('/') + 1]
+            cmd = 'ssh -p %s %s%s "mkdir -p %s"' % (self.port, self.sshuser, self.hostname, new_dir)
+            self.logger(logging.DEBUG, 'ssh.rename: mkdir cmd: {}'.format(cmd))
+            status, out, err = execute(cmd)
+            cmd = 'ssh -p %s %s%s mv %s %s' % (self.port, self.sshuser, self.hostname, path, new_path)
+            self.logger(logging.DEBUG, 'ssh.rename: rename cmd: {}'.format(cmd))
+            status, out, err = execute(cmd)
+            if status != 0:
+                raise exception.RucioException(err)
+        except Exception as e:
+            raise exception.ServiceUnavailable(e)
+
+
+class Rsync(Default):
+    """ Implementing access to RSEs using the ssh.Rsync implementation."""
+
+    def stat(self, path):
+        """
+        Returns the stats of a file.
+
+        :param path: path to file
+
         :raises ServiceUnavailable: if some generic error occured in the library.
-        :raises SourceNotFound: if the source file was not found on the referred storage.
+
+        :returns: a dict with two keys, filesize and an element of GLOBALLY_SUPPORTED_CHECKSUMS.
         """
+        self.logger(logging.DEBUG, 'rsync.stat: path: {}'.format(path))
+        ret = {}
+        chsum = None
+        path = self.pfn2path(path)
 
-        space_token = ''
-        if self.attributes['extended_attributes'] is not None and 'space_token' in self.attributes['extended_attributes'].keys():
-            space_token = '--dst %s' % self.attributes['extended_attributes']['space_token']
+        try:
+            # rsync stat for getting filesize
+            cmd = "rsync -an --size-only -e 'ssh -p {0}' --remove-source-files  {1}{2}:{3}".format(self.port, self.sshuser, self.hostname, path)
+            self.logger(logging.DEBUG, 'rsync.stat: filesize cmd: {}'.format(cmd))
+            status_stat, out, err = execute(cmd)
+            if status_stat == 0:
+                sizestr = out.split(" ")[-4]
+                ret['filesize'] = sizestr.replace(',', '')
+
+            # rsync query checksum for getting md5 checksum
+            cmd = 'ssh -p %s %s%s md5sum %s' % (self.port, self.sshuser, self.hostname, path)
+            self.logger(logging.DEBUG, 'rsync.stat: checksum cmd: {}'.format(cmd))
+            status_query, out, err = execute(cmd)
+
+            if status_query == 0:
+                chsum = 'md5'
+                val = out.strip(' ').split()
+                ret[chsum] = val[0]
+
+        except Exception as e:
+            raise exception.ServiceUnavailable(e)
+
+        if 'filesize' not in ret:
+            raise exception.ServiceUnavailable('Filesize could not be retrieved.')
+        if PREFERRED_CHECKSUM != chsum or not chsum:
+            msg = '{} does not match with {}'.format(chsum, PREFERRED_CHECKSUM)
+            raise exception.RSEChecksumUnavailable(msg)
+
+        return ret
 
+    def connect(self):
+        """ Establishes the actual connection to the referred RSE.
+
+            :raises RSEAccessDenied
+        """
+        self.logger(logging.DEBUG, 'rsync.connect: port: {}, hostname {}, ssh-user {}'.format(self.port, self.hostname, self.sshuser))
         try:
-            cmd = 'lcg-cp $LCGVO -v -b --srm-timeout 3600 -D srmv2 %s %s %s' % (space_token, path, new_path)
+            cmd = 'ssh -p %s %s%s echo ok 2>&1' % (self.port, self.sshuser, self.hostname)
             status, out, err = execute(cmd)
-            if status:
-                raise exception.RucioException(err)
+            checker = re.search(r'ok', out)
+            if not checker:
+                raise exception.RSEAccessDenied(err)
+            cmd = 'ssh -p %s %s%s type rsync' % (self.port, self.sshuser, self.hostname)
+            status, out, err = execute(cmd)
+            checker = re.search(r'rsync is', out)
+            if not checker:
+                raise exception.RSEAccessDenied(err)
+            self.path = out.split(" ")[2][:-1]
+
+        except Exception as e:
+            raise exception.RSEAccessDenied(e)
+
+    def get(self, pfn, dest, transfer_timeout=None):
+        """ Provides access to files stored inside connected the RSE.
 
-            cmd = 'lcg-del $LCGVO -v -b -l --srm-timeout 600 -D srmv2 %s' % (path)
+            :param pfn: Physical file name of requested file
+            :param dest: Name and path of the files when stored at the client
+            :param transfer_timeout: Transfer timeout (in seconds) - dummy
+
+            :raises DestinationNotAccessible, ServiceUnavailable, SourceNotFound
+        """
+        self.logger(logging.DEBUG, 'rsync.get: pfn: {}'.format(pfn))
+        try:
+            path = self.pfn2path(pfn)
+            destdir = os.path.dirname(dest)
+            cmd = 'mkdir -p %s && rsync -az -e "ssh -p %s" --append-verify %s%s:%s %s' % (destdir, self.port, self.sshuser, self.hostname, path, dest)
+            self.logger(logging.DEBUG, 'rsync.get: cmd: {}'.format(cmd))
             status, out, err = execute(cmd)
             if status:
                 raise exception.RucioException(err)
-        except Exception as error:
-            raise exception.ServiceUnavailable(error)
+        except Exception as e:
+            raise exception.ServiceUnavailable(e)
 
-    def exists(self, path):
+    def put(self, filename, target, source_dir, transfer_timeout=None):
         """
-        Checks if the requested file is known by the referred RSE.
+            Allows to store files inside the referred RSE.
+
+            :param source: path to the source file on the client file system
+            :param target: path to the destination file on the storage
+            :param source_dir: Path where the to be transferred files are stored in the local file system
+            :param transfer_timeout: Transfer timeout (in seconds) - dummy
 
-        :param path: Physical file name
-        :returns: True if the file exists, False if it doesn't
-        :raises SourceNotFound: if the source file was not found on the referred storage.
+            :raises DestinationNotAccessible: if the destination storage was not accessible.
+            :raises ServiceUnavailable: if some generic error occured in the library.
+            :raises SourceNotFound: if the source file was not found on the referred storage.
         """
+        self.logger(logging.DEBUG, 'rsync.put: filename: {} target: {}'.format(filename, target))
+        source_dir = source_dir or '.'
+        source_url = '%s/%s' % (source_dir, filename)
+        self.logger(logging.DEBUG, 'rsync.put: source url: {}'.format(source_url))
+
+        path = self.pfn2path(target)
+        pathdir = os.path.dirname(path)
+        if not os.path.exists(source_url):
+            raise exception.SourceNotFound()
 
         try:
-            cmd = 'lcg-ls $LCGVO -v -b --srm-timeout 60 -D srmv2  %s' % (path)
+            cmd = 'ssh -p %s %s%s "mkdir -p %s" && rsync -az -e "ssh -p %s" --append-verify %s %s%s:%s' % (self.port, self.sshuser, self.hostname, pathdir, self.port, source_url, self.sshuser, self.hostname, path)
+            self.logger(logging.DEBUG, 'rsync.put: cmd: {}'.format(cmd))
             status, out, err = execute(cmd)
             if status:
-                return False
-            return True
-        except Exception as error:
-            raise exception.ServiceUnavailable(error)
-
-    def __parse_srm_error__(self, err_code, out, err):
-        """Parse the error message to return error code."""
-        if out is not None and len(out) > 0:
-            if out.count(err_code) > 0:
-                return True
-        if err is not None and len(err) > 0:
-            if err.count(err_code) > 0:
-                return True
-        return False
-
-    def close(self):
-        """
-        Closes the connection to RSE.
-        """
-        pass
+                raise exception.RucioException(err)
+        except Exception as e:
+            raise exception.ServiceUnavailable(e)
```

### Comparing `rucio-clients-1.9.6/lib/rucio/rse/rsemanager.py` & `rucio-clients-32.0.0rc1/lib/rucio/rse/rsemanager.py`

 * *Files 26% similar despite different names*

```diff
@@ -1,405 +1,480 @@
-# Copyright European Organization for Nuclear Research (CERN)
+# -*- coding: utf-8 -*-
+# Copyright European Organization for Nuclear Research (CERN) since 2012
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
-# You may not use this file except in compliance with the License.
+# you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
-# http://www.apache.org/licenses/LICENSE-2.0
 #
-# Authors:
-# - Ralph Vigne, <ralph.vigne@cern.ch>, 2013-2015
-# - Mario Lassnig, <mario.lassnig@cern.ch>, 2012-2014, 2017
-# - Vincent Garonne, <vincent.garonne@cern.ch>, 2013-2014
-# - Cedric Serfon, <cedric.serfon@cern.ch>, 2013-2014
-# - Wen Guan, <wen.guan@cern.ch>, 2014-2015
+#    http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 
 import copy
-import os
-
-from urlparse import urlparse
-
-from rucio.common import exception, utils
-
-DEFAULT_PROTOCOL = 1
-
-
-def get_rse_info(rse, session=None):
-    """ Returns all protocol related RSE attributes.
-
-        :param rse: Name of the reqeusted RSE
+import logging
+import random
+from time import sleep
+from urllib.parse import urlparse
+
+from rucio.common import exception, utils, constants
+from rucio.common.config import config_get_int
+from rucio.common.constraints import STRING_TYPES
+from rucio.common.logging import formatted_logger
+from rucio.common.utils import make_valid_did, GLOBALLY_SUPPORTED_CHECKSUMS
+
+
+def get_rse_info(rse=None, vo='def', rse_id=None, session=None):
+    """
+        Returns all protocol related RSE attributes.
+        Call with either rse and vo, or (in server mode) rse_id
+
+        :param rse: Name of the requested RSE
+        :param vo: The VO for the RSE.
+        :param rse_id: The id of the rse (use in server mode to avoid db calls)
         :param session: The eventual database session.
 
-
         :returns: a dict object with the following attributes:
                     id                ...     an internal identifier
                     rse               ...     the name of the RSE as string
                     type              ...     the storage type odf the RSE e.g. DISK
                     volatile          ...     boolean indictaing if the RSE is volatile
+                    verify_checksum   ...     boolean indicating whether RSE supports requests for checksums
                     deteministic      ...     boolean indicating of the nameing of the files follows the defined determinism
-                    domain            ...     indictaing the domain that should bes assumed for transfers. Values are 'ALL', 'LAN', or 'WAN'
-                    delete_protocol   ...     the protocol to be used for deletion, if rsemanager.DEFAULT_PROTOCOL, the default of the site will be selected automatically
-                    write_protocol    ...     the protocol to be used for deletion, if rsemanager.DEFAULT_PROTOCOL, the default of the site will be selected automatically
-                    read_protocol     ...     the protocol to be used for deletion, if rsemanager.DEFAULT_PROTOCOL, the default of the site will be selected automatically
-                    third_party_copy  ...     the protocol to be used for third_party_copy, if rsemanager.DEFAULT_PROTOCOL, the default of the site will be selected automatically
-
-                    protocols       ...     all supported protocol in form of alist of dict objects with the followig structure
-                        scheme              ...     protocol scheme e.g. http, srm, ...
-                        hostname            ...     hostname of the site
-                        prefix              ...     path to the folder where the files are stored
-                        port                ...     port used for this protocol
-                        impl                ...     naming the python class of the protocol implementation
-                        extended_attributes ...     additional information for the protocol
-                        domains             ...     a dict naming each domain and the priority of the protocol for each operation (lower is better, zero is not upported)
+                    domain            ...     indictaing the domain that should be assumed for transfers. Values are 'ALL', 'LAN', or 'WAN'
+                    protocols         ...     all supported protocol in form of a list of dict objects with the followig structure
+                    - scheme              ...     protocol scheme e.g. http, srm, ...
+                    - hostname            ...     hostname of the site
+                    - prefix              ...     path to the folder where the files are stored
+                    - port                ...     port used for this protocol
+                    - impl                ...     naming the python class of the protocol implementation
+                    - extended_attributes ...     additional information for the protocol
+                    - domains             ...     a dict naming each domain and the priority of the protocol for each operation (lower is better, zero is not upported)
 
         :raises RSENotFound: if the provided RSE coud not be found in the database.
     """
     # __request_rse_info will be assigned when the module is loaded as it depends on the rucio environment (server or client)
     # __request_rse_info, rse_region are defined in /rucio/rse/__init__.py
-    rse_info = rse_region.get(str(rse))   # NOQA
+    key = '{}:{}'.format(rse, vo) if rse_id is None else str(rse_id)
+    key = 'rse_info_%s' % (key)
+    rse_info = RSE_REGION.get(key)   # NOQA pylint: disable=undefined-variable
     if not rse_info:  # no cached entry found
-        rse_info = __request_rse_info(str(rse), session=session)  # NOQA
-        rse_region.set(str(rse), rse_info)  # NOQA
+        rse_info = __request_rse_info(str(rse), vo=vo, rse_id=rse_id, session=session)  # NOQA pylint: disable=undefined-variable
+        RSE_REGION.set(key, rse_info)  # NOQA pylint: disable=undefined-variable
     return rse_info
 
 
-def select_protocol(rse_settings, operation, scheme=None):
+def _get_possible_protocols(rse_settings, operation, scheme=None, domain=None, impl=None):
+    """
+    Filter the list of available protocols or provided by the supported ones.
+
+    :param rse_settings: The rse settings.
+    :param operation:    The operation (write, read).
+    :param scheme:       Optional filter if no specific protocol is defined in
+                         rse_setting for the provided operation.
+    :param domain:       Optional domain (lan/wan), if not specified, both will be returned
+    :returns:            The list of possible protocols.
+    """
     operation = operation.lower()
-    candidates = copy.copy(rse_settings['protocols'])
-    if type(rse_settings['domain']) is not list:
-        raise exception.RSEProtocolDomainNotSupported('Domain setting must be list.')
-
-    for d in rse_settings['domain']:
-        if d not in utils.rse_supported_protocol_domains():
-            raise exception.RSEProtocolDomainNotSupported('Domain %s is not supported by Rucio.' % d)
+    candidates = rse_settings['protocols']
 
-    tbr = list()
+    # convert scheme to list, if given as string
+    if scheme and not isinstance(scheme, list):
+        scheme = scheme.split(',')
+
+    tbr = []
     for protocol in candidates:
+        # Check if impl given and filter if so
+        if impl and protocol['impl'] != impl:
+            tbr.append(protocol)
+            continue
+
         # Check if scheme given and filter if so
-        if scheme:
-            if not isinstance(scheme, list):
-                scheme = scheme.split(',')
-            if protocol['scheme'] not in scheme:
-                tbr.append(protocol)
-                continue
-        # Check if operation in domain is supported
-        for d in rse_settings['domain']:
-            if protocol['domains'][d][operation] == 0:
-                tbr.append(protocol)
-                break
-    for r in tbr:
-        candidates.remove(r)
+        if scheme and protocol['scheme'] not in scheme:
+            tbr.append(protocol)
+            continue
 
-    if not len(candidates):
-        raise exception.RSEProtocolNotSupported('No protocol for provided settings found : %s.' % str(rse_settings))
+        filtered = True
 
-    # Select the one with the highest priority
-    candidates = sorted(candidates, key=lambda k: k['scheme'])
-    best_choice = candidates[0]
-    candidates.remove(best_choice)
-    domain = rse_settings['domain'][0]
-    for p in candidates:
-        if p['domains'][domain][operation] < best_choice['domains'][domain][operation]:
-            best_choice = p
-    return best_choice
-
-
-def create_protocol(rse_settings, operation, scheme=None):
-    """ Instanciates the protocol defined for the given operation.
-
-        :param rse_attr: RSE attributes
-        :param operation: the intended operation for this protocol
-        :param scheme: optional filter if no specific protocol is defined in rse_setting for the provided operation
+        if not domain:
+            for d in list(protocol['domains'].keys()):
+                if protocol['domains'][d][operation]:
+                    filtered = False
+        else:
+            if protocol['domains'].get(domain, {operation: None}).get(operation):
+                filtered = False
+
+        if filtered:
+            tbr.append(protocol)
+
+    if len(candidates) <= len(tbr):
+        raise exception.RSEProtocolNotSupported('No protocol for provided settings'
+                                                ' found : %s.' % str(rse_settings))
+
+    return [c for c in candidates if c not in tbr]
+
+
+def get_protocols_ordered(rse_settings, operation, scheme=None, domain='wan', impl=None):
+    if operation not in utils.rse_supported_protocol_operations():
+        raise exception.RSEOperationNotSupported('Operation %s is not supported' % operation)
 
-        :returns: an instance of the requested protocol
+    if domain and domain not in utils.rse_supported_protocol_domains():
+        raise exception.RSEProtocolDomainNotSupported('Domain %s not supported' % domain)
+
+    candidates = _get_possible_protocols(rse_settings, operation, scheme, domain, impl)
+    candidates.sort(key=lambda k: k['domains'][domain][operation])
+    return candidates
+
+
+def select_protocol(rse_settings, operation, scheme=None, domain='wan'):
+    if operation not in utils.rse_supported_protocol_operations():
+        raise exception.RSEOperationNotSupported('Operation %s is not supported' % operation)
+
+    if domain and domain not in utils.rse_supported_protocol_domains():
+        raise exception.RSEProtocolDomainNotSupported('Domain %s not supported' % domain)
+
+    candidates = _get_possible_protocols(rse_settings, operation, scheme, domain)
+    # Shuffle candidates to load-balance over equal sources
+    random.shuffle(candidates)
+    return min(candidates, key=lambda k: k['domains'][domain][operation])
+
+
+def create_protocol(rse_settings, operation, scheme=None, domain='wan', auth_token=None, protocol_attr=None, logger=logging.log, impl=None):
+    """
+    Instanciates the protocol defined for the given operation.
+
+    :param rse_settings:  RSE attributes
+    :param operation:     Intended operation for this protocol
+    :param scheme:        Optional filter if no specific protocol is defined in rse_setting for the provided operation
+    :param domain:        Optional specification of the domain
+    :param auth_token:    Optionally passing JSON Web Token (OIDC) string for authentication
+    :param protocol_attr: Optionally passing the full protocol availability information to correctly select WAN/LAN
+    :param logger:        Optional decorated logger that can be passed from the calling daemons or servers.
+    :returns:             An instance of the requested protocol
     """
 
     # Verify feasibility of Protocol
     operation = operation.lower()
     if operation not in utils.rse_supported_protocol_operations():
         raise exception.RSEOperationNotSupported('Operation %s is not supported' % operation)
-    rse_settings['domain'] = [rse_settings['domain']] if type(rse_settings['domain']) is not list else rse_settings['domain']
-    for domain in rse_settings['domain']:
-        if domain.lower() not in utils.rse_supported_protocol_domains():
-            raise exception.RSEOperationNotSupported('Domain %s not supported' % rse_settings['domain'])
 
-    if rse_settings['%s_protocol' % operation] == DEFAULT_PROTOCOL:
-        protocol_attr = select_protocol(rse_settings, operation, scheme)
+    if domain and domain not in utils.rse_supported_protocol_domains():
+        raise exception.RSEProtocolDomainNotSupported('Domain %s not supported' % domain)
+
+    if impl:
+        candidate = _get_possible_protocols(rse_settings, operation, scheme, domain, impl=impl)
+        if len(candidate) == 0:
+            raise exception.RSEProtocolNotSupported('Protocol implementation %s operation %s on domain %s not supported' % (impl, operation, domain))
+        protocol_attr = candidate[0]
+    elif not protocol_attr:
+        protocol_attr = select_protocol(rse_settings, operation, scheme, domain)
     else:
-        protocol_attr = rse_settings['%s_protocol' % operation]
-        for d in rse_settings['domain']:
-            if protocol_attr['domains'][d][operation] == 0:
-                raise exception.RSEOperationNotSupported('Operation %s for domain %s not supported by %s' % (operation, rse_settings['domain'], protocol_attr['scheme']))
+        candidates = _get_possible_protocols(rse_settings, operation, scheme, domain)
+        if protocol_attr not in candidates:
+            raise exception.RSEProtocolNotSupported('Protocol %s operation %s on domain %s not supported' % (protocol_attr, operation, domain))
 
-    # Instanciate protocol
+    # Instantiate protocol
     comp = protocol_attr['impl'].split('.')
+    prefix = '.'.join(comp[-2:]) + ': '
+    logger = formatted_logger(logger, prefix + "%s")
     mod = __import__('.'.join(comp[:-1]))
     for n in comp[1:]:
         try:
             mod = getattr(mod, n)
-        except AttributeError:
-            print 'Protocol implementation not found'
-            raise  # TODO: provide proper rucio exception
-    protocol = mod(protocol_attr, rse_settings)
+        except AttributeError as e:
+            logger(logging.DEBUG, 'Protocol implementations not supported.')
+            raise exception.RucioException(str(e))  # TODO: provide proper rucio exception
+    protocol_attr['auth_token'] = auth_token
+    protocol = mod(protocol_attr, rse_settings, logger=logger)
     return protocol
 
 
-def lfns2pfns(rse_settings, lfns, operation='write', scheme=None):
+def lfns2pfns(rse_settings, lfns, operation='write', scheme=None, domain='wan', auth_token=None, logger=logging.log, impl=None):
     """
         Convert the lfn to a pfn
 
+        :rse_settings:      RSE attributes
         :param lfns:        logical file names as a dict containing 'scope' and 'name' as keys. For bulk a list of dicts can be provided
-        :param protocol:    instance of the protocol to be used to create the PFN
+        :param operation:   Intended operation for this protocol
+        :param scheme:      Optional filter if no specific protocol is defined in rse_setting for the provided operation
+        :param domain:      Optional specification of the domain
+        :param auth_token:  Optionally passing JSON Web Token (OIDC) string for authentication
+        :param logger:      Optional decorated logger that can be passed from the calling daemons or servers.
 
-        :returns: a dict with scope:name as key and the PFN as value
+        :returns:           a dict with scope:name as key and the PFN as value
 
     """
-    return create_protocol(rse_settings, operation, scheme).lfns2pfns(lfns)
+    return create_protocol(rse_settings, operation, scheme, domain, auth_token=auth_token, logger=logger, impl=impl).lfns2pfns(lfns)
 
 
-def parse_pfns(rse_settings, pfns, operation='read'):
+def parse_pfns(rse_settings, pfns, operation='read', domain='wan', auth_token=None):
     """
         Checks if a PFN is feasible for a given RSE. If so it splits the pfn in its various components.
 
+        :rse_settings:   RSE attributes
         :param pfns:        list of PFNs
-        :param protocol:    instance of the protocol to be used to create the PFN
+        :param operation: Intended operation for this protocol
+        :param domain:    Optional specification of the domain
+        :param auth_token: Optionally passing JSON Web Token (OIDC) string for authentication
 
         :returns: A dict with the parts known by the selected protocol e.g. scheme, hostname, prefix, path, name
 
         :raises RSEFileNameNotSupported: if provided PFN is not supported by the RSE/protocol
         :raises RSENotFound: if the referred storage is not found i the repository (rse_id)
         :raises InvalidObject: If the properties parameter doesn't include scheme, hostname, and port as keys
         :raises RSEOperationNotSupported: If no matching protocol was found for the requested operation
     """
     if len(set([urlparse(pfn).scheme for pfn in pfns])) != 1:
         raise ValueError('All PFNs must provide the same protocol scheme')
-    return create_protocol(rse_settings, operation, urlparse(pfns[0]).scheme).parse_pfns(pfns)
-
-
-def download(rse_settings, files, dest_dir=None, force_scheme=None, printstatements=False):
-    """
-        Copy a file from the connected storage to the local file system.
-        Providing a list indicates the bulk mode.
-
-
-        :param rse_settings:    RSE to use
-        :param files:           a single dict or a list with dicts containing 'scope' and 'name'
-                                if LFNs are provided and additional 'pfn' if PFNs are provided.
-                                E.g.  [{'name': '2_rse_remote_get.raw', 'scope': 'user.jdoe'},
-                                       {'name':'3_rse_remote_get.raw', 'scope': 'user.jdoe', 'pfn': 'user/jdoe/5a/98/3_rse_remote_get.raw'}]
-        :param dest_dir:        path to the directory where the downloaded files will be stored. If not given, each scope is represented by its own directory.
-        :param force_scheme:    normally the scheme is dictated by the RSE object, when specifying the PFN it must be forced to the one specified in the PFN, overruling the RSE description.
-
-        :returns: True/False for a single file or a dict object with 'scope:name' for LFNs or 'name' for PFNs as keys and True or the exception as value for each file in bulk mode
-
-        :raises SourceNotFound: remote source file can not be found on storage
-        :raises DestinationNotAccessible: local destination directory is not accessible
-        :raises FileConsistencyMismatch: the checksum of the downloaded file does not match the provided one
-        :raises ServiceUnavailable: for any other reason
-
-    """
-    ret = {}
-    gs = True  # gs represents the global status which inidcates if every operation workd in bulk mode
-
-    protocol = create_protocol(rse_settings, 'read', scheme=force_scheme)
-    protocol.connect()
-
-    files = [files] if not type(files) is list else files
-    for f in files:
-        pfn = f['pfn'] if 'pfn' in f else protocol.lfns2pfns(f).values()[0]
-        target_dir = "./%s" % f['scope'] if dest_dir is None else dest_dir
-        try:
-            if not os.path.exists(target_dir):
-                os.makedirs(target_dir)
-            # Each scope is stored into a separate folder
-            finalfile = '%s/%s' % (target_dir, f['name'])
-            # Check if the file already exists, if not download and validate it
-            if not os.path.isfile(finalfile):
-                if 'adler32' in f:
-                    tempfile = '%s/%s.part' % (target_dir, f['name'])
-                    if os.path.isfile(tempfile):
-                        if printstatements:
-                            print '%s already exists, probably from a failed attempt. Will remove it' % (tempfile)
-                        os.unlink(tempfile)
-                    protocol.get(pfn, tempfile)
-                    if printstatements:
-                        print 'File downloaded. Will be validated'
-                    localchecksum = utils.adler32(tempfile)
-                    if localchecksum == f['adler32']:
-                        if printstatements:
-                            print 'File validated'
-                        os.rename(tempfile, finalfile)
-                    else:
-                        os.unlink(tempfile)
-                        raise exception.FileConsistencyMismatch('Checksum mismatch : local %s vs recorded %s' % (str(localchecksum), str(f['adler32'])))
-                else:
-                    protocol.get(pfn, '%s/%s' % (target_dir, f['name']))
-                ret['%s:%s' % (f['scope'], f['name'])] = True
-            else:
-                ret['%s:%s' % (f['scope'], f['name'])] = True
-        except Exception as e:
-            gs = False
-            ret['%s:%s' % (f['scope'], f['name'])] = e
-
-    protocol.close()
-    if len(ret) == 1:
-        for x in ret:
-            if isinstance(ret[x], Exception):
-                raise ret[x]
-            else:
-                return ret[x]
-    return [gs, ret]
+    return create_protocol(rse_settings, operation, urlparse(pfns[0]).scheme, domain, auth_token=auth_token).parse_pfns(pfns)
 
 
-def exists(rse_settings, files):
+def exists(rse_settings, files, domain='wan', scheme=None, impl=None, auth_token=None, vo='def', logger=logging.log):
     """
         Checks if a file is present at the connected storage.
         Providing a list indicates the bulk mode.
 
-        :param files: a single dict or a list with dicts containing 'scope' and 'name'
-                      if LFNs are used and only 'name' if PFNs are used.
-                      E.g. {'name': '2_rse_remote_get.raw', 'scope': 'user.jdoe'}, {'name': 'user/jdoe/5a/98/3_rse_remote_get.raw'}
+        :rse_settings:      RSE attributes
+        :param files:       a single dict or a list with dicts containing 'scope' and 'name'
+                            if LFNs are used and only 'name' if PFNs are used.
+                            E.g. {'name': '2_rse_remote_get.raw', 'scope': 'user.jdoe'}, {'name': 'user/jdoe/5a/98/3_rse_remote_get.raw'}
+        :param domain:      The network domain, either 'wan' (default) or 'lan'
+        :param auth_token:  Optionally passing JSON Web Token (OIDC) string for authentication
+        :param vo:          The VO for the RSE
+        :param logger:      Optional decorated logger that can be passed from the calling daemons or servers.
 
-        :returns: True/False for a single file or a dict object with 'scope:name' for LFNs or 'name' for PFNs as keys and True or the exception as value for each file in bulk mode
+        :returns:           True/False for a single file or a dict object with 'scope:name' for LFNs or 'name' for PFNs as keys and True or the exception as value for each file in bulk mode
 
         :raises RSENotConnected: no connection to a specific storage has been established
     """
+
     ret = {}
     gs = True  # gs represents the global status which inidcates if every operation workd in bulk mode
 
-    protocol = create_protocol(rse_settings, 'read')
+    protocol = create_protocol(rse_settings, 'read', scheme=scheme, impl=impl, domain=domain, auth_token=auth_token, logger=logger)
     protocol.connect()
+    try:
+        protocol.exists(None)
+    except NotImplementedError:
+        protocol = create_protocol(rse_settings, 'write', scheme=scheme, domain=domain, auth_token=auth_token, logger=logger)
+        protocol.connect()
+    except:
+        pass
 
     files = [files] if not type(files) is list else files
     for f in files:
         exists = None
-        if (type(f) is str) or (type(f) is unicode):
+        if isinstance(f, STRING_TYPES):
             exists = protocol.exists(f)
             ret[f] = exists
         elif 'scope' in f:  # a LFN is provided
-            exists = protocol.exists(protocol.lfns2pfns(f).values()[0])
+            pfn = list(protocol.lfns2pfns(f).values())[0]
+            if isinstance(pfn, exception.RucioException):
+                raise pfn
+            logger(logging.DEBUG, 'Checking if %s exists', pfn)
+            # deal with URL signing if required
+            if rse_settings['sign_url'] is not None and pfn[:5] == 'https':
+                pfn = __get_signed_url(rse_settings['rse'], rse_settings['sign_url'], 'read', pfn, vo)    # NOQA pylint: disable=undefined-variable
+            exists = protocol.exists(pfn)
             ret[f['scope'] + ':' + f['name']] = exists
         else:
             exists = protocol.exists(f['name'])
             ret[f['name']] = exists
         if not exists:
             gs = False
 
     protocol.close()
     if len(ret) == 1:
         for x in ret:
             return ret[x]
     return [gs, ret]
 
 
-def upload(rse_settings, lfns, source_dir=None):
+def upload(rse_settings, lfns, domain='wan', source_dir=None, force_pfn=None, force_scheme=None, transfer_timeout=None, delete_existing=False, sign_service=None, auth_token=None, vo='def', logger=logging.log, impl=None):
     """
         Uploads a file to the connected storage.
         Providing a list indicates the bulk mode.
 
-        :param lfns:        a single dict or a list with dicts containing 'scope' and 'name'. E.g. [{'name': '1_rse_local_put.raw', 'scope': 'user.jdoe', 'filesize': 42, 'adler32': '87HS3J968JSNWID'},
-                                                                                                    {'name': '2_rse_local_put.raw', 'scope': 'user.jdoe', 'filesize': 4711, 'adler32': 'RSSMICETHMISBA837464F'}]
-        :param source_dir:  path to the local directory including the source files
+        :rse_settings:            RSE attributes
+        :param lfns:              a single dict or a list with dicts containing 'scope' and 'name'.
+                                  Examples:
+                                  [
+                                  {'name': '1_rse_local_put.raw', 'scope': 'user.jdoe', 'filesize': 42, 'adler32': '87HS3J968JSNWID'},
+                                  {'name': '2_rse_local_put.raw', 'scope': 'user.jdoe', 'filesize': 4711, 'adler32': 'RSSMICETHMISBA837464F'}
+                                  ]
+                                  If the 'filename' key is present, it will be used by Rucio as the actual name of the file on disk (separate from the Rucio 'name').
+        :param domain:            The network domain, either 'wan' (default) or 'lan'
+        :param source_dir:        path to the local directory including the source files
+        :param force_pfn:         use the given PFN -- can lead to dark data, use sparingly
+        :param force_scheme:      use the given protocol scheme, overriding the protocol priority in the RSE description
+        :param transfer_timeout:  set this timeout (in seconds) for the transfers, for protocols that support it
+        :param sign_service:      use the given service (e.g. gcs, s3, swift) to sign the URL
+        :param auth_token:        Optionally passing JSON Web Token (OIDC) string for authentication
+        :param vo:                The VO for the RSE
+        :param logger:            Optional decorated logger that can be passed from the calling daemons or servers.
 
-        :returns: True/False for a single file or a dict object with 'scope:name' as keys and True or the exception as value for each file in bulk mode
+        :returns:                 True/False for a single file or a dict object with 'scope:name' as keys and True or the exception as value for each file in bulk mode
 
         :raises RSENotConnected: no connection to a specific storage has been established
         :raises SourceNotFound: local source file can not be found
         :raises DestinationNotAccessible: remote destination directory is not accessible
         :raises ServiceUnavailable: for any other reason
     """
+
     ret = {}
-    gs = True  # gs represents the global status which inidcates if every operation workd in bulk mode
+    gs = True  # gs represents the global status which indicates if every operation worked in bulk mode
 
-    protocol = create_protocol(rse_settings, 'write')
+    protocol = create_protocol(rse_settings, 'write', scheme=force_scheme, domain=domain, auth_token=auth_token, logger=logger, impl=impl)
     protocol.connect()
-    protocol_delete = create_protocol(rse_settings, 'delete')
+    protocol_delete = create_protocol(rse_settings, 'delete', domain=domain, auth_token=auth_token, logger=logger, impl=impl)
     protocol_delete.connect()
-
     lfns = [lfns] if not type(lfns) is list else lfns
     for lfn in lfns:
-        name = lfn['name']
+        base_name = lfn.get('filename', lfn['name'])
+        name = lfn.get('name', base_name)
         scope = lfn['scope']
-        if 'adler32' not in lfn:
+        if 'adler32' not in lfn and 'md5' not in lfn:
             gs = False
-            ret['%s:%s' % (scope, name)] = exception.RucioException('Missing checksum for file %s:%s' % (lfn['scope'], lfn['name']))
+            ret['%s:%s' % (scope, name)] = exception.RucioException('Missing checksum for file %s:%s' % (lfn['scope'], name))
             continue
         if 'filesize' not in lfn:
             gs = False
-            ret['%s:%s' % (scope, name)] = exception.RucioException('Missing filesize for file %s:%s' % (lfn['scope'], lfn['name']))
+            ret['%s:%s' % (scope, name)] = exception.RucioException('Missing filesize for file %s:%s' % (lfn['scope'], name))
             continue
-
-        pfn = protocol.lfns2pfns(lfn).values()[0]
+        if force_pfn:
+            pfn = force_pfn
+            readpfn = force_pfn
+        else:
+            pfn = list(protocol.lfns2pfns(make_valid_did(lfn)).values())[0]
+            if isinstance(pfn, exception.RucioException):
+                raise pfn
+            readpfn = pfn
+            if sign_service is not None:
+                # need a separate signed URL for read operations (exists and stat)
+                readpfn = __get_signed_url(rse_settings['rse'], sign_service, 'read', pfn, vo)    # NOQA pylint: disable=undefined-variable
+                pfn = __get_signed_url(rse_settings['rse'], sign_service, 'write', pfn, vo)       # NOQA pylint: disable=undefined-variable
 
         # First check if renaming operation is supported
         if protocol.renaming:
 
             # Check if file replica is already on the storage system
-            if protocol.exists(pfn):
-                ret['%s:%s' % (scope, name)] = exception.FileReplicaAlreadyExists('File %s in scope %s already exists on storage' % (name, scope))
+            if protocol.overwrite is False and delete_existing is False and protocol.exists(pfn):
+                ret['%s:%s' % (scope, name)] = exception.FileReplicaAlreadyExists('File %s in scope %s already exists on storage as PFN %s' % (name, scope, pfn))
                 gs = False
             else:
                 if protocol.exists('%s.rucio.upload' % pfn):  # Check for left over of previous unsuccessful attempts
                     try:
-                        protocol_delete.delete('%s.rucio.upload', protocol_delete.lfns2pfns(lfn).values()[0])
+                        logger(logging.DEBUG, 'Deleting %s.rucio.upload', pfn)
+                        protocol_delete.delete('%s.rucio.upload' % list(protocol_delete.lfns2pfns(make_valid_did(lfn)).values())[0])
                     except Exception as e:
                         ret['%s:%s' % (scope, name)] = exception.RSEOperationNotSupported('Unable to remove temporary file %s.rucio.upload: %s' % (pfn, str(e)))
+                        gs = False
+                        continue
+
+                if delete_existing:
+                    if protocol.exists('%s' % pfn):  # Check for previous completed uploads that have to be removed before upload
+                        try:
+                            logger(logging.DEBUG, 'Deleting %s', pfn)
+                            protocol_delete.delete('%s' % list(protocol_delete.lfns2pfns(make_valid_did(lfn)).values())[0])
+                        except Exception as e:
+                            ret['%s:%s' % (scope, name)] = exception.RSEOperationNotSupported('Unable to remove file %s: %s' % (pfn, str(e)))
+                            gs = False
+                            continue
+
                 try:  # Try uploading file
-                    protocol.put(name, '%s.rucio.upload' % pfn, source_dir)
+                    logger(logging.DEBUG, 'Uploading to %s.rucio.upload', pfn)
+                    protocol.put(base_name, '%s.rucio.upload' % pfn, source_dir, transfer_timeout=transfer_timeout)
                 except Exception as e:
                     gs = False
                     ret['%s:%s' % (scope, name)] = e
                     continue
 
                 valid = None
+
                 try:  # Get metadata of file to verify if upload was successful
-                    stats = protocol.stat('%s.rucio.upload' % pfn)
-                    if ('adler32' in stats) and ('adler32' in lfn):
-                        valid = stats['adler32'] == lfn['adler32']
-                    if (valid is None) and ('filesize' in stats) and ('filesize' in lfn):
-                        valid = stats['filesize'] == lfn['filesize']
-                except NotImplementedError:
-                    valid = True  # If the protocol doesn't support stat of a file, we agreed on assuming that the file was uploaded without error
+                    try:
+                        stats = _retry_protocol_stat(protocol, '%s.rucio.upload' % pfn)
+                        # Verify all supported checksums and keep rack of the verified ones
+                        verified_checksums = []
+                        for checksum_name in GLOBALLY_SUPPORTED_CHECKSUMS:
+                            if (checksum_name in stats) and (checksum_name in lfn):
+                                verified_checksums.append(stats[checksum_name] == lfn[checksum_name])
+                        # Upload is successful if at least one checksum was found
+                        valid = any(verified_checksums)
+                        if not valid and ('filesize' in stats) and ('filesize' in lfn):
+                            valid = stats['filesize'] == lfn['filesize']
+                    except NotImplementedError:
+                        if rse_settings['verify_checksum'] is False:
+                            valid = True
+                        else:
+                            raise exception.RucioException('Checksum not validated')
+                    except exception.RSEChecksumUnavailable:
+                        if rse_settings['verify_checksum'] is False:
+                            valid = True
+                        else:
+                            raise exception.RucioException('Checksum not validated')
                 except Exception as e:
                     gs = False
                     ret['%s:%s' % (scope, name)] = e
                     continue
 
                 if valid:  # The upload finished successful and the file can be renamed
                     try:
+                        logger(logging.DEBUG, 'Renaming %s.rucio.upload to %s', pfn, pfn)
                         protocol.rename('%s.rucio.upload' % pfn, pfn)
                         ret['%s:%s' % (scope, name)] = True
                     except Exception as e:
                         gs = False
                         ret['%s:%s' % (scope, name)] = e
                 else:
                     gs = False
                     ret['%s:%s' % (scope, name)] = exception.RucioException('Replica %s is corrupted.' % pfn)
         else:
 
             # Check if file replica is already on the storage system
-            if protocol.exists(pfn):
-                ret['%s:%s' % (scope, name)] = exception.FileReplicaAlreadyExists('File %s in scope %s already exists on storage' % (name, scope))
+            if protocol.overwrite is False and delete_existing is False and protocol.exists(readpfn):
+                ret['%s:%s' % (scope, name)] = exception.FileReplicaAlreadyExists('File %s in scope %s already exists on storage as PFN %s' % (name, scope, pfn))
                 gs = False
             else:
                 try:  # Try uploading file
-                    protocol.put(name, pfn, source_dir)
+                    logger(logging.DEBUG, 'Uploading to %s', pfn)
+                    protocol.put(base_name, pfn, source_dir, transfer_timeout=transfer_timeout)
                 except Exception as e:
                     gs = False
                     ret['%s:%s' % (scope, name)] = e
                     continue
 
                 valid = None
                 try:  # Get metadata of file to verify if upload was successful
-                    stats = protocol.stat(pfn)
-                    if ('adler32' in stats) and ('adler32' in lfn):
-                        valid = stats['adler32'] == lfn['adler32']
-                    if (valid is None) and ('filesize' in stats) and ('filesize' in lfn):
-                        valid = stats['filesize'] == lfn['filesize']
-                except NotImplementedError:
-                    valid = True  # If the protocol doesn't support stat of a file, we agreed on assuming that the file was uploaded without error
+                    try:
+                        stats = _retry_protocol_stat(protocol, pfn)
+
+                        # Verify all supported checksums and keep rack of the verified ones
+                        verified_checksums = []
+                        for checksum_name in GLOBALLY_SUPPORTED_CHECKSUMS:
+                            if (checksum_name in stats) and (checksum_name in lfn):
+                                verified_checksums.append(stats[checksum_name] == lfn[checksum_name])
+
+                        # Upload is successful if at least one checksum was found
+                        valid = any(verified_checksums)
+                        if not valid and ('filesize' in stats) and ('filesize' in lfn):
+                            valid = stats['filesize'] == lfn['filesize']
+                    except NotImplementedError:
+                        if rse_settings['verify_checksum'] is False:
+                            valid = True
+                        else:
+                            raise exception.RucioException('Checksum not validated')
+                    except exception.RSEChecksumUnavailable:
+                        if rse_settings['verify_checksum'] is False:
+                            valid = True
+                        else:
+                            raise exception.RucioException('Checksum not validated')
                 except Exception as e:
                     gs = False
                     ret['%s:%s' % (scope, name)] = e
                     continue
 
                 if not valid:
                     gs = False
@@ -408,41 +483,44 @@
     protocol.close()
     protocol_delete.close()
     if len(ret) == 1:
         for x in ret:
             if isinstance(ret[x], Exception):
                 raise ret[x]
             else:
-                return ret[x]
-    return [gs, ret]
+                return {0: ret[x], 1: ret, 'success': ret[x], 'pfn': pfn}
+    return {0: gs, 1: ret, 'success': gs, 'pfn': pfn}
 
 
-def delete(rse_settings, lfns):
+def delete(rse_settings, lfns, domain='wan', auth_token=None, logger=logging.log, impl=None):
     """
         Delete a file from the connected storage.
         Providing a list indicates the bulk mode.
 
-        :param lfns:        a single dict or a list with dicts containing 'scope' and 'name'. E.g. [{'name': '1_rse_remote_delete.raw', 'scope': 'user.jdoe'}, {'name': '2_rse_remote_delete.raw', 'scope': 'user.jdoe'}]
-
-        :returns: True/False for a single file or a dict object with 'scope:name' as keys and True or the exception as value for each file in bulk mode
+        :rse_settings:     RSE attributes
+        :param lfns:       a single dict or a list with dicts containing 'scope' and 'name'. E.g. [{'name': '1_rse_remote_delete.raw', 'scope': 'user.jdoe'}, {'name': '2_rse_remote_delete.raw', 'scope': 'user.jdoe'}]
+        :param domain:     The network domain, either 'wan' (default) or 'lan'
+        :param auth_token: Optionally passing JSON Web Token (OIDC) string for authentication
+        :param logger:     Optional decorated logger that can be passed from the calling daemons or servers.
+        :returns:          True/False for a single file or a dict object with 'scope:name' as keys and True or the exception as value for each file in bulk mode
 
         :raises RSENotConnected: no connection to a specific storage has been established
         :raises SourceNotFound: remote source file can not be found on storage
         :raises ServiceUnavailable: for any other reason
 
     """
     ret = {}
     gs = True  # gs represents the global status which inidcates if every operation workd in bulk mode
 
-    protocol = create_protocol(rse_settings, 'delete')
+    protocol = create_protocol(rse_settings, 'delete', domain=domain, auth_token=auth_token, logger=logger, impl=impl)
     protocol.connect()
 
     lfns = [lfns] if not type(lfns) is list else lfns
     for lfn in lfns:
-        pfn = protocol.lfns2pfns(lfn).values()[0]
+        pfn = list(protocol.lfns2pfns(lfn).values())[0]
         try:
             protocol.delete(pfn)
             ret['%s:%s' % (lfn['scope'], lfn['name'])] = True
         except Exception as e:
             ret['%s:%s' % (lfn['scope'], lfn['name'])] = e
             gs = False
 
@@ -452,36 +530,43 @@
             if isinstance(ret[x], Exception):
                 raise ret[x]
             else:
                 return ret[x]
     return [gs, ret]
 
 
-def rename(rse_settings, files):
+def rename(rse_settings, files, domain='wan', auth_token=None, logger=logging.log, impl=None):
     """
         Rename files stored on the connected storage.
         Providing a list indicates the bulk mode.
 
-        :param files: a single dict or a list with dicts containing 'scope', 'name', 'new_scope' and 'new_name'
-                      if LFNs are used or only 'name' and 'new_name' if PFNs are used.
-                      If 'new_scope' or 'new_name' are not provided, the current one is used.
-                      E.g. [{'name': '3_rse_remote_rename.raw', 'scope': 'user.jdoe', 'new_name': '3_rse_new.raw', 'new_scope': 'user.jdoe'},
-                            {'name': 'user/jdoe/d9/cb/9_rse_remote_rename.raw', 'new_name': 'user/jdoe/c6/4a/9_rse_new.raw'}
+        :rse_settings:     RSE attributes
+        :param files:      a single dict or a list with dicts containing 'scope', 'name', 'new_scope' and 'new_name'
+                           if LFNs are used or only 'name' and 'new_name' if PFNs are used.
+                           If 'new_scope' or 'new_name' are not provided, the current one is used.
+                           Examples:
+                           [
+                           {'name': '3_rse_remote_rename.raw', 'scope': 'user.jdoe', 'new_name': '3_rse_new.raw', 'new_scope': 'user.jdoe'},
+                           {'name': 'user/jdoe/d9/cb/9_rse_remote_rename.raw', 'new_name': 'user/jdoe/c6/4a/9_rse_new.raw'}
+                           ]
+        :param domain:     The network domain, either 'wan' (default) or 'lan'
+        :param auth_token: Optionally passing JSON Web Token (OIDC) string for authentication
+        :param logger:     Optional decorated logger that can be passed from the calling daemons or servers.
 
-        :returns: True/False for a single file or a dict object with LFN (key) and True/False (value) in bulk mode
+        :returns:          True/False for a single file or a dict object with LFN (key) and True/False (value) in bulk mode
 
         :raises RSENotConnected: no connection to a specific storage has been established
         :raises SourceNotFound: remote source file can not be found on storage
         :raises DestinationNotAccessible: remote destination directory is not accessible
         :raises ServiceUnavailable: for any other reason
     """
     ret = {}
     gs = True  # gs represents the global status which inidcates if every operation workd in bulk mode
 
-    protocol = create_protocol(rse_settings, 'write')
+    protocol = create_protocol(rse_settings, 'write', domain=domain, auth_token=auth_token, logger=logger, impl=impl)
     protocol.connect()
 
     files = [files] if not type(files) is list else files
     for f in files:
         pfn = None
         new_pfn = None
         key = None
@@ -489,16 +574,16 @@
             key = '%s:%s' % (f['scope'], f['name'])
             # Check if new name is provided
             if 'new_name' not in f:
                 f['new_name'] = f['name']
             # Check if new scope is provided
             if 'new_scope' not in f:
                 f['new_scope'] = f['scope']
-            pfn = protocol.lfns2pfns({'name': f['name'], 'scope': f['scope']}).values()[0]
-            new_pfn = protocol.lfns2pfns({'name': f['new_name'], 'scope': f['new_scope']}).values()[0]
+            pfn = list(protocol.lfns2pfns({'name': f['name'], 'scope': f['scope']}).values())[0]
+            new_pfn = list(protocol.lfns2pfns({'name': f['new_name'], 'scope': f['new_scope']}).values())[0]
         else:
             pfn = f['name']
             new_pfn = f['new_name']
             key = pfn
         # Check if target is not on storage
         if protocol.exists(new_pfn):
             ret[key] = exception.FileReplicaAlreadyExists('File %s already exists on storage' % (new_pfn))
@@ -521,33 +606,146 @@
             if isinstance(ret[x], Exception):
                 raise ret[x]
             else:
                 return ret[x]
     return [gs, ret]
 
 
-def get_space_usage(rse_settings, scheme=None):
+def get_space_usage(rse_settings, scheme=None, domain='wan', auth_token=None, logger=logging.log, impl=None):
     """
         Get RSE space usage information.
 
-        :param scheme: optional filter to select which protocol to be used.
+        :rse_settings:     RSE attributes
+        :param scheme:     optional filter to select which protocol to be used.
+        :param domain:     The network domain, either 'wan' (default) or 'lan'
+        :param auth_token: Optionally passing JSON Web Token (OIDC) string for authentication
+        :param logger:     Optional decorated logger that can be passed from the calling daemons or servers.
 
-        :returns: a list with dict containing 'totalsize' and 'unusedsize'
+        :returns:          a list with dict containing 'totalsize' and 'unusedsize'
 
         :raises ServiceUnavailable: if some generic error occured in the library.
     """
     gs = True
     ret = {}
 
-    protocol = create_protocol(rse_settings, 'read', scheme)
+    protocol = create_protocol(rse_settings, 'read', scheme=scheme, domain=domain, auth_token=auth_token, logger=logger, impl=impl)
     protocol.connect()
 
     try:
         totalsize, unusedsize = protocol.get_space_usage()
         ret["totalsize"] = totalsize
         ret["unusedsize"] = unusedsize
     except Exception as e:
         ret = e
         gs = False
 
     protocol.close()
     return [gs, ret]
+
+
+def find_matching_scheme(rse_settings_dest, rse_settings_src, operation_src, operation_dest, domain='wan', scheme=None):
+    """
+    Find the best matching scheme between two RSEs
+
+    :param rse_settings_dest:    RSE settings for the destination RSE.
+    :param rse_settings_src:     RSE settings for the src RSE.
+    :param operation_src:        Source Operation such as read, write.
+    :param operation_dest:       Dest Operation such as read, write.
+    :param domain:               Domain such as lan, wan.
+    :param scheme:               List of supported schemes.
+    :returns:                    Tuple of matching schemes (dest_scheme, src_scheme, dest_scheme_priority, src_scheme_priority).
+    """
+    operation_src = operation_src.lower()
+    operation_dest = operation_dest.lower()
+
+    src_candidates = copy.copy(rse_settings_src['protocols'])
+    dest_candidates = copy.copy(rse_settings_dest['protocols'])
+
+    # Clean up src_candidates
+    tbr = list()
+    for protocol in src_candidates:
+        # Check if scheme given and filter if so
+        if scheme:
+            if not isinstance(scheme, list):
+                scheme = scheme.split(',')
+            if protocol['scheme'] not in scheme:
+                tbr.append(protocol)
+                continue
+        prot = protocol['domains'].get(domain, {}).get(operation_src, 1)
+        if prot is None or prot == 0:
+            tbr.append(protocol)
+    for r in tbr:
+        src_candidates.remove(r)
+
+    # Clean up dest_candidates
+    tbr = list()
+    for protocol in dest_candidates:
+        # Check if scheme given and filter if so
+        if scheme:
+            if not isinstance(scheme, list):
+                scheme = scheme.split(',')
+            if protocol['scheme'] not in scheme:
+                tbr.append(protocol)
+                continue
+        prot = protocol['domains'].get(domain, {}).get(operation_dest, 1)
+        if prot is None or prot == 0:
+            tbr.append(protocol)
+    for r in tbr:
+        dest_candidates.remove(r)
+
+    if not len(src_candidates) or not len(dest_candidates):
+        raise exception.RSEProtocolNotSupported('No protocol for provided settings found : %s.' % str(rse_settings_dest))
+
+    # Shuffle the candidates to load-balance across equal weights.
+    random.shuffle(dest_candidates)
+    random.shuffle(src_candidates)
+
+    # Select the one with the highest priority
+    dest_candidates = sorted(dest_candidates, key=lambda k: k['domains'][domain][operation_dest])
+    src_candidates = sorted(src_candidates, key=lambda k: k['domains'][domain][operation_src])
+
+    for dest_protocol in dest_candidates:
+        for src_protocol in src_candidates:
+            if __check_compatible_scheme(dest_protocol['scheme'], src_protocol['scheme']):
+                return (dest_protocol['scheme'], src_protocol['scheme'], dest_protocol['domains'][domain][operation_dest], src_protocol['domains'][domain][operation_src])
+
+    raise exception.RSEProtocolNotSupported('No protocol for provided settings found : %s.' % str(rse_settings_dest))
+
+
+def _retry_protocol_stat(protocol, pfn):
+    """
+    try to stat file, on fail try again 1s, 2s, 4s, 8s, 16s, 32s later. Fail is all fail
+
+    :param protocol:     The protocol to use to reach this file
+    :param pfn:          Physical file name of the target for the protocol stat
+    """
+    retries = config_get_int('client', 'protocol_stat_retries', raise_exception=False, default=6)
+    for attempt in range(retries):
+        try:
+            stats = protocol.stat(pfn)
+            return stats
+        except exception.RSEChecksumUnavailable as e:
+            # The stat succeeded here, but the checksum failed
+            raise e
+        except NotImplementedError:
+            break
+        except Exception:
+            sleep(2**attempt)
+    return protocol.stat(pfn)
+
+
+def __check_compatible_scheme(dest_scheme, src_scheme):
+    """
+    Check if two schemes are compatible, such as srm and gsiftp
+
+    :param dest_scheme:    Destination scheme
+    :param src_scheme:     Source scheme
+    :param scheme:         List of supported schemes
+    :returns:              True if schemes are compatible, False otherwise.
+    """
+
+    if dest_scheme == src_scheme:
+        return True
+    if src_scheme in constants.SCHEME_MAP.get(dest_scheme, []):
+        return True
+
+    return False
```

### Comparing `rucio-clients-1.9.6/lib/rucio/tests/emulation/usecases/panda_new.py` & `rucio-clients-32.0.0rc1/tests/test_did.py`

 * *Files 27% similar despite different names*

```diff
@@ -1,1082 +1,1178 @@
-# Copyright European Organization for Nuclear Research (CERN)
+# -*- coding: utf-8 -*-
+# Copyright European Organization for Nuclear Research (CERN) since 2012
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
-# You may not use this file except in compliance with the License.
+# you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
-#              http://www.apache.org/licenses/LICENSE-2.0
 #
-# Authors:
-# - Ralph Vigne, <ralph.vigne@cern.ch>, 2013
+#    http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+from datetime import datetime, timedelta
+
+import pytest
+
+from rucio.api import did
+from rucio.api import scope
+from rucio.db.sqla.util import json_implemented
+from rucio.common import exception
+from rucio.common.exception import (DataIdentifierNotFound, DataIdentifierAlreadyExists,
+                                    InvalidPath, UnsupportedOperation,
+                                    UnsupportedStatus, ScopeNotFound, FileAlreadyExists, FileConsistencyMismatch)
+from rucio.common.types import InternalAccount, InternalScope
+from rucio.common.utils import generate_uuid
+from rucio.core.did import (list_dids, add_did, delete_dids, get_did_atime, touch_dids, attach_dids, detach_dids,
+                            get_metadata, set_metadata, get_did, get_did_access_cnt, add_did_to_followed,
+                            get_users_following_did, remove_did_from_followed, set_status, list_new_dids,
+                            set_new_dids)
+from rucio.core.replica import add_replica, get_replica
+from rucio.db.sqla.constants import DIDType
+from rucio.tests.common import rse_name_generator, scope_name_generator, did_name_generator
+
+
+def skip_without_json():
+    if not json_implemented():
+        pytest.skip("JSON support is not implemented in this database")
+
+
+class TestDIDCore:
+
+    def test_list_dids(self, vo):
+        """ DATA IDENTIFIERS (CORE): List dids """
+        for d in list_dids(scope=InternalScope('data13_hip', vo=vo), filters={'name': '*'}, did_type='collection'):
+            print(d)
+
+    def test_delete_dids(self, mock_scope, root_account):
+        """ DATA IDENTIFIERS (CORE): Delete dids """
+        dsns = [{'name': did_name_generator('dataset'),
+                 'scope': mock_scope,
+                 'purge_replicas': False,
+                 'did_type': DIDType.DATASET} for i in range(5)]
+        for dsn in dsns:
+            add_did(scope=mock_scope, name=dsn['name'], did_type='DATASET', account=root_account)
+        delete_dids(dids=dsns, account=root_account)
+
+    def test_touch_dids_atime(self, mock_scope, root_account):
+        """ DATA IDENTIFIERS (CORE): Touch dids accessed_at timestamp"""
+        tmp_dsn1 = did_name_generator('dataset')
+        tmp_dsn2 = did_name_generator('dataset')
+
+        add_did(scope=mock_scope, name=tmp_dsn1, did_type=DIDType.DATASET, account=root_account)
+        add_did(scope=mock_scope, name=tmp_dsn2, did_type=DIDType.DATASET, account=root_account)
+        now = datetime.utcnow()
+
+        now -= timedelta(microseconds=now.microsecond)
+        assert get_did_atime(scope=mock_scope, name=tmp_dsn1) is None
+        assert get_did_atime(scope=mock_scope, name=tmp_dsn2) is None
+
+        touch_dids(dids=[{'scope': mock_scope, 'name': tmp_dsn1, 'type': DIDType.DATASET, 'accessed_at': now}])
+        assert now == get_did_atime(scope=mock_scope, name=tmp_dsn1)
+        assert get_did_atime(scope=mock_scope, name=tmp_dsn2) is None
+
+    def test_touch_dids_access_cnt(self, mock_scope, root_account):
+        """ DATA IDENTIFIERS (CORE): Increase dids access_cnt"""
+        tmp_dsn1 = did_name_generator('dataset')
+        tmp_dsn2 = did_name_generator('dataset')
+
+        add_did(scope=mock_scope, name=tmp_dsn1, did_type=DIDType.DATASET, account=root_account)
+        add_did(scope=mock_scope, name=tmp_dsn2, did_type=DIDType.DATASET, account=root_account)
+
+        assert get_did_access_cnt(scope=mock_scope, name=tmp_dsn1) is None
+        assert get_did_access_cnt(scope=mock_scope, name=tmp_dsn2) is None
+
+        for i in range(100):
+            touch_dids(dids=[{'scope': mock_scope, 'name': tmp_dsn1, 'type': DIDType.DATASET}])
+        assert 100 == get_did_access_cnt(scope=mock_scope, name=tmp_dsn1)
+        assert get_did_access_cnt(scope=mock_scope, name=tmp_dsn2) is None
+
+    def test_update_dids(self, vo, mock_scope, root_account, rse_factory):
+        """ DATA IDENTIFIERS (CORE): Update file size and checksum"""
+        rse, rse_id = rse_factory.make_mock_rse()
+        dsn = did_name_generator('dataset')
+        lfn = did_name_generator('file')
+        add_did(scope=mock_scope, name=dsn, did_type=DIDType.DATASET, account=root_account, meta='')
+
+        files = [{'scope': mock_scope, 'name': lfn,
+                  'bytes': 724963570, 'adler32': '0cc737eb',
+                  'meta': {'guid': str(generate_uuid()), 'events': 100}}]
+        attach_dids(scope=mock_scope, name=dsn, rse_id=rse_id, dids=files, account=root_account)
+        assert get_replica(rse_id=rse_id, scope=mock_scope, name=lfn)
+
+        set_metadata(scope=mock_scope, name=lfn, key='adler32', value='0cc737ee')
+        assert get_metadata(scope=mock_scope, name=lfn)['adler32'] == '0cc737ee'
+
+        with pytest.raises(DataIdentifierNotFound):
+            set_metadata(scope=mock_scope, name='Nimportnawak', key='adler32', value='0cc737ee')
+
+        set_metadata(scope=mock_scope, name=lfn, key='bytes', value=724963577)
+        assert get_metadata(scope=mock_scope, name=lfn)['bytes'] == 724963577
+
+    def test_get_did_with_dynamic(self, root_account, rse_factory, did_factory):
+        """ DATA IDENTIFIERS (CORE): Get did with dynamic resolve of size"""
+        rse_name, rse_id = rse_factory.make_mock_rse()
+
+        # make a dataset with 2 files in it and verify get_did(dynamic) on dataset
+        dataset1 = did_factory.make_dataset()
+        file1 = did_factory.random_file_did()
+        file2 = did_factory.random_file_did()
+        add_replica(rse_id=rse_id, bytes_=10, account=root_account, **file1)
+        add_replica(rse_id=rse_id, bytes_=10, account=root_account, **file2)
+        attach_dids(dids=[file1, file2], account=root_account, **dataset1)
+        did1 = get_did(dynamic_depth=DIDType.FILE, **dataset1)
+        assert did1['length'] == 2
+        assert did1['bytes'] == 20
+
+        # attach dataset to container and verify get_did(dynamic) on container
+        container = did_factory.make_container()
+        attach_dids(dids=[dataset1], account=root_account, **container)
+        did1 = get_did(dynamic_depth=DIDType.FILE, **container)
+        assert did1['length'] == 2
+        assert did1['bytes'] == 20
+
+        # make another dataset with one file in it and attach to previously created container
+        dataset2 = did_factory.make_dataset()
+        file3 = did_factory.random_file_did()
+        add_replica(rse_id=rse_id, bytes_=10, account=root_account, **file3)
+        attach_dids(dids=[dataset2], account=root_account, **container)
+        attach_dids(dids=[file3], account=root_account, **dataset2)
+        did1 = get_did(dynamic_depth=DIDType.FILE, **container)
+        assert did1['length'] == 3
+        assert did1['bytes'] == 30
+
+        # if resolve_depth is dataset, the result will be incorrect, but this is by design,
+        # dataset has to be re-evaluated
+        did1 = get_did(dynamic_depth=DIDType.DATASET, **container)
+        assert did1['length'] == 0
+        assert did1['bytes'] == 0
+
+    def test_reattach_dids(self, vo, mock_scope, root_account, rse_factory):
+        """ DATA IDENTIFIERS (CORE): Repeatedly attach and detach DIDs """
+        rse, rse_id = rse_factory.make_mock_rse()
+        parent_name = did_name_generator('dataset')
+        add_did(scope=mock_scope, name=parent_name, did_type=DIDType.DATASET, account=root_account)
+
+        child_name = did_name_generator('file')
+        files = [{'scope': mock_scope, 'name': child_name,
+                  'bytes': 12345, 'adler32': '0cc737eb'}]
+
+        attach_dids(scope=mock_scope, name=parent_name, rse_id=rse_id, dids=files, account=root_account)
+
+        detach_dids(scope=mock_scope, name=parent_name, dids=files)
+
+        attach_dids(scope=mock_scope, name=parent_name, rse_id=rse_id, dids=files, account=root_account)
+
+        detach_dids(scope=mock_scope, name=parent_name, dids=files)
+
+    @pytest.mark.dirty
+    def test_add_did_to_followed(self, mock_scope, root_account):
+        """ DATA IDENTIFIERS (CORE): Mark a did as followed """
+        dsn = did_name_generator('dataset')
+
+        add_did(scope=mock_scope, name=dsn, did_type=DIDType.DATASET, account=root_account)
+        add_did_to_followed(scope=mock_scope, name=dsn, account=root_account)
+        users = get_users_following_did(scope=mock_scope, name=dsn)
+        rows = 0
+        for user in users:
+            rows += 1
+
+        assert rows == 1
+
+    @pytest.mark.dirty
+    def test_get_users_following_did(self, mock_scope, root_account):
+        """ DATA IDENTIFIERS (CORE): Get the list of users following a did """
+        dsn = did_name_generator('dataset')
+
+        add_did(scope=mock_scope, name=dsn, did_type=DIDType.DATASET, account=root_account)
+        add_did_to_followed(scope=mock_scope, name=dsn, account=root_account)
+
+        users = get_users_following_did(scope=mock_scope, name=dsn)
+        rows = 0
+        for user in users:
+            rows += 1
+
+        assert rows == 1
+
+    @pytest.mark.dirty
+    def test_remove_did_from_followed(self, mock_scope, root_account):
+        """ DATA IDENTIFIERS (CORE): Mark a did as not followed """
+        dsn = did_name_generator('dataset')
+
+        add_did(scope=mock_scope, name=dsn, did_type=DIDType.DATASET, account=root_account)
+        add_did_to_followed(scope=mock_scope, name=dsn, account=root_account)
+
+        users = get_users_following_did(scope=mock_scope, name=dsn)
+        rows = 0
+        for user in users:
+            rows += 1
+
+        assert rows == 1
+
+        remove_did_from_followed(scope=mock_scope, name=dsn, account=root_account)
+
+        users = get_users_following_did(scope=mock_scope, name=dsn)
+        rows = 0
+        for user in users:
+            rows += 1
+
+        assert rows == 0
+
+    def test_circular_attach(self, root_account, rse_factory, did_factory):
+        """ Ensure that it's not possible to create a circular attachment of containers"""
+        container1 = did_factory.make_container()
+        container2 = did_factory.make_container()
+        container3 = did_factory.make_container()
+        attach_dids(dids=[container2], account=root_account, **container1)
+        with pytest.raises(UnsupportedOperation, match='Circular attachment detected'):
+            attach_dids(dids=[container1], account=root_account, **container2)
+
+        attach_dids(dids=[container3], account=root_account, **container2)
+        with pytest.raises(UnsupportedOperation, match='Circular attachment detected'):
+            attach_dids(dids=[container1], account=root_account, **container3)
+
+    @pytest.mark.dirty
+    @pytest.mark.parametrize("file_config_mock", [
+        {"overrides": [('subscriptions', 'reevaluate_dids_at_close', 'True')]},
+    ], indirect=True)
+    def test_reevaluate_after_close(self, mock_scope, root_account, file_config_mock):
+        """ DATA IDENTIFIERS (CORE): Test that the option reevaluate_close_did set is_new to True """
+        dsn = did_name_generator('dataset')
+        add_did(scope=mock_scope, name=dsn, did_type=DIDType.DATASET, account=root_account)
+        new_dids = [did for did in list_new_dids(did_type=None, thread=None, total_threads=None, chunk_size=100000, session=None)]
+        assert {'scope': mock_scope, 'name': dsn, 'did_type': DIDType.DATASET} in new_dids
+
+        set_new_dids([{'scope': mock_scope, 'name': dsn, 'did_type': DIDType.DATASET}], None)
+        new_dids = [did for did in list_new_dids(did_type=None, thread=None, total_threads=None, chunk_size=100000, session=None)]
+        assert {'scope': mock_scope, 'name': dsn, 'did_type': DIDType.DATASET} not in new_dids
+
+        set_status(mock_scope, dsn, open=False)
+        new_dids = [did for did in list_new_dids(did_type=None, thread=None, total_threads=None, chunk_size=100000, session=None)]
+        assert {'scope': mock_scope, 'name': dsn, 'did_type': DIDType.DATASET} in new_dids
+
+
+class TestDIDApi:
+
+    @pytest.mark.dirty
+    def test_list_new_dids(self, vo):
+        """ DATA IDENTIFIERS (API): List new identifiers """
+        tmp_scope = scope_name_generator()
+        tmp_dsn = did_name_generator('dataset')
+        scope.add_scope(tmp_scope, 'jdoe', 'root', vo=vo)
+        for i in range(0, 5):
+            did.add_did(scope=tmp_scope, name='%s-%i' % (tmp_dsn, i), did_type='DATASET', issuer='root', vo=vo)
+        for i in did.list_new_dids('DATASET', vo=vo):
+            assert i != {}
+            assert i['did_type'] == DIDType.DATASET
+            break
+        for i in did.list_new_dids(vo=vo):
+            assert i != {}
+            break
+
+    @pytest.mark.dirty
+    def test_update_new_dids(self, vo):
+        """ DATA IDENTIFIERS (API): List new identifiers and update the flag new """
+        tmp_scope = scope_name_generator()
+        tmp_dsn = did_name_generator('dataset')
+        scope.add_scope(tmp_scope, 'jdoe', 'root', vo=vo)
+        dids = []
+        for i in range(0, 5):
+            d = {'scope': tmp_scope, 'name': '%s-%i' % (tmp_dsn, i), 'did_type': DIDType.DATASET}
+            did.add_did(scope=tmp_scope, name='%s-%i' % (tmp_dsn, i), did_type='DATASET', issuer='root', vo=vo)
+            dids.append(d)
+        st = did.set_new_dids(dids, None, vo=vo)
+        assert st
+        with pytest.raises(DataIdentifierNotFound):
+            did.set_new_dids([{'scope': 'dummyscope', 'name': 'dummyname', 'did_type': DIDType.DATASET}], None, vo=vo)
+
+
+class TestDIDClients:
+
+    def test_list_dids(self, did_client, replica_client, scope_client, rse_factory):
+        """ DATA IDENTIFIERS (CLIENT): List dids by pattern."""
+        tmp_rse, rse_id = rse_factory.make_mock_rse()
+        tmp_scope = scope_name_generator()
+        tmp_files = []
+        file_prefix = did_name_generator('file')
+        tmp_files.append('%sfile_a_1%s' % (file_prefix, generate_uuid()))
+        tmp_files.append('%sfile_a_2%s' % (file_prefix, generate_uuid()))
+        tmp_files.append('%sfile_b_1%s' % (file_prefix, generate_uuid()))
+
+        scope_client.add_scope('jdoe', tmp_scope)
+        for tmp_file in tmp_files:
+            replica_client.add_replica(tmp_rse, tmp_scope, tmp_file, 1, '0cc737eb')
+
+        results = []
+        for result in did_client.list_dids(tmp_scope, {'name': '%sfile_a_*' % file_prefix}, did_type='file'):
+            results.append(result)
+        assert len(results) == 2
+        results = []
+        for result in did_client.list_dids(tmp_scope, {'name': '%sfile_a_1*' % file_prefix}, did_type='file'):
+            results.append(result)
+        assert len(results) == 1
+        results = []
+        for result in did_client.list_dids(tmp_scope, {'name': '%sfile_*_1*' % file_prefix}, did_type='file'):
+            results.append(result)
+        assert len(results) == 2
+        results = []
+        for result in did_client.list_dids(tmp_scope, {'name': '%sfile*' % file_prefix}, did_type='file'):
+            results.append(result)
+        assert len(results) == 3
+        results = []
+
+        filters = {'name': '%sfile* % file_prefix', 'created_after': datetime.utcnow() - timedelta(hours=1)}
+        for result in did_client.list_dids(tmp_scope, filters):
+            results.append(result)
+        assert len(results) == 0
+        with pytest.raises(UnsupportedOperation):
+            did_client.list_dids(tmp_scope, {'name': '%sfile*' % file_prefix}, did_type='whateverytype')
+
+    @pytest.mark.dirty
+    @pytest.mark.noparallel(reason='uses pre-defined scope names')
+    def test_list_recursive(self, did_client, scope_client):
+        """ DATA IDENTIFIERS (CLIENT): List did recursive """
+        # Create nested containers and datast
+        tmp_scope_1 = ('list-did-recursive-%s' % generate_uuid())[:25]
+        tmp_scope_2 = ('list-did-recursive-%s' % generate_uuid())[:25]
+        scope_client.add_scope('root', tmp_scope_1)
+        scope_client.add_scope('root', tmp_scope_2)
+
+        tmp_container_1 = did_name_generator('container')
+        did_client.add_container(scope=tmp_scope_1, name=tmp_container_1)
+
+        tmp_container_2 = did_name_generator('container')
+        did_client.add_container(scope=tmp_scope_1, name=tmp_container_2)
+
+        tmp_dataset_1 = did_name_generator('dataset')
+        did_client.add_dataset(scope=tmp_scope_2, name=tmp_dataset_1)
+
+        tmp_dataset_2 = did_name_generator('dataset')
+        did_client.add_dataset(scope=tmp_scope_1, name=tmp_dataset_2)
+
+        did_client.attach_dids(scope=tmp_scope_1, name=tmp_container_1, dids=[{'scope': tmp_scope_2, 'name': tmp_dataset_1}])
+        did_client.attach_dids(scope=tmp_scope_1, name=tmp_container_2, dids=[{'scope': tmp_scope_1, 'name': tmp_dataset_2}])
+        did_client.attach_dids(scope=tmp_scope_1, name=tmp_container_1, dids=[{'scope': tmp_scope_1, 'name': tmp_container_2}])
+
+        # List DIDs not recursive - only the first container is expected
+        dids = [str(did) for did in did_client.list_dids(scope=tmp_scope_1, recursive=False, did_type='all', filters={'name': tmp_container_1})]
+        assert dids == [tmp_container_1]
+
+        # List DIDs recursive - first container and all attached collections are expected
+        dids = [str(did) for did in did_client.list_dids(scope=tmp_scope_1, recursive=True, did_type='all', filters={'name': tmp_container_1})]
+        assert tmp_container_1 in dids
+        assert tmp_container_2 in dids
+        assert tmp_dataset_1 in dids
+        assert tmp_dataset_2 in dids
+        assert len(dids) == 4
+
+        # List DIDs recursive - only containers are expected
+        dids = [str(did) for did in did_client.list_dids(scope=tmp_scope_1, recursive=True, did_type='container', filters={'name': tmp_container_1})]
+        assert tmp_container_1 in dids
+        assert tmp_container_2 in dids
+        assert tmp_dataset_1 not in dids
+        assert tmp_dataset_2 not in dids
+        assert len(dids) == 2
+
+    @pytest.mark.dirty
+    def test_list_by_metadata(self, did_client):
+        """ DATA IDENTIFIERS (CLIENT): List did with metadata"""
+        dsns = []
+        tmp_scope = 'mock'
+        tmp_dsn1 = did_name_generator('dataset')
+        dsns.append(tmp_dsn1)
+
+        dataset_meta = {'project': 'data12_8TeV',
+                        'run_number': 400000,
+                        'stream_name': 'physics_CosmicCalo',
+                        'prod_step': 'merge',
+                        'datatype': 'NTUP_TRIG',
+                        'version': 'f392_m920',
+                        }
+        did_client.add_dataset(scope=tmp_scope, name=tmp_dsn1, meta=dataset_meta)
+        tmp_dsn2 = did_name_generator('dataset')
+        dsns.append(tmp_dsn2)
+        dataset_meta['run_number'] = 400001
+        did_client.add_dataset(scope=tmp_scope, name=tmp_dsn2, meta=dataset_meta)
+
+        tmp_dsn3 = did_name_generator('dataset')
+        dsns.append(tmp_dsn3)
+        dataset_meta['stream_name'] = 'physics_Egamma'
+        dataset_meta['datatype'] = 'NTUP_SMWZ'
+        did_client.add_dataset(scope=tmp_scope, name=tmp_dsn3, meta=dataset_meta)
+
+        dids = did_client.list_dids(tmp_scope, {'project': 'data12_8TeV', 'version': 'f392_m920'})
+        results = []
+        for d in dids:
+            results.append(d)
+        for dsn in dsns:
+            assert dsn in results
+        dsns.remove(tmp_dsn1)
+
+        dids = did_client.list_dids(tmp_scope, {'project': 'data12_8TeV', 'run_number': 400001})
+        results = []
+        for d in dids:
+            results.append(d)
+        for dsn in dsns:
+            assert dsn in results
+        dsns.remove(tmp_dsn2)
+
+        dids = did_client.list_dids(tmp_scope, {'project': 'data12_8TeV', 'stream_name': 'physics_Egamma', 'datatype': 'NTUP_SMWZ'})
+        results = []
+        for d in dids:
+            results.append(d)
+        for dsn in dsns:
+            assert dsn in results
+
+    @pytest.mark.dirty
+    @pytest.mark.noparallel(reason='uses pre-defined scope')
+    def test_add_did(self, vo, did_client, rse_factory):
+        """ DATA IDENTIFIERS (CLIENT): Add, populate, list did content"""
+        tmp_scope = 'mock'
+        tmp_rse, rse_id = rse_factory.make_mock_rse()
+        rse2, rse2_id = rse_factory.make_mock_rse()
+        tmp_dsn = did_name_generator('dataset')
+        root = InternalAccount('root', vo=vo)
+
+        # PFN example: rfio://castoratlas.cern.ch/castor/cern.ch/grid/atlas/tzero/xx/xx/xx/filename
+        dataset_meta = {'project': 'data13_hip',
+                        'run_number': 300000,
+                        'stream_name': 'physics_CosmicCalo',
+                        'prod_step': 'merge',
+                        'datatype': 'NTUP_TRIG',
+                        'version': 'f392_m927',
+                        }
+        rules = [{'copies': 1, 'rse_expression': tmp_rse, 'account': 'root'}]
+
+        with pytest.raises(ScopeNotFound):
+            did_client.add_dataset(scope='Nimportnawak', name=tmp_dsn, statuses={'monotonic': True}, meta=dataset_meta, rules=rules)
+
+        files = [{'scope': InternalScope(tmp_scope, vo=vo), 'name': did_name_generator('file'), 'bytes': 724963570, 'adler32': '0cc737eb'}, ]
+        with pytest.raises(DataIdentifierNotFound):
+            did_client.add_dataset(scope=tmp_scope, name=tmp_dsn, statuses={'monotonic': True}, meta=dataset_meta, rules=rules, files=files)
+
+        with pytest.raises(DataIdentifierNotFound):
+            did_client.add_files_to_dataset(scope=tmp_scope, name=tmp_dsn, files=files)
+
+        files = []
+        for i in range(5):
+            lfn = did_name_generator('file')
+            pfn = 'mock://localhost/tmp/rucio_rse/%(project)s/%(version)s/%(prod_step)s' % dataset_meta
+            # it doesn't work with mock: TBF
+            # pfn = 'srm://mock2.com:2880/pnfs/rucio/disk-only/scratchdisk/rucio_tests/%(project)s/%(version)s/%(prod_step)s' % dataset_meta
+            pfn += '%(tmp_dsn)s/%(lfn)s' % locals()
+            file_meta = {'guid': str(generate_uuid()), 'events': 10}
+            files.append({'scope': InternalScope(tmp_scope, vo=vo), 'name': lfn,
+                          'bytes': 724963570, 'adler32': '0cc737eb',
+                          'pfn': pfn, 'meta': file_meta})
+
+        rules = [{'copies': 1, 'rse_expression': rse2, 'lifetime': timedelta(days=2), 'account': 'root'}]
+
+        with pytest.raises(InvalidPath):
+            did_client.add_dataset(scope=tmp_scope, name=tmp_dsn, statuses={'monotonic': True}, meta=dataset_meta, rules=rules, files=files, rse=tmp_rse)
+
+        files_without_pfn = [{'scope': i['scope'], 'name': i['name'], 'bytes': i['bytes'], 'adler32': i['adler32'], 'meta': i['meta']} for i in files]
+        did_client.add_dataset(scope=tmp_scope, name=tmp_dsn, statuses={'monotonic': True}, meta=dataset_meta, rules=rules, files=files_without_pfn, rse=tmp_rse)
+
+        with pytest.raises(DataIdentifierAlreadyExists):
+            did_client.add_dataset(scope=tmp_scope, name=tmp_dsn, files=files, rse=tmp_rse)
+
+        files = []
+        for i in range(5):
+            lfn = '%(tmp_dsn)s.' % locals() + str(generate_uuid())
+            pfn = 'mock://localhost/tmp/rucio_rse/%(project)s/%(version)s/%(prod_step)s' % dataset_meta
+            # it doesn't work with mock: TBF
+            # pfn = 'srm://mock2.com:2880/pnfs/rucio/disk-only/scratchdisk/rucio_tests/%(project)s/%(version)s/%(prod_step)s' % dataset_meta
+            pfn += '%(tmp_dsn)s/%(lfn)s' % locals()
+            file_meta = {'guid': str(generate_uuid()), 'events': 100}
+            files.append({'scope': InternalScope(tmp_scope, vo=vo), 'name': lfn,
+                          'bytes': 724963570, 'adler32': '0cc737eb',
+                          'pfn': pfn, 'meta': file_meta})
+        rules = [{'copies': 1, 'rse_expression': rse2, 'lifetime': timedelta(days=2)}]
+
+        with pytest.raises(InvalidPath):
+            did_client.add_files_to_dataset(scope=tmp_scope, name=tmp_dsn, files=files, rse=tmp_rse)
+        files_without_pfn = [{'scope': i['scope'], 'name': i['name'], 'bytes': i['bytes'], 'adler32': i['adler32'], 'meta': i['meta']} for i in files]
+        did_client.add_files_to_dataset(scope=tmp_scope, name=tmp_dsn, files=files_without_pfn, rse=tmp_rse)
+
+        did_client.close(scope=tmp_scope, name=tmp_dsn)
+
+    @pytest.mark.dirty
+    @pytest.mark.noparallel(reason='uses pre-defined scope')
+    def test_create_sample(self, vo, root_account, did_client, rse_factory):
+        """ DATA IDENTIFIERS (CLIENT): Create a sample"""
+        tmp_scope = 'mock'
+        tmp_rse, rse_id = rse_factory.make_mock_rse()
+        tmp_dsn = did_name_generator('dataset')
+
+        # PFN example: rfio://castoratlas.cern.ch/castor/cern.ch/grid/atlas/tzero/xx/xx/xx/filename
+        dataset_meta = {'project': 'data13_hip',
+                        'run_number': 300000,
+                        'stream_name': 'physics_CosmicCalo',
+                        'prod_step': 'merge',
+                        'datatype': 'NTUP_TRIG',
+                        'version': 'f392_m927',
+                        }
+        rules = [{'copies': 1, 'rse_expression': tmp_rse, 'account': root_account.external}]
+
+        files = []
+        for i in range(5):
+            lfn = did_name_generator('file')
+            file_meta = {'guid': str(generate_uuid()), 'events': 10}
+            files.append({'scope': InternalScope(tmp_scope, vo=vo), 'name': lfn,
+                          'bytes': 724963570, 'adler32': '0cc737eb',
+                          'meta': file_meta})
+
+        did_client.add_dataset(scope=tmp_scope, name=tmp_dsn, statuses={'monotonic': True}, meta=dataset_meta, rules=rules, files=files, rse=tmp_rse)
+        did_client.close(scope=tmp_scope, name=tmp_dsn)
+
+        tmp_dsn_output = did_name_generator('dataset')
+        did_client.create_did_sample(input_scope=tmp_scope, input_name=tmp_dsn, output_scope=tmp_scope, output_name=tmp_dsn_output, nbfiles=2)
+        files = [f for f in did_client.list_files(scope=tmp_scope, name=tmp_dsn_output)]
+        assert len(files) == 2
+
+    @pytest.mark.dirty
+    @pytest.mark.noparallel(reason='uses pre-defined scope')
+    def test_attach_dids_to_dids(self, did_client, rse_factory):
+        """ DATA IDENTIFIERS (CLIENT): Attach dids to dids"""
+        tmp_scope = 'mock'
+        tmp_rse, rse_id = rse_factory.make_mock_rse()
+        nb_datasets = 5
+        nb_files = 5
+        attachments, dsns = list(), list()
+        guid_to_query = None
+        dsn = {}
+        for i in range(nb_datasets):
+            attachment = {}
+            attachment['scope'] = tmp_scope
+            attachment['name'] = did_name_generator('dataset')
+            attachment['rse'] = tmp_rse
+            files = []
+            for _ in range(nb_files):
+                files.append({'scope': tmp_scope, 'name': did_name_generator('file'),
+                              'bytes': 724963570, 'adler32': '0cc737eb',
+                              'meta': {'guid': str(generate_uuid()), 'events': 100}})
+            attachment['dids'] = files
+            guid_to_query = files[0]['meta']['guid']
+            dsn = {'scope': tmp_scope, 'name': attachment['name']}
+            dsns.append(dsn)
+            attachments.append(attachment)
+
+        did_client.add_datasets(dsns=dsns)
+        did_client.attach_dids_to_dids(attachments=attachments)
+        dsns_l = [i for i in did_client.get_dataset_by_guid(guid_to_query)]
+
+        assert [dsn] == dsns_l
+
+        cnt_name = did_name_generator('container')
+        did_client.add_container(scope='mock', name=cnt_name)
+        with pytest.raises(UnsupportedOperation):
+            did_client.attach_dids_to_dids([{'scope': 'mock', 'name': cnt_name, 'rse': tmp_rse, 'dids': attachment['dids']}])
+
+    @pytest.mark.dirty
+    @pytest.mark.noparallel(reason='uses pre-defined scope')
+    def test_add_files_to_datasets(self, did_client, rse_factory):
+        """ DATA IDENTIFIERS (CLIENT): Add files to Datasets"""
+        tmp_scope = 'mock'
+        tmp_rse, rse_id = rse_factory.make_mock_rse()
+        dsn1 = did_name_generator('dataset')
+        dsn2 = did_name_generator('dataset')
+        meta = {'transient': True}
+        files1, files2, nb_files = [], [], 5
+        for i in range(nb_files):
+            files1.append({'scope': tmp_scope, 'name': did_name_generator('file'),
+                           'bytes': 724963570, 'adler32': '0cc737eb',
+                           'meta': {'guid': str(generate_uuid()), 'events': 100}})
+            files2.append({'scope': tmp_scope, 'name': did_name_generator('file'),
+                           'bytes': 724963570, 'adler32': '0cc737eb',
+                           'meta': {'guid': str(generate_uuid()), 'events': 100}})
+
+        did_client.add_dataset(scope=tmp_scope, name=dsn1, files=files1,
+                               rse=tmp_rse, meta=meta)
+        did_client.add_dataset(scope=tmp_scope, name=dsn2, files=files2,
+                               rse=tmp_rse, meta=meta)
+
+        attachments = [{'scope': tmp_scope, 'name': dsn1, 'dids': files2, 'rse': tmp_rse},
+                       {'scope': tmp_scope, 'name': dsn2, 'dids': files1, 'rse': tmp_rse}]
+
+        did_client.add_files_to_datasets(attachments)
+
+        files = [f for f in did_client.list_files(scope=tmp_scope, name=dsn1)]
+        assert len(files) == 10
+
+        with pytest.raises(FileAlreadyExists):
+            did_client.add_files_to_datasets(attachments)
+
+        for attachment in attachments:
+            for i in range(nb_files):
+                attachment['dids'].append({'scope': tmp_scope,
+                                           'name': did_name_generator('file'),
+                                           'bytes': 724963570,
+                                           'adler32': '0cc737eb',
+                                           'meta': {'guid': str(generate_uuid()),
+                                                    'events': 100}})
+
+        did_client.add_files_to_datasets(attachments, ignore_duplicate=True)
+
+        files = [f for f in did_client.list_files(scope=tmp_scope, name=dsn1)]
+        assert len(files) == 15
+
+        # Corrupt meta-data
+        files = []
+        for attachment in attachments:
+            for file in attachment['dids']:
+                file['bytes'] = 1000
+                break
 
-import datetime
-import bisect
-import os
-import pickle
-import sys
-import threading
-import time
-
-from Queue import Queue
-from random import choice, gauss, sample, random, randint
-from requests.exceptions import ConnectionError
-
-from rucio.client import Client
-from rucio.common.exception import DatabaseException, DataIdentifierNotFound, UnsupportedOperation
-from rucio.common.utils import generate_uuid as uuid
-from rucio.core import monitor
-from rucio.tests.emulation.ucemulator import UCEmulator
-
-
-class UseCaseDefinition(UCEmulator):
-    """
-        Implements all PanDA use cases.
-    """
-
-    @UCEmulator.UseCase
-    def CREATE_TASK(self, task_type, rses, input, output, file_transfer_duration, bulk, threads, safety_delay):
-        target_rses = list()
-        task_type_id = task_type.split('.')[1].split('-')[0]
-        task_number = '%08d' % randint(0, 100000000)
-        if threads:
-            sem = threading.BoundedSemaphore(threads)
-        if 'output_datasets_per_datatype' in output.keys():
-            output_datasets_per_datatype = output['output_datasets_per_datatype']
-            if (output_datasets_per_datatype % 1) != 0:  # Fraction is a decimal, decide final number by chance
-                output_datasets_per_datatype = int(output_datasets_per_datatype) if ((output_datasets_per_datatype % 1) < random()) else int(output_datasets_per_datatype) + 1
-        else:
-            output_datasets_per_datatype = 1
-
-        input_ds_used = False if input['dss'] is None else True
-
-        if 'create_subs' in output.keys():
-            create_sub_ds = output['create_subs'] == "True"
-        else:
-            create_sub_ds = False
-
-        if (task_type.startswith('user') or task_type.startswith('group')):  # User task is created
-            ext = task_type.split('.')[0]
-            create_dis_ds = False
-            log_ds = False
-            rse = None
-            for i in range(output_datasets_per_datatype):
-                while (rse is None) or (rse in target_rses):
-                    rse = choice(rses)
-                target_rses.append(rse)
-        else:  # Production task output stuff is created
-            ext = 'out'
-            rse = choice(rses)
-            for i in range(output_datasets_per_datatype):
-                target_rses.append(rse)
-            if input_ds_used:
-                if input['dis_ds_probability'] == 0:
-                    create_dis_ds = False
-                elif input['dis_ds_probability'] == 1:
-                    create_dis_ds = True
-                else:
-                    create_dis_ds = (input['dis_ds_probability'] >= random())
-            else:
-                create_dis_ds = False
-            log_ds = True
-
-        client = Client(account='panda')
-
-        if 'lifetime' not in output.keys():
-            output['lifetime'] = None
-
-        # ----------------------- List replicas and derive list of files from it -------------------
-        replicas = list()
-        if input_ds_used:
-            while input_ds_used and len(input['dss']) and not len(replicas):
-                temp = input['dss'].pop()
-                now = time.time()
-                print '== PanDA: Checking %s as input' % temp
-                try:
-                    with monitor.record_timer_block('panda.list_replicas'):
-                        replicas = [f for f in client.list_replicas(scope=temp[0], name=temp[1])]
-                except (DatabaseException, DataIdentifierNotFound, ConnectionError):
-                    replicas = list()
-                    pass
-                delta = time.time() - now
-                if len(replicas):
-                    monitor.record_timer('panda.list_replicas.normalized', delta / len(replicas))
-            if len(replicas) == 0:
-                print '== PanDA: Empty input dataset provided'
-                monitor.record_counter('panda.tasks.%s.EmptyInputDataset' % task_type, 1)
-                return {'jobs': [], 'task': [], 'subs': []}
-            input['scope'] = temp[0]
-            input['ds_name'] = temp[1]
-            if log_ds:  # Production task
-                output['scope'] = temp[0]
-
-            # Should be changed when the response from list_replicas is updated
-            files = list()
-            file_keys = list()
-            cnt_rses = dict()
-            for r in replicas:
-                if '%s:%s' % (r['scope'], r['name']) not in file_keys:
-                    file_keys.append('%s:%s' % (r['scope'], r['name']))
-                    files.append({'scope': r['scope'], 'name': r['name'], 'bytes': r['bytes']})
-                    if ('max_jobs' in input.keys()) and (len(files) > (input['max_jobs'] * input['number_of_inputfiles_per_job'])):
-                        monitor.record_counter('panda.tasks.%s.limited_input' % task_type, 1)
-                        break
-                for tmp_rse in r['rses']:
-                    if tmp_rse not in cnt_rses.keys():
-                        cnt_rses[tmp_rse] = 0
-                    cnt_rses[tmp_rse] += 1
-            print '== PanDA: Replica distribution over RSEs: %s files -> %s' % (len(files), cnt_rses)
-            if not (task_type.startswith('user') or task_type.startswith('group')):  # User task is created
-                rse = sorted(cnt_rses, key=cnt_rses.get, reverse=True)[0]
-                for i in range(target_rses):
-                    target_rses[i] = rse
-
-            monitor.record_counter('panda.tasks.%s.input_files' % task_type, len(files))  # Reports the number of files in the intput dataset of the task type
-
-            # Release memory by cleaning the two objects
-            file_keys = None
-
-            # ------------------------------- Determine metadata for output dataset ------------------------------------
-            meta = dict()
-            success = False
-            retry = 1
-            print '---- List meta'
-            while not success:
-                try:
-                    with monitor.record_timer_block('panda.get_metadata'):
-                        meta_i = client.get_metadata(scope=input['scope'], name=input['ds_name'])
-                    success = True
-                except (DatabaseException, ConnectionError):
-                    monitor.record_counter('panda.retry.get_metadata.%s' % (retry), 1)
-                    retry += 1
-                    if retry > 5:
-                        monitor.record_counter('panda.tasks.%s.missing_input_meta.timeout' % (task_type), 1)
-                        raise
-            for key in ['stream_name', 'project']:
-                if meta_i[key] is not None:
-                    meta[key] = meta_i[key]
-                else:
-                    monitor.record_counter('panda.tasks.%s.missing_input_meta.%s' % (task_type, key), 1)
-                    if key == 'stream_name':
-                        meta[key] = 'physics_Egamma'
-                    elif key == 'project':
-                        meta[key] = 'mc12_8TeV'
-                    else:
-                        meta[key] = 'NotGivenByInput'
-        else:
-            output['scope'] = choice(['mc12_8TeV', 'mc13_14TeV'])
-            input['ds_name'] = uuid()
-            meta = {'stream_name': 'dummy', 'project': output['scope']}
-            input['number_of_inputfiles_per_job'] = 1
-            files = ['file_%s' % f for f in xrange(input['max_jobs'])]
-
-        meta['run_number'] = int(time.time() / (3600 * 24))
-        meta['version'] = uuid()
-        # ----------------------------------- Create final output - dataset(s) ---------------------------------------
-        print '-------------- Create output DS'
-        final_dss = {}
-        for out_ds in output['meta']:  # Create output containers(s)
-            meta['prod_step'] = out_ds.split('.')[0]
-            meta['datatype'] = out_ds.split('.')[1]
-            ds = '.'.join([meta['project'], str(meta['run_number']), meta['stream_name'], meta['prod_step'], meta['datatype'], meta['version'], ext])
-            final_dss[ds] = meta.copy()
-        if log_ds:
-            ds = '.'.join([meta['project'], str(meta['run_number']), meta['stream_name'], meta['prod_step'], meta['datatype'], meta['version'], 'log'])
-            final_dss[ds] = meta.copy()
-
-        temp_ds = list()
-        for fds in final_dss:
-            temp = list()
-            success = False
-            retry = 1
-            while not success:
-                print 'Creating container: cnt_%s' % fds
-                try:
-                    with monitor.record_timer_block('panda.add_container'):
-                        client.add_container(scope=output['scope'], name='cnt_%s' % (fds))
-                        monitor.record_counter('panda.tasks.%s.container' % task_type, 1)  # Reports the creation of a container
-                        success = True
-                except (DatabaseException, ConnectionError):
-                    monitor.record_counter('panda.retry.add_container.%s' % (retry), 1)
-                    retry += 1
-                    if retry > 5:
-                        raise
-                    time.sleep(randint(1, 2))
-
-            for i in range(output_datasets_per_datatype):
-                final_dss[fds].update({'guid': str(uuid())})
-                dsn2 = '%s.%s' % (fds, i)
-                out_ds = {'scope': output['scope'], 'name': dsn2, 'dids': [], 'meta': final_dss[fds].copy(),
-                          'rules': [{'account': output['account'], 'copies': 1, 'rse_expression': target_rses[i], 'grouping': 'DATASET', 'lifetime': output['lifetime']}]}
-                temp.append(out_ds)
-
-            success = False
-            retry = 1
-            while not success:
-                try:
-                    with monitor.record_timer_block(['panda.add_datasets', ('panda.add_datasets.normalized', len(temp))]):
-                        client.add_datasets(temp)
-                    monitor.record_counter('panda.tasks.%s.output_datasets' % task_type, len(temp))  # Reports the number of output datasets for the tasktype (including log datasets)
-                    success = True
-                except (DatabaseException, ConnectionError):
-                    monitor.record_counter('panda.retry.add_datasets.%s' % (retry), 1)
-                    retry += 1
-                    if retry > 5:
-                        raise
-                    time.sleep(randint(1, 2))
-
-            success = False
-            retry = 1
-            while not success:
-                try:
-                    with monitor.record_timer_block(['panda.add_datasets_to_container', ('panda.add_datasets_to_container.normailzed', len(temp))]):
-                        client.add_datasets_to_container(scope=output['scope'], name='cnt_%s' % (fds), dsns=[{'scope': d['scope'], 'name': d['name']} for d in temp])
-                    success = True
-                except (DatabaseException, ConnectionError):
-                    monitor.record_counter('panda.retry.add_datasets_to_container.%s' % (retry), 1)
-                    retry += 1
-                    if retry > 5:
-                        raise
-                    time.sleep(randint(1, 2))
-            temp_ds += temp
-        final_dss = [dsn['name'] for dsn in temp_ds]
-
-        # -------------------------------- Derive/Create dis and subdatasets ------------------------------------------
-        print '-------------- Create dis/sub DS'
-        jobs = []
-        files_in_ds = []
-        dis_ds = None
-        computing_rse = None
-        job_count = 0
-
-        inserts_dis = list()
-        inserts_sub = list()
-
-        if 'number_of_inputfiles_per_job' not in input.keys():
-            input['number_of_inputfiles_per_job'] = 1
-
-        # ----------------------- Derive number of jobs depending on the input dataset ------------------------
-        job_count = float(len(files)) / input['number_of_inputfiles_per_job']
-        if (job_count % 1) != 0:
-            job_count = int(job_count) + 1
-        if ('max_jobs' in input.keys()) and (job_count >= input['max_jobs']):
-            job_count = input['max_jobs']
-
-        used_rses = dict()
-        if create_dis_ds:  # Creating DIS - Datasets
-            count_dis = float(job_count) / input['jobs_per_dis']
-            if (count_dis % 1) != 0:
-                count_dis = int(count_dis) + 1
-            for i in range(int(count_dis)):
-                id = uuid()
-                dis_ds = '%s_DIS_%s' % (input['ds_name'], id)
-                fpd = float(input['jobs_per_dis']) * input['number_of_inputfiles_per_job']
-                start = int(i * fpd)  # If not int, remove digits to get the lower number
-                fpd = int(fpd) + 1 if (fpd % 1) != 0 else int(fpd)  # Must include every file that is (partly) used
-                end = start + fpd
-                if end > len(files):
-                    print '== PanDA Warning: Missing proper number of files per DIS (%s - %s (Files: %s))' % (start, end, len(files))
-                    end = len(files)
-                    start = end - fpd if (end - fpd) > 0 else 0
-                    print '== PanDA Warning: Chosen %s - %s instead' % (start, end)
-                files_in_ds = [files[r] for r in range(start, end)]
-                if not len(files_in_ds):
-                    break
-                if create_sub_ds:
-                    while (target_rses[0] == computing_rse) or (computing_rse is None):
-                        computing_rse = choice(rses)  # Random choice of the computing RSE
-                else:
-                    computing_rse = target_rses[0]  # If no sub, no output is moved, therefore target rse = computing rse
+        with pytest.raises(FileConsistencyMismatch):
+            did_client.add_files_to_datasets(attachments, ignore_duplicate=True)
 
-                temp_job_count = int(float(len(files_in_ds)) / input['number_of_inputfiles_per_job'])
-                if temp_job_count > input['jobs_per_dis']:
-                    temp_job_count = input['jobs_per_dis']
-
-                if computing_rse not in used_rses.keys():
-                    used_rses[computing_rse] = list()
-                used_rses[computing_rse].append((id, temp_job_count))
-
-                inserts_dis.append({'scope': 'Manure', 'name': dis_ds, 'lifetime': 172800,
-                                    'rules': [{'account': 'panda', 'copies': 1, 'rse_expression': computing_rse, 'grouping': 'DATASET'}],
-                                    'dids': files_in_ds})  # Create DIS-Datasets
-                monitor.record_counter('panda.tasks.%s.dis_datasets' % task_type, 1)  # Reports the creation of a dis dataset for the given task type
-                monitor.record_counter('panda.tasks.%s.dis_files' % task_type, len(files_in_ds))  # Reports the number of files in the dis - dataset
-                computing_rse = None
-        else:  # No Dis created, protect files by rules from deletion
-            if task_type.startswith('prod'):  # T1 job, single RSE
-                if input_ds_used:  # Create rules to protect replicas from deletion
-                    with monitor.record_timer_block(['panda.add_replication_rule', ('panda.add_replication_rule.normalized', len(files))]):
-                        client.add_replication_rule(files, copies=1, rse_expression=target_rses[0],
-                                                    grouping='NONE', account='panda', lifetime=172800)
-                temp_job_count = int(float(len(files)) / input['number_of_inputfiles_per_job'])
-                temp_job_count = int(temp_job_count) + 1 if (temp_job_count % 1) != 0 else int(temp_job_count)
-                used_rses[target_rses[0]] = [(None, temp_job_count)]
-            else:  # User or Group, each out-ds on different RSE
-                fpd = float(len(files)) / output_datasets_per_datatype
-                if (fpd % 1) != 0:
-                    fpd = int(fpd) + 1
-                for i in range(int(output_datasets_per_datatype)):
-                    files_in_ds = []
-                    start = int(i * fpd) if ((i * fpd) < len(files)) else int(len(files) - fpd)
-                    end = int(start + fpd) if (start + fpd) < len(files) else len(files)
-                    try:
-                        files_in_ds = [files[f] for f in range(start, end)]
-                    except IndexError:
-                        print '== PanDA Warning: Missing proper number of files per out-DS (%s - %s (%s))' % (start, end, len(files))
-                    if not len(files_in_ds):
-                        break
-
-                    computing_rse = target_rses[i]
-
-                    if input_ds_used:  # Create rules to protect replicas from deletion
-                        with monitor.record_timer_block(['panda.add_replication_rule', ('panda.add_replication_rule.normalized', len(files_in_ds))]):
-                            client.add_replication_rule(files_in_ds, copies=1, rse_expression=computing_rse,
-                                                        grouping='NONE', account='panda', lifetime=172800)
-                    temp_job_count = int(float(len(files_in_ds)) / input['number_of_inputfiles_per_job']) + 1
-
-                    if computing_rse not in used_rses.keys():
-                        used_rses[computing_rse] = list()
-                    used_rses[computing_rse].append((None, temp_job_count))
-                    computing_rse = None
-
-        for computing_rse in used_rses:
-            for temp in used_rses[computing_rse]:
-                jobs.append((output['scope'], final_dss, int(temp[1]), computing_rse))
-
-        # -------------------------------------- Perform bulk inserts ----------------------------------------
-        ts = list()
-        ts_res = Queue()
-        for ds in inserts_dis:
-            if threads:
-                t = threading.Thread(target=self.add_files_ds, kwargs={'client': client, 'ds': ds, 'ret': ts_res, 'sem': sem})
-                t.start()
-                ts.append(t)
-            else:
-                self.add_files_ds(client, ds)
-        if threads:
-            for t in ts:
-                t.join()
-        while not ts_res.empty():
-            ret = ts_res.get()
-            if not ret[0]:
-                print ret[1][2]
-                raise ret[1][0]
-
-        # --------------------------------------- Calculate finishing times ----------------------------------
-        job_finish = []         # When each job finishes -> register output files(s)
-
-        # When jobs are finished for dataset
-        sub_finish = dict()
-        max_completion = 0
-        job_number = 0
-        for job_set in jobs:
-            # job_set: (scope, [target datasets], number of jobs, computing_rse, task_type, log_ds)
-            dis_completion = time.time()
-            if create_dis_ds:
-                dis_completion += gauss(**file_transfer_duration)  # Determines the time it takes to move all files to the target RSE
-
-            # Determine the finishing time of each job using again a gaussian distribution
-            max_target_completion = 0
-            temp = float(job_set[2]) / output_datasets_per_datatype
-            temp = int(temp) + 1 if (temp % 1 != 0) else int(temp)
-            for i in xrange(temp):
-                job_completion = dis_completion + gauss(**output['duration_job'])
-                if job_completion > max_target_completion:
-                    max_target_completion = job_completion
-                job_number += 1
-                job_finish.append((float(job_completion), {'scope': job_set[0], 'targets': job_set[1], 'computing_rse': job_set[3],
-                                                           'task_type': task_type, 'log_ds': log_ds, 'task_type_id': task_type_id,
-                                                           'task_number': task_number, 'job_number': '%06d' % job_number}))
-
-            # Remeber last access to target dataset
-            max_target_completion += safety_delay
-            for dsn in job_set[1]:
-                if (dsn not in sub_finish.keys()) or (sub_finish[dsn][0] < max_target_completion):
-                    for fin_ds in final_dss:
-                        if dsn.endswith(fin_ds):
-                            sub_finish[dsn] = (float(max_target_completion), {'source': {'scope': job_set[0], 'name': dsn}, 'target': {'scope': output['scope'], 'name': fin_ds}, 'task_type': task_type})
-
-            # Update task completion
-            if max_completion < max_target_completion:
-                max_completion = max_target_completion
-
-        max_completion += safety_delay  # Note: Triggers FINISH_TASK some time later to avoid conflicts if job is stuck in gearman queue
-
-        if create_sub_ds:
-            max_completion += gauss(**file_transfer_duration)
-        else:
-            sub_finish = {}  # Empty list of sub datasets to avoid data moving when task is finished
-        task_finish = (float(max_completion), {'scope': output['scope'], 'targets': final_dss, 'task_type': task_type, 'log_ds': log_ds})
-        monitor.record_counter('panda.tasks.%s.dispatched' % task_type, 1)  # Reports the task type which is dipsatched
-        monitor.record_counter('panda.tasks.%s.number_job' % task_type, len(job_finish) * output_datasets_per_datatype)  # Reports the number of jobs spawned from the given task
-        print '== PanDA: Create %s task with %s files (%s repl.) with output scope %s (dis: %s / sub: %s (%s)/ log_ds: %s / out_ds: %s / jobs: %s (%s))' % (task_type, len(files), len(replicas),
-                                                                                                                                                            output['scope'], len(inserts_dis),
-                                                                                                                                                            len(inserts_sub), len(sub_finish), log_ds,
-                                                                                                                                                            final_dss, job_count, len(job_finish) * output_datasets_per_datatype)
-        # print '-', job_finish
-        # print '-', sub_finish
-        # print '-', task_finish
-        return {'jobs': job_finish, 'subs': sub_finish.values(), 'task': task_finish}
-
-    def add_files_ds(self, client, ds, ret=None, sem=None):
-        print '+' * 100
-        print ds
-        print '+' * 100
-        if not client:
-            client = Client(account='panda')
-        success = False
-        retry = 1
-        for rule in ds['rules']:
-            while not success:
-                try:
-                    if sem:
-                        sem.acquire()
-                        with monitor.record_timer_block(['panda.client.add_replication_rule', ('panda.client.add_replication_rule.normalized', len(ds['dids']))]):
-                            bla = client.add_replication_rule(dids=ds['dids'], rse_expression=rule['rse_expression'], account=rule['account'], copies=rule['copies'], lifetime=172800)
-                            print bla
-                    success = True
-                except (DatabaseException, ConnectionError):
-                    e = sys.exc_info()
-                    monitor.record_counter('panda.retry.add_files_to_dataset.%s' % (retry), 1)
-                    retry += 1
-                    if retry > 5:
-                        if ret:
-                            ret.put((False, e))
-                            return
-                        else:
-                            print e
-                            raise
-                    print '== PanDA Warning [%s]: Failed %s times when adding files to dataset (%s:%s). Will retry in 5 seconds.' % (time.strftime('%D %H:%M:%S', time.localtime()), retry, ds['scope'], ds['name'])
-                    time.sleep(randint(1, 2))
-                except:
-                    e = sys.exc_info()
-                    if ret:
-                        ret.put((False, e))
-                    else:
-                        print e
-                        raise
-                finally:
-                    if sem:
-                        sem.release()
-        if ret:
-            ret.put((True, None))
-
-    def CREATE_TASK_input(self, ctx):
-        try:
-            # Select input DS from file provided by Cedric using observed age distribution from Thomas
-            # Select task type
-            success = False
-            task_type = ''
-            while not success:
-                exit = False
-                while not exit:
-                    tt = choice(ctx.task_distribution)
-                    exit = (tt.startswith(task_type.split('-')[0]) or (task_type is ''))
-                # print '== PanDA [%s]: Selecting task from group %s' % (time.strftime('%D %H:%M:%S', time.localtime()), tt.split('-')[0])
-                task_type = tt
-                ret = {'input': ctx.tasks[task_type]['input'],
-                       'output': ctx.tasks[task_type]['output'],
-                       'task_type': task_type,
-                       'rses': [ctx.rses[i] for i in sample(xrange(len(ctx.rses)), 20)],
-                       'file_transfer_duration': ctx.file_transfer_duration,
-                       'safety_delay': ctx.safety_delay,
-                       }
-                if ('meta' in ctx.tasks[task_type]['input'].keys()) and (len(ctx.tasks[task_type]['input']['meta'])):  # Task depends on input dataset
-                    ret['input']['dss'] = list()
-                    for i in range(10):
-                        input_ds = self.select_input_ds(task_type, ctx)
-                        if not input_ds:
-                            continue
-                        ret['input']['dss'].append(input_ds)
-                else:  # Task activity is base on max_jobs
-                    ret['input']['dss'] = None
-                success = True
-            if task_type.split('.')[0] == 'user':
-                user = choice(ctx.users)
-                ret['output']['scope'] = 'user.%s' % user
-                ret['output']['account'] = user
-            elif task_type.split('.')[0] == 'group':
-                group = choice(ctx.groups)
-                ret['output']['scope'] = 'group.%s' % group
-                ret['output']['account'] = group
-            else:
-                ret['output']['account'] = 'panda'
-            ret['bulk'] = ctx.bulk == 'True'
-            if (ctx.threads == 'False') or int(ctx.threads) < 2:
-                ret['threads'] = None
-            else:
-                ret['threads'] = int(ctx.threads)
-            return ret
-        except Exception, e:
-            print e
-
-    def CREATE_TASK_output(self, ctx, output):
-        for key in ['jobs', 'subs', 'task']:
-            if key not in output.keys():
-                return
-        now = time.time()
-        with ctx.job_queue_mutex:
-            monitor.record_timer('panda.helper.waiting.job_queue_mutex.sorting', (time.time() - now))
-            for job in output['jobs']:
-                with monitor.record_timer_block('panda.helper.sorting_jobs'):
-                    bisect.insort(ctx.job_queue, job)
-        now = time.time()
-        with ctx.sub_queue_mutex:
-            monitor.record_timer('panda.helper.waiting.sub_queue_mutex.sorting', (time.time() - now))
-            for sub in output['subs']:
-                with monitor.record_timer_block('panda.helper.sorting_subs'):
-                    bisect.insort(ctx.sub_queue, sub)
-        if len(output['task']):
-            now = time.time()
-            with ctx.task_queue_mutex:
-                monitor.record_timer('panda.helper.waiting.task_queue_mutex.sorting', (time.time() - now))
-                with monitor.record_timer_block('panda.helper.sorting_tasks'):
-                    bisect.insort(ctx.task_queue, output['task'])
-
-    @UCEmulator.UseCase
-    def FINISH_JOB(self, jobs, threads):
-        client = Client(account='panda')
-        if threads:
-            sem = threading.BoundedSemaphore(threads)
-
-        # Group jobs by sub: if the frequency on the DB should be decreased
-        ts = list()
-        ts_res = Queue()
-        for job in jobs:
-            if threads:
-                t = threading.Thread(target=self.register_replica, kwargs={'client': client, 'job': job, 'ret': ts_res, 'sem': sem})
-                t.start()
-                ts.append(t)
-            else:
-                self.register_replica(client, job)
-        if threads:
-            for t in ts:
-                t.join()
-        while not ts_res.empty():
-            ret = ts_res.get()
-            if ret[0] is False:
-                print ret[1][2]
-                raise ret[1][0]
-        targets = []
-        replicas = 0
-        for job in jobs:
-            targets += job['targets']
-            replicas += len(job['targets']) if job['log_ds'] else (2 * len(job['targets']))
-        print '== PanDA [%s]: Registering %s replicas from %s jobs over %s different datasets' % (time.strftime('%D %H:%M:%S', time.localtime()), replicas, len(jobs), len(set(targets)))
-
-    def register_replica(self, client, job, ret=None, sem=None):
-        if not client:
-            client = Client(account='panda')
-        count = 0
-        # TODO: Instead of this loop the attach_dids_to_dids method should be used
-        attachments = list()
-        for tds in job['targets']:
-            # Create output files of the job
-            out_name = '%s.%s._%s.pool.root.1' % (job['task_type_id'], job['task_number'], job['job_numner'])
-            log_name = 'log.%s.%s._%s.job.log.tgz.1' % (job['task_number'], job['job_numner'])
-            files = list()
-            if not job['log_ds']:  # Add log file for each datatype if task doesn't have LOG dataset
-                files.append({'scope': job['scope'], 'name': out_name, 'bytes': 12345L, 'adler32': '0cc737eb', 'meta': {'guid': str(uuid())}})
-                files.append({'scope': job['scope'], 'name': log_name, 'bytes': 12345L, 'adler32': '0cc737eb', 'meta': {'guid': str(uuid())}})
-            else:
-                fn = out_name if tds.split('.')[-2] != 'log' else log_name
-                files.append({'scope': job['scope'], 'name': fn, 'bytes': 12345L, 'adler32': '0cc737eb', 'meta': {'guid': str(uuid())}})
-            attachments.append({'scope': job['scope'], 'name': tds, 'rse': job['computing_rse'], 'dids': files})
-            count += len(files)
-
-        success = False
-        retry = 1
-        e = None
-        now = time.time()
-        while not success:
-            try:
-                if sem:
-                    sem.acquire()
-                with monitor.record_timer_block('panda.attach_dids_to_dids'):
-                    client.attach_dids_to_dids(attachments=attachments)
-                success = True
-            except DatabaseException:
-                e = sys.exc_info()
-                monitor.record_counter('panda.retry.add_files_to_dataset.%s' % (retry), 1)
-                retry += 1
-                if retry > 5:
-                    break
-                print '== PanDA Warning: Failed %s times when adding files to datasets: %s' % (retry, attachments)
-                time.sleep(randint(1, 2))
-            except:
-                e = sys.exc_info()
-                break
-            finally:
-                if sem:
-                    sem.release()
-
-        if not success:
-            print '-' * 80
-            print '- Failed after %s seconds (retries: %s)' % ((time.time() - now), retry)
-            print '- %s:%s' % (job['scope'], tds)
-            print '-', files
-            print '-', job['log_ds']
-            print '-', e
-            print '-', count
-            print '-' * 80
-            if ret:
-                ret.put((False, e))
-        monitor.record_counter('panda.tasks.%s.replicas' % job['task_type'], count)  # Reports the creation of a new replica (including log files) fof the given task type
-        print '== PanDA: Job (%s) added %s files to %s datasets (%s:%s)' % (job['task_type'], count, len(job['targets']), job['scope'], job['targets'])
-        if ret:
-            ret.put((True, count))
-
-    def FINISH_JOB_input(self, ctx):
-        ctx.job_print += 1
-        if not len(ctx.job_queue):
-            if not ctx.job_print % 100:
-                print '== PanDA [%s]: No jobs scheduled so far.' % (time.strftime('%D %H:%M:%S', time.localtime()))
-            return None
-        jobs = []
-        if ctx.job_queue_select.acquire(False):  # Check if there is already one thread waiting to select items from queue
-            now = time.time()
-            with ctx.job_queue_mutex:
-                monitor.record_timer('panda.helper.waiting.job_queue_mutex.selecting', (time.time() - now))
-                now = time.time()
-                tmp_cnt = 0
-                with monitor.record_timer_block('panda.helper.selecting_jobs'):
-                    for job in ctx.job_queue:
-                        tmp_cnt += 1
-                        if job[0] < now:
-                            jobs.append(job[1])
-                        else:
-                            if not len(jobs):
-                                ctx.job_queue = sorted(ctx.job_queue, key=lambda job: job[0])
-                            break
-                    del ctx.job_queue[0:len(jobs)]
-            ctx.job_queue_select.release()
-        else:
-                print '== PanDA [%s]: Already one thread waiting for pending jobs.' % (time.strftime('%D %H:%M:%S', time.localtime()))
-        if (ctx.threads == 'False') or int(ctx.threads) < 2:
-            threads = None
-        else:
-            threads = int(ctx.threads)
-        if len(jobs):
-            print '== PanDA [%s]: Finishing %s jobs.' % (time.strftime('%D %H:%M:%S', time.localtime()), len(jobs))
-            monitor.record_counter('panda.helper.jobs_block', len(jobs))
-            return {'jobs': jobs, 'threads': threads}
-        else:
-            if not ctx.job_print % 100:
-                print '== PanDA [%s]: Next job finishes in %.1f minutes (%s)' % (time.strftime('%D %H:%M:%S', time.localtime()), ((ctx.job_queue[0][0] - now) / 60), time.strftime('%D %H:%M:%S', time.localtime(ctx.job_queue[0][0])))
-            return None
-
-    @UCEmulator.UseCase
-    def POPULATE_SUB(self, subs, threads, safety_delay):
-        client = Client(account='panda')
-        ts = list()
-        ts_res = Queue()
-        if threads:
-            sem = threading.BoundedSemaphore(threads)
-        for sub in subs:
-            print sub
-            print '== PanDA [%s]: Populating SUB-DS (%s) to target (%s) for job %s' % (time.strftime('%D %H:%M:%S', time.localtime()), sub['source'], sub['target'], sub['task_type'])
-            if threads:
-                t = threading.Thread(target=self.aggregate_output, kwargs={'client': client, 'source': sub['source'], 'target': sub['target'],
-                                                                           'task_type': sub['task_type'], 'ret': ts_res, 'sem': sem})
-                t.start()
-                ts.append(t)
-            else:
-                self.aggregate_output(client, sub['source'], sub['target'], sub['task_type'])
-        if threads:
-            for t in ts:
-                t.join()
-        while not ts_res.empty():
-            ret = ts_res.get()
-            if not ret[0]:
-                print ret[1][2]
-                raise ret[1][0]
-
-    def POPULATE_SUB_input(self, ctx):
-        ctx.sub_print += 1
-        if not len(ctx.sub_queue):
-            if not ctx.sub_print % 100:
-                print '== PanDA [%s]: No subs scheduled so far.' % (time.strftime('%D %H:%M:%S', time.localtime()))
-            return None
-        subs = []
-        if ctx.sub_queue_select.acquire(False):  # Check if there is already one thread waiting to select items from queue
-            now = time.time()
-            with ctx.sub_queue_mutex:
-                monitor.record_timer('panda.helper.waiting.sub_queue_mutex.selecting', (time.time() - now))
-                now = time.time()
-                with monitor.record_timer_block('panda.helper.selecting_subs'):
-                    for sub in ctx.sub_queue:
-                        if sub[0] < now:
-                            subs.append(sub[1])
-                        else:
-                            if not len(subs):
-                                ctx.sub_queue = sorted(ctx.sub_queue, key=lambda sub: sub[0])
-                            break
-                    del ctx.sub_queue[0:len(subs)]
-            ctx.sub_queue_select.release()
-        if (ctx.threads == 'False') or int(ctx.threads) < 2:
-            threads = None
-        else:
-            threads = int(ctx.threads)
-        if len(subs):
-            monitor.record_counter('panda.helper.subs_block', len(subs))
-            return {'subs': subs, 'threads': threads, 'safety_delay': ctx.safety_delay}
-        else:
-            if not ctx.sub_print % 100:
-                print '== PanDA [%s]: Next sub datset is populated in  %.1f minutes (%s)' % (time.strftime('%D %H:%M:%S', time.localtime()), ((ctx.sub_queue[0][0] - now) / 60), time.strftime('%D %H:%M:%S', time.localtime(ctx.sub_queue[0][0])))
-            return None
-
-    def aggregate_output(self, client, source, target, task_type, ret=None, sem=None):
-        now = time.time()
-        if not client:
-            client = Client(account='panda')
-        retry = 1
-        fs = list()
-        exc = None
-
-        # List files in SUB
-        while not len(fs):
-            try:
-                with monitor.record_timer_block('panda.list_files'):
-                    fs = [f for f in client.list_files(**source)]
-                if len(fs):
-                    monitor.record_timer('panda.list_files.normalized', (time.time() - now) / len(fs))
-                    monitor.record_counter('panda.tasks.%s.sub_files' % task_type, len(fs))
-                    print '== PanDA [%s]: Adding %s files from SUB (%s) to TID (%s)' % (time.strftime('%D %H:%M:%S', time.localtime()), len(fs), source, target)
+    @pytest.mark.dirty
+    def test_add_dataset(self, did_client):
+        """ DATA IDENTIFIERS (CLIENT): Add dataset """
+        tmp_scope = 'mock'
+        tmp_dsn = did_name_generator('dataset')
+
+        did_client.add_dataset(scope=tmp_scope, name=tmp_dsn, meta={'project': 'data13_hip'})
+
+        did = did_client.get_did(tmp_scope, tmp_dsn)
+
+        assert did['scope'] == tmp_scope
+        assert did['name'] == tmp_dsn
+
+        with pytest.raises(DataIdentifierNotFound):
+            did_client.get_did('i_dont_exist', 'neither_do_i')
+
+    @pytest.mark.dirty
+    def test_add_datasets(self, did_client):
+        """ DATA IDENTIFIERS (CLIENT): Bulk add datasets """
+        tmp_scope = 'mock'
+        dsns = list()
+        for i in range(500):
+            tmp_dsn = {'name': did_name_generator('dataset'), 'scope': tmp_scope, 'meta': {'project': 'data13_hip'}, 'account': 'root'}
+            dsns.append(tmp_dsn)
+        did_client.add_datasets(dsns)
+
+    @pytest.mark.dirty
+    @pytest.mark.noparallel(reason='uses pre-defined scope')
+    def test_exists(self, did_client, replica_client, rse_factory):
+        """ DATA IDENTIFIERS (CLIENT): Check if data identifier exists """
+        tmp_scope = 'mock'
+        tmp_file = did_name_generator('file')
+        tmp_rse, rse_id = rse_factory.make_mock_rse()
+
+        replica_client.add_replica(tmp_rse, tmp_scope, tmp_file, 1, '0cc737eb')
+
+        did = did_client.get_did(tmp_scope, tmp_file)
+
+        assert did['scope'] == tmp_scope
+        assert did['name'] == tmp_file
+
+        with pytest.raises(DataIdentifierNotFound):
+            did_client.get_did('i_dont_exist', 'neither_do_i')
+
+    @pytest.mark.dirty
+    def test_did_hierarchy(self, did_client, replica_client, scope_client, rse_factory):
+        """ DATA IDENTIFIERS (CLIENT): Check did hierarchy rule """
+
+        account = 'jdoe'
+        rse, rse_id = rse_factory.make_mock_rse()
+        scope = scope_name_generator()
+        file_ = [did_name_generator('file') for _ in range(10)]
+        dst = [did_name_generator('dataset') for _ in range(4)]
+        cnt = [did_name_generator('container') for _ in range(4)]
+
+        scope_client.add_scope(account, scope)
+
+        for i in range(10):
+            replica_client.add_replica(rse, scope, file_[i], 1, '0cc737eb')
+        for i in range(4):
+            did_client.add_did(scope, dst[i], 'DATASET', statuses=None, meta=None, rules=None)
+        for i in range(4):
+            did_client.add_did(scope, cnt[i], 'CONTAINER', statuses=None, meta=None, rules=None)
+
+        for i in range(4):
+            did_client.add_files_to_dataset(scope, dst[i], [{'scope': scope, 'name': file_[2 * i], 'bytes': 1, 'adler32': '0cc737eb'},
+                                                            {'scope': scope, 'name': file_[2 * i + 1], 'bytes': 1, 'adler32': '0cc737eb'}])
+
+        did_client.add_containers_to_container(scope, cnt[1], [{'scope': scope, 'name': cnt[2]}, {'scope': scope, 'name': cnt[3]}])
+        did_client.add_datasets_to_container(scope, cnt[0], [{'scope': scope, 'name': dst[1]}, {'scope': scope, 'name': dst[2]}])
+
+        result = did_client.scope_list(scope, recursive=True)
+        for r in result:
+            pass
+            # TODO: fix, fix, fix
+            # if r['name'] == cnt[1]:
+            #    assert r['type'] == 'container'
+            #    assert r['level'] == 0
+            # if (r['name'] == cnt[0]) or (r['name'] == dst[0]) or (r['name'] == file[8]) or (r['name'] == file[9]):
+            #    assert r['level'] == 0
+            # else:
+            #     assert r['level'] == 1
+
+    def test_detach_did(self, did_client, replica_client, scope_client, rse_factory):
+        """ DATA IDENTIFIERS (CLIENT): Detach dids from a did"""
+
+        account = 'jdoe'
+        rse, rse_id = rse_factory.make_mock_rse()
+        scope = scope_name_generator()
+        file_ = [did_name_generator('file') for _ in range(10)]
+        dst = [did_name_generator('dataset') for _ in range(5)]
+        cnt = [did_name_generator('container') for _ in range(2)]
+
+        scope_client.add_scope(account, scope)
+
+        for i in range(10):
+            replica_client.add_replica(rse, scope, file_[i], 1, '0cc737eb')
+        for i in range(5):
+            did_client.add_dataset(scope, dst[i], statuses=None, meta=None, rules=None)
+        for i in range(2):
+            did_client.add_container(scope, cnt[i], statuses=None, meta=None, rules=None)
+
+        for i in range(5):
+            did_client.add_files_to_dataset(scope, dst[i], [{'scope': scope, 'name': file_[2 * i], 'bytes': 1, 'adler32': '0cc737eb'},
+                                                            {'scope': scope, 'name': file_[2 * i + 1], 'bytes': 1, 'adler32': '0cc737eb'}])
+
+        did_client.add_containers_to_container(scope, cnt[1], [{'scope': scope, 'name': dst[2]}, {'scope': scope, 'name': dst[3]}])
+
+        with pytest.raises(UnsupportedOperation):
+            did_client.add_datasets_to_container(scope, cnt[0], [{'scope': scope, 'name': dst[1]}, {'scope': scope, 'name': cnt[1]}])
+
+        did_client.add_datasets_to_container(scope, cnt[0], [{'scope': scope, 'name': dst[1]}, {'scope': scope, 'name': dst[2]}])
+
+        did_client.detach_dids(scope, cnt[0], [{'scope': scope, 'name': dst[1]}])
+        did_client.detach_dids(scope, dst[3], [{'scope': scope, 'name': file_[6]}, {'scope': scope, 'name': file_[7]}])
+        result = did_client.scope_list(scope, recursive=True)
+        for r in result:
+            if r['name'] == dst[1]:
+                assert r['level'] == 0
+            if r['type'] == 'file':
+                if (r['name'] in file_[6:9]):
+                    assert r['level'] == 0
                 else:
-                    print '== PanDA Warning [%s]: No data task arrived for %s. Will Retry later.' % (time.strftime('%D %H:%M:%S', time.localtime()), source)
-                    retry += 1
-                    if retry > 5:
-                        print '== PanDA Warning [%s]: No data task arrived for %s. Gave up' % (time.strftime('%D %H:%M:%S', time.localtime()), source)
-                        monitor.record_counter('panda.tasks.%s.EmptySubDataset' % task_type, 1)
-                        with monitor.record_timer_block('panda.close'):
-                            client.close(**source)
-                        return
-                    time.sleep(randint(3, 5))
-            except DatabaseException:
-                exc = sys.exc_info()
-                fs = []
-                print '== PanDA [%s]: Waiting 5 seconds for task data to arrive in %s (retry count: %s / task-type: %s)' % (time.strftime('%D %H:%M:%S', time.localtime()), source, retry, task_type)
-                monitor.record_counter('panda.retry.list_files.%s' % (retry), 1)
-                retry += 1
-                if retry > 5:
-                    print '== PanDA [%s]: No data task arrived for %s. Gave up' % (time.strftime('%D %H:%M:%S', time.localtime()), source)
-                    monitor.record_counter('panda.tasks.%s.EmptySubDataset' % task_type, 1)
-                    with monitor.record_timer_block('panda.close'):
-                        client.close(**source)
-                    if ret:
-                        ret.put((False, exc))
-                    return
-                time.sleep(randint(1, 2))
-            except Exception:
-                exc = sys.exc_info()
-                if ret:
-                    ret.put((False, exc))
-                else:
-                    raise
+                    assert r['level'] != 0
+
+        with pytest.raises(UnsupportedOperation):
+            did_client.detach_dids(scope=scope, name=cnt[0], dids=[{'scope': scope, 'name': cnt[0]}])
 
-        # Append files to TID
-        try:
-            if sem:
-                sem.acquire()
-            success = False
-            retry = 1
-            while not success:
-                with monitor.record_timer_block(['panda.add_files_to_dataset', ('panda.add_files_to_dataset.normalized', len(fs))]):
-                    client.add_files_to_dataset(scope=target['scope'], name=target['name'], files=fs)
-                success = True
-        except Exception:
-            exc = sys.exc_info()
-            print '== PanDA: Waiting 5 seconds for task data to arrive in %s (retry count: %s / task-type: %s)' % (source, retry, task_type)
-            monitor.record_counter('panda.retry.add_files_to_dataset.%s' % (retry), 1)
-            retry += 1
-            if retry > 5:
-                if ret:
-                    ret.put((False, exc))
-                return
-        finally:
-            if sem:
-                sem.release()
-        print '== PanDA [%s]: Populated %s files from %s to %s' % (time.strftime('%D %H:%M:%S', time.localtime()), len(fs), source, target)
-
-        # Close SUB dataset
-        try:
-            if sem:
-                sem.acquire()
-            success = False
-            retry = 1
-            while not success:
-                with monitor.record_timer_block('panda.close'):
-                    client.close(**source)
-                success = True
-        except Exception:
-            exc = sys.exc_info()
-            print '== PanDA: Waiting 5 seconds for task data to arrive in %s (retry count: %s / task-type: %s)' % (source, retry, task_type)
-            monitor.record_counter('panda.retry.close.%s' % (retry), 1)
-            retry += 1
-            if retry > 5:
-                if ret:
-                    ret.put((False, exc))
-                return
-        finally:
-            if sem:
-                sem.release()
-        print '== PanDA [%s]: Closed sub dataset: %s' % (time.strftime('%D %H:%M:%S', time.localtime()), source)
-        if ret:
-            ret.put((True, None))
-
-    def FINISH_TASK(self, tasks, threads, safety_delay):
-        client = Client(account='panda')
-        for task in tasks:
-            task_type = task['task_type']
-            for target in task['targets']:
-                retry = 1
-                success = False
-                while not success:
-                    try:
-                        now = time.time()
-                        with monitor.record_timer_block('panda.list_files'):
-                            fs = [f for f in client.list_files(scope=task['scope'], name=target)]
-                        if len(fs):
-                            monitor.record_timer('panda.list_files.normalized', (time.time() - now) / len(fs))
-                            monitor.record_counter('panda.tasks.%s.output_ds_size' % task_type, len(fs))  # Reports the number of files added to the output dataset
-                        else:
-                            monitor.record_counter('panda.tasks.%s.EmptyOutputDataset' % task_type, 1)
-                        success = True
-                    except DatabaseException:
-                        monitor.record_counter('panda.retry.close.%s' % (retry), 1)
-                        retry += 1
-                        if retry > 5:
-                            raise
-                        print '== PanDA Warning [%s]: Failed %s times to list files in dataset (%s:%s). Will rertry in 5 seconds.' % (time.strftime('%D %H:%M:%S', time.localtime()), retry, task['scope'], target)
-                        time.sleep(randint(1, 2))
-                    except Exception:
-                        e = sys.exc_info()
-                        print '-' * 80
-                        print '- Failed listing files in TID: %s:%s' % (task['scope'], target)
-                        print '-', e
-                        print '-' * 80
-                        raise
-                retry = 1
-                success = False
-                while not success:
-                    try:
-                        with monitor.record_timer_block('panda.close'):
-                            client.close(scope=task['scope'], name=target)
-                        success = True
-                    except UnsupportedOperation:
-                        break
-                    except DatabaseException:
-                        monitor.record_counter('panda.retry.close.%s' % (retry), 1)
-                        retry += 1
-                        if retry > 5:
-                            raise
-                        print '== PanDA Warning: Failed %s times to close the dataset (%s:%s). Will rertry in 5 seconds.' % (retry, task['scope'], target)
-                        time.sleep(randint(1, 2))
-                print '== PanDA [%s]: Closed output dataset %s:%s from task (%s) including %s files' % (time.strftime('%D %H:%M:%S', time.localtime()), task['scope'], target, task_type, len(fs))
-            monitor.record_counter('panda.tasks.%s.finished' % task_type, 1)
-
-    def FINISH_TASK_input(self, ctx):
-        ctx.task_print += 1
-        if not len(ctx.task_queue):
-            if not ctx.task_print % 100:
-                print '== PanDA [%s]: No tasks scheduled so far.' % (time.strftime('%D %H:%M:%S', time.localtime()))
-            return None
-        tasks = []
-        if ctx.task_queue_select.acquire(False):  # Check if there is already one thread waiting to select items from queue
-            now = time.time()
-            with ctx.task_queue_mutex:
-                monitor.record_timer('panda.helper.waiting.task_queue_mutex.selecting', (time.time() - now))
-                now = time.time()
-                with monitor.record_timer_block('panda.helper.selecting_tasks'):
-                    for task in ctx.task_queue:
-                        if task[0] < now:
-                            tasks.append(task[1])
-                        else:
-                            if not len(tasks):
-                                ctx.task_queue = sorted(ctx.task_queue, key=lambda task: task[0])
-                            break
-                    del ctx.task_queue[0:len(tasks)]
-            ctx.task_queue_select.release()
-        if (ctx.threads == 'False') or int(ctx.threads) < 2:
-            threads = None
-        else:
-            threads = int(ctx.threads)
-        if len(tasks):
-            # print '== PanDA [%s]: Finishing %s tasks.' % (time.strftime('%D %H:%M:%S', time.localtime()), len(tasks))
-            monitor.record_counter('panda.helper.tasks_block', len(tasks))
-            return {'tasks': tasks, 'threads': threads, 'safety_delay': ctx.safety_delay}
-        else:
-            if not ctx.task_print % 100:
-                print '== PanDA [%s]: Next task is finsihed in  %.1f minutes (%s)' % (time.strftime('%D %H:%M:%S', time.localtime()), ((ctx.task_queue[0][0] - now) / 60), time.strftime('%D %H:%M:%S', time.localtime(ctx.task_queue[0][0])))
-            return None
-
-    def RESET_input(self, ctx):
-        print '== PanDA [%s]: Reseting input files cache' % time.strftime('%D %H:%M:%S', time.localtime())
-        monitor.record_counter('panda.tasks.reset', 1)
-        ctx.input_files = {}
-        return None
-
-    def RESET(self):
-        pass  # Will never be executed, only here for sematic reasons
-
-    def QUEUE_OBSERVER(self):
-        pass  # Will never be executed, only here for sematic reasons
-
-    def QUEUE_OBSERVER_input(self, ctx):
-        monitor.record_gauge('panda.tasks.queue', len(ctx.task_queue))
-        monitor.record_gauge('panda.jobs.queue', len(ctx.job_queue))
-        monitor.record_gauge('panda.subs.queue', len(ctx.sub_queue))
-        print '== PanDA [%s]: Task-Queue: %s / Job-Queue: %s / Sub-Queue: %s' % (time.strftime('%D %H:%M:%S', time.localtime()), len(ctx.task_queue), len(ctx.job_queue), len(ctx.sub_queue))
-        tmp_str = 'Job Queue\n'
-        tmp_str += '---------\n'
-        if len(ctx.job_queue) > 11:
-            for i in range(10):
-                tmp_str += '\t%s: %s\n' % (i, time.strftime('%D %H:%M:%S', time.localtime(ctx.job_queue[i][0])))
-            tmp_str += '---------'
-            print tmp_str
-
-        return None  # Indicates that no further action is required
-
-    def setup(self, ctx):
-        """
-            Sets up shared information/objects between the use cases and creates between one
-            and ten empty datasets for the UC_TZ_REGISTER_APPEND use case.
-
-            :param cfg: the context of etc/emulation.cfg
-        """
-        # As long as there is no database filler, one dataset and n files are created here
-        ctx.job_queue = []
-        ctx.job_queue_mutex = threading.Lock()
-        ctx.job_queue_select = threading.Lock()
-        ctx.job_print = 0
-        ctx.sub_queue = []
-        ctx.sub_queue_mutex = threading.Lock()
-        ctx.sub_queue_select = threading.Lock()
-        ctx.sub_print = 0
-        ctx.task_queue = []
-        ctx.task_queue_mutex = threading.Lock()
-        ctx.task_queue_select = threading.Lock()
-        ctx.task_print = 0
-        try:
-            print '== PanDA [%s]: Loading context file' % (time.strftime('%D %H:%M:%S', time.localtime()))
-            with open('/data/emulator/panda.ctx', 'r') as f:
-                stuff = pickle.load(f)
-            delta = (time.time() - stuff[0]) + 135  # safety
-            print '== PanDA [%s]: Start importing previous context (written at: %s / delta: %.2f min)' % (time.strftime('%D %H:%M:%S', time.localtime()),
-                                                                                                          time.strftime('%D %H:%M:%S', time.localtime(stuff[0])), (delta / 60))
-            ctx.job_queue = sorted(stuff[1])
-            for job in ctx.job_queue:
-                job[0] += delta
-            print '== PanDA [%s]: Re-imported %s jobs to queue (min: %s / max: %s).' % (time.strftime('%D %H:%M:%S', time.localtime()), len(ctx.job_queue),
-                                                                                        time.strftime('%D %H:%M:%S', time.localtime(ctx.job_queue[0][0])), time.strftime('%D %H:%M:%S', time.localtime(ctx.job_queue[-1][0])))
-            ctx.sub_queue = sorted(stuff[2])
-            for sub in ctx.sub_queue:
-                sub[0] += delta
-            print '== PanDA [%s]: Re-imported %s subs to queue (min: %s / max: %s).' % (time.strftime('%D %H:%M:%S', time.localtime()), len(ctx.sub_queue),
-                                                                                        time.strftime('%D %H:%M:%S', time.localtime(ctx.sub_queue[0][0])), time.strftime('%D %H:%M:%S', time.localtime(ctx.sub_queue[-1][0])))
-            ctx.task_queue = sorted(stuff[3])
-            for task in ctx.task_queue:
-                task[0] += delta
-            print '== PanDA [%s]: Re-imported %s tasks to queue (min: %s / max: %s).' % (time.strftime('%D %H:%M:%S', time.localtime()), len(ctx.task_queue),
-                                                                                         time.strftime('%D %H:%M:%S', time.localtime(ctx.task_queue[0][0])), time.strftime('%D %H:%M:%S', time.localtime(ctx.task_queue[-1][0])))
-            del stuff
-        except IOError:
-            print '== PanDA: No information about former execution found'
-        except EOFError:
-            print '== PanDA: Panda context file found, but unable to load it.'
-        ctx.input_files = {}
-
-        client = Client(account='panda')
-        ctx.users = list()
-        ctx.groups = list()
-        for a in client.list_accounts():
-            if a['type'] == 'USER' and a['account'].startswith('user'):
-                ctx.users.append(a['account'])
-            if a['type'] == 'GROUP':
-                ctx.groups.append(a['account'])
-
-        ctx.rses = []
-        for rse in client.list_rses():
-            if rse['deterministic']:
-                ctx.rses.append(rse['rse'])
-
-        # TODO: Could be done in a more elegant way I guess
-        ctx.task_distribution = list()
-        for task in ctx.tasks:
-            for i in xrange(ctx.tasks[task]['probability']):
-                ctx.task_distribution.append(task)
-
-    def update_ctx(self, key_chain, value):
-        ctx = super(UseCaseDefinition, self).update_ctx(key_chain, value)
-        # Update task distribution
-        if key_chain[0] == 'tasks' and key_chain[-1] == 'probability':
-            print '== PanDA: Updating task distribution'
-            # TODO: Could be done in a more elegant way I guess
-            task_distribution = list()
-            for task in ctx.tasks:
-                for i in xrange(ctx.tasks[task]['probability']):
-                    task_distribution.append(task)
-            ctx.task_distribution = task_distribution
-
-    def select_input_ds(self, task_type, ctx):
-        dist_prefix = '/data/mounted_hdfs/user/serfon/listdatasets2/'
-
-        success = False
-        retry = 0
-        while not success:
-            retry += 1
-            try:
-                # Derive dataset age
-                cluster = random()
-                i = 0
-                distr = ctx.input_distribution
-                for age_cluster in distr:
-                    if cluster < age_cluster[1]:
-                        break
-                    i += 1
-
-                if i == 0:  # First element
-                    age = randint(0, distr[0][0])
-                elif i == len(distr):  # Last element
-                    age = randint(distr[i - 1][0] + 1, distr[-1][0])
-                else:  # Some in between element
-                    age = randint(distr[i - 1][0] + 1, distr[i][0])
-
-                # Select random input ds-type
-                input_ds_type = choice(ctx.tasks[task_type]['input']['meta'])
-
-                # Select random dataset from file with according age
-                date = datetime.date.today() - datetime.timedelta(days=age)
-                dist_file = '%s/%02d/%02d/listfiles.%s.%s.txt' % (date.year, date.month, date.day, input_ds_type.split('.')[0], input_ds_type.split('.')[1])
-                path = dist_prefix + dist_file
-                if dist_file not in ctx.input_files:  # File is used for the first time
-                    ctx.input_files[dist_file] = (os.path.getsize(path) / 287)
-                if ctx.input_files[dist_file] is False:  # It is known that this file doen't exists
-                    continue
-                ds = None
-                with open(path) as f:
-                    f.seek(randint(0, ctx.input_files[dist_file] - 1) * 287)
-                    ds = f.readline().split()
-                success = True
-            except Exception, e:
-                ctx.input_files[dist_file] = False  # Remeber that this file doen't exist
-                print '!! ERROR !! Can read dataset name from distribution file: %s' % e
-                if retry > 5:
-                    return 0
-        return ds
-
-    def shutdown(self, ctx):
-        monitor.record_gauge('panda.tasks.queue', 0)
-        monitor.record_gauge('panda.jobs.queue', 0)
-        monitor.record_gauge('panda.subs.queue', 0)
-        print '== PanDA [%s]: Persisting jobs: %s (first: %s, last: %s)' % (time.strftime('%D %H:%M:%S', time.localtime()), len(ctx.job_queue), time.strftime('%D %H:%M:%S', time.localtime(ctx.job_queue[0][0])),
-                                                                            time.strftime('%D %H:%M:%S', time.localtime(ctx.job_queue[-1][0])))
-        print '== PanDA [%s]: Persisting subs: %s (first: %s, last: %s)' % (time.strftime('%D %H:%M:%S', time.localtime()), len(ctx.sub_queue), time.strftime('%D %H:%M:%S', time.localtime(ctx.sub_queue[0][0])),
-                                                                            time.strftime('%D %H:%M:%S', time.localtime(ctx.sub_queue[-1][0])))
-        print '== PanDA [%s]: Persisting tasks: %s (first: %s, last: %s)' % (time.strftime('%D %H:%M:%S', time.localtime()), len(ctx.task_queue), time.strftime('%D %H:%M:%S', time.localtime(ctx.task_queue[0][0])),
-                                                                             time.strftime('%D %H:%M:%S', time.localtime(ctx.task_queue[-1][0])))
-
-        with ctx.job_queue_mutex:
-            with ctx.sub_queue_mutex:
-                with ctx.task_queue_mutex:
-                    with open('/data/emulator/panda.ctx', 'w') as f:
-                        pickle.dump([time.time(), ctx.job_queue, ctx.sub_queue, ctx.task_queue], f, pickle.HIGHEST_PROTOCOL)
-        print '== PanDA [%s]: Persisted context file.' % (time.strftime('%D %H:%M:%S', time.localtime()))
+        did_client.close(scope, dst[4])
+        metadata = did_client.get_metadata(scope, dst[4])
+        i_bytes, i_length = metadata['bytes'], metadata['length']
+        metadata = did_client.get_metadata(scope, file_[8])
+        file1_bytes = metadata['bytes']
+        metadata = did_client.get_metadata(scope, file_[9])
+        file2_bytes = metadata['bytes']
+        did_client.detach_dids(scope, dst[4], [{'scope': scope, 'name': file_[8]}, {'scope': scope, 'name': file_[9]}])
+        metadata = did_client.get_metadata(scope, dst[4])
+        f_bytes, f_length = metadata['bytes'], metadata['length']
+        assert i_bytes == f_bytes + file1_bytes + file2_bytes
+        assert i_length == f_length + 1 + 1
+
+    @pytest.mark.dirty
+    @pytest.mark.noparallel(reason='uses pre-defined RSE')
+    def test_scope_list(self, did_client, replica_client, rse_client, scope_client):
+        """ DATA IDENTIFIERS (CLIENT): Add, aggregate, and list data identifiers in a scope """
+
+        # create some dummy data
+        tmp_accounts = ['jdoe' for _ in range(3)]
+        tmp_scopes = [scope_name_generator() for _ in range(3)]
+        tmp_rses = [rse_name_generator() for _ in range(3)]
+        tmp_files = [did_name_generator('file') for _ in range(3)]
+        tmp_datasets = [did_name_generator('dataset') for _ in range(3)]
+        tmp_containers = [did_name_generator('container') for _ in range(3)]
+
+        # add dummy data to the catalogue
+        for i in range(3):
+            scope_client.add_scope(tmp_accounts[i], tmp_scopes[i])
+            rse_client.add_rse(tmp_rses[i])
+            replica_client.add_replica(tmp_rses[i], tmp_scopes[i], tmp_files[i], 1, '0cc737eb')
+
+        # put files in datasets
+        for i in range(3):
+            for j in range(3):
+                files = [{'scope': tmp_scopes[j], 'name': tmp_files[j], 'bytes': 1, 'adler32': '0cc737eb'}]
+                did_client.add_dataset(tmp_scopes[i], tmp_datasets[j])
+                did_client.add_files_to_dataset(tmp_scopes[i], tmp_datasets[j], files)
+
+        # put datasets in containers
+        for i in range(3):
+            for j in range(3):
+                datasets = [{'scope': tmp_scopes[j], 'name': tmp_datasets[j]}]
+                did_client.add_container(tmp_scopes[i], tmp_containers[j])
+                did_client.add_datasets_to_container(tmp_scopes[i], tmp_containers[j], datasets)
+
+        # reverse check if everything is in order
+        for i in range(3):
+            result = did_client.scope_list(tmp_scopes[i], recursive=True)
+
+            r_topdids = []
+            r_otherscopedids = []
+            r_scope = []
+            for r in result:
+                if r['level'] == 0:
+                    r_topdids.append(r['scope'] + ':' + r['name'])
+                    r_scope.append(r['scope'])
+                if r['scope'] != tmp_scopes[i]:
+                    r_otherscopedids.append(r['scope'] + ':' + r['name'])
+                    assert r['level'] in [1, 2]
+
+            for j in range(3):
+                assert tmp_scopes[i] == r_scope[j]
+                if j != i:
+                    assert tmp_scopes[j] + ':' + tmp_files[j] in r_otherscopedids
+            assert tmp_scopes[i] + ':' + tmp_files[i] not in r_topdids
+
+    @pytest.mark.dirty
+    @pytest.mark.noparallel(reason='uses pre-defined scope')
+    def test_get_did(self, did_client, replica_client, rse_factory):
+        """ DATA IDENTIFIERS (CLIENT): add a new data identifier and try to retrieve it back"""
+        rse, rse_id = rse_factory.make_mock_rse()
+        scope = 'mock'
+        file_ = did_name_generator('file')
+        dsn = did_name_generator('dataset')
+
+        replica_client.add_replica(rse, scope, file_, 1, '0cc737eb')
+
+        did = did_client.get_did(scope, file_)
+
+        assert did['scope'] == scope
+        assert did['name'] == file_
+
+        did_client.add_dataset(scope=scope, name=dsn, lifetime=10000000)
+        did2 = did_client.get_did(scope, dsn)
+        assert type(did2['expired_at']) == datetime
+
+    @pytest.mark.dirty
+    @pytest.mark.noparallel(reason='uses pre-defined scope')
+    def test_get_meta(self, did_client, replica_client, rse_factory):
+        """ DATA IDENTIFIERS (CLIENT): add a new meta data for an identifier and try to retrieve it back"""
+        rse, rse_id = rse_factory.make_mock_rse()
+        scope = 'mock'
+        file_ = did_name_generator('file')
+        keys = ['project', 'run_number']
+        values = ['data13_hip', 12345678]
+
+        replica_client.add_replica(rse, scope, file_, 1, '0cc737eb')
+        for i in range(2):
+            did_client.set_metadata(scope, file_, keys[i], values[i])
+
+        meta = did_client.get_metadata(scope, file_)
+
+        for i in range(2):
+            assert meta[keys[i]] == values[i]
+
+    @pytest.mark.dirty
+    @pytest.mark.noparallel(reason='uses pre-defined scope')
+    def test_list_content(self, did_client, rse_factory):
+        """ DATA IDENTIFIERS (CLIENT): test to list contents for an identifier"""
+        rse, rse_id = rse_factory.make_mock_rse()
+        scope = 'mock'
+        nbfiles = 5
+        dataset1 = did_name_generator('dataset')
+        dataset2 = did_name_generator('dataset')
+        container = did_name_generator('container')
+        files1 = [{'scope': scope, 'name': did_name_generator('file'), 'bytes': 1, 'adler32': '0cc737eb'} for i in range(nbfiles)]
+        files2 = [{'scope': scope, 'name': did_name_generator('file'), 'bytes': 1, 'adler32': '0cc737eb'} for i in range(nbfiles)]
+
+        did_client.add_dataset(scope, dataset1)
+
+        with pytest.raises(DataIdentifierAlreadyExists):
+            did_client.add_dataset(scope, dataset1)
+
+        did_client.add_files_to_dataset(scope, dataset1, files1, rse=rse)
+
+        did_client.add_dataset(scope, dataset2)
+        did_client.add_files_to_dataset(scope, dataset2, files2, rse=rse)
+
+        did_client.add_container(scope, container)
+        datasets = [{'scope': scope, 'name': dataset1}, {'scope': scope, 'name': dataset2}]
+        did_client.add_datasets_to_container(scope, container, datasets)
+
+        contents = did_client.list_content(scope, container)
+
+        datasets_s = [d['name'] for d in contents]
+        assert dataset1 in datasets_s
+        assert dataset2 in datasets_s
+
+    @pytest.mark.dirty
+    @pytest.mark.noparallel(reason='uses pre-defined scope')
+    def test_list_files(self, did_client, replica_client, rse_factory):
+        """ DATA IDENTIFIERS (CLIENT): List files for a container"""
+        rse, rse_id = rse_factory.make_mock_rse()
+        scope = 'mock'
+        dataset1 = did_name_generator('dataset')
+        dataset2 = did_name_generator('dataset')
+        container = did_name_generator('container')
+        files1 = []
+        files2 = []
+        for i in range(10):
+            files1.append({'scope': scope, 'name': did_name_generator('file'), 'bytes': 1, 'adler32': '0cc737eb'})
+            files2.append({'scope': scope, 'name': did_name_generator('file'), 'bytes': 1, 'adler32': '0cc737eb'})
+
+        for i in range(10):
+            replica_client.add_replica(rse, scope, files1[i]['name'], 1, '0cc737eb')
+            replica_client.add_replica(rse, scope, files2[i]['name'], 1, '0cc737eb')
+
+        did_client.add_dataset(scope, dataset1)
+        did_client.add_files_to_dataset(scope, dataset1, files1)
+
+        did_client.add_dataset(scope, dataset2)
+        did_client.add_files_to_dataset(scope, dataset2, files2)
+        datasets = [{'scope': scope, 'name': dataset1}, {'scope': scope, 'name': dataset2}]
+        did_client.add_container(scope, container)
+        did_client.add_datasets_to_container(scope, container, datasets)
+
+        # List file content
+        content = did_client.list_files(scope, files1[i]['name'])
+        assert content is not None
+        for d in content:
+            assert d['name'] == files1[i]['name']
+
+        # List container content
+        for d in [{'name': x['name'], 'scope': x['scope'], 'bytes': x['bytes'], 'adler32': x['adler32']} for x in did_client.list_files(scope, container)]:
+            assert d in files1 + files2
+
+        # List non-existing data identifier content
+        with pytest.raises(DataIdentifierNotFound):
+            did_client.list_files(scope, 'Nimportnawak')
+
+    @pytest.mark.dirty
+    @pytest.mark.noparallel(reason='uses pre-defined scope')
+    def test_list_replicas(self, did_client, replica_client, rse_factory):
+        """ DATA IDENTIFIERS (CLIENT): List replicas for a container"""
+        rse, rse_id = rse_factory.make_mock_rse()
+        scope = 'mock'
+        dsn1 = did_name_generator('dataset')
+        dsn2 = did_name_generator('dataset')
+        cnt = did_name_generator('container')
+        files1 = []
+        files2 = []
+        for i in range(10):
+            files1.append({'scope': scope, 'name': did_name_generator('file'), 'bytes': 1, 'adler32': '0cc737eb'})
+            files2.append({'scope': scope, 'name': did_name_generator('file'), 'bytes': 1, 'adler32': '0cc737eb'})
+
+        did_client.add_dataset(scope, dsn1)
+        did_client.add_files_to_dataset(scope, dsn1, files1, rse=rse)
+
+        did_client.add_dataset(scope, dsn2)
+        did_client.add_files_to_dataset(scope, dsn2, files2, rse=rse)
+
+        did_client.add_container(scope, cnt)
+        did_client.add_datasets_to_container(scope, cnt, [{'scope': scope, 'name': dsn1}, {'scope': scope, 'name': dsn2}])
+
+        replicas = replica_client.list_replicas(dids=[{'scope': scope, 'name': dsn1}])
+        assert replicas is not None
+
+        replicas = replica_client.list_replicas(dids=[{'scope': scope, 'name': cnt}])
+        assert replicas is not None
+
+    @pytest.mark.dirty
+    @pytest.mark.noparallel(reason='uses pre-defined scope')
+    def test_close(self, did_client, replica_client, rse_factory):
+        """ DATA IDENTIFIERS (CLIENT): test to close data identifiers"""
+
+        tmp_rse, rse_id = rse_factory.make_mock_rse()
+        tmp_scope = 'mock'
+
+        # Add dataset
+        tmp_dataset = did_name_generator('dataset')
+
+        # Add file replica
+        tmp_file = did_name_generator('file')
+        replica_client.add_replica(rse=tmp_rse, scope=tmp_scope, name=tmp_file, bytes_=1, adler32='0cc737eb')
+
+        # Add dataset
+        did_client.add_dataset(scope=tmp_scope, name=tmp_dataset)
+
+        # Add files to dataset
+        files = [{'scope': tmp_scope, 'name': tmp_file, 'bytes': 1, 'adler32': '0cc737eb'}, ]
+        did_client.add_files_to_dataset(scope=tmp_scope, name=tmp_dataset, files=files)
+
+        # Add a second file replica
+        tmp_file = did_name_generator('file')
+        replica_client.add_replica(tmp_rse, tmp_scope, tmp_file, 1, '0cc737eb')
+        # Add files to dataset
+        files = [{'scope': tmp_scope, 'name': tmp_file, 'bytes': 1, 'adler32': '0cc737eb'}, ]
+        did_client.add_files_to_dataset(scope=tmp_scope, name=tmp_dataset, files=files)
+
+        # Close dataset
+        with pytest.raises(UnsupportedStatus):
+            did_client.set_status(scope=tmp_scope, name=tmp_dataset, close=False)
+        did_client.set_status(scope=tmp_scope, name=tmp_dataset, open=False)
+
+        # Add a third file replica
+        tmp_file = did_name_generator('file')
+        replica_client.add_replica(tmp_rse, tmp_scope, tmp_file, 1, '0cc737eb')
+        # Add files to dataset
+        files = [{'scope': tmp_scope, 'name': tmp_file, 'bytes': 1, 'adler32': '0cc737eb'}, ]
+        with pytest.raises(exception.UnsupportedOperation):
+            did_client.attach_dids(scope=tmp_scope, name=tmp_dataset, dids=files)
+
+    @pytest.mark.dirty
+    @pytest.mark.noparallel(reason='uses pre-defined scope')
+    def test_open(self, did_client, replica_client, rse_factory):
+        """ DATA IDENTIFIERS (CLIENT): test to re-open data identifiers for priv account"""
+
+        tmp_rse, rse_id = rse_factory.make_mock_rse()
+        tmp_scope = 'mock'
+
+        # Add dataset
+        tmp_dataset = did_name_generator('dataset')
+
+        # Add file replica
+        tmp_file = did_name_generator('file')
+        replica_client.add_replica(rse=tmp_rse, scope=tmp_scope, name=tmp_file, bytes_=1, adler32='0cc737eb')
+
+        # Add dataset
+        did_client.add_dataset(scope=tmp_scope, name=tmp_dataset)
+
+        # Add files to dataset
+        files = [{'scope': tmp_scope, 'name': tmp_file, 'bytes': 1, 'adler32': '0cc737eb'}, ]
+        did_client.add_files_to_dataset(scope=tmp_scope, name=tmp_dataset, files=files)
+
+        # Add a second file replica
+        tmp_file = did_name_generator('file')
+        replica_client.add_replica(tmp_rse, tmp_scope, tmp_file, 1, '0cc737eb')
+        # Add files to dataset
+        files = [{'scope': tmp_scope, 'name': tmp_file, 'bytes': 1, 'adler32': '0cc737eb'}, ]
+        did_client.add_files_to_dataset(scope=tmp_scope, name=tmp_dataset, files=files)
+
+        # Close dataset
+        with pytest.raises(UnsupportedStatus):
+            did_client.set_status(scope=tmp_scope, name=tmp_dataset, close=False)
+        did_client.set_status(scope=tmp_scope, name=tmp_dataset, open=False)
+
+        # Add a third file replica
+        did_client.set_status(scope=tmp_scope, name=tmp_dataset, open=True)
+
+    @pytest.mark.dirty
+    @pytest.mark.noparallel(reason='uses pre-defined scope')
+    def test_bulk_get_meta(self, did_client, replica_client, rse_factory):
+        """ DATA IDENTIFIERS (CLIENT): Add a new meta data for a list of DIDs and try to retrieve them back"""
+        key = 'project'
+        rse, _ = rse_factory.make_mock_rse()
+        scope = 'mock'
+        files = [did_name_generator('file') for _ in range(4)]
+        dst = [did_name_generator('dataset') for _ in range(4)]
+        cnt = [did_name_generator('container') for _ in range(4)]
+        meta_mapping = {}
+        list_dids = []
+
+        # Set file metadata
+        for idx in range(4):
+            replica_client.add_replica(rse, scope, files[idx], 1, '0cc737eb')
+            did_client.set_metadata(scope, files[idx], key, 'file_%s' % idx)
+            list_dids.append({'scope': scope, 'name': files[idx]})
+            meta_mapping['%s:%s' % (scope, files[idx])] = (key, 'file_%s' % idx)
+
+        # Set dataset metadata
+        for idx in range(4):
+            did_client.add_did(scope, dst[idx], 'DATASET', statuses=None, meta={key: 'dsn_%s' % idx}, rules=None)
+            list_dids.append({'scope': scope, 'name': dst[idx]})
+            meta_mapping['%s:%s' % (scope, dst[idx])] = (key, 'dsn_%s' % idx)
+
+        # Set container metadata
+        for idx in range(4):
+            did_client.add_did(scope, cnt[idx], 'CONTAINER', statuses=None, meta={key: 'cnt_%s' % idx}, rules=None)
+            list_dids.append({'scope': scope, 'name': cnt[idx]})
+            meta_mapping['%s:%s' % (scope, cnt[idx])] = (key, 'cnt_%s' % idx)
+        # List metadata using the bulk method
+        list_meta = [_ for _ in did_client.get_metadata_bulk(list_dids)]
+        res_list_dids = [{'scope': entry['scope'], 'name': entry['name']} for entry in list_meta]
+        res_list_dids.sort(key=lambda item: item['name'])
+        list_dids.sort(key=lambda item: item['name'])
+        assert list_dids == res_list_dids
+        for meta in list_meta:
+            did = '%s:%s' % (meta['scope'], meta['name'])
+            met = meta_mapping[did]
+            assert (key, meta[key]) == met
+
+        # Create new containers without metadata
+        cnt = [did_name_generator('container') for _ in range(4)]
+        for idx in range(4):
+            list_dids.append({'scope': scope, 'name': cnt[idx]})
+        list_meta = [_ for _ in did_client.get_metadata_bulk(list_dids)]
+        assert len(list_meta) == 12
+        list_dids = []
+        for idx in range(4):
+            list_dids.append({'scope': scope, 'name': cnt[idx]})
+        list_meta = [_ for _ in did_client.get_metadata_bulk(list_dids)]
+        assert len(list_meta) == 0
+
+
+@pytest.mark.noparallel(reason='uses mock scope')
+def test_bulk_get_meta_inheritance(vo, rse_factory, mock_scope, did_factory, rucio_client):
+    """ DATA IDENTIFIERS (CLIENT): Add metadata for a hierarchical list of DIDs and check that the metadata are inherited"""
+    skip_without_json()
+    key = 'generic_metadata'
+    nb_dids = 4
+    scope = mock_scope.external
+    rse, _ = rse_factory.make_posix_rse()
+    meta_mapping = {}
+    list_dids = []
+
+    files = [did_factory.upload_test_file(rse) for _ in range(nb_dids)]
+    datasets = [did_factory.make_dataset() for _ in range(nb_dids)]
+    containers = [did_factory.make_container() for _ in range(nb_dids)]
+
+    # Set the files metadata
+    for idx, file_ in enumerate(files):
+        fkey = 'file_%s' % key
+        rucio_client.set_metadata(scope, file_['name'], fkey, 'file_%s' % idx)
+        list_dids.append({'scope': scope, 'name': file_['name']})
+        meta_mapping['%s:%s' % (scope, file_['name'])] = {fkey: 'file_%s' % idx}
+
+    # Set the datasets metadata
+    for idx, dataset in enumerate(datasets):
+        dkey = 'dst_%s' % key
+        rucio_client.set_metadata(scope, dataset['name'], dkey, 'dsn_%s' % idx)
+        rucio_client.attach_dids_to_dids([{'scope': scope, 'name': dataset['name'], 'dids': [{'scope': scope, 'name': files[idx]['name']}]}])
+        meta_mapping['%s:%s' % (scope, files[idx]['name'])][dkey] = 'dsn_%s' % idx
+
+    # Set the containers metadata
+    for idx, container in enumerate(containers):
+        ckey = 'cnt_%s' % key
+        rucio_client.set_metadata(scope, container['name'], ckey, 'cnt_%s' % idx)
+        rucio_client.attach_dids_to_dids([{'scope': scope, 'name': container['name'], 'dids': [{'scope': scope, 'name': datasets[idx]['name']}]}])
+        meta_mapping['%s:%s' % (scope, files[idx]['name'])][ckey] = 'cnt_%s' % idx
+
+    list_meta = [meta for meta in rucio_client.get_metadata_bulk(list_dids, inherit=True)]
+
+    for meta in list_meta:
+        did = '%s:%s' % (meta['scope'], meta['name'])
+        met = meta_mapping[did]
+        for key in met:
+            assert met[key] == meta[key]
+
+
+@pytest.mark.dirty
+@pytest.mark.noparallel(reason='uses pre-defined scope')
+def test_list_by_length(vo, root_account, rse_factory, mock_scope, did_factory, did_client):
+    """ DATA IDENTIFIERS (CLIENT): List did with length """
+
+    tmp_scope = scope_name_generator()
+    scope.add_scope(tmp_scope, 'root', 'root', vo)
+    rse, rse_id = rse_factory.make_posix_rse()
+    dataset = did_factory.upload_test_dataset(rse, tmp_scope)
+    set_status(InternalScope(tmp_scope, vo), dataset[0]['dataset_name'], open=False)
+
+    dids = did_client.list_dids(tmp_scope, {'length.gt': 0})
+    results = []
+    for d in dids:
+        results.append(d)
+    print(results)
+    assert len(results) != 0
+
+    dids = did_client.list_dids(tmp_scope, {'length.gt': -1, 'length.lt': 1})
+    results = []
+    for d in dids:
+        results.append(d)
+    assert len(results) == 0
+
+    dids = did_client.list_dids(tmp_scope, {'length': 0})
+    results = []
+    for d in dids:
+        results.append(d)
+    assert len(results) == 0
```

### Comparing `rucio-clients-1.9.6/lib/rucio/tests/test_curl.py` & `rucio-clients-32.0.0rc1/tests/test_curl.py`

 * *Files 21% similar despite different names*

```diff
@@ -1,135 +1,139 @@
-# Copyright European Organization for Nuclear Research (CERN)
+# -*- coding: utf-8 -*-
+# Copyright European Organization for Nuclear Research (CERN) since 2012
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
-# You may not use this file except in compliance with the License.
-# You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
 #
-# Authors:
-# - Vincent Garonne, <vincent.garonne@cern.ch>, 2012-2013
-# - Mario Lassnig, <mario.lassnig@cern.ch>, 2012, 2014
-
-# How to generate test outputs:
-#   nosetests --verbose --with-outputsave --save-directory=doc/source/example_outputs/ lib/rucio/tests/test_curl.py
+#    http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 
 import json
 import os
-import nose.tools
-
-from rucio.common.config import config_get
-from rucio.tests.common import account_name_generator, rse_name_generator, execute
-
 
-class TestCurlRucio():
+import pytest
 
-    @classmethod
-    def setUpClass(cls):
-        pass
+from rucio.common.config import config_get, config_get_bool
+from rucio.tests.common import account_name_generator, rse_name_generator, execute, get_long_vo
 
-    @classmethod
-    def tearDownClass(cls):
-        pass
 
-    def setup(self):
-        self.cacert = config_get('test', 'cacert')
-        self.usercert = config_get('test', 'usercert')
-        self.host = config_get('client', 'rucio_host')
-        self.auth_host = config_get('client', 'auth_host')
-        self.marker = '$> '
+class TestCurlRucio:
+    '''
+    class TestCurlRucio
+    '''
+
+    vo_header = ''
+    if config_get_bool('common', 'multi_vo', raise_exception=False, default=False):
+        vo_header = '-H "X-Rucio-VO: %s"' % get_long_vo()
+    cacert = config_get('test', 'cacert')
+    usercert = config_get('test', 'usercert')
+    userkey = config_get('test', 'userkey')
+    host = config_get('client', 'rucio_host')
+    auth_host = config_get('client', 'auth_host')
+    marker = '$> '
 
     def test_ping(self):
         """PING (CURL): Get Version"""
         cmd = 'curl --cacert %s -s -X GET %s/ping' % (self.cacert, self.host)
-        print self.marker + cmd
+        print(self.marker + cmd)
         exitcode, out, err = execute(cmd)
-        print out,
+        print(out, )
         ret = json.loads(out)
-        nose.tools.assert_true('version' in ret)
-        nose.tools.assert_is_instance(ret, dict)
+        assert 'version' in ret
+        assert isinstance(ret, dict)
 
     def test_get_auth_userpass(self):
         """AUTH (CURL): Test auth token retrieval with via username and password"""
-        cmd = 'curl -s -i --cacert %s -X GET -H "X-Rucio-Account: root" -H "X-Rucio-Username: ddmlab" -H "X-Rucio-Password: secret" %s/auth/userpass' % (self.cacert, self.auth_host)
-        print self.marker + cmd
+        cmd = 'curl -s -i --cacert %s -X GET -H "X-Rucio-Account: root" -H "X-Rucio-Username: ddmlab" -H "X-Rucio-Password: secret" %s %s/auth/userpass' % (self.cacert, self.vo_header, self.auth_host)
+        print(self.marker + cmd)
         exitcode, out, err = execute(cmd)
-        print out,
-        nose.tools.assert_in('X-Rucio-Auth-Token', out)
+        print(out, )
+        assert 'X-Rucio-Auth-Token' in out
 
     def test_get_auth_x509(self):
         """AUTH (CURL): Test auth token retrieval with via x509"""
-        cmd = 'curl -s -i --cacert %s -H "X-Rucio-Account: root" -E %s -X GET %s/auth/x509' % (self.cacert, self.usercert, self.auth_host)
-        print self.marker + cmd
+        cmd = 'curl -s -i --cacert %s -H "X-Rucio-Account: root" %s -cert %s --key %s -X GET %s/auth/x509' % (self.cacert, self.vo_header, self.usercert, self.userkey, self.auth_host)
+        print(self.marker + cmd)
         exitcode, out, err = execute(cmd)
-        print out,
-        nose.tools.assert_in('X-Rucio-Auth-Token', out)
+        print(out, )
+        assert 'X-Rucio-Auth-Token' in out
 
     def test_get_auth_GSS(self):
         """AUTH (CURL): Test auth token retrieval with via gss"""
-        cmd = 'curl -s -i --cacert %s -H "X-Rucio-Account: root" -E %s -X GET %s/auth/x509 | grep X-Rucio-Auth-Token' % (self.cacert, self.usercert, self.auth_host)
+        cmd = 'curl -s -i --cacert %s -H "X-Rucio-Account: root" %s --cert %s --key %s -X GET %s/auth/x509 | tr -d \'\r\' | grep X-Rucio-Auth-Token' % (self.cacert, self.vo_header, self.usercert, self.userkey, self.auth_host)
         exitcode, out, err = execute(cmd)
-        nose.tools.assert_in('X-Rucio-Auth-Token', out)
-        os.environ['RUCIO_TOKEN'] = out[len('X-Rucio-Auth-Token: '):-1]
-        cmd = 'curl -s -i --cacert %s -H "X-Rucio-Account: root" --negotiate -u: -X GET %s/auth/gss' % (self.cacert, self.auth_host)
-        print self.marker + cmd
+        assert 'X-Rucio-Auth-Token' in out
+        os.environ['RUCIO_TOKEN'] = out[len('X-Rucio-Auth-Token: '):].rstrip()
+        cmd = 'curl -s -i --cacert %s -H "X-Rucio-Account: root" %s --negotiate -u: -X GET %s/auth/gss' % (self.cacert, self.vo_header, self.auth_host)
+        print(self.marker + cmd)
         exitcode, out, err = execute(cmd)
-        print out,
-        # nose.tools.assert_in('X-Rucio-Auth-Token', out)
+        print(out, )
+        # assert 'X-Rucio-Auth-Token' in out
 
     def test_get_auth_x509_proxy(self):
         """AUTH (CURL): Test auth token retrieval with via proxy"""
-        cmd = 'curl -s -i --cacert %s -H "X-Rucio-Account: root" -E %s -X GET %s/auth/x509 | grep X-Rucio-Auth-Token' % (self.cacert, self.usercert, self.auth_host)
+        cmd = 'curl -s -i --cacert %s -H "X-Rucio-Account: root" %s --cert %s --key %s -X GET %s/auth/x509 | tr -d \'\r\' | grep X-Rucio-Auth-Token' % (self.cacert, self.vo_header, self.usercert, self.userkey, self.auth_host)
         exitcode, out, err = execute(cmd)
-        nose.tools.assert_in('X-Rucio-Auth-Token', out)
-        os.environ['RUCIO_TOKEN'] = out[len('X-Rucio-Auth-Token: '):-1]
-        cmd = 'curl -s -i --cacert %s -H "X-Rucio-Account: vgaronne" --cert $X509_USER_PROXY --key $X509_USER_PROXY -X GET %s/auth/x509_proxy' % (self.cacert, self.auth_host)
-        print self.marker + cmd
+        assert 'X-Rucio-Auth-Token' in out
+        os.environ['RUCIO_TOKEN'] = out[len('X-Rucio-Auth-Token: '):].rstrip()
+        cmd = 'curl -s -i --cacert %s -H "X-Rucio-Account: vgaronne" %s --cert $X509_USER_PROXY --key $X509_USER_PROXY -X GET %s/auth/x509_proxy' % (self.cacert, self.vo_header, self.auth_host)
+        print(self.marker + cmd)
         exitcode, out, err = execute(cmd)
-        print out,
-        # nose.tools.assert_in('X-Rucio-Auth-Token', out)
+        print(out, )
+        # assert 'X-Rucio-Auth-Token' in out
 
     def test_get_auth_validate(self):
         """AUTH (CURL): Test if token is valid"""
-        cmd = 'curl -s -i --cacert %s -H "X-Rucio-Account: root" -E %s -X GET %s/auth/x509 | grep X-Rucio-Auth-Token:' % (self.cacert, self.usercert, self.auth_host)
+        cmd = 'curl -s -i --cacert %s -H "X-Rucio-Account: root" %s --cert %s --key %s -X GET %s/auth/x509 | tr -d \'\r\' | grep X-Rucio-Auth-Token:' % (self.cacert, self.vo_header, self.usercert, self.userkey, self.auth_host)
         exitcode, out, err = execute(cmd)
-        nose.tools.assert_in('X-Rucio-Auth-Token', out)
-        os.environ['RUCIO_TOKEN'] = out[len('X-Rucio-Auth-Token: '):-1]
+        assert 'X-Rucio-Auth-Token' in out
+        os.environ['RUCIO_TOKEN'] = out[len('X-Rucio-Auth-Token: '):].rstrip()
         cmd = 'curl -s -i --cacert %s  -H "X-Rucio-Auth-Token: $RUCIO_TOKEN" -X GET %s/auth/validate' % (self.cacert, self.auth_host)
-        print self.marker + cmd
+        print(self.marker + cmd)
         exitcode, out, err = execute(cmd)
-        print out
-        nose.tools.assert_in('datetime.datetime', out)
+        print(out)
+        assert 'datetime.datetime' in out
 
+    @pytest.mark.dirty
     def test_post_account(self):
         """ACCOUNT (CURL): add account"""
-        cmd = 'curl -s -i --cacert %s -H "X-Rucio-Account: root" -E %s -X GET %s/auth/x509 | grep X-Rucio-Auth-Token:' % (self.cacert, self.usercert, self.auth_host)
+        cmd = 'curl -s -i --cacert %s -H "X-Rucio-Account: root" %s --cert %s --key %s -X GET %s/auth/x509 | tr -d \'\r\' | grep X-Rucio-Auth-Token:' % (self.cacert, self.vo_header, self.usercert, self.userkey, self.auth_host)
         exitcode, out, err = execute(cmd)
-        nose.tools.assert_in('X-Rucio-Auth-Token', out)
-        os.environ['RUCIO_TOKEN'] = out[len('X-Rucio-Auth-Token: '):-1]
-        cmd = '''curl -s -i --cacert %s -H "X-Rucio-Auth-Token: $RUCIO_TOKEN" -H "Rucio-Type: user" -d '{"type": "USER", "email": "rucio@email.com"}' -X POST %s/accounts/%s''' % (self.cacert, self.host, account_name_generator())
-        print self.marker + cmd
+        assert 'X-Rucio-Auth-Token' in out
+        os.environ['RUCIO_TOKEN'] = out[len('X-Rucio-Auth-Token: '):].rstrip()
+        cmd = '''curl -s -i --cacert %s -H "X-Rucio-Auth-Token: $RUCIO_TOKEN" -H "Rucio-Type: user" -H "Content-Type: application/json" -d '{"type": "USER", "email": "rucio@email.com"}' -X POST %s/accounts/%s''' % (self.cacert, self.host, account_name_generator())
+        print(self.marker + cmd)
         exitcode, out, err = execute(cmd)
-        print out
-        nose.tools.assert_in('201 Created', out)
+        print(out)
+        assert '201 Created'.lower() in out.lower()
 
     def test_get_accounts_whoami(self):
         """ACCOUNT (CURL): Test whoami method"""
-        cmd = 'curl -s -i --cacert %s -H "X-Rucio-Account: root" -E %s -X GET %s/auth/x509 | grep X-Rucio-Auth-Token:' % (self.cacert, self.usercert, self.auth_host)
+        cmd = 'curl -s -i --cacert %s -H "X-Rucio-Account: root" %s --cert %s --key %s -X GET %s/auth/x509 | tr -d \'\r\' | grep X-Rucio-Auth-Token:' % (self.cacert, self.vo_header, self.usercert, self.userkey, self.auth_host)
+        print(cmd)
         exitcode, out, err = execute(cmd)
-        nose.tools.assert_in('X-Rucio-Auth-Token', out)
-        os.environ['RUCIO_TOKEN'] = out[len('X-Rucio-Auth-Token: '):-1]
+        assert 'X-Rucio-Auth-Token' in out
+        os.environ['RUCIO_TOKEN'] = out[len('X-Rucio-Auth-Token: '):].rstrip()
         cmd = '''curl -s -i -L --cacert %s -H "X-Rucio-Auth-Token: $RUCIO_TOKEN" -X GET %s/accounts/whoami''' % (self.cacert, self.host)
-        print self.marker + cmd
+        print(self.marker + cmd)
         exitcode, out, err = execute(cmd)
-        print out
-        nose.tools.assert_in('303 See Other', out)
+        print(out)
+        assert '303 See Other'.lower() in out.lower()
 
+    @pytest.mark.dirty
     def test_post_rse(self):
         """RSE (CURL): add RSE"""
-        cmd = 'curl -s -i --cacert %s -H "X-Rucio-Account: root" -E %s -X GET %s/auth/x509 | grep X-Rucio-Auth-Token:' % (self.cacert, self.usercert, self.auth_host)
+        cmd = 'curl -s -i --cacert %s -H "X-Rucio-Account: root" %s --cert %s --key %s -X GET %s/auth/x509 | tr -d \'\r\' | grep X-Rucio-Auth-Token:' % (self.cacert, self.vo_header, self.usercert, self.userkey, self.auth_host)
         exitcode, out, err = execute(cmd)
-        nose.tools.assert_in('X-Rucio-Auth-Token', out)
-        os.environ['RUCIO_TOKEN'] = out[len('X-Rucio-Auth-Token: '):-1]
+        assert 'X-Rucio-Auth-Token' in out
+        os.environ['RUCIO_TOKEN'] = out[len('X-Rucio-Auth-Token: '):].rstrip()
         cmd = '''curl -s -i --cacert %s -H "X-Rucio-Auth-Token: $RUCIO_TOKEN" -X POST %s/rses/%s''' % (self.cacert, self.host, rse_name_generator())
-        print self.marker + cmd
+        print(self.marker + cmd)
         exitcode, out, err = execute(cmd)
-        print out
-        nose.tools.assert_in('201 Created', out)
+        print(out)
+        assert '201 Created'.lower() in out.lower()
```

### Comparing `rucio-clients-1.9.6/lib/rucio/tests/test_dumper_consistency.py` & `rucio-clients-32.0.0rc1/tests/test_dumper_consistency.py`

 * *Files 18% similar despite different names*

```diff
@@ -1,35 +1,77 @@
-
-# Copyright European Organization for Nuclear Research (CERN)
+# -*- coding: utf-8 -*-
+# Copyright European Organization for Nuclear Research (CERN) since 2012
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
-# You may not use this file except in compliance with the License.
-# You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#    http://www.apache.org/licenses/LICENSE-2.0
 #
-# Authors:
-# - Fernando Lopez, <felopez@cern.ch>, 2015
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+import json
+import os
 from datetime import datetime
-from nose.tools import eq_
-from nose.tools import ok_
-from rucio.common import dumper
+from unittest import mock
+
 from rucio.common.dumper.consistency import Consistency
 from rucio.common.dumper.consistency import _try_to_advance
 from rucio.common.dumper.consistency import compare3
 from rucio.common.dumper.consistency import gnu_sort
 from rucio.common.dumper.consistency import min3
 from rucio.common.dumper.consistency import parse_and_filter_file
 from rucio.tests.common import make_temp_file
-from rucio.tests.common import stubbed
-import os
-import requests
-import shutil
-import tempfile
 
-
-class TestConsistency(object):
+RSEPROTOCOL = {
+    "hostname": "example.com",
+    "scheme": "root",
+    "port": 1094,
+    "prefix": "//atlasdatadisk/rucio/",
+    "domains": {
+        "wan": {
+            "read": 1,
+        }
+    },
+}
+
+
+def mocked_requests(*args, **kwargs):
+    class MockResponse:
+        def __init__(self, json_data, status_code):
+            self.status_code = status_code
+            self.json_data = json_data
+            self.text = json.dumps(json_data)
+            self.iter_content = lambda _: json_data
+
+        def json(self):
+            return self.json_data
+
+    rucio_dump_1 = (
+        'MOCK_SCRATCHDISK\tuser.someuser\tuser.someuser.filename\t19028d77\t189468\t2015-09-20 21:22:04\tuser/someuser/aa/bb/user.someuser.filename\t2015-09-20 21:22:17\tA\n'
+        'MOCK_SCRATCHDISK\tuser.someuser\tuser.someuser.lost\t19028d77\t189468\t2015-09-20 21:22:04\tuser/someuser/aa/bb/user.someuser.lost\t2015-09-20 21:22:17\tA\n'
+    )
+    rucio_dump_2 = (
+        'MOCK_SCRATCHDISK\tuser.someuser\tuser.someuser.filename\t19028d77\t189468\t2015-09-20 21:22:04\tuser/someuser/aa/bb/user.someuser.filename\t2015-09-20 21:22:17\tA\n'
+        'MOCK_SCRATCHDISK\tuser.someuser\tuser.someuser.lost\t19028d77\t189468\t2015-09-20 21:22:04\tuser/someuser/aa/bb/user.someuser.lost\t2015-09-20 21:22:17\tA\n'
+    )
+    if '29-09-2015' in args[0]:
+        return MockResponse([rucio_dump_1], 200)
+    else:
+        return MockResponse([rucio_dump_2], 200)
+
+
+class TestConsistency:
+    '''
+    TestConsistency
+    '''
     case_mixed_rrd_1 = [
         'path1,A',
         'path20,U',
         'path01,U',
         'path23,U',
         'path26,A',
         'path6,A',
@@ -45,347 +87,327 @@
         'path1,A',
         'path26,A',
         'path01,U',
         'path6,A',
         'path20,A',
     ]
 
-    def setUp(self):
-        self.tmp_dir = tempfile.mkdtemp()
-        self.fake_agis_data = lambda _: [{
-            'name': 'MOCK_SCRATCHDISK',
-            'se': 'srm://example.com:8446/',
-            'endpoint': '/pnfs/example.com/atlas/atlasdatadisk/'
-        }]
-
-    def teardown(self):
-        shutil.rmtree(self.tmp_dir)
-
-    def test_consistency_manual_correct_file_default_args(self):
+    @mock.patch('rucio.common.dumper.ddmendpoint_preferred_protocol')
+    def test_consistency_manual_correct_file_default_args(self, mock_get, tmp_path):
+        ''' DUMPER '''
         rucio_dump = 'MOCK_SCRATCHDISK\tuser.someuser\tuser.someuser.filename\t19028d77\t189468\t2015-09-20 21:22:04\tuser/someuser/aa/bb/user.someuser.filename\t2015-09-20 21:22:17\tA\n'
         storage_dump = 'user/someuser/aa/bb/user.someuser.filename\n'
 
-        rrdf1 = make_temp_file(self.tmp_dir, rucio_dump)
-        rrdf2 = make_temp_file(self.tmp_dir, rucio_dump)
-        sdf = make_temp_file(self.tmp_dir, storage_dump)
-
-        with stubbed(dumper.agis_endpoints_data, self.fake_agis_data):
-            consistency = Consistency.dump(
-                'consistency-manual',
-                'MOCK_SCRATCHDISK',
-                sdf,
-                prev_date_fname=rrdf1,
-                next_date_fname=rrdf2,
-                cache_dir=self.tmp_dir,
-            )
-            eq_(len(list(consistency)), 0)
+        rrdf1 = make_temp_file(tmp_path, rucio_dump)
+        rrdf2 = make_temp_file(tmp_path, rucio_dump)
+        sdf = make_temp_file(tmp_path, storage_dump)
+
+        mock_get.return_value = RSEPROTOCOL
+        consistency = Consistency.dump(
+            'consistency-manual',
+            'MOCK_SCRATCHDISK',
+            sdf,
+            prev_date_fname=rrdf1,
+            next_date_fname=rrdf2,
+            cache_dir=tmp_path,
+        )
+        assert len(list(consistency)) == 0
 
-    def test_consistency_manual_lost_file(self):
+    @mock.patch('rucio.common.dumper.ddmendpoint_preferred_protocol')
+    def test_consistency_manual_lost_file(self, mock_get, tmp_path):
+        ''' DUMPER '''
         rucio_dump = 'MOCK_SCRATCHDISK\tuser.someuser\tuser.someuser.filename\t19028d77\t189468\t2015-09-20 21:22:04\tuser/someuser/aa/bb/user.someuser.filename\t2015-09-20 21:22:17\tA\n'
         rucio_dump += 'MOCK_SCRATCHDISK\tuser.someuser\tuser.someuser.filename2\t19028d77\t189468\t2015-09-20 21:22:04\tuser/someuser/aa/bb/user.someuser.filename2\t2015-09-20 21:22:17\tA\n'
         storage_dump = 'user/someuser/aa/bb/user.someuser.filename\n'
 
-        rrdf1 = make_temp_file(self.tmp_dir, rucio_dump)
-        rrdf2 = make_temp_file(self.tmp_dir, rucio_dump)
-        sdf = make_temp_file(self.tmp_dir, storage_dump)
-
-        with stubbed(dumper.agis_endpoints_data, self.fake_agis_data):
-            consistency = Consistency.dump(
-                'consistency-manual',
-                'MOCK_SCRATCHDISK',
-                sdf,
-                prev_date_fname=rrdf1,
-                next_date_fname=rrdf2,
-                cache_dir=self.tmp_dir,
-            )
-            consistency = list(consistency)
-        eq_(len(consistency), 1)
-        eq_(consistency[0].apparent_status, 'LOST')
-        eq_(consistency[0].path, 'user/someuser/aa/bb/user.someuser.filename2')
-
-    def test_consistency_manual_transient_file_is_not_lost(self):
+        rrdf1 = make_temp_file(tmp_path, rucio_dump)
+        rrdf2 = make_temp_file(tmp_path, rucio_dump)
+        sdf = make_temp_file(tmp_path, storage_dump)
+
+        mock_get.return_value = RSEPROTOCOL
+
+        consistency = Consistency.dump(
+            'consistency-manual',
+            'MOCK_SCRATCHDISK',
+            sdf,
+            prev_date_fname=rrdf1,
+            next_date_fname=rrdf2,
+            cache_dir=tmp_path,
+        )
+        consistency = list(consistency)
+        assert len(consistency) == 1
+        assert consistency[0].apparent_status == 'LOST'
+        assert consistency[0].path == 'user/someuser/aa/bb/user.someuser.filename2'
+
+    @mock.patch('rucio.common.dumper.ddmendpoint_preferred_protocol')
+    def test_consistency_manual_transient_file_is_not_lost(self, mock_get, tmp_path):
+        ''' DUMPER '''
         rucio_dump = 'MOCK_SCRATCHDISK\tuser.someuser\tuser.someuser.filename\t19028d77\t189468\t2015-09-20 21:22:04\tuser/someuser/aa/bb/user.someuser.filename\t2015-09-20 21:22:17\tA\n'
         rucio_dump_1 = rucio_dump + 'MOCK_SCRATCHDISK\tuser.someuser\tuser.someuser.filename2\t19028d77\t189468\t2015-09-20 21:22:04\tuser/someuser/aa/bb/user.someuser.filename2\t2015-09-20 21:22:17\tU\n'
         rucio_dump_2 = rucio_dump + 'MOCK_SCRATCHDISK\tuser.someuser\tuser.someuser.filename2\t19028d77\t189468\t2015-09-20 21:22:04\tuser/someuser/aa/bb/user.someuser.filename2\t2015-09-20 21:22:17\tA\n'
         storage_dump = 'user/someuser/aa/bb/user.someuser.filename\n'
 
-        rrdf1 = make_temp_file(self.tmp_dir, rucio_dump_1)
-        rrdf2 = make_temp_file(self.tmp_dir, rucio_dump_2)
-        sdf = make_temp_file(self.tmp_dir, storage_dump)
-
-        with stubbed(dumper.agis_endpoints_data, self.fake_agis_data):
-            consistency = Consistency.dump(
-                'consistency-manual',
-                'MOCK_SCRATCHDISK',
-                sdf,
-                prev_date_fname=rrdf1,
-                next_date_fname=rrdf2,
-                cache_dir=self.tmp_dir,
-            )
-            eq_(len(list(consistency)), 0)
+        rrdf1 = make_temp_file(tmp_path, rucio_dump_1)
+        rrdf2 = make_temp_file(tmp_path, rucio_dump_2)
+        sdf = make_temp_file(tmp_path, storage_dump)
+
+        mock_get.return_value = RSEPROTOCOL
+
+        consistency = Consistency.dump(
+            'consistency-manual',
+            'MOCK_SCRATCHDISK',
+            sdf,
+            prev_date_fname=rrdf1,
+            next_date_fname=rrdf2,
+            cache_dir=tmp_path,
+        )
+        assert len(list(consistency)) == 0
 
-    def test_consistency_manual_dark_file(self):
+    @mock.patch('rucio.common.dumper.ddmendpoint_preferred_protocol')
+    def test_consistency_manual_dark_file(self, mock_get, tmp_path):
+        ''' DUMPER '''
         rucio_dump = 'MOCK_SCRATCHDISK\tuser.someuser\tuser.someuser.filename\t19028d77\t189468\t2015-09-20 21:22:04\tuser/someuser/aa/bb/user.someuser.filename\t2015-09-20 21:22:17\tA\n'
         storage_dump = 'user/someuser/aa/bb/user.someuser.filename\n'
         storage_dump += 'user/someuser/aa/bb/user.someuser.filename2\n'
 
-        rrdf1 = make_temp_file(self.tmp_dir, rucio_dump)
-        rrdf2 = make_temp_file(self.tmp_dir, rucio_dump)
-        sdf = make_temp_file(self.tmp_dir, storage_dump)
-        with stubbed(dumper.agis_endpoints_data, self.fake_agis_data):
-            consistency = Consistency.dump(
-                'consistency-manual',
-                'MOCK_SCRATCHDISK',
-                sdf,
-                prev_date_fname=rrdf1,
-                next_date_fname=rrdf2,
-                cache_dir=self.tmp_dir,
-            )
-            consistency = list(consistency)
-
-        eq_(len(consistency), 1)
-        eq_(consistency[0].apparent_status, 'DARK')
-        eq_(consistency[0].path, 'user/someuser/aa/bb/user.someuser.filename2')
+        rrdf1 = make_temp_file(tmp_path, rucio_dump)
+        rrdf2 = make_temp_file(tmp_path, rucio_dump)
+        sdf = make_temp_file(tmp_path, storage_dump)
+
+        mock_get.return_value = RSEPROTOCOL
+
+        consistency = Consistency.dump(
+            'consistency-manual',
+            'MOCK_SCRATCHDISK',
+            sdf,
+            prev_date_fname=rrdf1,
+            next_date_fname=rrdf2,
+            cache_dir=tmp_path,
+        )
+        consistency = list(consistency)
 
-    def test_consistency_manual_multiple_slashes_in_storage_dump_do_not_generate_false_positive(self):
+        assert len(consistency) == 1
+        assert consistency[0].apparent_status == 'DARK'
+        assert consistency[0].path == 'user/someuser/aa/bb/user.someuser.filename2'
+
+    @mock.patch('rucio.common.dumper.ddmendpoint_preferred_protocol')
+    def test_consistency_manual_multiple_slashes_in_storage_dump_do_not_generate_false_positive(self, mock_get, tmp_path):
+        ''' DUMPER '''
         rucio_dump = 'MOCK_SCRATCHDISK\tuser.someuser\tuser.someuser.filename\t19028d77\t189468\t2015-09-20 21:22:04\tuser/someuser/aa/bb/user.someuser.filename\t2015-09-20 21:22:17\tA\n'
-        storage_dump = '/pnfs/example.com/atlas///atlasdatadisk/rucio//user/someuser/aa/bb/user.someuser.filename\n'
+        storage_dump = '/example.com:1094////atlasdatadisk/rucio//user/someuser/aa/bb/user.someuser.filename\n'
 
-        rrdf1 = make_temp_file(self.tmp_dir, rucio_dump)
-        rrdf2 = make_temp_file(self.tmp_dir, rucio_dump)
-        sdf = make_temp_file(self.tmp_dir, storage_dump)
-        with stubbed(dumper.agis_endpoints_data, self.fake_agis_data):
-            consistency = Consistency.dump(
-                'consistency-manual',
-                'MOCK_SCRATCHDISK',
-                sdf,
-                prev_date_fname=rrdf1,
-                next_date_fname=rrdf2,
-                cache_dir=self.tmp_dir,
-            )
-            consistency = list(consistency)
-
-        eq_(len(consistency), 0, [e.csv() for e in consistency])
-
-    def test_consistency(self):
-        rucio_dump_1 = (
-            'MOCK_SCRATCHDISK\tuser.someuser\tuser.someuser.filename\t19028d77\t189468\t2015-09-20 21:22:04\tuser/someuser/aa/bb/user.someuser.filename\t2015-09-20 21:22:17\tA\n'
-            'MOCK_SCRATCHDISK\tuser.someuser\tuser.someuser.lost\t19028d77\t189468\t2015-09-20 21:22:04\tuser/someuser/aa/bb/user.someuser.lost\t2015-09-20 21:22:17\tA\n'
-        )
-        rucio_dump_2 = (
-            'MOCK_SCRATCHDISK\tuser.someuser\tuser.someuser.filename\t19028d77\t189468\t2015-09-20 21:22:04\tuser/someuser/aa/bb/user.someuser.filename\t2015-09-20 21:22:17\tA\n'
-            'MOCK_SCRATCHDISK\tuser.someuser\tuser.someuser.lost\t19028d77\t189468\t2015-09-20 21:22:04\tuser/someuser/aa/bb/user.someuser.lost\t2015-09-20 21:22:17\tA\n'
+        rrdf1 = make_temp_file(tmp_path, rucio_dump)
+        rrdf2 = make_temp_file(tmp_path, rucio_dump)
+        sdf = make_temp_file(tmp_path, storage_dump)
+
+        mock_get.return_value = RSEPROTOCOL
+
+        consistency = Consistency.dump(
+            'consistency-manual',
+            'MOCK_SCRATCHDISK',
+            sdf,
+            prev_date_fname=rrdf1,
+            next_date_fname=rrdf2,
+            cache_dir=tmp_path,
         )
+        consistency = list(consistency)
+
+        assert len(consistency) == 0
+
+    @mock.patch('requests.Session.head', side_effect=mocked_requests)
+    @mock.patch('requests.Session.get', side_effect=mocked_requests)
+    @mock.patch('rucio.common.dumper.ddmendpoint_preferred_protocol', return_value=RSEPROTOCOL)
+    def test_consistency(self, mock_dumper_get, mock_request_get, mock_request_head, tmp_path):
+        ''' DUMPER '''
         storage_dump = (
-            '/pnfs/example.com/atlas///atlasdatadisk/rucio//user/someuser/aa/bb/user.someuser.filename\n'
-            '/pnfs/example.com/atlas///atlasdatadisk/rucio//user/someuser/aa/bb/user.someuser.dark\n'
+            '//atlasdatadisk/rucio/user/someuser/aa/bb/user.someuser.filename\n'
+            '//atlasdatadisk/rucio/user/someuser/aa/bb/user.someuser.dark\n'
         )
-        sd = make_temp_file(self.tmp_dir, storage_dump)
+        sd = make_temp_file(tmp_path, storage_dump)
 
-        def fake_get(slf, url, stream=False):
-            response = requests.Response()
-            response.status_code = 200
-            if '29-09-2015' in url:
-                response.iter_content = lambda _: [rucio_dump_1]
-            else:
-                response.iter_content = lambda _: [rucio_dump_2]
-            return response
-
-        def fake_head(slf, url):
-            response = requests.Response()
-            response.status_code = 200
-            return response
-
-        agisdata = [{
-            'name': 'MOCK_SCRATCHDISK',
-            'se': 'srm://example.com/',
-            'endpoint': 'pnfs/example.com/atlas/atlasdatadisk/',
-        }]
-        with stubbed(dumper.agis_endpoints_data, lambda: agisdata):
-            with stubbed(requests.Session.get, fake_get):
-                with stubbed(requests.Session.head, fake_head):
-                        consistency = Consistency.dump(
-                            'consistency',
-                            'MOCK_SCRATCHDISK',
-                            storage_dump=sd,
-                            prev_date=datetime(2015, 9, 29),
-                            next_date=datetime(2015, 10, 4),
-                            cache_dir=self.tmp_dir,
-                        )
-                        consistency = list(consistency)
+        consistency = Consistency.dump('consistency',
+                                       'MOCK_SCRATCHDISK',
+                                       storage_dump=sd,
+                                       prev_date=datetime(2015, 9, 29),
+                                       next_date=datetime(2015, 10, 4),
+                                       cache_dir=tmp_path)
+        consistency = list(consistency)
 
-        eq_(len(consistency), 2)
+        assert len(consistency) == 2
         dark = next(
             entry.path for entry in consistency if entry.apparent_status == 'DARK'
         )
         lost = next(
             entry.path for entry in consistency if entry.apparent_status == 'LOST'
         )
-        ok_('user.someuser.dark' in dark)
-        ok_('user.someuser.lost' in lost)
+        assert 'user.someuser.dark' in dark
+        assert 'user.someuser.lost' in lost
 
-    def test__try_to_advance(self):
+    def test__try_to_advance(self, tmp_path):
+        ''' DUMPER '''
         i = iter(['   abc  '])
-        eq_(_try_to_advance(i), 'abc')
-        eq_(_try_to_advance(i), None)
-        eq_(_try_to_advance(i, 42), 42)
+        assert _try_to_advance(i) == 'abc'
+        assert _try_to_advance(i) is None
+        assert _try_to_advance(i, 42) == 42
 
-    def test_compare3(self):
+    def test_compare3(self, tmp_path):
+        ''' DUMPER '''
         sorted_rdd_1 = sorted(self.case_mixed_rrd_1, key=lambda s: s.split(',')[0])
         sorted_rdd_2 = sorted(self.case_mixed_rrd_2, key=lambda s: s.split(',')[0])
         sorted_sed = sorted(self.case_mixed_sed)
 
-        comp = list(compare3(sorted_rdd_1, sorted_sed, sorted_rdd_2))
-
-        eq_(
-            sorted(comp),
-            sorted([
-                ('path1', (True, True, True), ('A', 'A')),
-                ('path20', (True, True, True), ('U', 'A')),
-                ('path01', (True, False, True), ('U', 'U')),
-                ('path23', (True, False, False), ('U', None)),
-                ('path26', (True, False, True), ('A', 'A')),
-                ('path6', (True, False, True), ('A', 'A')),
-                ('path66', (False, True, False), (None, None)),
-                ('path46', (False, True, False), (None, None)),
-                ('pathsda', (False, True, False), (None, None)),
-            ]),
-        )
+        value = sorted(list(compare3(sorted_rdd_1, sorted_sed, sorted_rdd_2)))
+        expected = sorted([
+            ('path1', (True, True, True), ('A', 'A')),
+            ('path20', (True, True, True), ('U', 'A')),
+            ('path01', (True, False, True), ('U', 'U')),
+            ('path23', (True, False, False), ('U', None)),
+            ('path26', (True, False, True), ('A', 'A')),
+            ('path6', (True, False, True), ('A', 'A')),
+            ('path66', (False, True, False), (None, None)),
+            ('path46', (False, True, False), (None, None)),
+            ('pathsda', (False, True, False), (None, None)),
+        ])
+        assert value == expected
 
-    def test_compare3_file_name_with_comma_in_storage_dump_ATLDDMOPS_4105(self):
+    def test_compare3_file_name_with_comma_in_storage_dump_ATLDDMOPS_4105(self, tmp_path):
+        ''' DUMPER '''
         rucio_replica_dump = 'user/mfauccig/8d/46/user.mfauccig.410000.PowhegPythiaEvtGen.DAOD_TOPQ1.e3698_s2608_s2183_r6630_r6264_p2377.v1.log.6466214.000001.log.tgz,A'
         storage_dump = 'user/mdobre/01/6b/user.mdobre.C1C1bkg.WWVBH,nometcut.0711.log.4374089.000029.log.tgz'
-        results = list(compare3([rucio_replica_dump], [storage_dump], [rucio_replica_dump]))
-        eq_(
-            results,
-            [
-                (
-                    'user/mdobre/01/6b/user.mdobre.C1C1bkg.WWVBH,nometcut.0711.log.4374089.000029.log.tgz',
-                    (False, True, False),
-                    (None, None),
-                ),
-                (
-                    'user/mfauccig/8d/46/user.mfauccig.410000.PowhegPythiaEvtGen.DAOD_TOPQ1.e3698_s2608_s2183_r6630_r6264_p2377.v1.log.6466214.000001.log.tgz',
-                    (True, False, True),
-                    ('A', 'A'),
-                ),
-            ],
-        )
-
-    def test_min3_simple_strings(self):
-        eq_(min3('a', 'b', 'c'), 'a')
+        value = list(compare3([rucio_replica_dump], [storage_dump], [rucio_replica_dump]))
+        expected = [
+            (
+                'user/mdobre/01/6b/user.mdobre.C1C1bkg.WWVBH,nometcut.0711.log.4374089.000029.log.tgz',
+                (False, True, False),
+                (None, None),
+            ),
+            (
+                'user/mfauccig/8d/46/user.mfauccig.410000.PowhegPythiaEvtGen.DAOD_TOPQ1.e3698_s2608_s2183_r6630_r6264_p2377.v1.log.6466214.000001.log.tgz',
+                (True, False, True),
+                ('A', 'A'),
+            ),
+        ]
+        assert value == expected
+
+    def test_min3_simple_strings(self, tmp_path):
+        ''' DUMPER '''
+        assert min3('a', 'b', 'c') == 'a'
+
+    def test_min3_repeated_strings(self, tmp_path):
+        ''' DUMPER '''
+        assert min3('b', 'a', 'a') == 'a'
+
+    def test_min3_parsing_the_strings_is_not_a_responsability_of_this_function(self, tmp_path):
+        ''' DUMPER '''
+        assert min3('a,b', 'cab', 'b,a') == 'a,b'
 
-    def test_min3_repeated_strings(self):
-        eq_(min3('b', 'a', 'a'), 'a')
-
-    def test_min3_parsing_the_strings_is_not_a_responsability_of_this_function(self):
-        # FIXME: Remove
-        eq_(min3('a,b', 'cab', 'b,a'), 'a,b')
-
-    def test_parse_and_filter_file_default_parameters(self):
+    def test_parse_and_filter_file_default_parameters(self, tmp_path):
+        ''' DUMPER '''
         fake_data = 'asd\nasda\n'
-        path = make_temp_file(self.tmp_dir, fake_data)
+        path = make_temp_file(tmp_path, fake_data)
 
-        parsed_file = parse_and_filter_file(path, cache_dir=self.tmp_dir)
+        parsed_file = parse_and_filter_file(path, cache_dir=tmp_path)
         with open(parsed_file) as f:
             data = f.read()
 
-        eq_(fake_data.replace('\n', '\n\n'), data)
+        assert fake_data.replace('\n', '\n\n') == data
 
         os.unlink(path)
         os.unlink(parsed_file)
 
-    def test_parse_and_filter_file_parser_function(self):
+    def test_parse_and_filter_file_parser_function(self, tmp_path):
+        ''' DUMPER '''
         fake_data = 'asd\nasda\n'
-        path = make_temp_file(self.tmp_dir, fake_data)
+        path = make_temp_file(tmp_path, fake_data)
 
-        parsed_file = parse_and_filter_file(path, parser=str.strip, cache_dir=self.tmp_dir)
+        parsed_file = parse_and_filter_file(path, parser=str.strip, cache_dir=tmp_path)
         with open(parsed_file) as f:
             data = f.read()
-        eq_(fake_data, data)
+        assert fake_data == data
 
         os.unlink(path)
         os.unlink(parsed_file)
 
-    def test_parse_and_filter_file_filter_function(self):
+    def test_parse_and_filter_file_filter_function(self, tmp_path):
+        ''' DUMPER '''
         fake_data = 'asd\nasda\n'
-        path = make_temp_file(self.tmp_dir, fake_data)
+        path = make_temp_file(tmp_path, fake_data)
 
-        parsed_file = parse_and_filter_file(path, filter_=lambda s: s == 'asd\n', cache_dir=self.tmp_dir)
+        parsed_file = parse_and_filter_file(path, filter_=lambda s: s == 'asd\n', cache_dir=tmp_path)
         with open(parsed_file) as f:
             data = f.read()
 
-        eq_('asd\n\n', data)
+        assert 'asd\n\n' == data
 
         os.unlink(path)
         os.unlink(parsed_file)
 
-    def test_parse_and_filter_file_default_naming(self):
-        path = make_temp_file(self.tmp_dir, 'x\n')
+    def test_parse_and_filter_file_default_naming(self, tmp_path):
+        ''' DUMPER '''
+        path = make_temp_file(tmp_path, 'x\n')
 
-        parsed_file = parse_and_filter_file(path, cache_dir=self.tmp_dir)
+        parsed_file = parse_and_filter_file(path, cache_dir=tmp_path)
 
-        eq_(parsed_file, os.path.join(self.tmp_dir, os.path.basename(path) + '_parsed'))
+        assert parsed_file == os.path.join(tmp_path, os.path.basename(path) + '_parsed')
 
         os.unlink(path)
         os.unlink(parsed_file)
 
-    def test_parse_and_filter_file_prefix_specified(self):
-        path = make_temp_file(self.tmp_dir, 'x\n')
+    def test_parse_and_filter_file_prefix_specified(self, tmp_path):
+        ''' DUMPER '''
+        path = make_temp_file(tmp_path, 'x\n')
 
-        parsed_file = parse_and_filter_file(path, prefix=path + 'X', cache_dir=self.tmp_dir)
+        parsed_file = parse_and_filter_file(path, prefix=path + 'X', cache_dir=tmp_path)
 
-        eq_(parsed_file, path + 'X_parsed')
+        assert parsed_file == path + 'X_parsed'
 
         os.unlink(path)
         os.unlink(parsed_file)
 
-    def test_parse_and_filter_file_prefix_and_postfix_specified(self):
-        path = make_temp_file(self.tmp_dir, 'x\n')
+    def test_parse_and_filter_file_prefix_and_postfix_specified(self, tmp_path):
+        ''' DUMPER '''
+        path = make_temp_file(tmp_path, 'x\n')
 
-        parsed_file = parse_and_filter_file(path, prefix=path + 'X', postfix='Y', cache_dir=self.tmp_dir)
+        parsed_file = parse_and_filter_file(path, prefix=path + 'X', postfix='Y', cache_dir=tmp_path)
 
-        eq_(parsed_file, path + 'X_Y')
+        assert parsed_file == path + 'X_Y'
 
         os.unlink(path)
         os.unlink(parsed_file)
 
-    def test_gnu_sort_and_the_current_version_of_python_sort_strings_using_byte_value(self):
+    def test_gnu_sort_and_the_current_version_of_python_sort_strings_using_byte_value(self, tmp_path):
+        ''' DUMPER '''
         unsorted_data_list = ['z\n', 'a\n', '\xc3\xb1\n']
         unsorted_data = ''.join(unsorted_data_list)
         sorted_data = ''.join(['a\n', 'z\n', '\xc3\xb1\n'])
 
-        path = make_temp_file(self.tmp_dir, unsorted_data)
-        sorted_file = gnu_sort(path, cache_dir=self.tmp_dir)
+        path = make_temp_file(tmp_path, unsorted_data)
+        sorted_file = gnu_sort(path, cache_dir=tmp_path)
 
-        with open(sorted_file) as f:
-            eq_(
-                f.read(),
-                sorted_data,
-                'GNU Sort must sort comparing byte by byte (export '
-                'LC_ALL=C) to be faster and consistent with Python 2.'
-            )
+        assertion_msg = ('GNU Sort must sort comparing byte by byte (export '
+                         'LC_ALL=C) to be faster and consistent with Python 2.')
+        with open(sorted_file, encoding='utf-8') as f:
+            assert f.read() == sorted_data, assertion_msg
 
         os.unlink(path)
         os.unlink(sorted_file)
 
         python_sort = ''.join(sorted(unsorted_data_list))
-        eq_(
-            python_sort,
-            sorted_data,
-            'Current Python interpreter must sort strings comparing byte by '
-            'byte, it is important to use the same ordering as the one used '
-            'with GNU Sort. Note Python 3 uses unicode by default.'
-        )
+        assertion_msg = ('Current Python interpreter must sort strings '
+                         'comparing byte by byte, it is important to use the '
+                         'same ordering as the one used with GNU Sort. Note '
+                         'Python 3 uses unicode by default.')
+        assert python_sort == sorted_data, assertion_msg
 
-    def test_gnu_sort_can_sort_by_field(self):
+    def test_gnu_sort_can_sort_by_field(self, tmp_path):
+        ''' DUMPER '''
         unsorted_data = ''.join(['1,z\n', '2,a\n', '3,\xc3\xb1\n'])
         sorted_data = ''.join(['2,a\n', '1,z\n', '3,\xc3\xb1\n'])
 
-        path = make_temp_file(self.tmp_dir, unsorted_data)
-        sorted_file = gnu_sort(path, delimiter=',', fieldspec='2', cache_dir=self.tmp_dir)
+        path = make_temp_file(tmp_path, unsorted_data)
+        sorted_file = gnu_sort(path, delimiter=',', fieldspec='2', cache_dir=tmp_path)
 
-        with open(sorted_file) as f:
-            eq_(f.read(), sorted_data)
+        with open(sorted_file, encoding='utf-8') as f:
+            assert f.read() == sorted_data
 
         os.unlink(path)
         os.unlink(sorted_file)
```

### Comparing `rucio-clients-1.9.6/lib/rucio/tests/test_dumper_data_model.py` & `rucio-clients-32.0.0rc1/tests/test_dumper_data_model.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,30 +1,46 @@
-# Copyright European Organization for Nuclear Research (CERN)
+# -*- coding: utf-8 -*-
+# Copyright European Organization for Nuclear Research (CERN) since 2012
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
-# You may not use this file except in compliance with the License.
-# You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
 #
-# Authors:
-# - Fernando Lopez, <felopez@cern.ch>, 2015
-from datetime import datetime
-from nose.tools import eq_
-from nose.tools import ok_
-from nose.tools import raises
-from rucio.common import dumper
-from rucio.common.dumper import data_models
-from rucio.tests.common import stubbed
+#    http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
 import glob
 import os
+from datetime import datetime
+from unittest import mock
+
+import pytest
 import requests
-import shutil
-import tempfile
 
+from rucio.common import dumper
+from rucio.common.dumper import data_models
+
+
+def mocked_requests_head(*args, **kwargs):
+    response = requests.Response()
+    response.status_code = 200
+    response._content = 'content'
+    response.headers['content-disposition'] = 'filename=01-01-2015'
+    response.iter_content = lambda _: [response._content]
+
+    assert args[0] == 'https://rucio-hadoop.cern.ch/data_concrete?rse=SOMEENDPOINT'
+    return response
 
-class TestDataModel(object):
+
+class TestDataModel:
     VALID_DUMP = '''\
 CERN-PROD_DATADISK	data12_8TeV	AOD.04972924._000218.pool.root.1	1045a406	127508132	2015-03-10 14:00:24	data12_8TeV/5b/ea/AOD.04972924._000218.pool.root.1	2015-03-15 08:33:09
 CERN-PROD_DATADISK	data12_8TeV	ESD.04972924._000218.pool.root.1	a6152bbc	2498690922	2015-03-10 14:00:24	 data12_8TeV/7a/a6/ESD.04972924._000218.pool.root.1	2015-03-10 14:00:35
 '''
     VALID_DUMP_NO_EOL = '''\
 CERN-PROD_DATADISK	data12_8TeV	AOD.04972924._000218.pool.root.1	1045a406	127508132	2015-03-10 14:00:24	data12_8TeV/5b/ea/AOD.04972924._000218.pool.root.1	2015-03-15 08:33:09
 CERN-PROD_DATADISK	data12_8TeV	ESD.04972924._000218.pool.root.1	a6152bbc	2498690922	2015-03-10 14:00:24	 data12_8TeV/7a/a6/ESD.04972924._000218.pool.root.1	2015-03-10 14:00:35'''
@@ -41,261 +57,279 @@
             ('d', str),
             ('e', int),
             ('f', dumper.to_datetime),
             ('g', str),
             ('h', dumper.to_datetime),
         )
 
-    def setUp(self):
-        self.data_list = [
-            'aa',
-            'bb',
-            'cc',
-            'dd',
-            '42',
-            self.DATE_SECONDS,
-            'ee',
-            self.DATE_TENTHS,
-        ]
-        self.data_concrete = self._DataConcrete(*self.data_list)
-        self.tmp_dir = tempfile.mkdtemp()
-
-    def teardown(self):
-        shutil.rmtree(self.tmp_dir)
+    data_list = [
+        'aa',
+        'bb',
+        'cc',
+        'dd',
+        '42',
+        DATE_SECONDS,
+        'ee',
+        DATE_TENTHS,
+    ]
+    data_concrete = _DataConcrete(*data_list)
 
     def test_field_names(self):
-        eq_(self._DataConcrete.get_fieldnames(), list('abcdefgh'))
+        """ test field names """
+        assert self._DataConcrete.get_fieldnames() == list('abcdefgh')
 
     def test_pprint(self):
+        """ Testint pprint """
         expected_format = ''.join([
             'a: aa\n',
             'b: bb\n',
             'c: cc\n',
             'd: dd\n',
             'e: 42\n',
             'f: 2015-03-10 14:00:35\n',
             'g: ee\n',
             'h: 2015-03-10 14:00:35\n',
         ])
-        eq_(self.data_concrete.pprint(), expected_format, self.data_concrete.pprint())
+        assert self.data_concrete.pprint() == expected_format
 
     def test_data_models_are_indexable(self):
-        eq_(self.data_concrete[0], 'aa')
+        """ test data models are indexable """
+        assert self.data_concrete[0] == 'aa'
 
     def test_csv_header(self):
-        eq_(self._DataConcrete.csv_header(), 'a,b,c,d,e,f,g,h')
+        """ test csv header """
+        assert self._DataConcrete.csv_header() == 'a,b,c,d,e,f,g,h'
 
     def test_formated_fields(self):
-        eq_(self.data_concrete.formated_fields(print_fields=('a', 'e')), ['aa', '42'])
+        """ test formated fields """
+        assert self.data_concrete.formated_fields(print_fields=('a', 'e')) == ['aa', '42']
 
     def test_csv(self):
-        eq_(self.data_concrete.csv(fields=('a', 'e')), 'aa,42')
+        """ test csv """
+        assert self.data_concrete.csv(fields=('a', 'e')) == 'aa,42'
 
     def test_csv_default_formatting(self):
-        eq_(
-            self.data_concrete.csv(),
-            'aa,bb,cc,dd,42,2015-03-10T14:00:35,ee,2015-03-10T14:00:35'
-        )
+        """ test csv default formatting"""
+        assert self.data_concrete.csv() == 'aa,bb,cc,dd,42,2015-03-10T14:00:35,ee,2015-03-10T14:00:35'
 
     def test_each(self):
+        """ test each"""
         tsv_dump = ['\t'.join(self.data_list)]
         records = list(self._DataConcrete.each(tsv_dump))
-        eq_(len(records), 1)
-        eq_(records[0].a, 'aa')
+        assert len(records) == 1
+        assert records[0].a == 'aa'
 
     def test_each_with_filter(self):
+        """ test each with filter"""
         tsv_dump = ['\t'.join(self.data_list)]
         tsv_dump.append(tsv_dump[0].replace('aa', 'xx'))
         records = list(self._DataConcrete.each(tsv_dump, filter_=lambda x: x.a == 'xx'))
-        eq_(len(records), 1)
-        eq_(records[0].a, 'xx')
+        assert len(records) == 1
+        assert records[0].a == 'xx'
 
-    def test_each_iterates_tough_all_lines_even_without_eol(self):
+    def test_each_without_eol(self):
+        """ test each without eol """
         dump_file = self.VALID_DUMP.splitlines(True)
-        eq_(
-            2,
-            len(list(self._DataConcrete.each(dump_file))),
-        )
+        assert 2 == len(list(self._DataConcrete.each(dump_file)))
 
     def test_parse_line_valid_line(self):
+        """ test parse line valid line """
         for line in self.VALID_DUMP.splitlines(True):
             self._DataConcrete.parse_line(line)
 
-    @raises(TypeError)
-    def test_parse_line_wrong_number_of_fields(self):
-        self._DataConcrete.parse_line('asdasd\taasdsa\n')
-
-    @raises(ValueError)
-    def test_parse_line_wrong_format_of_fields(self):
-        self._DataConcrete.parse_line('a\ta\ta\ta\ta\ta\ta\ta\n')
-
-    def test_download_with_fixed_date(self):
+    def test_wrong_number_of_fields(self):
+        """ test wrong number of fields """
+        with pytest.raises(TypeError):
+            self._DataConcrete.parse_line('asdasd\taasdsa\n')
+
+    def test_wrong_format_of_fields(self):
+        """ test wrong format of fields """
+        with pytest.raises(ValueError):
+            self._DataConcrete.parse_line('a\ta\ta\ta\ta\ta\ta\ta\n')
+
+    @mock.patch('requests.Session.get')
+    @mock.patch('requests.Session.head')
+    def test_download_with_fixed_date(self, mock_request_head, mock_request_get, tmp_path):
+        """ test download with fixed date"""
         response = requests.Response()
         response.status_code = 200
         response._content = 'content'
         response.iter_content = lambda _: [response._content]
+        mock_request_get.return_value = response
+        mock_request_head.return_value = response
 
-        with stubbed(requests.Session.get, lambda _, __: response):
-            with stubbed(requests.Session.head, lambda _, __: response):
-                self._DataConcrete.download(
-                    'SOMEENDPOINT',
-                    date=datetime.strptime('01-01-2015', '%d-%m-%Y'),
-                    cache_dir=self.tmp_dir,
-                )
+        self._DataConcrete.download(
+            'SOMEENDPOINT',
+            date=datetime.strptime('01-01-2015', '%d-%m-%Y'),
+            cache_dir=tmp_path,
+        )
         downloaded = glob.glob(
             os.path.join(
-                self.tmp_dir,
+                tmp_path,
                 '_dataconcrete_SOMEENDPOINT_01-01-2015_*',
             )
         )
-        eq_(len(downloaded), 1)
-        with open(downloaded[0]) as f:
-            eq_(f.read(), 'content')
-
-    def test_download_with_date_latest_should_make_a_head_query_with_empty_date_and_name_the_output_file_according_to_the_content_disposition_header(self):
+        assert len(downloaded) == 1
+        with open(downloaded[0]) as fil:
+            assert fil.read() == 'content'
+
+    @mock.patch('requests.Session.get')
+    @mock.patch('requests.Session.head', side_effect=mocked_requests_head)
+    def test_download_empty_date(self, mock_request_head, mock_request_get, tmp_path):
+        """ test_download_with_date_latest_should_make_a_head_query_with_empty_date_and_name_the_output_file_according_to_the_content_disposition_header """
         response = requests.Response()
         response.status_code = 200
         response._content = 'content'
         response.headers['content-disposition'] = 'filename=01-01-2015'
         response.iter_content = lambda _: [response._content]
 
-        def fake_head(slf, url):
-            eq_(
-                url,
-                'https://rucio-hadoop.cern.ch/data_concrete?rse=SOMEENDPOINT',
-            )
-            return response
+        mock_request_get.return_value = response
 
-        with stubbed(requests.Session.get, fake_head):
-            with stubbed(requests.Session.head, lambda _, __: response):
-                self._DataConcrete.download(
-                    'SOMEENDPOINT',
-                    date='latest',
-                    cache_dir=self.tmp_dir,
-                )
+        self._DataConcrete.download(
+            'SOMEENDPOINT',
+            date='latest',
+            cache_dir=tmp_path,
+        )
         downloaded = glob.glob(
             os.path.join(
-                self.tmp_dir,
+                tmp_path,
                 '_dataconcrete_SOMEENDPOINT_01-01-2015_*',
             )
         )
-        eq_(len(downloaded), 1)
-        with open(downloaded[0]) as f:
-            eq_(f.read(), 'content')
-
-    @raises(dumper.HTTPDownloadFailed)
-    def test_download_error_raises_exception(self):
+        assert len(downloaded) == 1
+        with open(downloaded[0]) as fil:
+            assert fil.read() == 'content'
+
+    @mock.patch('requests.Session.get')
+    @mock.patch('requests.Session.head')
+    def test_raises_exception(self, mock_session_head, mock_session_get, tmp_path):
+        """ test raise exception """
         response = requests.Response()
         response.status_code = 500
+        mock_session_get.return_value = response
+        mock_session_head.return_value = response
 
-        with stubbed(requests.Session.get, lambda _, __: response):
-            with stubbed(requests.Session.head, lambda _, __: response):
-                self._DataConcrete.download(
-                    'SOMEENDPOINT',
-                    date=datetime.strptime('01-01-2015', '%d-%m-%Y'),
-                    cache_dir=self.tmp_dir,
-                )
+        with pytest.raises(dumper.HTTPDownloadFailed):
+            self._DataConcrete.download(
+                'SOMEENDPOINT',
+                date=datetime.strptime('01-01-2015', '%d-%m-%Y'),
+                cache_dir=tmp_path,
+            )
 
 
 class TestCompleteDataset(object):
-    def test_creation_with_7_parameters(self):
+
+    @staticmethod
+    def test_creation_with_7_parameters():
+        """ test ceation with 7 parameters """
         complete_dataset = data_models.CompleteDataset(
             'RSE',
             'scope',
             'name',
             'owner',
             '42',
             '2015-01-01 23:00:00',
             '2015-01-01 23:00:00',
         )
         assert complete_dataset.state is None
 
-    def test_creation_with_8_parameters(self):
+    @staticmethod
+    def test_creation_with_8_parameters():
+        """ test creation with 8 parameters """
         complete_dataset = data_models.CompleteDataset(
             'RSE',
             'scope',
             'name',
             'owner',
             '42',
             '2015-01-01 23:00:00',
             '2015-01-01 23:00:00',
             'A',
         )
-        eq_(complete_dataset.state, 'A')
+        assert complete_dataset.state == 'A'
 
-    def test_empty_size_is_saved_as_none(self):
+    @staticmethod
+    def test_empty_size_is_():
+        """ test empty size """
         complete_dataset = data_models.CompleteDataset(
             'RSE',
             'scope',
             'name',
             'owner',
             '',
             '2015-01-01 23:00:00',
             '2015-01-01 23:00:00',
             'A',
         )
-        assert complete_dataset.size is None
+        assert complete_dataset.size is None  # pylint: disable=no-member
 
 
 class TestReplica(object):
-    def test_replica_creation_with_8_parameters(self):
+
+    @staticmethod
+    def test_replica_with_8_parameters():
+        """ test replica with 8 parameters """
         replica = data_models.Replica(
             'RSE',
             'scope',
             'name',
             'checksum',
             '42',
             '2015-01-01 23:00:00',
             'path',
             '2015-01-01 23:00:00',
         )
-        eq_(replica.state, 'None')
+        assert replica.state == 'None'  # pylint: disable=no-member
 
-    def test_replica_creation_with_9_parameters(self):
+    @staticmethod
+    def test_replica_with_9_parameters():
+        """ test replica with 9 parameters """
         replica = data_models.Replica(
             'RSE',
             'scope',
             'name',
             'checksum',
             '42',
             '2015-01-01 23:00:00',
             'path',
             '2015-01-01 23:00:00',
             'A',
         )
-        eq_(replica.state, 'A')
+        assert replica.state == 'A'  # pylint: disable=no-member
 
 
-class TestFilter(object):
-    def setUp(self):
-        self.replica_1 = data_models.Replica(
-            'RSE',
-            'scope',
-            'name',
-            'checksum',
-            '42',
-            '2015-01-01 23:00:00',
-            'path',
-            '2015-01-01 23:00:00',
-            'A',
-        )
-        self.replica_2 = data_models.Replica(
-            'RSE',
-            'scope',
-            'name',
-            'checksum',
-            '42',
-            '2015-01-01 23:00:00',
-            'path',
-            '2015-01-01 23:00:00',
-            'U',
-        )
+class TestFilter:
+
+    replica_1 = data_models.Replica(
+        'RSE',
+        'scope',
+        'name',
+        'checksum',
+        '42',
+        '2015-01-01 23:00:00',
+        'path',
+        '2015-01-01 23:00:00',
+        'A',
+    )
+    replica_2 = data_models.Replica(
+        'RSE',
+        'scope',
+        'name',
+        'checksum',
+        '42',
+        '2015-01-01 23:00:00',
+        'path',
+        '2015-01-01 23:00:00',
+        'U',
+    )
 
     def test_simple_condition(self):
+        """ test simple condition """
         filter_ = data_models.Filter('state=A', data_models.Replica)
-        ok_(filter_.match(self.replica_1))
-        ok_(not filter_.match(self.replica_2))
+        assert filter_.match(self.replica_1)
+        assert not filter_.match(self.replica_2)
 
-    def test_multiple_conditions_are_evaluated_as_an_and_expresion(self):
+    def test_multiple_conditions(self):
+        """ test multiple conditions """
         filter_ = data_models.Filter('size=42,state=A', data_models.Replica)
-        ok_(filter_.match(self.replica_1), self.replica_1.size)
-        ok_(not filter_.match(self.replica_2), self.replica_2.size)
+        assert filter_.match(self.replica_1)
+        assert not filter_.match(self.replica_2)
```

### Comparing `rucio-clients-1.9.6/lib/rucio/tests/test_judge_evaluator.py` & `rucio-clients-32.0.0rc1/tests/test_judge_injector.py`

 * *Files 24% similar despite different names*

```diff
@@ -1,259 +1,225 @@
-# Copyright European Organization for Nuclear Research (CERN)
+# -*- coding: utf-8 -*-
+# Copyright European Organization for Nuclear Research (CERN) since 2012
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
-# You may not use this file except in compliance with the License.
+# you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
-# http://www.apache.org/licenses/LICENSE-2.0
 #
-# Authors:
-# - Martin Barisits, <martin.barisits@cern.ch>, 2014-2015
-# - Mario Lassnig, <mario.lassnig@cern.ch>, 2014
-
+#    http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+from datetime import datetime, timedelta
+
+import pytest
+
+from rucio.common.config import config_get_bool
+from rucio.common.exception import RuleNotFound
+from rucio.common.types import InternalAccount, InternalScope
 from rucio.common.utils import generate_uuid as uuid
-from rucio.core.account_counter import get_counter
-from rucio.core.account_limit import set_account_limit
-from rucio.core.did import add_did, attach_dids, detach_dids
-from rucio.core.lock import get_replica_locks, get_dataset_locks
-from rucio.core.rse import add_rse_attribute, get_rse
-from rucio.core.rule import add_rule, get_rule
-from rucio.daemons.judge.evaluator import re_evaluator
-from rucio.daemons.abacus.account import account_update
-from rucio.db.sqla.constants import DIDType
-from rucio.tests.test_rule import create_files, tag_generator
-
-
-class TestJudgeEvaluator():
+from rucio.core.account_limit import set_local_account_limit
+from rucio.core.did import add_did, attach_dids
+from rucio.core.lock import get_replica_locks
+from rucio.core.rse import add_rse_attribute
+from rucio.core.rule import add_rule, get_rule, approve_rule, deny_rule, list_rules
+from rucio.daemons.judge.injector import rule_injector
+from rucio.db.sqla.constants import DIDType, RuleState
+from rucio.db.sqla.models import ReplicationRule
+from rucio.db.sqla.session import transactional_session
+from rucio.tests.common_server import get_vo
+from .test_rule import create_files, tag_generator
+
+
+@pytest.fixture(scope="class")
+def setup_class(request, rse_factory_unittest):
+    request.cls.setUpClass()
+
+
+@pytest.mark.noparallel(reason='uses pre-defined RSE, sets account limits, adds global rse attributes')
+@pytest.mark.usefixtures("setup_class")
+class TestJudgeEvaluator:
 
     @classmethod
     def setUpClass(cls):
+        if config_get_bool('common', 'multi_vo', raise_exception=False, default=False):
+            cls.vo = {'vo': get_vo()}
+        else:
+            cls.vo = {}
+
         # Add test RSE
-        cls.rse1 = 'MOCK'
-        cls.rse3 = 'MOCK3'
-        cls.rse4 = 'MOCK4'
-        cls.rse5 = 'MOCK5'
-
-        cls.rse1_id = get_rse(cls.rse1).id
-        cls.rse3_id = get_rse(cls.rse3).id
-        cls.rse4_id = get_rse(cls.rse4).id
-        cls.rse5_id = get_rse(cls.rse5).id
+        cls.rse1, cls.rse1_id = cls.rse_factory.make_mock_rse()
+        cls.rse3, cls.rse3_id = cls.rse_factory.make_mock_rse()
+        cls.rse4, cls.rse4_id = cls.rse_factory.make_mock_rse()
+        cls.rse5, cls.rse5_id = cls.rse_factory.make_mock_rse()
 
         # Add Tags
         cls.T1 = tag_generator()
         cls.T2 = tag_generator()
-        add_rse_attribute(cls.rse1, cls.T1, True)
-        add_rse_attribute(cls.rse3, cls.T1, True)
-        add_rse_attribute(cls.rse4, cls.T2, True)
-        add_rse_attribute(cls.rse5, cls.T1, True)
+        add_rse_attribute(cls.rse1_id, cls.T1, True)
+        add_rse_attribute(cls.rse3_id, cls.T1, True)
+        add_rse_attribute(cls.rse4_id, cls.T2, True)
+        add_rse_attribute(cls.rse5_id, cls.T1, True)
 
         # Add fake weights
-        add_rse_attribute(cls.rse1, "fakeweight", 10)
-        add_rse_attribute(cls.rse3, "fakeweight", 0)
-        add_rse_attribute(cls.rse4, "fakeweight", 0)
-        add_rse_attribute(cls.rse5, "fakeweight", 0)
+        add_rse_attribute(cls.rse1_id, "fakeweight", 10)
+        add_rse_attribute(cls.rse3_id, "fakeweight", 0)
+        add_rse_attribute(cls.rse4_id, "fakeweight", 0)
+        add_rse_attribute(cls.rse5_id, "fakeweight", 0)
 
         # Add quota
-        set_account_limit('jdoe', cls.rse1_id, -1)
-        set_account_limit('jdoe', cls.rse3_id, -1)
-        set_account_limit('jdoe', cls.rse4_id, -1)
-        set_account_limit('jdoe', cls.rse5_id, -1)
-
-        set_account_limit('root', cls.rse1_id, -1)
-        set_account_limit('root', cls.rse3_id, -1)
-        set_account_limit('root', cls.rse4_id, -1)
-        set_account_limit('root', cls.rse5_id, -1)
-
-    def test_judge_add_files_to_dataset(self):
-        """ JUDGE EVALUATOR: Test the judge when adding files to dataset"""
-        scope = 'mock'
-        files = create_files(3, scope, self.rse1)
+        cls.jdoe = InternalAccount('jdoe', **cls.vo)
+        cls.root = InternalAccount('root', **cls.vo)
+        set_local_account_limit(cls.jdoe, cls.rse1_id, -1)
+        set_local_account_limit(cls.jdoe, cls.rse3_id, -1)
+        set_local_account_limit(cls.jdoe, cls.rse4_id, -1)
+        set_local_account_limit(cls.jdoe, cls.rse5_id, -1)
+
+        set_local_account_limit(cls.jdoe, cls.rse1_id, -1)
+        set_local_account_limit(cls.jdoe, cls.rse3_id, -1)
+        set_local_account_limit(cls.jdoe, cls.rse4_id, -1)
+        set_local_account_limit(cls.jdoe, cls.rse5_id, -1)
+
+    def test_judge_inject_rule(self):
+        """ JUDGE INJECTOR: Test the judge when injecting a rule"""
+        scope = InternalScope('mock', **self.vo)
+        files = create_files(3, scope, self.rse1_id)
         dataset = 'dataset_' + str(uuid())
-        add_did(scope, dataset, DIDType.from_sym('DATASET'), 'jdoe')
+        add_did(scope, dataset, DIDType.DATASET, self.jdoe)
+        attach_dids(scope, dataset, files, self.jdoe)
 
         # Add a first rule to the DS
-        add_rule(dids=[{'scope': scope, 'name': dataset}], account='jdoe', copies=2, rse_expression=self.T1, grouping='DATASET', weight=None, lifetime=None, locked=False, subscription_id=None)
-
-        attach_dids(scope, dataset, files, 'jdoe')
-        re_evaluator(once=True)
+        rule_id = add_rule(dids=[{'scope': scope, 'name': dataset}], account=self.jdoe, copies=2, rse_expression=self.T1, grouping='DATASET', weight=None, lifetime=None, locked=False, subscription_id=None, asynchronous=True)[0]
 
-        files = create_files(3, scope, self.rse1)
-        attach_dids(scope, dataset, files, 'jdoe')
-
-        # Fake judge
-        re_evaluator(once=True)
-
-        # Check if the Locks are created properly
-        for file in files:
-            assert(len(get_replica_locks(scope=file['scope'], name=file['name'])) == 2)
-
-    def test_judge_add_dataset_to_container(self):
-        """ JUDGE EVALUATOR: Test the judge when adding dataset to container"""
-        scope = 'mock'
-        files = create_files(3, scope, self.rse1)
-        dataset = 'dataset_' + str(uuid())
-        add_did(scope, dataset, DIDType.from_sym('DATASET'), 'jdoe')
-        attach_dids(scope, dataset, files, 'jdoe')
+        assert (get_rule(rule_id)['state'] == RuleState.INJECT)
 
-        parent_container = 'dataset_' + str(uuid())
-        add_did(scope, parent_container, DIDType.from_sym('CONTAINER'), 'jdoe')
-        # Add a first rule to the DS
-        add_rule(dids=[{'scope': scope, 'name': parent_container}], account='jdoe', copies=2, rse_expression=self.T1, grouping='DATASET', weight=None, lifetime=None, locked=False, subscription_id=None)
-        attach_dids(scope, parent_container, [{'scope': scope, 'name': dataset}], 'jdoe')
-        # Fake judge
-        re_evaluator(once=True)
+        rule_injector(once=True)
 
         # Check if the Locks are created properly
         for file in files:
-            assert(len(get_replica_locks(scope=file['scope'], name=file['name'])) == 2)
-
-        # Check if the DatasetLocks are created properly
-        dataset_locks = [lock for lock in get_dataset_locks(scope=scope, name=dataset)]
-        assert(len(dataset_locks) == 2)
-
-    def test_account_counter_judge_evaluate_attach(self):
-        """ JUDGE EVALUATOR: Test if the account counter is updated correctly when a file is added to a DS"""
-        re_evaluator(once=True)
-        account_update(once=True)
-
-        scope = 'mock'
-        files = create_files(3, scope, self.rse1, bytes=100)
-        dataset = 'dataset_' + str(uuid())
-        add_did(scope, dataset, DIDType.from_sym('DATASET'), 'jdoe')
-
-        # Add a first rule to the DS
-        add_rule(dids=[{'scope': scope, 'name': dataset}], account='jdoe', copies=1, rse_expression=self.rse1, grouping='ALL', weight=None, lifetime=None, locked=False, subscription_id=None)
+            assert (len(get_replica_locks(scope=file['scope'], name=file['name'])) == 2)
+        assert (get_rule(rule_id)['state'] == RuleState.REPLICATING)
 
-        account_counter_before = get_counter(self.rse1_id, 'jdoe')
-        attach_dids(scope, dataset, files, 'jdoe')
-
-        # Fake judge
-        re_evaluator(once=True)
-        account_update(once=True)
-
-        account_counter_after = get_counter(self.rse1_id, 'jdoe')
-        assert(account_counter_before['bytes'] + 3 * 100 == account_counter_after['bytes'])
-        assert(account_counter_before['files'] + 3 == account_counter_after['files'])
-
-    def test_account_counter_judge_evaluate_detach(self):
-        """ JUDGE EVALUATOR: Test if the account counter is updated correctly when a file is removed from a DS"""
-        re_evaluator(once=True)
-        account_update(once=True)
-
-        scope = 'mock'
-        files = create_files(3, scope, self.rse1, bytes=100)
+    def test_judge_inject_delayed_rule(self):
+        """ JUDGE INJECTOR: Test the judge when injecting a delayed rule"""
+        scope = InternalScope('mock', **self.vo)
+        files = create_files(1, scope, self.rse1_id)
+        dataset = 'dataset_' + str(uuid())
+        add_did(scope, dataset, DIDType.DATASET, self.jdoe)
+        attach_dids(scope, dataset, files, self.jdoe)
+        [file] = files
+
+        # Add a delayed rule
+        rule_id = add_rule(dids=[{'scope': scope, 'name': dataset}], account=self.jdoe, copies=2, rse_expression=self.T1, grouping='DATASET', weight=None, lifetime=None, locked=False, subscription_id=None, delay_injection=3600)[0]
+
+        rule = get_rule(rule_id)
+        assert rule['state'] == RuleState.INJECT
+        assert rule['updated_at'] < rule['created_at']
+        assert datetime.utcnow() + timedelta(seconds=3550) < rule['created_at'] < datetime.utcnow() + timedelta(seconds=3650)
+
+        # The time to create the rule has not yet arrived. The injector must skip this rule, no locks must be created
+        rule_injector(once=True)
+        assert get_rule(rule_id)['state'] == RuleState.INJECT
+        assert not get_replica_locks(scope=file['scope'], name=file['name'])
+
+        # simulate that time to inject the rule has arrived
+        @transactional_session
+        def __update_created_at(*, session=None):
+            session.query(ReplicationRule).filter_by(id=rule_id).one().created_at = datetime.utcnow()
+        __update_created_at()
+
+        # The injector must create the locks now
+        rule_injector(once=True)
+        assert get_rule(rule_id)['state'] == RuleState.REPLICATING
+        assert len(get_replica_locks(scope=file['scope'], name=file['name'])) == 2
+
+    def test_judge_ask_approval(self):
+        """ JUDGE INJECTOR: Test the judge when asking approval for a rule"""
+        scope = InternalScope('mock', **self.vo)
+        files = create_files(3, scope, self.rse1_id)
         dataset = 'dataset_' + str(uuid())
-        add_did(scope, dataset, DIDType.from_sym('DATASET'), 'jdoe')
-        attach_dids(scope, dataset, files, 'jdoe')
+        add_did(scope, dataset, DIDType.DATASET, self.jdoe)
+        attach_dids(scope, dataset, files, self.jdoe)
 
         # Add a first rule to the DS
-        add_rule(dids=[{'scope': scope, 'name': dataset}], account='jdoe', copies=1, rse_expression=self.rse1, grouping='ALL', weight=None, lifetime=None, locked=False, subscription_id=None)
-
-        account_update(once=True)
+        rule_id = add_rule(dids=[{'scope': scope, 'name': dataset}], account=self.jdoe, copies=1, rse_expression=self.rse4, grouping='DATASET', weight=None, lifetime=None, locked=False, subscription_id=None, ask_approval=True)[0]
 
-        account_counter_before = get_counter(self.rse1_id, 'jdoe')
+        assert (get_rule(rule_id)['state'] == RuleState.WAITING_APPROVAL)
 
-        detach_dids(scope, dataset, [files[0]])
+        approve_rule(rule_id=rule_id, approver=self.jdoe)
 
-        # Fake judge
-        re_evaluator(once=True)
-        account_update(once=True)
+        assert (get_rule(rule_id)['state'] == RuleState.INJECT)
 
-        account_counter_after = get_counter(self.rse1_id, 'jdoe')
-        assert(account_counter_before['bytes'] - 100 == account_counter_after['bytes'])
-        assert(account_counter_before['files'] - 1 == account_counter_after['files'])
-
-    def test_judge_evaluate_detach_datasetlock(self):
-        """ JUDGE EVALUATOR: Test if the a datasetlock is detached correctly when removing a dataset from a container"""
-        re_evaluator(once=True)
-
-        scope = 'mock'
-        files = create_files(3, scope, self.rse1, bytes=100)
-        dataset = 'dataset_' + str(uuid())
-        add_did(scope, dataset, DIDType.from_sym('DATASET'), 'jdoe')
-        attach_dids(scope, dataset, files, 'jdoe')
+        rule_injector(once=True)
 
-        container = 'container_' + str(uuid())
-        add_did(scope, container, DIDType.from_sym('CONTAINER'), 'jdoe')
-        attach_dids(scope, container, [{'scope': scope, 'name': dataset}], 'jdoe')
-
-        # Add a rule to the Container
-        add_rule(dids=[{'scope': scope, 'name': container}], account='jdoe', copies=1, rse_expression=self.rse1, grouping='DATASET', weight=None, lifetime=None, locked=False, subscription_id=None)
-
-        # Check if the datasetlock is there
-        locks = [ds_lock for ds_lock in get_dataset_locks(scope=scope, name=dataset)]
-        assert(len(locks) > 0)
-
-        detach_dids(scope, container, [{'scope': scope, 'name': dataset}])
-
-        # Fake judge
-        re_evaluator(once=True)
-
-        locks = [ds_lock for ds_lock in get_dataset_locks(scope=scope, name=dataset)]
-        assert(len(locks) == 0)
-
-    def test_judge_evaluate_detach(self):
-        """ JUDGE EVALUATOR: Test if the detach is done correctly"""
-        re_evaluator(once=True)
-
-        scope = 'mock'
-        container = 'container_' + str(uuid())
-        add_did(scope, container, DIDType.from_sym('CONTAINER'), 'jdoe')
-
-        scope = 'mock'
-        files = create_files(3, scope, self.rse1, bytes=100)
-        dataset = 'dataset_' + str(uuid())
-        add_did(scope, dataset, DIDType.from_sym('DATASET'), 'jdoe')
-        attach_dids(scope, dataset, files, 'jdoe')
-        attach_dids(scope, container, [{'scope': scope, 'name': dataset}], 'jdoe')
-
-        scope = 'mock'
-        files = create_files(3, scope, self.rse1, bytes=100)
-        dataset = 'dataset_' + str(uuid())
-        add_did(scope, dataset, DIDType.from_sym('DATASET'), 'jdoe')
-        attach_dids(scope, dataset, files, 'jdoe')
-        attach_dids(scope, container, [{'scope': scope, 'name': dataset}], 'jdoe')
-
-        scope = 'mock'
-        files = create_files(3, scope, self.rse1, bytes=100)
-        dataset = 'dataset_' + str(uuid())
-        add_did(scope, dataset, DIDType.from_sym('DATASET'), 'jdoe')
-        attach_dids(scope, dataset, files, 'jdoe')
-        attach_dids(scope, container, [{'scope': scope, 'name': dataset}], 'jdoe')
-
-        # Add a first rule to the Container
-        rule_id = add_rule(dids=[{'scope': scope, 'name': container}], account='jdoe', copies=1, rse_expression=self.rse1, grouping='ALL', weight=None, lifetime=None, locked=False, subscription_id=None)[0]
-
-        # Fake judge
-        re_evaluator(once=True)
-
-        assert(9 == get_rule(rule_id)['locks_ok_cnt'])
-
-        detach_dids(scope, dataset, [files[0]])
-
-        # Fake judge
-        re_evaluator(once=True)
-
-        assert(8 == get_rule(rule_id)['locks_ok_cnt'])
+        # Check if the Locks are created properly
+        for file in files:
+            assert (len(get_replica_locks(scope=file['scope'], name=file['name'])) == 1)
+        assert (get_rule(rule_id)['state'] == RuleState.REPLICATING)
 
-    def test_judge_add_files_to_dataset_with_2_rules(self):
-        """ JUDGE EVALUATOR: Test the judge when adding files to dataset with 2 rules"""
-        scope = 'mock'
-        files = create_files(3, scope, self.rse1)
+    def test_judge_deny_rule(self):
+        """ JUDGE INJECTOR: Test the judge when asking approval for a rule and denying it"""
+        scope = InternalScope('mock', **self.vo)
+        files = create_files(3, scope, self.rse1_id)
         dataset = 'dataset_' + str(uuid())
-        add_did(scope, dataset, DIDType.from_sym('DATASET'), 'jdoe')
+        add_did(scope, dataset, DIDType.DATASET, self.jdoe)
+        attach_dids(scope, dataset, files, self.jdoe)
 
         # Add a first rule to the DS
-        add_rule(dids=[{'scope': scope, 'name': dataset}], account='jdoe', copies=1, rse_expression=self.rse5, grouping='DATASET', weight=None, lifetime=None, locked=False, subscription_id=None)
-        add_rule(dids=[{'scope': scope, 'name': dataset}], account='root', copies=1, rse_expression=self.rse5, grouping='DATASET', weight=None, lifetime=None, locked=False, subscription_id=None)
+        rule_id = add_rule(dids=[{'scope': scope, 'name': dataset}], account=self.jdoe, copies=1, rse_expression=self.rse4, grouping='DATASET', weight=None, lifetime=None, locked=False, subscription_id=None, ask_approval=True)[0]
 
-        attach_dids(scope, dataset, files, 'jdoe')
-        re_evaluator(once=True)
+        assert (get_rule(rule_id)['state'] == RuleState.WAITING_APPROVAL)
 
-        files = create_files(3, scope, self.rse1)
-        attach_dids(scope, dataset, files, 'jdoe')
+        deny_rule(rule_id=rule_id, approver=self.jdoe)
 
-        # Fake judge
-        re_evaluator(once=True)
+        pytest.raises(RuleNotFound, get_rule, rule_id)
 
-        # Check if the Locks are created properly
-        for file in files:
-            assert(len(get_replica_locks(scope=file['scope'], name=file['name'])) == 2)
+    def test_add_rule_with_r2d2_container_treating(self):
+        """ JUDGE INJECTOR (CORE): Add a replication rule with an r2d2 container treatment"""
+        scope = InternalScope('mock', **self.vo)
+        container = 'asdf.r2d2_request.2016-04-01-15-00-00.ads.' + str(uuid())
+        add_did(scope, container, DIDType.CONTAINER, self.jdoe)
+        datasets = []
+        for i in range(3):
+            files = create_files(3, scope, self.rse1_id)
+            dataset = 'dataset_' + str(uuid())
+            datasets.append(dataset)
+            add_did(scope, dataset, DIDType.DATASET, self.jdoe)
+            attach_dids(scope, dataset, files, self.jdoe)
+            attach_dids(scope, container, [{'scope': scope, 'name': dataset}], self.jdoe)
+        rule_id = add_rule(dids=[{'scope': scope, 'name': container}], account=self.jdoe, copies=1, rse_expression=self.rse1, grouping='DATASET', weight=None, lifetime=900, locked=False, subscription_id=None, ask_approval=True)[0]
+        approve_rule(rule_id, approver=self.jdoe)
+        assert (get_rule(rule_id)['state'] == RuleState.INJECT)
+        rule_injector(once=True)
+        # Check if there is a rule for each file
+        with pytest.raises(RuleNotFound):
+            get_rule(rule_id)
+        for dataset in datasets:
+            assert (len([r for r in list_rules({'scope': scope, 'name': dataset})]) > 0)
+
+    def test_add_rule_with_r2d2_container_treating_and_duplicate_rule(self):
+        """ JUDGE INJECTOR (CORE): Add a replication rule with an r2d2 container treatment and duplicate rule"""
+        scope = InternalScope('mock', **self.vo)
+        container = 'asdf.r2d2_request.2016-04-01-15-00-00.ads.' + str(uuid())
+        add_did(scope, container, DIDType.CONTAINER, self.jdoe)
+        datasets = []
+        for i in range(3):
+            files = create_files(3, scope, self.rse1_id)
+            dataset = 'dataset_' + str(uuid())
+            datasets.append(dataset)
+            add_did(scope, dataset, DIDType.DATASET, self.jdoe)
+            attach_dids(scope, dataset, files, self.jdoe)
+            attach_dids(scope, container, [{'scope': scope, 'name': dataset}], self.jdoe)
+        add_rule(dids=[{'scope': scope, 'name': dataset}], account=self.jdoe, copies=1, rse_expression=self.rse1, grouping='DATASET', weight=None, lifetime=900, locked=False, subscription_id=None, ask_approval=False)
+        rule_id = add_rule(dids=[{'scope': scope, 'name': container}], account=self.jdoe, copies=1, rse_expression=self.rse1, grouping='DATASET', weight=None, lifetime=900, locked=False, subscription_id=None, ask_approval=True)[0]
+        approve_rule(rule_id, approver=self.jdoe)
+        assert (get_rule(rule_id)['state'] == RuleState.INJECT)
+        rule_injector(once=True)
+        # Check if there is a rule for each file
+        with pytest.raises(RuleNotFound):
+            get_rule(rule_id)
+        for dataset in datasets:
+            assert (len([r for r in list_rules({'scope': scope, 'name': dataset})]) > 0)
```

### Comparing `rucio-clients-1.9.6/lib/rucio/tests/test_judge_repairer.py` & `rucio-clients-32.0.0rc1/tests/test_judge_repairer.py`

 * *Files 16% similar despite different names*

```diff
@@ -1,199 +1,226 @@
-# Copyright European Organization for Nuclear Research (CERN)
+# -*- coding: utf-8 -*-
+# Copyright European Organization for Nuclear Research (CERN) since 2012
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
-# You may not use this file except in compliance with the License.
+# you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
-# http://www.apache.org/licenses/LICENSE-2.0
 #
-# Authors:
-# - Martin Barisits, <martin.barisits@cern.ch>, 2014-2015
-# - Mario Lassnig, <mario.lassnig@cern.ch>, 2014
-
-from rucio.common.utils import generate_uuid as uuid
-from rucio.core.account_limit import set_account_limit
+#    http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+import itertools
+from hashlib import sha256
+
+import pytest
+from dogpile.cache import make_region
+
+from rucio.common.config import config_get
+from rucio.common.config import config_get_bool
+from rucio.common.types import InternalAccount, InternalScope
+from rucio.core.account_limit import set_local_account_limit
 from rucio.core.did import add_did, attach_dids
 from rucio.core.lock import successful_transfer, failed_transfer, get_replica_locks
 from rucio.core.replica import get_replica
 from rucio.core.request import cancel_request_did
-from rucio.core.rse import add_rse_attribute, get_rse
+from rucio.core.transfer import cancel_transfers
+from rucio.core.rse import add_rse_attribute, add_rse, update_rse
 from rucio.core.rule import get_rule, add_rule
-from rucio.daemons.judge.repairer import rule_repairer
 from rucio.daemons.judge.evaluator import re_evaluator
+from rucio.daemons.judge.repairer import rule_repairer
+from rucio.db.sqla import models
 from rucio.db.sqla.constants import DIDType, RuleState, ReplicaState
 from rucio.db.sqla.session import get_session
-from rucio.db.sqla import models
-from rucio.tests.test_rule import create_files, tag_generator
+from rucio.tests.common import rse_name_generator, did_name_generator
+from rucio.tests.common_server import get_vo
+from .test_rule import create_files, tag_generator
+
 
+@pytest.fixture(scope="class")
+def setup_class(request, rse_factory_unittest):
+    request.cls.setUpClass()
 
-class TestJudgeRepairer():
+
+@pytest.mark.dirty
+@pytest.mark.noparallel(reason='uses pre-defined rses, sets rse attributes, sets account limits')
+@pytest.mark.usefixtures("setup_class")
+class TestJudgeRepairer:
 
     @classmethod
     def setUpClass(cls):
+        if config_get_bool('common', 'multi_vo', raise_exception=False, default=False):
+            cls.vo = {'vo': get_vo()}
+        else:
+            cls.vo = {}
+
         # Add test RSE
-        cls.rse1 = 'MOCK'
-        cls.rse3 = 'MOCK3'
-        cls.rse4 = 'MOCK4'
-        cls.rse5 = 'MOCK5'
-
-        cls.rse1_id = get_rse(cls.rse1).id
-        cls.rse3_id = get_rse(cls.rse3).id
-        cls.rse4_id = get_rse(cls.rse4).id
-        cls.rse5_id = get_rse(cls.rse5).id
+        cls.rse1, cls.rse1_id = cls.rse_factory.make_mock_rse()
+        cls.rse3, cls.rse3_id = cls.rse_factory.make_mock_rse()
+        cls.rse4, cls.rse4_id = cls.rse_factory.make_mock_rse()
+        cls.rse5, cls.rse5_id = cls.rse_factory.make_mock_rse()
 
         # Add Tags
         cls.T1 = tag_generator()
         cls.T2 = tag_generator()
-        add_rse_attribute(cls.rse1, cls.T1, True)
-        add_rse_attribute(cls.rse3, cls.T1, True)
-        add_rse_attribute(cls.rse4, cls.T2, True)
-        add_rse_attribute(cls.rse5, cls.T1, True)
+        add_rse_attribute(cls.rse1_id, cls.T1, True)
+        add_rse_attribute(cls.rse3_id, cls.T1, True)
+        add_rse_attribute(cls.rse4_id, cls.T2, True)
+        add_rse_attribute(cls.rse5_id, cls.T1, True)
 
         # Add fake weights
-        add_rse_attribute(cls.rse1, "fakeweight", 10)
-        add_rse_attribute(cls.rse3, "fakeweight", 0)
-        add_rse_attribute(cls.rse4, "fakeweight", 0)
-        add_rse_attribute(cls.rse5, "fakeweight", 0)
+        add_rse_attribute(cls.rse1_id, "fakeweight", 10)
+        add_rse_attribute(cls.rse3_id, "fakeweight", 0)
+        add_rse_attribute(cls.rse4_id, "fakeweight", 0)
+        add_rse_attribute(cls.rse5_id, "fakeweight", 0)
 
         # Add quota
-        set_account_limit('jdoe', cls.rse1_id, -1)
-        set_account_limit('jdoe', cls.rse3_id, -1)
-        set_account_limit('jdoe', cls.rse4_id, -1)
-        set_account_limit('jdoe', cls.rse5_id, -1)
-
-        set_account_limit('root', cls.rse1_id, -1)
-        set_account_limit('root', cls.rse3_id, -1)
-        set_account_limit('root', cls.rse4_id, -1)
-        set_account_limit('root', cls.rse5_id, -1)
+        cls.jdoe = InternalAccount('jdoe', **cls.vo)
+        cls.root = InternalAccount('root', **cls.vo)
+        set_local_account_limit(cls.jdoe, cls.rse1_id, -1)
+        set_local_account_limit(cls.jdoe, cls.rse3_id, -1)
+        set_local_account_limit(cls.jdoe, cls.rse4_id, -1)
+        set_local_account_limit(cls.jdoe, cls.rse5_id, -1)
+
+        set_local_account_limit(cls.root, cls.rse1_id, -1)
+        set_local_account_limit(cls.root, cls.rse3_id, -1)
+        set_local_account_limit(cls.root, cls.rse4_id, -1)
+        set_local_account_limit(cls.root, cls.rse5_id, -1)
 
     def test_to_repair_a_rule_with_NONE_grouping_whose_transfer_failed(self):
         """ JUDGE REPAIRER: Test to repair a rule with 1 failed transfer (lock)"""
 
         rule_repairer(once=True)  # Clean out the repairer
-        scope = 'mock'
-        files = create_files(3, scope, self.rse4, bytes=100)
-        dataset = 'dataset_' + str(uuid())
-        add_did(scope, dataset, DIDType.from_sym('DATASET'), 'jdoe')
-        attach_dids(scope, dataset, files, 'jdoe')
+        scope = InternalScope('mock', **self.vo)
+        files = create_files(3, scope, self.rse4_id, bytes_=100)
+        dataset = did_name_generator('dataset')
+        add_did(scope, dataset, DIDType.DATASET, self.jdoe)
+        attach_dids(scope, dataset, files, self.jdoe)
 
-        rule_id = add_rule(dids=[{'scope': scope, 'name': dataset}], account='jdoe', copies=1, rse_expression=self.T1, grouping='NONE', weight=None, lifetime=None, locked=False, subscription_id=None)[0]
+        rule_id = add_rule(dids=[{'scope': scope, 'name': dataset}], account=self.jdoe, copies=1, rse_expression=self.T1, grouping='NONE', weight=None, lifetime=None, locked=False, subscription_id=None)[0]
 
         failed_rse_id = get_replica_locks(scope=files[2]['scope'], name=files[2]['name'])[0].rse_id
-        assert(get_replica(rse=None, scope=files[2]['scope'], name=files[2]['name'], rse_id=failed_rse_id)['state'] == ReplicaState.COPYING)
-        assert(get_replica(rse=None, scope=files[2]['scope'], name=files[2]['name'], rse_id=failed_rse_id)['lock_cnt'] == 1)
+        assert (get_replica(scope=files[2]['scope'], name=files[2]['name'], rse_id=failed_rse_id)['state'] == ReplicaState.COPYING)
+        assert (get_replica(scope=files[2]['scope'], name=files[2]['name'], rse_id=failed_rse_id)['lock_cnt'] == 1)
 
         successful_transfer(scope=scope, name=files[0]['name'], rse_id=get_replica_locks(scope=files[0]['scope'], name=files[2]['name'])[0].rse_id, nowait=False)
         successful_transfer(scope=scope, name=files[1]['name'], rse_id=get_replica_locks(scope=files[1]['scope'], name=files[2]['name'])[0].rse_id, nowait=False)
         failed_transfer(scope=scope, name=files[2]['name'], rse_id=get_replica_locks(scope=files[2]['scope'], name=files[2]['name'])[0].rse_id)
 
-        assert(rule_id == get_rule(rule_id)['id'].replace('-', '').lower())
-        assert(RuleState.STUCK == get_rule(rule_id)['state'])
+        assert (rule_id == get_rule(rule_id)['id'].replace('-', '').lower())
+        assert (RuleState.STUCK == get_rule(rule_id)['state'])
         rule_repairer(once=True)
-        assert(RuleState.REPLICATING == get_rule(rule_id)['state'])
-        assert(get_replica(rse=None, scope=files[2]['scope'], name=files[2]['name'], rse_id=failed_rse_id)['state'] == ReplicaState.UNAVAILABLE)
-        assert(get_replica(rse=None, scope=files[2]['scope'], name=files[2]['name'], rse_id=failed_rse_id)['lock_cnt'] == 0)
+        assert (RuleState.REPLICATING == get_rule(rule_id)['state'])
+        assert (get_replica(scope=files[2]['scope'], name=files[2]['name'], rse_id=failed_rse_id)['state'] == ReplicaState.UNAVAILABLE)
+        assert (get_replica(scope=files[2]['scope'], name=files[2]['name'], rse_id=failed_rse_id)['lock_cnt'] == 0)
 
     def test_to_repair_a_rule_with_ALL_grouping_whose_transfer_failed(self):
         """ JUDGE REPAIRER: Test to repair a rule with 1 failed transfer (lock)"""
 
         rule_repairer(once=True)  # Clean out the repairer
-        scope = 'mock'
-        files = create_files(4, scope, self.rse4, bytes=100)
-        dataset = 'dataset_' + str(uuid())
-        add_did(scope, dataset, DIDType.from_sym('DATASET'), 'jdoe')
-        attach_dids(scope, dataset, files, 'jdoe')
+        scope = InternalScope('mock', **self.vo)
+        files = create_files(4, scope, self.rse4_id, bytes_=100)
+        dataset = did_name_generator('dataset')
+        add_did(scope, dataset, DIDType.DATASET, self.jdoe)
+        attach_dids(scope, dataset, files, self.jdoe)
 
-        rule_id = add_rule(dids=[{'scope': scope, 'name': dataset}], account='jdoe', copies=1, rse_expression=self.T1, grouping='ALL', weight=None, lifetime=None, locked=False, subscription_id=None)[0]
+        rule_id = add_rule(dids=[{'scope': scope, 'name': dataset}], account=self.jdoe, copies=1, rse_expression=self.T1, grouping='ALL', weight=None, lifetime=None, locked=False, subscription_id=None, activity='DebugJudge')[0]
 
         successful_transfer(scope=scope, name=files[0]['name'], rse_id=get_replica_locks(scope=files[0]['scope'], name=files[2]['name'])[0].rse_id, nowait=False)
         successful_transfer(scope=scope, name=files[1]['name'], rse_id=get_replica_locks(scope=files[1]['scope'], name=files[2]['name'])[0].rse_id, nowait=False)
         failed_transfer(scope=scope, name=files[2]['name'], rse_id=get_replica_locks(scope=files[2]['scope'], name=files[2]['name'])[0].rse_id)
         failed_transfer(scope=scope, name=files[3]['name'], rse_id=get_replica_locks(scope=files[3]['scope'], name=files[3]['name'])[0].rse_id)
 
-        assert(rule_id == get_rule(rule_id)['id'].replace('-', '').lower())
-        assert(RuleState.STUCK == get_rule(rule_id)['state'])
+        assert (rule_id == get_rule(rule_id)['id'].replace('-', '').lower())
+        assert (RuleState.STUCK == get_rule(rule_id)['state'])
         rule_repairer(once=True)
-        assert(RuleState.REPLICATING == get_rule(rule_id)['state'])
-        assert(get_replica_locks(scope=files[2]['scope'], name=files[2]['name'])[0].rse_id == get_replica_locks(scope=files[3]['scope'], name=files[3]['name'])[0].rse_id)
+        assert (RuleState.REPLICATING == get_rule(rule_id)['state'])
+        assert (get_replica_locks(scope=files[2]['scope'], name=files[2]['name'])[0].rse_id == get_replica_locks(scope=files[3]['scope'], name=files[3]['name'])[0].rse_id)
+        assert (get_replica_locks(scope=files[1]['scope'], name=files[1]['name'])[0].rse_id == get_replica_locks(scope=files[3]['scope'], name=files[3]['name'])[0].rse_id)
 
     def test_to_repair_a_rule_with_DATASET_grouping_whose_transfer_failed(self):
         """ JUDGE REPAIRER: Test to repair a rule with 1 failed transfer (lock)"""
 
         rule_repairer(once=True)  # Clean out the repairer
-        scope = 'mock'
-        files = create_files(4, scope, self.rse4, bytes=100)
-        dataset = 'dataset_' + str(uuid())
-        add_did(scope, dataset, DIDType.from_sym('DATASET'), 'jdoe')
-        attach_dids(scope, dataset, files, 'jdoe')
+        scope = InternalScope('mock', **self.vo)
+        files = create_files(4, scope, self.rse4_id, bytes_=100)
+        dataset = did_name_generator('dataset')
+        add_did(scope, dataset, DIDType.DATASET, self.jdoe)
+        attach_dids(scope, dataset, files, self.jdoe)
 
-        rule_id = add_rule(dids=[{'scope': scope, 'name': dataset}], account='jdoe', copies=1, rse_expression=self.T1, grouping='DATASET', weight=None, lifetime=None, locked=False, subscription_id=None)[0]
+        rule_id = add_rule(dids=[{'scope': scope, 'name': dataset}], account=self.jdoe, copies=1, rse_expression=self.T1, grouping='DATASET', weight=None, lifetime=None, locked=False, subscription_id=None, activity='DebugJudge')[0]
 
         successful_transfer(scope=scope, name=files[0]['name'], rse_id=get_replica_locks(scope=files[0]['scope'], name=files[2]['name'])[0].rse_id, nowait=False)
         successful_transfer(scope=scope, name=files[1]['name'], rse_id=get_replica_locks(scope=files[1]['scope'], name=files[2]['name'])[0].rse_id, nowait=False)
         failed_transfer(scope=scope, name=files[2]['name'], rse_id=get_replica_locks(scope=files[2]['scope'], name=files[2]['name'])[0].rse_id)
         failed_transfer(scope=scope, name=files[3]['name'], rse_id=get_replica_locks(scope=files[3]['scope'], name=files[3]['name'])[0].rse_id)
 
-        assert(rule_id == get_rule(rule_id)['id'].replace('-', '').lower())
-        assert(RuleState.STUCK == get_rule(rule_id)['state'])
+        assert (rule_id == get_rule(rule_id)['id'].replace('-', '').lower())
+        assert (RuleState.STUCK == get_rule(rule_id)['state'])
         rule_repairer(once=True)
-        assert(RuleState.REPLICATING == get_rule(rule_id)['state'])
-        assert(get_replica_locks(scope=files[2]['scope'], name=files[2]['name'])[0].rse_id == get_replica_locks(scope=files[3]['scope'], name=files[3]['name'])[0].rse_id)
+        assert (RuleState.REPLICATING == get_rule(rule_id)['state'])
+        assert (get_replica_locks(scope=files[2]['scope'], name=files[2]['name'])[0].rse_id == get_replica_locks(scope=files[3]['scope'], name=files[3]['name'])[0].rse_id)
+        assert (get_replica_locks(scope=files[1]['scope'], name=files[1]['name'])[0].rse_id == get_replica_locks(scope=files[3]['scope'], name=files[3]['name'])[0].rse_id)
 
     def test_repair_a_rule_with_missing_locks(self):
         """ JUDGE EVALUATOR: Test the judge when a rule gets STUCK from re_evaluating and there are missing locks"""
-        scope = 'mock'
-        files = create_files(3, scope, self.rse4)
-        dataset = 'dataset_' + str(uuid())
-        add_did(scope, dataset, DIDType.from_sym('DATASET'), 'jdoe')
+        scope = InternalScope('mock', **self.vo)
+        files = create_files(3, scope, self.rse4_id)
+        dataset = did_name_generator('dataset')
+        add_did(scope, dataset, DIDType.DATASET, self.jdoe)
 
         # Add a first rule to the DS
-        rule_id = add_rule(dids=[{'scope': scope, 'name': dataset}], account='jdoe', copies=2, rse_expression=self.T1, grouping='DATASET', weight=None, lifetime=None, locked=False, subscription_id=None)[0]
+        rule_id = add_rule(dids=[{'scope': scope, 'name': dataset}], account=self.jdoe, copies=2, rse_expression=self.T1, grouping='DATASET', weight=None, lifetime=None, locked=False, subscription_id=None)[0]
 
-        attach_dids(scope, dataset, files, 'jdoe')
+        attach_dids(scope, dataset, files, self.jdoe)
 
         # Fake judge
-        re_evaluator(once=True)
+        re_evaluator(once=True, did_limit=None)
 
         # Check if the Locks are created properly
         for file in files:
-            assert(len(get_replica_locks(scope=file['scope'], name=file['name'])) == 2)
+            assert (len(get_replica_locks(scope=file['scope'], name=file['name'])) == 2)
 
         # Add more files to the DID
-        files2 = create_files(3, scope, self.rse4)
-        attach_dids(scope, dataset, files2, 'jdoe')
+        files2 = create_files(3, scope, self.rse4_id)
+        attach_dids(scope, dataset, files2, self.jdoe)
 
         # Mark the rule STUCK to fake that the re-evaluation failed
         session = get_session()
         rule = session.query(models.ReplicationRule).filter_by(id=rule_id).one()
         rule.state = RuleState.STUCK
         session.commit()
 
         rule_repairer(once=True)
 
         for file in files:
-            assert(len(get_replica_locks(scope=file['scope'], name=file['name'])) == 2)
+            assert (len(get_replica_locks(scope=file['scope'], name=file['name'])) == 2)
         for file in files2:
-            assert(len(get_replica_locks(scope=file['scope'], name=file['name'])) == 2)
-            assert(len(set([lock.rse_id for lock in get_replica_locks(scope=files[0]['scope'], name=files[0]['name'])]).intersection(set([lock.rse_id for lock in get_replica_locks(scope=file['scope'], name=file['name'])]))) == 2)
-        assert(12 == get_rule(rule_id)['locks_replicating_cnt'])
+            assert (len(get_replica_locks(scope=file['scope'], name=file['name'])) == 2)
+            assert (len(set([lock.rse_id for lock in get_replica_locks(scope=files[0]['scope'], name=files[0]['name'])]).intersection(set([lock.rse_id for lock in get_replica_locks(scope=file['scope'], name=file['name'])]))) == 2)
+        assert (12 == get_rule(rule_id)['locks_replicating_cnt'])
 
     def test_repair_a_rule_with_source_replica_expression(self):
         """ JUDGE EVALUATOR: Test the judge when a with two rules with source_replica_expression"""
-        scope = 'mock'
-        files = create_files(3, scope, self.rse4)
-        dataset = 'dataset_' + str(uuid())
-        add_did(scope, dataset, DIDType.from_sym('DATASET'), 'jdoe')
-        attach_dids(scope, dataset, files, 'jdoe')
+        scope = InternalScope('mock', **self.vo)
+        files = create_files(3, scope, self.rse4_id)
+        dataset = did_name_generator('dataset')
+        add_did(scope, dataset, DIDType.DATASET, self.jdoe)
+        attach_dids(scope, dataset, files, self.jdoe)
 
         # Add a first rule to the DS
-        rule_id1 = add_rule(dids=[{'scope': scope, 'name': dataset}], account='jdoe', copies=1, rse_expression=self.rse1, grouping='DATASET', weight=None, lifetime=None, locked=False, subscription_id=None)[0]
-        rule_id2 = add_rule(dids=[{'scope': scope, 'name': dataset}], account='jdoe', copies=1, rse_expression=self.rse3, grouping='DATASET', weight=None, lifetime=None, locked=False, subscription_id=None, source_replica_expression=self.rse1)[0]
+        rule_id1 = add_rule(dids=[{'scope': scope, 'name': dataset}], account=self.jdoe, copies=1, rse_expression=self.rse1, grouping='DATASET', weight=None, lifetime=None, locked=False, subscription_id=None)[0]
+        rule_id2 = add_rule(dids=[{'scope': scope, 'name': dataset}], account=self.jdoe, copies=1, rse_expression=self.rse3, grouping='DATASET', weight=None, lifetime=None, locked=False, subscription_id=None, source_replica_expression=self.rse1)[0]
 
-        assert(RuleState.REPLICATING == get_rule(rule_id1)['state'])
-        assert(RuleState.STUCK == get_rule(rule_id2)['state'])
+        assert (RuleState.REPLICATING == get_rule(rule_id1)['state'])
+        assert (RuleState.STUCK == get_rule(rule_id2)['state'])
 
         successful_transfer(scope=scope, name=files[0]['name'], rse_id=self.rse1_id, nowait=False)
         successful_transfer(scope=scope, name=files[1]['name'], rse_id=self.rse1_id, nowait=False)
         successful_transfer(scope=scope, name=files[2]['name'], rse_id=self.rse1_id, nowait=False)
         # Also make replicas AVAILABLE
         session = get_session()
         replica = session.query(models.RSEFileAssociation).filter_by(scope=scope, name=files[0]['name'], rse_id=self.rse1_id).one()
@@ -202,64 +229,113 @@
         replica.state = ReplicaState.AVAILABLE
         replica = session.query(models.RSEFileAssociation).filter_by(scope=scope, name=files[2]['name'], rse_id=self.rse1_id).one()
         replica.state = ReplicaState.AVAILABLE
         session.commit()
 
         rule_repairer(once=True)
 
-        assert(RuleState.OK == get_rule(rule_id1)['state'])
-        assert(RuleState.REPLICATING == get_rule(rule_id2)['state'])
+        assert (RuleState.OK == get_rule(rule_id1)['state'])
+        assert (RuleState.REPLICATING == get_rule(rule_id2)['state'])
 
     def test_to_repair_a_rule_with_only_1_rse_whose_transfers_failed(self):
         """ JUDGE REPAIRER: Test to repair a rule with only 1 rse whose transfers failed (lock)"""
 
         rule_repairer(once=True)  # Clean out the repairer
-        scope = 'mock'
-        files = create_files(4, scope, self.rse4, bytes=100)
-        dataset = 'dataset_' + str(uuid())
-        add_did(scope, dataset, DIDType.from_sym('DATASET'), 'jdoe')
-        attach_dids(scope, dataset, files, 'jdoe')
+        scope = InternalScope('mock', **self.vo)
+        files = create_files(4, scope, self.rse4_id, bytes_=100)
+        dataset = did_name_generator('dataset')
+        add_did(scope, dataset, DIDType.DATASET, self.jdoe)
+        attach_dids(scope, dataset, files, self.jdoe)
 
-        rule_id = add_rule(dids=[{'scope': scope, 'name': dataset}], account='jdoe', copies=1, rse_expression=self.rse1, grouping='DATASET', weight=None, lifetime=None, locked=False, subscription_id=None)[0]
+        rule_id = add_rule(dids=[{'scope': scope, 'name': dataset}], account=self.jdoe, copies=1, rse_expression=self.rse1, grouping='DATASET', weight=None, lifetime=None, locked=False, subscription_id=None)[0]
 
         successful_transfer(scope=scope, name=files[0]['name'], rse_id=get_replica_locks(scope=files[0]['scope'], name=files[2]['name'])[0].rse_id, nowait=False)
         successful_transfer(scope=scope, name=files[1]['name'], rse_id=get_replica_locks(scope=files[1]['scope'], name=files[2]['name'])[0].rse_id, nowait=False)
         failed_transfer(scope=scope, name=files[2]['name'], rse_id=get_replica_locks(scope=files[2]['scope'], name=files[2]['name'])[0].rse_id)
         failed_transfer(scope=scope, name=files[3]['name'], rse_id=get_replica_locks(scope=files[3]['scope'], name=files[3]['name'])[0].rse_id)
-        cancel_request_did(scope=scope, name=files[2]['name'], dest_rse_id=get_replica_locks(scope=files[2]['scope'], name=files[2]['name'])[0].rse_id)
-        cancel_request_did(scope=scope, name=files[3]['name'], dest_rse_id=get_replica_locks(scope=files[3]['scope'], name=files[2]['name'])[0].rse_id)
+        transfs = cancel_request_did(scope=scope, name=files[2]['name'], dest_rse_id=get_replica_locks(scope=files[2]['scope'], name=files[2]['name'])[0].rse_id)
+        cancel_transfers(transfs)
+        transfs = cancel_request_did(scope=scope, name=files[3]['name'], dest_rse_id=get_replica_locks(scope=files[3]['scope'], name=files[2]['name'])[0].rse_id)
+        cancel_transfers(transfs)
 
-        assert(rule_id == get_rule(rule_id)['id'].replace('-', '').lower())
-        assert(RuleState.STUCK == get_rule(rule_id)['state'])
+        assert (rule_id == get_rule(rule_id)['id'].replace('-', '').lower())
+        assert (RuleState.STUCK == get_rule(rule_id)['state'])
         rule_repairer(once=True)
 
         # Stil assert STUCK because of delays:
-        assert(RuleState.STUCK == get_rule(rule_id)['state'])
-        assert(get_replica_locks(scope=files[2]['scope'], name=files[2]['name'])[0].rse_id == get_replica_locks(scope=files[3]['scope'], name=files[3]['name'])[0].rse_id)
-        # assert(RuleState.REPLICATING == get_rule(rule_id)['state'])
-        # assert(get_replica_locks(scope=files[2]['scope'], name=files[2]['name'])[0].rse_id == get_replica_locks(scope=files[3]['scope'], name=files[3]['name'])[0].rse_id)
+        assert (RuleState.STUCK == get_rule(rule_id)['state'])
+        assert (get_replica_locks(scope=files[2]['scope'], name=files[2]['name'])[0].rse_id == get_replica_locks(scope=files[3]['scope'], name=files[3]['name'])[0].rse_id)
+        # assert (RuleState.REPLICATING == get_rule(rule_id)['state'])
+        # assert (get_replica_locks(scope=files[2]['scope'], name=files[2]['name'])[0].rse_id == get_replica_locks(scope=files[3]['scope'], name=files[3]['name'])[0].rse_id)
 
-    def test_to_repair_a_rule_with_DATASET_grouping_whose_transfer_failed_and_flipping_to_other_rse(self):
+    def test_to_repair_a_rule_with_NONE_grouping_whose_transfer_failed_and_flipping_to_other_rse(self):
         """ JUDGE REPAIRER: Test to repair a rule with 1 failed transfer and flip to other rse(lock)"""
 
         rule_repairer(once=True)  # Clean out the repairer
-        scope = 'mock'
-        files = create_files(4, scope, self.rse4, bytes=100)
-        dataset = 'dataset_' + str(uuid())
-        add_did(scope, dataset, DIDType.from_sym('DATASET'), 'jdoe')
-        attach_dids(scope, dataset, files, 'jdoe')
+        scope = InternalScope('mock', **self.vo)
+        files = create_files(4, scope, self.rse4_id, bytes_=100)
+        dataset = did_name_generator('dataset')
+        add_did(scope, dataset, DIDType.DATASET, self.jdoe)
+        attach_dids(scope, dataset, files, self.jdoe)
 
-        rule_id = add_rule(dids=[{'scope': scope, 'name': dataset}], account='jdoe', copies=1, rse_expression=self.T1, grouping='DATASET', weight=None, lifetime=None, locked=False, subscription_id=None)[0]
+        rule_id = add_rule(dids=[{'scope': scope, 'name': dataset}], account=self.jdoe, copies=1, rse_expression=self.T1, grouping='NONE', weight=None, lifetime=None, locked=False, subscription_id=None)[0]
 
         successful_transfer(scope=scope, name=files[0]['name'], rse_id=get_replica_locks(scope=files[0]['scope'], name=files[2]['name'])[0].rse_id, nowait=False)
         successful_transfer(scope=scope, name=files[1]['name'], rse_id=get_replica_locks(scope=files[1]['scope'], name=files[2]['name'])[0].rse_id, nowait=False)
         failed_transfer(scope=scope, name=files[2]['name'], rse_id=get_replica_locks(scope=files[2]['scope'], name=files[2]['name'])[0].rse_id)
         failed_transfer(scope=scope, name=files[3]['name'], rse_id=get_replica_locks(scope=files[3]['scope'], name=files[3]['name'])[0].rse_id)
 
         old_rse_id = get_replica_locks(scope=files[3]['scope'], name=files[3]['name'])[0].rse_id
 
-        assert(rule_id == get_rule(rule_id)['id'].replace('-', '').lower())
-        assert(RuleState.STUCK == get_rule(rule_id)['state'])
+        assert (rule_id == get_rule(rule_id)['id'].replace('-', '').lower())
+        assert (RuleState.STUCK == get_rule(rule_id)['state'])
         rule_repairer(once=True)
-        assert(RuleState.REPLICATING == get_rule(rule_id)['state'])
-        assert(get_replica_locks(scope=files[2]['scope'], name=files[2]['name'])[0].rse_id == get_replica_locks(scope=files[3]['scope'], name=files[3]['name'])[0].rse_id)
-        assert(get_replica_locks(scope=files[3]['scope'], name=files[3]['name'])[0].rse_id != old_rse_id)
+        assert (RuleState.REPLICATING == get_rule(rule_id)['state'])
+        assert (get_replica_locks(scope=files[3]['scope'], name=files[3]['name'])[0].rse_id != old_rse_id)
+
+    def test_to_repair_a_rule_with_only_1_rse_whose_site_is_blocklisted(self):
+        """ JUDGE REPAIRER: Test to repair a rule with only 1 rse whose site is blocklisted"""
+
+        rse = rse_name_generator()
+        rse_id = add_rse(rse, **self.vo)
+        set_local_account_limit(self.jdoe, rse_id, -1)
+        rule_repairer(once=True)  # Clean out the repairer
+
+        region = make_region().configure(
+            'dogpile.cache.pymemcache',
+            expiration_time=900,
+            arguments={'url': config_get('cache', 'url', False, '127.0.0.1:11211'), 'distributed_lock': True}
+        )
+
+        def change_availability(new_value):
+            update_rse(rse_id, {'availability_write': new_value})
+            # clear cache
+            region.delete(sha256(rse.encode()).hexdigest())
+
+        for grouping, ignore_availability in itertools.product(["NONE", "DATASET", "ALL"], [True, False]):
+            scope = InternalScope('mock', **self.vo)
+            files = create_files(1, scope, self.rse4_id, bytes_=100)
+            dataset = did_name_generator('dataset')
+            add_did(scope, dataset, DIDType.DATASET, self.jdoe)
+            attach_dids(scope, dataset, files, self.jdoe)
+
+            if ignore_availability:
+                change_availability(False)
+                rule_id = add_rule(dids=[{'scope': scope, 'name': dataset}], account=self.jdoe, copies=1, rse_expression=rse, grouping=grouping, weight=None, lifetime=None, locked=False, subscription_id=None, ignore_availability=ignore_availability, activity='DebugJudge')[0]
+                assert (RuleState.STUCK == get_rule(rule_id)['state'])
+
+                rule_repairer(once=True)
+                assert (RuleState.REPLICATING == get_rule(rule_id)['state'])
+
+                change_availability(True)
+            else:
+                rule_id = add_rule(dids=[{'scope': scope, 'name': dataset}], account=self.jdoe, copies=1, rse_expression=rse, grouping=grouping, weight=None, lifetime=None, locked=False, subscription_id=None, ignore_availability=ignore_availability, activity='DebugJudge')[0]
+                failed_transfer(scope=scope, name=files[0]['name'], rse_id=get_replica_locks(scope=files[0]['scope'], name=files[0]['name'])[0].rse_id)
+                change_availability(False)
+                assert (RuleState.STUCK == get_rule(rule_id)['state'])
+
+                rule_repairer(once=True)
+                assert (RuleState.STUCK == get_rule(rule_id)['state'])
+
+                change_availability(True)
+                rule_repairer(once=True)
+                assert (RuleState.REPLICATING == get_rule(rule_id)['state'])
```

### Comparing `rucio-clients-1.9.6/lib/rucio/tests/test_naming_convention.py` & `rucio-clients-32.0.0rc1/tests/test_naming_convention.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,70 +1,68 @@
-"""
-   Copyright European Organization for Nuclear Research (CERN)
+# -*- coding: utf-8 -*-
+# Copyright European Organization for Nuclear Research (CERN) since 2012
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#    http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 
-  Licensed under the Apache License, Version 2.0 (the "License");
-  You may not use this file except in compliance with the License.
-  You may obtain a copy of the License at
-  http://www.apache.org/licenses/LICENSE-2.0
-
-  Authors:
-  - Vincent Garonne, <vincent.garonne@cern.ch>, 2015
-"""
+import pytest
 
-# pylint: disable=E0611
-from nose.tools import assert_equal, assert_raises
-
-from rucio.client.didclient import DIDClient
 from rucio.common.exception import InvalidObject
+from rucio.common.types import InternalScope
 from rucio.common.utils import generate_uuid
 from rucio.core.naming_convention import (add_naming_convention,
                                           validate_name,
                                           list_naming_conventions,
                                           delete_naming_convention)
 from rucio.db.sqla.constants import KeyType
 
 
+@pytest.mark.noparallel(reason='changes global naming conventions, breaks other tests')
 class TestNamingConventionCore:
     '''
     Class to test naming convention enforcement.
     '''
 
-    def __init__(self):
-        """ Constructor."""
-        self.did_client = DIDClient()
-
-    def test_naming_convention(self):
+    def test_naming_convention(self, vo, mock_scope, did_client):
         """ NAMING_CONVENTION(CORE): Add and validate naming convention."""
         conventions = {}
         for convention in list_naming_conventions():
             conventions[convention['scope']] = convention['regexp']
 
-        if 'mock' not in conventions:
-            add_naming_convention(scope='mock',
-                                  regexp='^(?P<project>mock)\.(?P<datatype>\w+)\.\w+$',
+        scope = mock_scope
+        if scope not in conventions:
+            add_naming_convention(scope=scope,
+                                  regexp=r'^(?P<project>mock)\.(?P<datatype>\w+)\.\w+$',
                                   convention_type=KeyType.DATASET)
 
-        meta = validate_name(scope='mck', name='mock.DESD.yipeeee', did_type='D')
-        assert_equal(meta, None)
+        meta = validate_name(scope=InternalScope('mck', vo=vo), name='mock.DESD.yipeeee', did_type='D')
+        assert meta is None
 
-        meta = validate_name(scope='mock', name='mock.DESD.yipeeee', did_type='D')
-        assert_equal(meta, {u'project': 'mock', u'datatype': 'DESD'})
+        meta = validate_name(scope=scope, name='mock.DESD.yipeeee', did_type='D')
+        assert meta == {'project': 'mock', 'datatype': 'DESD'}
 
-        with assert_raises(InvalidObject):
-            validate_name(scope='mock', name='mockyipeeee', did_type='D')
+        with pytest.raises(InvalidObject):
+            validate_name(scope=scope, name='mockyipeeee', did_type='D')
 
         # Register a dataset
         tmp_dataset = 'mock.AD.' + str(generate_uuid())
-        with assert_raises(InvalidObject):
-            self.did_client.add_dataset(scope='mock', name=tmp_dataset, meta={'datatype': 'DESD'})
+        with pytest.raises(InvalidObject):
+            did_client.add_dataset(scope='mock', name=tmp_dataset, meta={'datatype': 'DESD'})
 
-        with assert_raises(InvalidObject):
-            self.did_client.add_dataset(scope='mock', name=tmp_dataset)
+        with pytest.raises(InvalidObject):
+            did_client.add_dataset(scope='mock', name=tmp_dataset)
 
         tmp_dataset = 'mock.AOD.' + str(generate_uuid())
-        self.did_client.add_dataset(scope='mock', name=tmp_dataset)
-        observed_datatype = self.did_client.get_metadata(scope='mock', name=tmp_dataset)['datatype']
-        assert_equal(observed_datatype, 'AOD')
-
-        delete_naming_convention(scope='mock',
-                                 regexp='(?P<project>mock)\.(\w+)$',
-                                 convention_type=KeyType.DATASET)
+        did_client.add_dataset(scope='mock', name=tmp_dataset)
+        observed_datatype = did_client.get_metadata(scope='mock', name=tmp_dataset)['datatype']
+        assert observed_datatype == 'AOD'
+
+        delete_naming_convention(scope=scope, convention_type=KeyType.DATASET)
```

### Comparing `rucio-clients-1.9.6/lib/rucio/tests/test_replica.py` & `rucio-clients-32.0.0rc1/tests/test_bad_replica.py`

 * *Files 22% similar despite different names*

```diff
@@ -1,544 +1,478 @@
-# Copyright European Organization for Nuclear Research (CERN)
+# -*- coding: utf-8 -*-
+# Copyright European Organization for Nuclear Research (CERN) since 2012
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
-# You may not use this file except in compliance with the License.
+# you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
-# http://www.apache.org/licenses/LICENSE-2.0
 #
-# Authors:
-# - Vincent Garonne, <vincent.garonne@cern.ch>, 2013-2015
-# - Mario Lassnig, <mario.lassnig@cern.ch>, 2013-2014, 2016
-# - Cedric Serfon, <cedric.serfon@cern.ch>, 2014-2016
-# - Thomas Beermann, <thomas.beermann@cern.ch>, 2014
-
-import xmltodict
+#    http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 
 from datetime import datetime, timedelta
 from json import dumps, loads
-from nose.tools import assert_equal, assert_in, assert_raises
-from paste.fixture import TestApp
-
 
-from rucio.db.sqla.constants import DIDType, ReplicaState
-from rucio.client.baseclient import BaseClient
-from rucio.client.didclient import DIDClient
-from rucio.client.replicaclient import ReplicaClient
-from rucio.common.config import config_get
-from rucio.common.exception import DataIdentifierNotFound, AccessDenied, UnsupportedOperation
-from rucio.common.utils import generate_uuid
-from rucio.core.did import add_did, attach_dids, get_did, set_status, list_files, get_did_atime
-from rucio.core.replica import (add_replica, add_replicas, delete_replicas,
-                                update_replica_lock_counter, get_replica, list_replicas,
-                                declare_bad_file_replicas, list_bad_replicas,
-                                update_replicas_paths, update_replica_state,
-                                get_replica_atime, touch_replica)
-from rucio.daemons.necromancer import run
-from rucio.rse import rsemanager as rsemgr
-from rucio.web.rest.authentication import app as auth_app
-from rucio.web.rest.replica import app as rep_app
-
-
-class TestReplicaCore:
-
-    def test_update_replicas_paths(self):
-        """ REPLICA (CORE): Force update the replica path """
-        tmp_scope = 'mock'
-        nbfiles = 5
-        rse_info = rsemgr.get_rse_info('MOCK')
-        files = [{'scope': tmp_scope,
-                  'name': 'file_%s' % generate_uuid(),
-                  'pfn': 'srm://mock2.com:8443/srm/managerv2?SFN=/rucio/tmpdisk/rucio_tests/does/not/really/matter/where',
-                  'bytes': 1L,
-                  'adler32': '0cc737eb',
-                  'meta': {'events': 10},
-                  'rse_id': rse_info['id'],
-                  'path': '/does/not/really/matter/where'} for i in xrange(nbfiles)]
-        add_replicas(rse='MOCK2', files=files, account='root', ignore_availability=True)
-        update_replicas_paths(files)
-        for replica in list_replicas(dids=[{'scope': f['scope'],
-                                            'name': f['name'],
-                                            'type': DIDType.FILE} for f in files],
-                                     schemes=['srm']):
-            # force the changed string - if we look it up from the DB, then we're not testing anything :-D
-            assert_equal(replica['rses']['MOCK2'][0], 'srm://mock2.com:8443/srm/managerv2?SFN=/rucio/tmpdisk/rucio_tests/does/not/really/matter/where')
-
-    def test_add_list_bad_replicas(self):
-        """ REPLICA (CORE): Add bad replicas and list them"""
-        tmp_scope = 'mock'
-        nbfiles = 5
-        # Adding replicas to deterministic RSE
-        files = [{'scope': tmp_scope, 'name': 'file_%s' % generate_uuid(), 'bytes': 1L, 'adler32': '0cc737eb', 'meta': {'events': 10}} for i in xrange(nbfiles)]
-        rse_info = rsemgr.get_rse_info('MOCK')
-        rse_id1 = rse_info['id']
-        add_replicas(rse='MOCK', files=files, account='root', ignore_availability=True)
-
-        # Listing replicas on deterministic RSE
-        replicas = []
-        list_rep = []
-        for replica in list_replicas(dids=[{'scope': f['scope'], 'name': f['name'], 'type': DIDType.FILE} for f in files], schemes=['srm']):
-            replicas.extend(replica['rses']['MOCK'])
-            list_rep.append(replica)
-        r = declare_bad_file_replicas(replicas, 'This is a good reason', 'root')
-        assert_equal(r, {})
-        bad_replicas = list_bad_replicas()
-        nbbadrep = 0
-        for rep in list_rep:
-            for badrep in bad_replicas:
-                if badrep['rse_id'] == rse_id1:
-                    if badrep['scope'] == rep['scope'] and badrep['name'] == rep['name']:
-                        nbbadrep += 1
-        assert_equal(len(replicas), nbbadrep)
-
-        # Adding replicas to non-deterministic RSE
-        files = [{'scope': tmp_scope, 'name': 'file_%s' % generate_uuid(), 'bytes': 1L, 'adler32': '0cc737eb',
-                  'pfn': 'srm://mock2.com:8443/srm/managerv2?SFN=/rucio/tmpdisk/rucio_tests/%s/%s' % (tmp_scope, generate_uuid()), 'meta': {'events': 10}} for i in xrange(nbfiles)]
-        rse_info = rsemgr.get_rse_info('MOCK2')
-        rse_id2 = rse_info['id']
-        add_replicas(rse='MOCK2', files=files, account='root', ignore_availability=True)
-
-        # Listing replicas on non-deterministic RSE
-        replicas = []
-        list_rep = []
-        for replica in list_replicas(dids=[{'scope': f['scope'], 'name': f['name'], 'type': DIDType.FILE} for f in files], schemes=['srm']):
-            replicas.extend(replica['rses']['MOCK2'])
-            list_rep.append(replica)
-        r = declare_bad_file_replicas(replicas, 'This is a good reason', 'root')
-        assert_equal(r, {})
-        bad_replicas = list_bad_replicas()
-        nbbadrep = 0
-        for rep in list_rep:
-            for badrep in bad_replicas:
-                if badrep['rse_id'] == rse_id2:
-                    if badrep['scope'] == rep['scope'] and badrep['name'] == rep['name']:
-                        nbbadrep += 1
-        assert_equal(len(replicas), nbbadrep)
-
-        # Now adding non-existing bad replicas
-        files = ['srm://mock2.com/rucio/tmpdisk/rucio_tests/%s/%s' % (tmp_scope, generate_uuid()), ]
-        r = declare_bad_file_replicas(files, 'This is a good reason', 'root')
-        output = ['%s Unknown replica' % rep for rep in files]
-        assert_equal(r, {'MOCK2': output})
-
-    def test_add_list_replicas(self):
-        """ REPLICA (CORE): Add and list file replicas """
-        tmp_scope = 'mock'
-        nbfiles = 13
-        files = [{'scope': tmp_scope, 'name': 'file_%s' % generate_uuid(), 'bytes': 1L, 'adler32': '0cc737eb', 'meta': {'events': 10}} for i in xrange(nbfiles)]
-        rses = ['MOCK', 'MOCK3']
-        for rse in rses:
-            add_replicas(rse=rse, files=files, account='root', ignore_availability=True)
-
-        replica_cpt = 0
-        for replica in list_replicas(dids=[{'scope': f['scope'], 'name': f['name'], 'type': DIDType.FILE} for f in files], schemes=['srm']):
-            replica_cpt += 1
-
-        assert_equal(nbfiles, replica_cpt)
-
-    def test_delete_replicas(self):
-        """ REPLICA (CORE): Delete replicas """
-        tmp_scope = 'mock'
-        nbfiles = 5
-        files1 = [{'scope': tmp_scope, 'name': 'file_%s' % generate_uuid(), 'bytes': 1L, 'adler32': '0cc737eb', 'meta': {'events': 10}} for i in xrange(nbfiles)]
-        add_replicas(rse='MOCK', files=files1, account='root', ignore_availability=True)
-
-        files2 = [{'scope': tmp_scope, 'name': 'file_%s' % generate_uuid(), 'bytes': 1L, 'adler32': '0cc737eb', 'meta': {'events': 10}} for i in xrange(nbfiles)]
-        add_replicas(rse='MOCK', files=files2, account='root', ignore_availability=True)
-        add_replicas(rse='MOCK3', files=files2, account='root', ignore_availability=True)
-
-        delete_replicas(rse='MOCK', files=files1 + files2)
-
-        for file in files1:
-            with assert_raises(DataIdentifierNotFound):
-                print get_did(scope=file['scope'], name=file['name'])
-
-        for file in files2:
-            get_did(scope=file['scope'], name=file['name'])
-
-    def test_delete_replicas_from_datasets(self):
-        """ REPLICA (CORE): Delete replicas from dataset """
-        tmp_scope = 'mock'
-        tmp_dsn1 = 'dsn_%s' % generate_uuid()
-        tmp_dsn2 = 'dsn_%s' % generate_uuid()
-        nbfiles = 5
-        files1 = [{'scope': tmp_scope, 'name': 'file_%s' % generate_uuid(), 'bytes': 1L, 'adler32': '0cc737eb', 'meta': {'events': 10}} for i in xrange(nbfiles)]
-
-        add_did(scope=tmp_scope, name=tmp_dsn1, type=DIDType.DATASET, account='root')
-        add_did(scope=tmp_scope, name=tmp_dsn2, type=DIDType.DATASET, account='root')
-
-        attach_dids(scope=tmp_scope, name=tmp_dsn1, rse='MOCK', dids=files1, account='root')
-        attach_dids(scope=tmp_scope, name=tmp_dsn2, dids=files1, account='root')
-
-        set_status(scope=tmp_scope, name=tmp_dsn1, open=False)
-
-        delete_replicas(rse='MOCK', files=files1)
-
-        with assert_raises(DataIdentifierNotFound):
-            get_did(scope=tmp_scope, name=tmp_dsn1)
-
-        get_did(scope=tmp_scope, name=tmp_dsn2)
-
-        assert_equal([f for f in list_files(scope=tmp_scope, name=tmp_dsn2)], [])
-
-    def test_update_lock_counter(self):
-        """ RSE (CORE): Test the update of a replica lock counter """
-        rse = 'MOCK'
-        tmp_scope = 'mock'
-        tmp_file = 'file_%s' % generate_uuid()
-        add_replica(rse=rse, scope=tmp_scope, name=tmp_file, bytes=1L, adler32='0cc737eb', account='jdoe')
-
-        values = (1, 1, 1, -1, -1, -1, 1, 1, -1)
-        tombstones = (True, True, True, True, True, False, True, True, True)
-        lock_counters = (1, 2, 3, 2, 1, 0, 1, 2, 1)
-        for value, tombstone, lock_counter in zip(values, tombstones, lock_counters):
-            status = update_replica_lock_counter(rse=rse, scope=tmp_scope, name=tmp_file, value=value)
-            assert_equal(status, True)
-            replica = get_replica(rse=rse, scope=tmp_scope, name=tmp_file)
-            assert_equal(replica['tombstone'] is None, tombstone)
-            assert_equal(lock_counter, replica['lock_cnt'])
-
-    def test_touch_replicas(self):
-        """ REPLICA (CORE): Touch replicas accessed_at timestamp"""
-        tmp_scope = 'mock'
-        nbfiles = 5
-        files1 = [{'scope': tmp_scope, 'name': 'file_%s' % generate_uuid(), 'bytes': 1L, 'adler32': '0cc737eb', 'meta': {'events': 10}} for i in xrange(nbfiles)]
-        files2 = [{'scope': tmp_scope, 'name': 'file_%s' % generate_uuid(), 'bytes': 1L, 'adler32': '0cc737eb', 'meta': {'events': 10}} for i in xrange(nbfiles)]
-        files2.append(files1[0])
-        add_replicas(rse='MOCK', files=files1, account='root', ignore_availability=True)
-        add_replicas(rse='MOCK', files=files2, account='root', ignore_availability=True)
-
-        now = datetime.utcnow()
-
-        now -= timedelta(microseconds=now.microsecond)
-
-        assert_equal(None, get_replica_atime({'scope': files1[0]['scope'], 'name': files1[0]['name'], 'rse': 'MOCK'}))
-        assert_equal(None, get_did_atime(scope=tmp_scope, name=files1[0]['name']))
-
-        for r in [{'scope': files1[0]['scope'], 'name': files1[0]['name'], 'rse': 'MOCK', 'accessed_at': now}]:
-            touch_replica(r)
-
-        assert_equal(now, get_replica_atime({'scope': files1[0]['scope'], 'name': files1[0]['name'], 'rse': 'MOCK'}))
-        assert_equal(now, get_did_atime(scope=tmp_scope, name=files1[0]['name']))
-
-        for i in range(1, nbfiles):
-            assert_equal(None, get_replica_atime({'scope': files1[i]['scope'], 'name': files1[i]['name'], 'rse': 'MOCK'}))
-
-        for i in range(0, nbfiles - 1):
-            assert_equal(None, get_replica_atime({'scope': files2[i]['scope'], 'name': files2[i]['name'], 'rse': 'MOCK'}))
-
-    def test_list_replicas_all_states(self):
-        """ REPLICA (CORE): list file replicas with all_states"""
-        tmp_scope = 'mock'
-        nbfiles = 13
-        files = [{'scope': tmp_scope, 'name': 'file_%s' % generate_uuid(), 'bytes': 1L, 'adler32': '0cc737eb', 'meta': {'events': 10}} for i in xrange(nbfiles)]
-        rses = ['MOCK', 'MOCK3']
-        for rse in rses:
-            add_replicas(rse=rse, files=files, account='root', ignore_availability=True)
-
-        for file in files:
-            update_replica_state('MOCK', tmp_scope, file['name'], ReplicaState.COPYING)
-
-        replica_cpt = 0
-        for replica in list_replicas(dids=[{'scope': f['scope'], 'name': f['name'], 'type': DIDType.FILE} for f in files], schemes=['srm'], all_states=True):
-            assert_in('states', replica)
-            assert_equal(replica['states']['MOCK'], str(ReplicaState.COPYING))
-            assert_equal(replica['states']['MOCK3'], str(ReplicaState.AVAILABLE))
-            replica_cpt += 1
-
-        assert_equal(nbfiles, replica_cpt)
-
-
-class TestReplicaClients:
-
-    def setup(self):
-        self.replica_client = ReplicaClient()
-        self.did_client = DIDClient()
-
-    def test_add_list_bad_replicas(self):
-        """ REPLICA (CLIENT): Add bad replicas"""
-        tmp_scope = 'mock'
-        nbfiles = 5
-        # Adding replicas to deterministic RSE
-        files = [{'scope': tmp_scope, 'name': 'file_%s' % generate_uuid(), 'bytes': 1L, 'adler32': '0cc737eb', 'meta': {'events': 10}} for i in xrange(nbfiles)]
-        rse_info = rsemgr.get_rse_info('MOCK')
-        rse_id1 = rse_info['id']
-        self.replica_client.add_replicas(rse='MOCK', files=files)
-
-        # Listing replicas on deterministic RSE
-        replicas, list_rep = [], []
-        for replica in self.replica_client.list_replicas(dids=[{'scope': f['scope'], 'name': f['name']} for f in files], schemes=['srm'], unavailable=True):
-            replicas.extend(replica['rses']['MOCK'])
-            list_rep.append(replica)
-        r = self.replica_client.declare_bad_file_replicas(replicas, 'This is a good reason')
-        assert_equal(r, {})
-        bad_replicas = list_bad_replicas()
-        nbbadrep = 0
-        for rep in list_rep:
-            for badrep in bad_replicas:
-                if badrep['rse_id'] == rse_id1:
-                    if badrep['scope'] == rep['scope'] and badrep['name'] == rep['name']:
-                        nbbadrep += 1
-        assert_equal(len(replicas), nbbadrep)
-
-        # Run necromancer once
-        run(threads=1, bulk=10000, once=True)
-
-        # Try to attach a lost file
-        tmp_dsn = 'dataset_%s' % generate_uuid()
-        self.did_client.add_dataset(scope=tmp_scope, name=tmp_dsn)
-        with assert_raises(UnsupportedOperation):
-            self.did_client.add_files_to_dataset(tmp_scope, name=tmp_dsn, files=files, rse='MOCK')
-
-        # Adding replicas to non-deterministic RSE
-        files = [{'scope': tmp_scope, 'name': 'file_%s' % generate_uuid(), 'bytes': 1L, 'adler32': '0cc737eb',
-                  'pfn': 'srm://mock2.com:8443/srm/managerv2?SFN=/rucio/tmpdisk/rucio_tests/%s/%s' % (tmp_scope, generate_uuid()), 'meta': {'events': 10}} for i in xrange(nbfiles)]
-        rse_info = rsemgr.get_rse_info('MOCK2')
-        rse_id2 = rse_info['id']
-        self.replica_client.add_replicas(rse='MOCK2', files=files)
-
-        # Listing replicas on non-deterministic RSE
-        replicas, list_rep = [], []
-        for replica in self.replica_client.list_replicas(dids=[{'scope': f['scope'], 'name': f['name']} for f in files], schemes=['srm'], unavailable=True):
-            replicas.extend(replica['rses']['MOCK2'])
-            list_rep.append(replica)
-        print replicas, list_rep
-        r = self.replica_client.declare_bad_file_replicas(replicas, 'This is a good reason')
-        print r
-        assert_equal(r, {})
-        bad_replicas = list_bad_replicas()
-        nbbadrep = 0
-        for rep in list_rep:
-            for badrep in bad_replicas:
-                if badrep['rse_id'] == rse_id2:
-                    if badrep['scope'] == rep['scope'] and badrep['name'] == rep['name']:
-                        nbbadrep += 1
-        assert_equal(len(replicas), nbbadrep)
-
-        # Now adding non-existing bad replicas
-        files = ['srm://mock2.com/rucio/tmpdisk/rucio_tests/%s/%s' % (tmp_scope, generate_uuid()), ]
-        r = self.replica_client.declare_bad_file_replicas(files, 'This is a good reason')
-        output = ['%s Unknown replica' % rep for rep in files]
-        assert_equal(r, {'MOCK2': output})
-
-    def test_add_suspicious_replicas(self):
-        """ REPLICA (CLIENT): Add suspicious replicas"""
-        tmp_scope = 'mock'
-        nbfiles = 5
-        # Adding replicas to deterministic RSE
-        files = [{'scope': tmp_scope, 'name': 'file_%s' % generate_uuid(), 'bytes': 1L, 'adler32': '0cc737eb', 'meta': {'events': 10}} for i in xrange(nbfiles)]
-        self.replica_client.add_replicas(rse='MOCK', files=files)
-
-        # Listing replicas on deterministic RSE
-        replicas = []
-        list_rep = []
-        for replica in self.replica_client.list_replicas(dids=[{'scope': f['scope'], 'name': f['name']} for f in files], schemes=['srm'], unavailable=True):
-            replicas.extend(replica['rses']['MOCK'])
-            list_rep.append(replica)
-        r = self.replica_client.declare_suspicious_file_replicas(replicas, 'This is a good reason')
-        assert_equal(r, {})
-
-        # Adding replicas to non-deterministic RSE
-        files = [{'scope': tmp_scope, 'name': 'file_%s' % generate_uuid(), 'bytes': 1L, 'adler32': '0cc737eb',
-                  'pfn': 'srm://mock2.com:8443/srm/managerv2?SFN=/rucio/tmpdisk/rucio_tests/%s/%s' % (tmp_scope, generate_uuid()), 'meta': {'events': 10}} for i in xrange(nbfiles)]
-        self.replica_client.add_replicas(rse='MOCK2', files=files)
-
-        # Listing replicas on non-deterministic RSE
-        replicas = []
-        list_rep = []
-        for replica in self.replica_client.list_replicas(dids=[{'scope': f['scope'], 'name': f['name']} for f in files], schemes=['srm'], unavailable=True):
-            replicas.extend(replica['rses']['MOCK2'])
-            list_rep.append(replica)
-        r = self.replica_client.declare_suspicious_file_replicas(replicas, 'This is a good reason')
-        assert_equal(r, {})
-
-        # Now adding non-existing bad replicas
-        files = ['srm://mock2.com/rucio/tmpdisk/rucio_tests/%s/%s' % (tmp_scope, generate_uuid()), ]
-        r = self.replica_client.declare_suspicious_file_replicas(files, 'This is a good reason')
-        output = ['%s Unknown replica' % rep for rep in files]
-        assert_equal(r, {'MOCK2': output})
-
-    def test_bad_replica_methods_for_UI(self):
-        """ REPLICA (REST): Test the listing of bad and suspicious replicas """
-        mw = []
-        headers1 = {'X-Rucio-Account': 'root', 'X-Rucio-Username': 'ddmlab', 'X-Rucio-Password': 'secret'}
-        r1 = TestApp(auth_app.wsgifunc(*mw)).get('/userpass', headers=headers1, expect_errors=True)
-        assert_equal(r1.status, 200)
-        token = str(r1.header('X-Rucio-Auth-Token'))
-        headers2 = {'X-Rucio-Auth-Token': str(token)}
-
-        data = dumps({})
-        r2 = TestApp(rep_app.wsgifunc(*mw)).get('/bad/states', headers=headers2, params=data, expect_errors=True)
-        assert_equal(r2.status, 200)
-        tot_files = []
-        for line in r2.body.split('\n'):
-            if line != '':
-                tot_files.append(dumps(line))
-        nb_tot_files = len(tot_files)
-
-        data = dumps({'state': 'B'})
-        r2 = TestApp(rep_app.wsgifunc(*mw)).get('/bad/states', headers=headers2, params=data, expect_errors=True)
-        assert_equal(r2.status, 200)
-        tot_bad_files = []
-        for line in r2.body.split('\n'):
-            if line != '':
-                tot_bad_files.append(dumps(line))
-        nb_tot_bad_files1 = len(tot_bad_files)
+import pytest
 
-        data = dumps({'state': 'S', 'list_pfns': 'True'})
-        r2 = TestApp(rep_app.wsgifunc(*mw)).get('/bad/states', headers=headers2, params=data, expect_errors=True)
-        assert_equal(r2.status, 200)
-        tot_suspicious_files = []
-        for line in r2.body.split('\n'):
-            if line != '':
-                tot_suspicious_files.append(dumps(line))
-        nb_tot_suspicious_files = len(tot_suspicious_files)
-
-        assert_equal(nb_tot_files, nb_tot_bad_files1 + nb_tot_suspicious_files)
-
-        tomorrow = datetime.utcnow() + timedelta(2)
-        data = dumps({'state': 'B', 'younger_than': tomorrow.isoformat()})
-        r2 = TestApp(rep_app.wsgifunc(*mw)).get('/bad/states', headers=headers2, params=data, expect_errors=True)
-        assert_equal(r2.status, 200)
-        tot_bad_files = []
-        for line in r2.body.split('\n'):
-            if line != '':
-                tot_bad_files.append(dumps(line))
-        nb_tot_bad_files = len(tot_bad_files)
-        assert_equal(nb_tot_bad_files, 0)
-
-        data = dumps({})
-        r2 = TestApp(rep_app.wsgifunc(*mw)).get('/bad/summary', headers=headers2, params=data, expect_errors=True)
-        assert_equal(r2.status, 200)
+from rucio.common.exception import RucioException, UnsupportedOperation, InvalidType
+from rucio.common.utils import generate_uuid, clean_surls
+from rucio.core.did import delete_dids
+from rucio.core.replica import (add_replicas, get_replicas_state, list_replicas,
+                                declare_bad_file_replicas, list_bad_replicas, get_bad_pfns,
+                                get_bad_replicas_backlog, list_bad_replicas_status, get_pfn_to_rse)
+from rucio.client.rseclient import RSEClient
+from rucio.daemons.badreplicas.minos import run as minos_run
+from rucio.daemons.badreplicas.minos_temporary_expiration import run as minos_temp_run
+from rucio.daemons.badreplicas.necromancer import run as necromancer_run
+from rucio.daemons.badreplicas.necromancer import REGION
+from rucio.db.sqla.constants import DIDType, ReplicaState, BadPFNStatus, BadFilesStatus
+from rucio.tests.common import headers, auth
+
+
+@pytest.fixture
+def rse_client():
+    return RSEClient()
+
+
+@pytest.mark.noparallel(reason='calls list_bad_replicas() which acts on all bad replicas without any filtering')
+def test_add_list_bad_replicas(rse_factory, mock_scope, root_account):
+    """ REPLICA (CORE): Add bad replicas and list them"""
+
+    nbfiles = 5
+    # Adding replicas to deterministic RSE
+    _, rse1_id = rse_factory.make_srm_rse(deterministic=True)
+    files = [{'scope': mock_scope, 'name': 'file_%s' % generate_uuid(), 'bytes': 1, 'adler32': '0cc737eb', 'meta': {'events': 10}} for _ in range(nbfiles)]
+    add_replicas(rse_id=rse1_id, files=files, account=root_account, ignore_availability=True)
+
+    # Listing replicas on deterministic RSE
+    replicas = []
+    list_rep = []
+    for replica in list_replicas(dids=[{'scope': f['scope'], 'name': f['name'], 'type': DIDType.FILE} for f in files], schemes=['srm']):
+        replicas.extend(replica['rses'][rse1_id])
+        list_rep.append(replica)
+    r = declare_bad_file_replicas(replicas, 'This is a good reason', root_account)
+    assert r == {}
+    bad_replicas = list_bad_replicas()
+    nbbadrep = 0
+    for rep in list_rep:
+        for badrep in bad_replicas:
+            if badrep['rse_id'] == rse1_id:
+                if badrep['scope'] == rep['scope'] and badrep['name'] == rep['name']:
+                    nbbadrep += 1
+    assert len(replicas) == nbbadrep
+
+    # Adding replicas to non-deterministic RSE
+    _, rse2_id = rse_factory.make_srm_rse(deterministic=False)
+    files = [{'scope': mock_scope, 'name': 'file_%s' % generate_uuid(), 'bytes': 1, 'adler32': '0cc737eb',
+              'pfn': 'srm://%s.cern.ch/srm/managerv2?SFN=/test_%s/%s/%s' % (rse2_id, rse2_id, mock_scope, generate_uuid()), 'meta': {'events': 10}} for _ in range(nbfiles)]
+    add_replicas(rse_id=rse2_id, files=files, account=root_account, ignore_availability=True)
+
+    # Listing replicas on non-deterministic RSE
+    replicas = []
+    list_rep = []
+    for replica in list_replicas(dids=[{'scope': f['scope'], 'name': f['name'], 'type': DIDType.FILE} for f in files], schemes=['srm']):
+        replicas.extend(replica['rses'][rse2_id])
+        list_rep.append(replica)
+    r = declare_bad_file_replicas(replicas, 'This is a good reason', root_account)
+    assert r == {}
+    bad_replicas = list_bad_replicas()
+    nbbadrep = 0
+    for rep in list_rep:
+        for badrep in bad_replicas:
+            if badrep['rse_id'] == rse2_id:
+                if badrep['scope'] == rep['scope'] and badrep['name'] == rep['name']:
+                    nbbadrep += 1
+    assert len(replicas) == nbbadrep
+
+    # Try adding replicas already declared bad
+    r = declare_bad_file_replicas(replicas, 'This is a good reason', root_account)
+    output = ['%s Unknown replica' % rep for rep in replicas]
+    assert list(r.keys()) == [rse2_id]
+    list1 = r[rse2_id]
+    list1.sort()
+    list2 = ['%s Already declared' % clean_surls([rep])[0] for rep in replicas]
+    list2.sort()
+    assert list1 == list2
+
+    # Now adding non-existing bad replicas
+    files = ['srm://%s.cern.ch/test_%s/%s/%s' % (rse2_id, rse2_id, mock_scope, generate_uuid()), ]
+    r = declare_bad_file_replicas(files, 'This is a good reason', root_account)
+    output = ['%s Unknown replica' % rep for rep in files]
+    assert r == {rse2_id: output}
+
+
+@pytest.mark.noparallel(reason='runs necromancer which acts on all bad replicas without any filtering')
+@pytest.mark.parametrize("file_config_mock", [{
+    "overrides": [('necromancer', 'max_bad_replicas_backlog_count', '20')]
+}], indirect=True)
+def test_get_bad_replicas_backlog(rse_factory, mock_scope, root_account, file_config_mock):
+    """ REPLICA (CORE): Check the behaviour of the necromancer in case of backlog on an RSE"""
+
+    # Run necromancer once
+    necromancer_run(threads=1, bulk=10000, once=True)
+
+    nbfiles1 = 100
+    nbfiles2 = 20
+    # Adding replicas to deterministic RSE
+    rse1, rse1_id = rse_factory.make_srm_rse(deterministic=True)
+    _, rse2_id = rse_factory.make_srm_rse(deterministic=True)
+
+    # Create bad replicas on rse1
+    files = [{'scope': mock_scope, 'name': 'file_%s' % generate_uuid(), 'bytes': 1, 'adler32': '0cc737eb', 'meta': {'events': 10}} for _ in range(nbfiles1)]
+    add_replicas(rse_id=rse1_id, files=files, account=root_account, ignore_availability=True)
+
+    replicas = []
+    list_rep = []
+    for replica in list_replicas(dids=[{'scope': f['scope'], 'name': f['name'], 'type': DIDType.FILE} for f in files], schemes=['srm']):
+        replicas.extend(replica['rses'][rse1_id])
+        list_rep.append({'scope': replica['scope'], 'name': replica['name'], 'rse': rse1, 'rse_id': rse1_id})
+    res = declare_bad_file_replicas(replicas, 'This is a good reason', root_account)
+    assert res == {}
+
+    result = get_bad_replicas_backlog()
+    assert rse1_id in result
+    assert result[rse1_id] == nbfiles1
+
+    # Create more bad replicas on rse2
+    files = [{'scope': mock_scope, 'name': 'file_%s' % generate_uuid(), 'bytes': 1, 'adler32': '0cc737eb', 'meta': {'events': 10}} for _ in range(nbfiles2)]
+    add_replicas(rse_id=rse2_id, files=files, account=root_account, ignore_availability=True)
+
+    repl = []
+    for replica in list_replicas(dids=[{'scope': f['scope'], 'name': f['name'], 'type': DIDType.FILE} for f in files], schemes=['srm']):
+        repl.extend(replica['rses'][rse2_id])
+    res = declare_bad_file_replicas(repl, 'This is a good reason', root_account)
+    assert res == {}
+
+    # List bad replicas on rse1
+    bad_replicas = list_bad_replicas(rses=[{'id': rse1_id}])
+    assert len(bad_replicas) == nbfiles1
+    for rep in bad_replicas:
+        assert rep in list_rep
+
+    # Run necromancer once, all the files on RSE2 should be gone, 80 files should stay on RSE1
+    REGION.invalidate()
+    get_bad_replicas_backlog()
+    necromancer_run(threads=1, bulk=20, once=True)
+
+    bad_replicas = list_bad_replicas(rses=[{'id': rse1_id}, {'id': rse2_id}])
+    assert len(bad_replicas) == 80
+    for rep in bad_replicas:
+        assert rep['rse_id'] == rse1_id
+
+
+@pytest.mark.noparallel(reason='calls list_bad_replicas() and runs necromancer. Both act on all bad replicas without any filtering')
+def test_client_add_list_bad_replicas(rse_factory, replica_client, did_client):
+    """ REPLICA (CLIENT): Add bad replicas"""
+    tmp_scope = 'mock'
+    nbfiles = 5
+    # Adding replicas to deterministic RSE
+    files = [{'scope': tmp_scope, 'name': 'file_%s' % generate_uuid(), 'bytes': 1, 'adler32': '0cc737eb', 'meta': {'events': 10}} for _ in range(nbfiles)]
+    rse1, rse1_id = rse_factory.make_srm_rse(deterministic=True)
+    replica_client.add_replicas(rse=rse1, files=files)
+
+    # Listing replicas on deterministic RSE
+    replicas, list_rep = [], []
+    for replica in replica_client.list_replicas(dids=[{'scope': f['scope'], 'name': f['name']} for f in files], schemes=['srm'], all_states=True):
+        replicas.extend(replica['rses'][rse1])
+        list_rep.append(replica)
+    r = replica_client.declare_bad_file_replicas(replicas, 'This is a good reason')
+    assert r == {}
+    bad_replicas = list_bad_replicas()
+    nbbadrep = 0
+    for rep in list_rep:
+        for badrep in bad_replicas:
+            if badrep['rse_id'] == rse1_id:
+                if badrep['scope'].external == rep['scope'] and badrep['name'] == rep['name']:
+                    nbbadrep += 1
+    assert len(replicas) == nbbadrep
+
+    # Run necromancer once
+    necromancer_run(threads=1, bulk=10000, once=True)
+
+    # Try to attach a lost file
+    tmp_dsn = 'dataset_%s' % generate_uuid()
+    did_client.add_dataset(scope=tmp_scope, name=tmp_dsn)
+    with pytest.raises(UnsupportedOperation):
+        did_client.add_files_to_dataset(tmp_scope, name=tmp_dsn, files=files, rse=rse1)
+
+    # Adding replicas to non-deterministic RSE
+    rse2, rse2_id = rse_factory.make_srm_rse(deterministic=False)
+    files = [{'scope': tmp_scope, 'name': 'file_%s' % generate_uuid(), 'bytes': 1, 'adler32': '0cc737eb',
+              'pfn': 'srm://%s.cern.ch/srm/managerv2?SFN=/test_%s/%s/%s' % (rse2_id, rse2_id, tmp_scope, generate_uuid()), 'meta': {'events': 10}} for _ in range(nbfiles)]
+    replica_client.add_replicas(rse=rse2, files=files)
+
+    # Listing replicas on non-deterministic RSE
+    replicas, list_rep = [], []
+    for replica in replica_client.list_replicas(dids=[{'scope': f['scope'], 'name': f['name']} for f in files], schemes=['srm'], all_states=True):
+        replicas.extend(replica['rses'][rse2])
+        list_rep.append(replica)
+    r = replica_client.declare_bad_file_replicas(replicas, 'This is a good reason')
+    assert r == {}
+    bad_replicas = list_bad_replicas()
+    nbbadrep = 0
+    for rep in list_rep:
+        for badrep in bad_replicas:
+            if badrep['rse_id'] == rse2_id:
+                if badrep['scope'].external == rep['scope'] and badrep['name'] == rep['name']:
+                    nbbadrep += 1
+    assert len(replicas) == nbbadrep
+
+    # Now adding non-existing bad replicas
+    files = ['srm://%s.cern.ch/test_%s/%s/%s' % (rse2_id, rse2_id, tmp_scope, generate_uuid()), ]
+    r = replica_client.declare_bad_file_replicas(files, 'This is a good reason')
+    output = ['%s Unknown replica' % rep for rep in files]
+    assert r == {rse2: output}
+
+    # Now test adding bad_replicas with a list of replicas instead of PFNs
+    # Adding replicas to deterministic RSE
+    rse3, rse3_id = rse_factory.make_srm_rse(deterministic=True)
+    files = [{'scope': tmp_scope, 'name': 'file_%s' % generate_uuid(), 'bytes': 1, 'adler32': '0cc737eb', 'meta': {'events': 10}} for _ in range(nbfiles)]
+    replica_client.add_replicas(rse=rse3, files=files)
+    list_rep = [{'scope': file_['scope'], 'name': file_['name'], 'rse': rse3} for file_ in files]
+
+    # Listing replicas on deterministic RSE
+    replicas = []
+    for replica in replica_client.list_replicas(dids=[{'scope': f['scope'], 'name': f['name']} for f in files], schemes=['srm'], all_states=True):
+        replicas.extend(replica['rses'][rse3])
+    r = replica_client.declare_bad_file_replicas(list_rep, 'This is a good reason')
+    assert r == {}
+    bad_replicas = list_bad_replicas()
+    nbbadrep = 0
+    for rep in list_rep:
+        for badrep in bad_replicas:
+            if badrep['rse_id'] == rse3_id:
+                if badrep['scope'].external == rep['scope'] and badrep['name'] == rep['name']:
+                    nbbadrep += 1
+    assert len(replicas) == nbbadrep
+
+    # InvalidType is raised if list_rep contains a mixture of replicas and PFNs
+    list_rep.extend(['srm://%s.cern.ch/test_%s/%s/%s' % (rse2_id, rse2_id, tmp_scope, generate_uuid()), ])
+    with pytest.raises(InvalidType):
+        r = replica_client.declare_bad_file_replicas(list_rep, 'This is a good reason')
+
+
+def test_client_add_suspicious_replicas(rse_factory, replica_client):
+    """ REPLICA (CLIENT): Add suspicious replicas"""
+    tmp_scope = 'mock'
+    nbfiles = 5
+    # Adding replicas to deterministic RSE
+    rse1, _ = rse_factory.make_srm_rse(deterministic=True)
+    files = [{'scope': tmp_scope, 'name': 'file_%s' % generate_uuid(), 'bytes': 1, 'adler32': '0cc737eb', 'meta': {'events': 10}} for _ in range(nbfiles)]
+    replica_client.add_replicas(rse=rse1, files=files)
+
+    # Listing replicas on deterministic RSE
+    replicas = []
+    list_rep = []
+    for replica in replica_client.list_replicas(dids=[{'scope': f['scope'], 'name': f['name']} for f in files], schemes=['srm'], all_states=True):
+        replicas.extend(replica['rses'][rse1])
+        list_rep.append(replica)
+    r = replica_client.declare_suspicious_file_replicas(replicas, 'This is a good reason')
+    assert r == {}
+
+    # Adding replicas to non-deterministic RSE
+    rse2, rse2_id = rse_factory.make_srm_rse(deterministic=False)
+    files = [{'scope': tmp_scope, 'name': 'file_%s' % generate_uuid(), 'bytes': 1, 'adler32': '0cc737eb',
+              'pfn': 'srm://%s.cern.ch/srm/managerv2?SFN=/test_%s/%s/%s' % (rse2_id, rse2_id, tmp_scope, generate_uuid()), 'meta': {'events': 10}} for _ in range(nbfiles)]
+    replica_client.add_replicas(rse=rse2, files=files)
+
+    # Listing replicas on non-deterministic RSE
+    replicas = []
+    list_rep = []
+    for replica in replica_client.list_replicas(dids=[{'scope': f['scope'], 'name': f['name']} for f in files], schemes=['srm'], all_states=True):
+        replicas.extend(replica['rses'][rse2])
+        list_rep.append(replica)
+    r = replica_client.declare_suspicious_file_replicas(replicas, 'This is a good reason')
+    assert r == {}
+    # Now adding non-existing bad replicas
+    files = ['srm://%s.cern.ch/test_%s/%s/%s' % (rse2_id, rse2_id, tmp_scope, generate_uuid()), ]
+    r = replica_client.declare_suspicious_file_replicas(files, 'This is a good reason')
+    output = ['%s Unknown replica' % rep for rep in files]
+    assert r == {rse2: output}
+
+
+@pytest.mark.noparallel(reason='Lists bad replicas multiple times. If the list changes between calls, test fails.')
+def test_rest_bad_replica_methods_for_ui(rest_client, auth_token):
+    __test_rest_bad_replica_methods_for_ui(rest_client, auth_token, list_pfns=False)
+    __test_rest_bad_replica_methods_for_ui(rest_client, auth_token, list_pfns=True)
+
+
+def __test_rest_bad_replica_methods_for_ui(rest_client, auth_token, list_pfns):
+    """ REPLICA (REST): Test the listing of bad and suspicious replicas """
+
+    def _fetch_bad_replicas(query_data):
+        response = rest_client.get('/replicas/bad/states', headers=headers(auth(auth_token)), query_string=query_data)
+        assert response.status_code == 200
+        _files = []
+        for _line in response.get_data(as_text=True).split('\n'):
+            if _line != '':
+                _files.append(dumps(_line))
+        return _files
+
+    if list_pfns:
+        common_data = {'list_pfns': 'True'}
+    else:
+        common_data = {}
+
+    total = _fetch_bad_replicas(query_data={**common_data})
+    bad = _fetch_bad_replicas(query_data={'state': 'B', **common_data})
+    suspicious = _fetch_bad_replicas(query_data={'state': 'S', **common_data})
+    temporary_unavailable = _fetch_bad_replicas(query_data={'state': 'T', **common_data})
+    lost = _fetch_bad_replicas(query_data={'state': 'L', **common_data})
+    assert len(total) == len(bad) + len(suspicious) + len(temporary_unavailable) + len(lost)
+
+    tomorrow = datetime.utcnow() + timedelta(days=1)
+    assert len(_fetch_bad_replicas(query_data={'state': 'B', 'younger_than': tomorrow.isoformat(), **common_data})) == 0
+
+    if not list_pfns:
+        response = rest_client.get('/replicas/bad/summary', headers=headers(auth(auth_token)))
+        assert response.status_code == 200
         nb_tot_bad_files2 = 0
-        for line in r2.body.split('\n'):
+        for line in response.get_data(as_text=True).split('\n'):
             if line != '':
                 line = loads(line)
-                nb_tot_bad_files2 += int(line['BAD'])
-        assert_equal(nb_tot_bad_files1, nb_tot_bad_files2)
+                nb_tot_bad_files2 += int(line.get('BAD', 0))
+        assert len(bad) == nb_tot_bad_files2
+
 
-    def test_add_list_replicas(self):
-        """ REPLICA (CLIENT): Add, change state and list file replicas """
-        tmp_scope = 'mock'
-        nbfiles = 5
-
-        files1 = [{'scope': tmp_scope, 'name': 'file_%s' % generate_uuid(), 'bytes': 1L, 'adler32': '0cc737eb', 'meta': {'events': 10}} for i in xrange(nbfiles)]
-        self.replica_client.add_replicas(rse='MOCK', files=files1)
-
-        files2 = [{'scope': tmp_scope, 'name': 'file_%s' % generate_uuid(), 'bytes': 1L, 'adler32': '0cc737eb', 'meta': {'events': 10}} for i in xrange(nbfiles)]
-        self.replica_client.add_replicas(rse='MOCK3', files=files2)
-
-        replicas = [r for r in self.replica_client.list_replicas(dids=[{'scope': i['scope'], 'name': i['name']} for i in files1])]
-        assert_equal(len(replicas), len(files1))
-
-        replicas = [r for r in self.replica_client.list_replicas(dids=[{'scope': i['scope'], 'name': i['name']} for i in files2], schemes=['file'])]
-        assert_equal(len(replicas), 5)
-
-        replicas = [r for r in self.replica_client.list_replicas(dids=[{'scope': i['scope'], 'name': i['name']} for i in files2], schemes=['srm'])]
-        assert_equal(len(replicas), 5)
-
-        files3 = [{'scope': tmp_scope, 'name': 'file_%s' % generate_uuid(), 'bytes': 1L, 'adler32': '0cc737eb', 'state': 'U', 'meta': {'events': 10}} for i in xrange(nbfiles)]
-        self.replica_client.add_replicas(rse='MOCK3', files=files3)
-        replicas = [r for r in self.replica_client.list_replicas(dids=[{'scope': i['scope'], 'name': i['name']} for i in files3], schemes=['file'])]
-        for i in xrange(nbfiles):
-            assert_equal(replicas[i]['rses'], {})
-        files4 = []
-        for file in files3:
-            file['state'] = 'A'
-            files4.append(file)
-        self.replica_client.update_replicas_states('MOCK3', files=files4)
-        replicas = [r for r in self.replica_client.list_replicas(dids=[{'scope': i['scope'], 'name': i['name']} for i in files3], schemes=['file'], unavailable=True)]
-        assert_equal(len(replicas), 5)
-        for i in xrange(nbfiles):
-            assert_in('MOCK3', replicas[i]['rses'])
-
-    def test_delete_replicas(self):
-        """ REPLICA (CLIENT): Add and delete file replicas """
-        tmp_scope = 'mock'
-        nbfiles = 5
-        files = [{'scope': tmp_scope, 'name': 'file_%s' % generate_uuid(), 'bytes': 1L, 'adler32': '0cc737eb', 'meta': {'events': 10}} for i in xrange(nbfiles)]
-        self.replica_client.add_replicas(rse='MOCK', files=files)
-        with assert_raises(AccessDenied):
-            self.replica_client.delete_replicas(rse='MOCK', files=files)
-
-        # replicas = [r for r in self.replica_client.list_replicas(dids=[{'scope': i['scope'], 'name': i['name']} for i in files])]
-        # assert_equal(len(replicas), 0)
-
-
-class TestReplicaMetalink:
-
-    def setup(self):
-        self.did_client = DIDClient()
-        self.replica_client = ReplicaClient()
-        self.base_client = BaseClient(account='root',
-                                      ca_cert=config_get('client', 'ca_cert'),
-                                      auth_type='x509')
-        self.token = self.base_client.headers['X-Rucio-Auth-Token']
-
-        self.fname = generate_uuid()
-
-        rses = ['MOCK', 'MOCK3', 'MOCK4']
-        dsn = generate_uuid()
-        self.files = [{'scope': 'mock', 'name': self.fname, 'bytes': 1L, 'adler32': '0cc737eb'}]
-
-        self.did_client.add_dataset(scope='mock', name=dsn)
-        self.did_client.add_files_to_dataset('mock', name=dsn, files=self.files, rse='MOCK')
-        for r in rses:
-            self.replica_client.add_replicas(r, self.files)
-
-    def test_list_replicas_metalink_3(self):
-        """ REPLICA (METALINK): List replicas as metalink version 3 """
-        ml = xmltodict.parse(self.replica_client.list_replicas(self.files,
-                                                               metalink=3,
-                                                               unavailable=True,
-                                                               schemes=['https', 'sftp', 'file']),
-                             xml_attribs=False)
-        assert_equal(3, len(ml['metalink']['files']['file']['resources']['url']))
-
-    def test_list_replicas_metalink_4(self):
-        """ REPLICA (METALINK): List replicas as metalink version 4 """
-        ml = xmltodict.parse(self.replica_client.list_replicas(self.files,
-                                                               metalink=4,
-                                                               unavailable=True,
-                                                               schemes=['https', 'sftp', 'file']),
-                             xml_attribs=False)
-        assert_equal(3, len(ml['metalink']['file']['url']))
-
-    def test_get_did_from_pfns_nondeterministic(self):
-        """ REPLICA (CLIENT): Get list of DIDs associated to PFNs for non-deterministic sites"""
-        rse = 'MOCK2'
-        tmp_scope = 'mock'
-        nbfiles = 3
-        pfns = []
-        input = {}
-        rse_info = rsemgr.get_rse_info(rse)
-        assert_equal(rse_info['deterministic'], False)
-        files = [{'scope': tmp_scope, 'name': 'file_%s' % generate_uuid(), 'bytes': 1L, 'adler32': '0cc737eb',
-                  'pfn': 'srm://mock2.com:8443/srm/managerv2?SFN=/rucio/tmpdisk/rucio_tests/%s/%s' % (tmp_scope, generate_uuid()), 'meta': {'events': 10}} for i in xrange(nbfiles)]
-        for f in files:
-            input[f['pfn']] = {'scope': f['scope'], 'name': f['name']}
-        add_replicas(rse=rse, files=files, account='root', ignore_availability=True)
-        for replica in list_replicas(dids=[{'scope': f['scope'], 'name': f['name'], 'type': DIDType.FILE} for f in files], schemes=['srm'], ignore_availability=True):
-            for rse in replica['rses']:
-                pfns.extend(replica['rses'][rse])
-        for result in self.replica_client.get_did_from_pfns(pfns, rse):
-            pfn = result.keys()[0]
-            assert_equal(input[pfn], result.values()[0])
-
-    def test_get_did_from_pfns_deterministic(self):
-        """ REPLICA (CLIENT): Get list of DIDs associated to PFNs for deterministic sites"""
-        tmp_scope = 'mock'
-        rse = 'MOCK3'
-        nbfiles = 3
-        pfns = []
-        input = {}
-        rse_info = rsemgr.get_rse_info(rse)
-        assert_equal(rse_info['deterministic'], True)
-        files = [{'scope': tmp_scope, 'name': 'file_%s' % generate_uuid(), 'bytes': 1L, 'adler32': '0cc737eb', 'meta': {'events': 10}} for i in xrange(nbfiles)]
-        p = rsemgr.create_protocol(rse_info, 'read', scheme='srm')
-        for f in files:
-            pfn = p.lfns2pfns(lfns={'scope': f['scope'], 'name': f['name']}).values()[0]
-            pfns.append(pfn)
-            input[pfn] = {'scope': f['scope'], 'name': f['name']}
-        add_replicas(rse=rse, files=files, account='root', ignore_availability=True)
-        for result in self.replica_client.get_did_from_pfns(pfns, rse):
-            pfn = result.keys()[0]
-            assert_equal(input[pfn], result.values()[0])
+@pytest.mark.dirty
+@pytest.mark.noparallel(reason='runs minos, which acts on all bad pfns')
+def test_client_add_temporary_unavailable_pfns(rse_factory, mock_scope, replica_client):
+    """ REPLICA (CLIENT): Add temporary unavailable PFNs"""
+    rse, rse_id = rse_factory.make_posix_rse()
+    nbfiles = 5
+    # Adding replicas to deterministic RSE
+    files = [{'scope': mock_scope.external, 'name': 'file_%s' % generate_uuid(), 'bytes': 1, 'adler32': '0cc737eb', 'meta': {'events': 10}} for _ in range(nbfiles)]
+    replica_client.add_replicas(rse=rse, files=files)
+
+    # Listing replicas on deterministic RSE
+    list_rep = []
+    for replica in replica_client.list_replicas(dids=[{'scope': f['scope'], 'name': f['name']} for f in files], schemes=['file'], all_states=True):
+        pfn = list(replica['pfns'].keys())[0]
+        list_rep.append(pfn)
+
+    # Submit bad PFNs
+    now = datetime.utcnow()
+    reason_str = generate_uuid()
+    replica_client.add_bad_pfns(pfns=list_rep, reason=str(reason_str), state='TEMPORARY_UNAVAILABLE', expires_at=now.isoformat())
+    result = get_bad_pfns(limit=10000, thread=None, total_threads=None, session=None)
+    bad_pfns = {}
+    for res in result:
+        bad_pfns[res['pfn']] = (res['state'], res['reason'], res['expires_at'])
+
+    for pfn in list_rep:
+        pfn = str(clean_surls([pfn])[0])
+        assert pfn in bad_pfns
+        assert bad_pfns[pfn][0] == BadPFNStatus.TEMPORARY_UNAVAILABLE
+        assert bad_pfns[pfn][1] == reason_str
+
+    # Submit with wrong state
+    with pytest.raises(RucioException):
+        replica_client.add_bad_pfns(pfns=list_rep, reason=str(reason_str), state='BADSTATE', expires_at=now.isoformat())
+
+    # Run minos once
+    minos_run(threads=1, bulk=10000, once=True)
+    result = get_bad_pfns(limit=10000, thread=None, total_threads=None, session=None)
+    pfns = [res['pfn'] for res in result if res['pfn'] in bad_pfns]
+    res_pfns = []
+    for replica in list_rep:
+        if replica in pfns:
+            res_pfns.append(replica)
+    assert res_pfns == []
+
+    # Check the state in the replica table
+    for did in files:
+        rep = get_replicas_state(scope=mock_scope, name=did['name'])
+        assert list(rep.keys())[0] == ReplicaState.TEMPORARY_UNAVAILABLE
+
+    rep = []
+    for did in files:
+        did['state'] = ReplicaState.TEMPORARY_UNAVAILABLE
+        rep.append(did)
+
+    # Run the minos expiration
+    minos_temp_run(threads=1, once=True)
+    # Check the state in the replica table
+    for did in files:
+        rep = get_replicas_state(scope=mock_scope, name=did['name'])
+        assert list(rep.keys())[0] == ReplicaState.AVAILABLE
+
+
+def test_add_and_delete_bad_replicas(rse_factory, mock_scope, root_account, did_client, vo):
+    """ REPLICA (CORE): Add bad replicas and delete them"""
+    # Adding replicas to deterministic RSE
+    nbfiles = 5
+    rse1, rse1_id = rse_factory.make_srm_rse(deterministic=True, vo=vo)
+    files = [{'scope': mock_scope, 'name': 'file_%s' % generate_uuid(), 'bytes': 1, 'adler32': '0cc737eb', 'meta': {'events': 10}} for _ in range(nbfiles)]
+    client_files = [{'scope': file_['scope'].external, 'name': file_['name']} for file_ in files]
+    add_replicas(rse_id=rse1_id, files=files, account=root_account, ignore_availability=True)
+    tmp_dsn = 'dataset_%s' % generate_uuid()
+    did_client.add_dataset(scope=mock_scope.external, name=tmp_dsn)
+    did_client.add_files_to_dataset(mock_scope.external, name=tmp_dsn, files=client_files, rse=rse1)
+
+    # Declare replica bad
+    replicas = []
+    for replica in list_replicas(dids=[{'scope': f['scope'], 'name': f['name'], 'type': DIDType.FILE} for f in files], schemes=['srm']):
+        replicas.extend(replica['rses'][rse1_id])
+    r = declare_bad_file_replicas(replicas, 'This is a good reason', root_account)
+    assert r == {}
+
+    # Check state of bad replicas
+    list_bad_rep = [{'scope': rep['scope'].external, 'name': rep['name']} for rep in list_bad_replicas_status(state=BadFilesStatus.BAD, rse_id=rse1_id, vo=vo)]
+    for rep in client_files:
+        assert rep in list_bad_rep
+    assert [rep for rep in list_bad_replicas_status(state=BadFilesStatus.DELETED, rse_id=rse1_id, vo=vo)] == []
+
+    # Now delete the dataset
+    delete_dids([{'scope': mock_scope, 'name': tmp_dsn, 'did_type': DIDType.DATASET, 'purge_replicas': True}], account=root_account)
+    assert [rep for rep in list_bad_replicas_status(state=BadFilesStatus.BAD, rse_id=rse1_id, vo=vo)] == []
+    list_deleted_rep = [{'scope': rep['scope'].external, 'name': rep['name']} for rep in list_bad_replicas_status(state=BadFilesStatus.DELETED, rse_id=rse1_id, vo=vo)]
+    for rep in client_files:
+        assert rep in list_deleted_rep
+
+
+def test_get_pfn_to_rse(rse_factory, rse_client, vo):
+    """ REPLICA (CORE): Test that get_pfn_to_rse is able to handle same scheme with different port"""
+
+    # Adding replicas to deterministic RSE
+    rse1, rse1_id = rse_factory.make_rse(scheme='file', protocol_impl='rucio.rse.protocols.posix.Default', vo=vo)
+    protocols = [{'scheme': 'MOCK',
+                  'hostname': 'get-pfn-to-rse',
+                  'port': 17,
+                  'prefix': '/the/one/with/all/the/files',
+                  'impl': 'rucio.rse.protocols.SomeProtocol.SomeImplementation',
+                  'domains': {
+                      'lan': {'read': 4,
+                              'write': 1,
+                              'delete': None}
+                  },
+                  'extended_attributes': 'TheOneWithAllTheRest'},
+                 {'scheme': 'MOCK',
+                  'hostname': 'get-pfn-to-rse',
+                  'port': 18,
+                  'prefix': '/the/one/with/all/the/files',
+                  'impl': 'rucio.rse.protocols.SomeProtocol.SomeImplementation',
+                  'domains': {
+                      'lan': {'read': 1,
+                              'write': 1,
+                              'delete': None}},
+                  'extended_attributes': 'TheOneWithAllTheRest'}, ]
+    for prot in protocols:
+        rse_client.add_protocol(rse1, prot)
+
+    pfn = 'MOCK://get-pfn-to-rse:17/the/one/with/all/the/files/file1'
+    pfn_no_port = 'MOCK://get-pfn-to-rse/the/one/with/all/the/files/file1'
+    pfn_other_port = 'MOCK://get-pfn-to-rse:18/the/one/with/all/the/files/file1'
+    pfn_non_existing_port = 'MOCK://get-pfn-to-rse:19/the/one/with/all/the/files/file1'
+
+    res = get_pfn_to_rse([pfn], vo=vo)
+    assert res == ('MOCK', {rse1_id: [pfn]}, {})
+    res = get_pfn_to_rse([pfn_no_port], vo=vo)
+    assert res == ('MOCK', {rse1_id: [pfn_no_port]}, {})
+    res = get_pfn_to_rse([pfn_other_port], vo=vo)
+    assert res == ('MOCK', {rse1_id: [pfn_other_port]}, {})
+    res = get_pfn_to_rse([pfn_non_existing_port], vo=vo)
+    assert res == ('MOCK', {}, {'unknown': [pfn_non_existing_port]})
```

### Comparing `rucio-clients-1.9.6/lib/rucio/tests/test_rse_expression_parser.py` & `rucio-clients-32.0.0rc1/tests/test_rse_expression_parser.py`

 * *Files 25% similar despite different names*

```diff
@@ -1,209 +1,259 @@
-# Copyright European Organization for Nuclear Research (CERN)
+# -*- coding: utf-8 -*-
+# Copyright European Organization for Nuclear Research (CERN) since 2012
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
-# You may not use this file except in compliance with the License.
+# you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
-# http://www.apache.org/licenses/LICENSE-2.0
 #
-# Authors:
-# - Martin Barisits, <martin.barisits@cern.ch>, 2013-2016
+#    http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 
 from random import choice
-from string import ascii_uppercase, digits, ascii_lowercase
+from string import ascii_uppercase, ascii_lowercase
 
-from nose.tools import assert_equal, raises, assert_raises
+import pytest
 
+from rucio.common.exception import InvalidRSEExpression, RSEWriteBlocked
 from rucio.core import rse
 from rucio.core import rse_expression_parser
-from rucio.client.rseclient import RSEClient
-from rucio.common.exception import InvalidRSEExpression, RSEBlacklisted
-
-
-def rse_name_generator(size=10):
-    return 'MOCK_' + ''.join(choice(ascii_uppercase) for x in xrange(size))
-
-
-def tag_generator(size_s=10, size_d=2):
-    return ''.join(choice(ascii_uppercase) for x in xrange(size_s)).join(choice(digits) for x in xrange(size_d))
 
 
 def attribute_name_generator(size=10):
-    return ''.join(choice(ascii_uppercase)).join(choice(ascii_lowercase) for x in xrange(size - 1))
+    return ''.join(choice(ascii_uppercase)).join(choice(ascii_lowercase) for x in range(size - 1))
 
 
-class TestRSEExpressionParserCore():
+@pytest.mark.noparallel(reason='uses pre-defined RSE, test_all_rse fails when run in parallel')
+class TestRSEExpressionParserCore:
 
-    def setup(self):
-        self.rse1 = rse_name_generator()
-        self.rse2 = rse_name_generator()
-        self.rse3 = rse_name_generator()
-        self.rse4 = rse_name_generator()
-        self.rse5 = rse_name_generator()
-
-        self.rse1_id = rse.add_rse(self.rse1)
-        self.rse2_id = rse.add_rse(self.rse2)
-        self.rse3_id = rse.add_rse(self.rse3)
-        self.rse4_id = rse.add_rse(self.rse4)
-        self.rse5_id = rse.add_rse(self.rse5)
+    @classmethod
+    @pytest.fixture(scope='class')
+    def setup_test_env(cls, rse_factory_unittest, tag_factory_class):
+        already_existing_rses = rse.list_rses()
+
+        rse1, rse1_id = rse_factory_unittest.make_mock_rse()
+        rse2, rse2_id = rse_factory_unittest.make_mock_rse()
+        rse3, rse3_id = rse_factory_unittest.make_mock_rse()
+        rse4, rse4_id = rse_factory_unittest.make_mock_rse()
+        rse5, rse5_id = rse_factory_unittest.make_mock_rse()
 
         # Add Attributes
-        self.attribute = attribute_name_generator()
+        attribute = attribute_name_generator()
 
-        rse.add_rse_attribute(self.rse1, self.attribute, "at")
-        rse.add_rse_attribute(self.rse2, self.attribute, "de")
-        rse.add_rse_attribute(self.rse3, self.attribute, "fr")
-        rse.add_rse_attribute(self.rse4, self.attribute, "uk")
-        rse.add_rse_attribute(self.rse5, self.attribute, "us")
+        rse.add_rse_attribute(rse1_id, attribute, "at")
+        rse.add_rse_attribute(rse2_id, attribute, "de")
+        rse.add_rse_attribute(rse3_id, attribute, "fr")
+        rse.add_rse_attribute(rse4_id, attribute, "uk")
+        rse.add_rse_attribute(rse5_id, attribute, "us")
 
         # Add numeric Attributes
-        self.attribute_numeric = attribute_name_generator()
+        attribute_numeric = attribute_name_generator()
 
-        rse.add_rse_attribute(self.rse1, self.attribute_numeric, 10)
-        rse.add_rse_attribute(self.rse2, self.attribute_numeric, 20)
-        rse.add_rse_attribute(self.rse3, self.attribute_numeric, 30)
-        rse.add_rse_attribute(self.rse4, self.attribute_numeric, 40)
-        rse.add_rse_attribute(self.rse5, self.attribute_numeric, 50)
+        rse.add_rse_attribute(rse1_id, attribute_numeric, 10)
+        rse.add_rse_attribute(rse2_id, attribute_numeric, 20)
+        rse.add_rse_attribute(rse3_id, attribute_numeric, 30)
+        rse.add_rse_attribute(rse4_id, attribute_numeric, 40)
+        rse.add_rse_attribute(rse5_id, attribute_numeric, 50)
 
         # Add Tags
-        self.tag1 = tag_generator()
-        self.tag2 = tag_generator()
-        rse.add_rse_attribute(self.rse1, self.tag1, True)
-        rse.add_rse_attribute(self.rse2, self.tag1, True)
-        rse.add_rse_attribute(self.rse3, self.tag1, True)
-        rse.add_rse_attribute(self.rse4, self.tag2, True)
-        rse.add_rse_attribute(self.rse5, self.tag2, True)
+        tag1 = tag_factory_class.new_tag()
+        tag2 = tag_factory_class.new_tag()
+        rse.add_rse_attribute(rse1_id, tag1, True)
+        rse.add_rse_attribute(rse2_id, tag1, True)
+        rse.add_rse_attribute(rse3_id, tag1, True)
+        rse.add_rse_attribute(rse4_id, tag2, True)
+        rse.add_rse_attribute(rse5_id, tag2, True)
+
+        return (rse1, rse1_id, rse2, rse2_id, rse3, rse3_id, rse4, rse4_id, rse5, rse5_id,
+                attribute, attribute_numeric, tag1, tag2, already_existing_rses)
+
+    @pytest.fixture(autouse=True)
+    def setup_obj(self, setup_test_env, vo):
+        self.filter = {'filter_': {'vo': vo}}
+        (
+            self.rse1, self.rse1_id, self.rse2, self.rse2_id, self.rse3, self.rse3_id, self.rse4, self.rse4_id, self.rse5, self.rse5_id,
+            self.attribute, self.attribute_numeric, self.tag1, self.tag2, self.already_existing_rses,
+        ) = setup_test_env
 
-    @raises(InvalidRSEExpression)
-    def test_invalid_expression_unconnected_operator(self):
+    def test_unconnected_operator(self):
         """ RSE_EXPRESSION_PARSER (CORE) Test invalid rse expression: unconnected operator"""
-        rse_expression_parser.parse_expression("TEST_RSE1|")
+        with pytest.raises(InvalidRSEExpression):
+            rse_expression_parser.parse_expression("TEST_RSE1|", **self.filter)
 
-    @raises(InvalidRSEExpression)
-    def test_invalid_expression_wrong_parantheses(self):
+    def test_wrong_parantheses(self):
         """ RSE_EXPRESSION_PARSER (CORE) Test invalid rse expression: wrong parantheses """
-        rse_expression_parser.parse_expression("TEST_RSE1)")
+        with pytest.raises(InvalidRSEExpression):
+            rse_expression_parser.parse_expression("TEST_RSE1)", **self.filter)
 
-    @raises(InvalidRSEExpression)
-    def test_unknown_RSE(self):
+    def test_unknown_rse(self):
         """ RSE_EXPRESSION_PARSER (CORE) Test unknown RSE """
-        rse_expression_parser.parse_expression("TEST_RSE999")
+        with pytest.raises(InvalidRSEExpression):
+            rse_expression_parser.parse_expression("TEST_RSE999", **self.filter)
 
     def test_simple_rse_reference(self):
         """ RSE_EXPRESSION_PARSER (CORE) Test simple RSE reference """
-        assert_equal([rse['id'] for rse in rse_expression_parser.parse_expression(self.rse1)], [self.rse1_id])
+        value = [t_rse['id'] for t_rse in rse_expression_parser.parse_expression(self.rse1, **self.filter)]
+        assert value == [self.rse1_id]
 
     def test_attribute_reference(self):
         """ RSE_EXPRESSION_PARSER (CORE) Test simple RSE attribute reference """
-        assert_equal([rse['id'] for rse in rse_expression_parser.parse_expression("%s=uk" % self.attribute)], [self.rse4_id])
+        value = [t_rse['id'] for t_rse in rse_expression_parser.parse_expression("%s=uk" % self.attribute, **self.filter)]
+        assert value == [self.rse4_id]
+
+    def test_all_rse(self):
+        """ RSE_EXPRESSION_PARSER (CORE) Test reference on all RSE """
+        all_rses = rse.list_rses(filters=self.filter['filter_'])
+        rse_expression_parser.REGION.invalidate()
+        value = rse_expression_parser.parse_expression("*", **self.filter)
+        for rse_ in self.already_existing_rses:
+            if rse_ in all_rses:
+                all_rses.remove(rse_)
+            if rse_ in value:
+                value.remove(rse_)
+        value = sorted(value, key=lambda rse_: rse_['rse'])
+        expected = sorted(all_rses, key=lambda rse_: rse_['rse'])
+        assert len(value) == 5
+        assert value == expected
 
     def test_tag_reference(self):
         """ RSE_EXPRESSION_PARSER (CORE) Test simple RSE tag reference """
-        assert_equal(sorted([rse['id'] for rse in rse_expression_parser.parse_expression(self.tag1)]), sorted([self.rse1_id, self.rse2_id, self.rse3_id]))
+        value = sorted([t_rse['id'] for t_rse in rse_expression_parser.parse_expression(self.tag1, **self.filter)])
+        expected = sorted([self.rse1_id, self.rse2_id, self.rse3_id])
+        assert value == expected
 
     def test_parantheses(self):
         """ RSE_EXPRESSION_PARSER (CORE) Test parantheses """
-        assert_equal(sorted([rse['id'] for rse in rse_expression_parser.parse_expression("(%s)" % self.tag1)]), sorted([self.rse1_id, self.rse2_id, self.rse3_id]))
+        value = sorted([t_rse['id'] for t_rse in rse_expression_parser.parse_expression("(%s)" % self.tag1, **self.filter)])
+        expected = sorted([self.rse1_id, self.rse2_id, self.rse3_id])
+        assert value == expected
 
     def test_union(self):
         """ RSE_EXPRESSION_PARSER (CORE) Test union operator """
-        assert_equal(sorted([rse['id'] for rse in rse_expression_parser.parse_expression("%s|%s" % (self.tag1, self.tag2))]), sorted([self.rse1_id, self.rse2_id, self.rse3_id, self.rse4_id, self.rse5_id]))
+        value = sorted([t_rse['id'] for t_rse in rse_expression_parser.parse_expression("%s|%s" % (self.tag1, self.tag2), **self.filter)])
+        expected = sorted([self.rse1_id, self.rse2_id, self.rse3_id, self.rse4_id, self.rse5_id])
+        assert value == expected
 
     def test_complement(self):
         """ RSE_EXPRESSION_PARSER (CORE) Test complement operator """
-        assert_equal(sorted([rse['id'] for rse in rse_expression_parser.parse_expression("%s\\%s" % (self.tag1, self.rse3))]), sorted([self.rse1_id, self.rse2_id]))
+        value = sorted([t_rse['id'] for t_rse in rse_expression_parser.parse_expression("%s\\%s" % (self.tag1, self.rse3), **self.filter)])
+        expected = sorted([self.rse1_id, self.rse2_id])
+        assert value == expected
 
     def test_intersect(self):
         """ RSE_EXPRESSION_PARSER (CORE) Test intersect operator """
-        assert_equal([rse['id'] for rse in rse_expression_parser.parse_expression("%s&%s=uk" % (self.tag2, self.attribute))], [self.rse4_id])
+        value = [t_rse['id'] for t_rse in rse_expression_parser.parse_expression("%s&%s=uk" % (self.tag2, self.attribute), **self.filter)]
+        assert value == [self.rse4_id]
 
     def test_order_of_operations(self):
         """ RSE_EXPRESSION_PARSER (CORE) Test order of operations """
-        assert_equal(sorted([rse['id'] for rse in rse_expression_parser.parse_expression("%s\\%s|%s=fr" % (self.tag1, self.rse3, self.attribute))]), sorted([self.rse1_id, self.rse2_id, self.rse3_id]))
-        assert_equal(sorted([rse['id'] for rse in rse_expression_parser.parse_expression("%s\\(%s|%s=fr)" % (self.tag1, self.rse3, self.attribute))]), sorted([self.rse1_id, self.rse2_id]))
+        value = sorted([t_rse['id'] for t_rse in rse_expression_parser.parse_expression("%s\\%s|%s=fr" % (self.tag1, self.rse3, self.attribute), **self.filter)])
+        expected = sorted([self.rse1_id, self.rse2_id, self.rse3_id])
+        assert value == expected
+        value = sorted([t_rse['id'] for t_rse in rse_expression_parser.parse_expression("%s\\(%s|%s=fr)" % (self.tag1, self.rse3, self.attribute), **self.filter)])
+        expected = sorted([self.rse1_id, self.rse2_id])
+        assert value == expected
 
     def test_complicated_expression_1(self):
         """ RSE_EXPRESSION_PARSER (CORE) Test some complicated expression 1"""
-        assert_equal(sorted([rse['id'] for rse in rse_expression_parser.parse_expression("(%s|%s)\\%s|%s&%s" % (self.tag1, self.tag2, self.tag2, self.tag2, self.tag1))]), sorted([self.rse1_id, self.rse2_id, self.rse3_id]))
+        value = sorted([t_rse['id'] for t_rse in rse_expression_parser.parse_expression("(%s|%s)\\%s|%s&%s" % (self.tag1, self.tag2, self.tag2, self.tag2, self.tag1), **self.filter)])
+        expected = sorted([self.rse1_id, self.rse2_id, self.rse3_id])
+        assert value == expected
 
     def test_complicated_expression_2(self):
         """ RSE_EXPRESSION_PARSER (CORE) Test some complicated expression 2"""
-        assert_equal(sorted([rse['id'] for rse in rse_expression_parser.parse_expression("(((((%s))))|%s=us)&%s|(%s=at|%s=de)" % (self.tag1, self.attribute, self.tag2, self.attribute, self.attribute))]), sorted([self.rse1_id, self.rse2_id, self.rse5_id]))
+        value = sorted([t_rse['id'] for t_rse in rse_expression_parser.parse_expression("(((((%s))))|%s=us)&%s|(%s=at|%s=de)" % (self.tag1, self.attribute, self.tag2, self.attribute, self.attribute), **self.filter)])
+        expected = sorted([self.rse1_id, self.rse2_id, self.rse5_id])
+        assert value == expected
+
+    def test_complicated_expression_3(self):
+        """ RSE_EXPRESSION_PARSER (CORE) Test some complicated expression 3"""
+        value = sorted([t_rse['id'] for t_rse in rse_expression_parser.parse_expression("(*)&%s=at" % self.attribute, **self.filter)])
+        expected = sorted([self.rse1_id])
+        assert value == expected
 
-    def test_list_rses_based_on_availability(self):
+    def test_list_on_availability(self, rse_factory):
         """ RSE_EXPRESSION_PARSER (CORE) List rses based on availability filter"""
-        rseWRITE_name = rse_name_generator()
-        rseNOWRITE_name = rse_name_generator()
-
-        rseWRITE_id = rse.add_rse(rseWRITE_name)
-        rseNOWRITE_id = rse.add_rse(rseNOWRITE_name)
+        rsewrite_name, rsewrite_id = rse_factory.make_mock_rse()
+        rsenowrite_name, rsenowrite_id = rse_factory.make_mock_rse()
 
         attribute = attribute_name_generator()
 
-        rse.add_rse_attribute(rseWRITE_name, attribute, "de")
-        rse.add_rse_attribute(rseNOWRITE_name, attribute, "de")
+        rse.add_rse_attribute(rsewrite_id, attribute, "de")
+        rse.add_rse_attribute(rsenowrite_id, attribute, "de")
 
-        rse.update_rse(rseWRITE_name, {'availability_write': True})
-        rse.update_rse(rseNOWRITE_name, {'availability_write': False})
+        rse.update_rse(rsewrite_id, {'availability_write': True})
+        rse.update_rse(rsenowrite_id, {'availability_write': False})
 
-        assert_equal(sorted([item['id'] for item in rse_expression_parser.parse_expression("%s=de" % attribute)]),
-                     sorted([rseWRITE_id, rseNOWRITE_id]))
+        value = sorted([item['id'] for item in rse_expression_parser.parse_expression("%s=de" % attribute, **self.filter)])
+        expected = sorted([rsewrite_id, rsenowrite_id])
+        assert value == expected
+
+        filters = self.filter
+        filters['availability_write'] = True
+        value = sorted([item['id'] for item in rse_expression_parser.parse_expression("%s=de" % attribute, filters)])
+        expected = sorted([rsewrite_id])
+        assert value == expected
 
-        assert_equal(sorted([item['id'] for item in rse_expression_parser.parse_expression("%s=de" % attribute, {'availability_write': True})]),
-                     sorted([rseWRITE_id]))
+        filters['availability_write'] = False
+        pytest.raises(RSEWriteBlocked, rse_expression_parser.parse_expression, "%s=de" % attribute, filters)
 
-        assert_raises(RSEBlacklisted, rse_expression_parser.parse_expression, "%s=de" % attribute, {'availability_write': False})
-
-    def test_attribute_with_numeric_operators(self):
+    def test_numeric_operators(self):
         """ RSE_EXPRESSION_PARSER (CORE) Test RSE attributes with numeric operations """
-        assert_equal([rse['id'] for rse in rse_expression_parser.parse_expression("%s<11" % self.attribute_numeric)], [self.rse1_id])
-        assert_raises(InvalidRSEExpression, rse_expression_parser.parse_expression, "%s<9" % self.attribute_numeric)
-        assert_equal(sorted([rse['id'] for rse in rse_expression_parser.parse_expression("%s<21" % self.attribute_numeric)]), sorted([self.rse1_id, self.rse2_id]))
-        assert_equal([rse['id'] for rse in rse_expression_parser.parse_expression("%s>49" % self.attribute_numeric)], [self.rse5_id])
-        assert_raises(InvalidRSEExpression, rse_expression_parser.parse_expression, "%s>51" % self.attribute_numeric)
-        assert_equal(sorted([rse['id'] for rse in rse_expression_parser.parse_expression("%s>30" % self.attribute_numeric)]), sorted([self.rse4_id, self.rse5_id]))
-
-
-class TestRSEExpressionParserClient():
-
-    def setup(self):
-        self.rse1 = rse_name_generator()
-        self.rse2 = rse_name_generator()
-        self.rse3 = rse_name_generator()
-        self.rse4 = rse_name_generator()
-        self.rse5 = rse_name_generator()
-
-        self.rse1_id = rse.add_rse(self.rse1)
-        self.rse2_id = rse.add_rse(self.rse2)
-        self.rse3_id = rse.add_rse(self.rse3)
-        self.rse4_id = rse.add_rse(self.rse4)
-        self.rse5_id = rse.add_rse(self.rse5)
+        value = [t_rse['id'] for t_rse in rse_expression_parser.parse_expression("%s<11" % self.attribute_numeric, **self.filter)]
+        assert value == [self.rse1_id]
+        pytest.raises(InvalidRSEExpression, rse_expression_parser.parse_expression, "%s<9" % self.attribute_numeric, **self.filter)
+        value = sorted([t_rse['id'] for t_rse in rse_expression_parser.parse_expression("%s<21" % self.attribute_numeric, **self.filter)])
+        expected = sorted([self.rse1_id, self.rse2_id])
+        assert value == expected
+        value = [t_rse['id'] for t_rse in rse_expression_parser.parse_expression("%s>49" % self.attribute_numeric, **self.filter)]
+        assert value == [self.rse5_id]
+        pytest.raises(InvalidRSEExpression, rse_expression_parser.parse_expression, "%s>51" % self.attribute_numeric, **self.filter)
+        value = sorted([t_rse['id'] for t_rse in rse_expression_parser.parse_expression("%s>30" % self.attribute_numeric, **self.filter)])
+        expected = sorted([self.rse4_id, self.rse5_id])
+        assert value == expected
+
+
+@pytest.mark.noparallel(reason='uses pre-defined RSE')
+class TestRSEExpressionParserClient:
+
+    @pytest.fixture(autouse=True)
+    def setup_obj(self, rse_factory, tag_factory):
+        self.rse1, self.rse1_id = rse_factory.make_mock_rse()
+        self.rse2, self.rse2_id = rse_factory.make_mock_rse()
+        self.rse3, self.rse3_id = rse_factory.make_mock_rse()
+        self.rse4, self.rse4_id = rse_factory.make_mock_rse()
+        self.rse5, self.rse5_id = rse_factory.make_mock_rse()
 
         # Add Attributes
         self.attribute = attribute_name_generator()
 
-        rse.add_rse_attribute(self.rse1, self.attribute, "at")
-        rse.add_rse_attribute(self.rse2, self.attribute, "de")
-        rse.add_rse_attribute(self.rse3, self.attribute, "fr")
-        rse.add_rse_attribute(self.rse4, self.attribute, "uk")
-        rse.add_rse_attribute(self.rse5, self.attribute, "us")
+        rse.add_rse_attribute(self.rse1_id, self.attribute, "at")
+        rse.add_rse_attribute(self.rse2_id, self.attribute, "de")
+        rse.add_rse_attribute(self.rse3_id, self.attribute, "fr")
+        rse.add_rse_attribute(self.rse4_id, self.attribute, "uk")
+        rse.add_rse_attribute(self.rse5_id, self.attribute, "us")
 
         # Add Tags
-        self.tag1 = tag_generator()
-        self.tag2 = tag_generator()
-        rse.add_rse_attribute(self.rse1, self.tag1, True)
-        rse.add_rse_attribute(self.rse2, self.tag1, True)
-        rse.add_rse_attribute(self.rse3, self.tag1, True)
-        rse.add_rse_attribute(self.rse4, self.tag2, True)
-        rse.add_rse_attribute(self.rse5, self.tag2, True)
-
-        self.rse_client = RSEClient()
+        self.tag1 = tag_factory.new_tag()
+        self.tag2 = tag_factory.new_tag()
+        rse.add_rse_attribute(self.rse1_id, self.tag1, True)
+        rse.add_rse_attribute(self.rse2_id, self.tag1, True)
+        rse.add_rse_attribute(self.rse3_id, self.tag1, True)
+        rse.add_rse_attribute(self.rse4_id, self.tag2, True)
+        rse.add_rse_attribute(self.rse5_id, self.tag2, True)
 
-    def test_complicated_expression(self):
+    def test_complicated_expression(self, rse_client):
         """ RSE_EXPRESSION_PARSER (CLIENT) Test some complicated expression"""
-        rses = [item['rse'] for item in self.rse_client.list_rses("(((((%s))))|%s=us)&%s|(%s=at|%s=de)" % (self.tag1, self.attribute, self.tag2, self.attribute, self.attribute))]
-        assert_equal(sorted(rses), sorted([self.rse1, self.rse2, self.rse5]))
+        rses = sorted([item['rse'] for item in rse_client.list_rses("(((((%s))))|%s=us)&%s|(%s=at|%s=de)" % (self.tag1, self.attribute, self.tag2, self.attribute, self.attribute))])
+        expected = sorted([self.rse1, self.rse2, self.rse5])
+        assert rses == expected
 
-    def test_complicated_expression_1(self):
+    def test_complicated_expression_1(self, rse_client):
         """ RSE_EXPRESSION_PARSER (CORE) Test some complicated expression 1"""
-        rses = [item['rse'] for item in self.rse_client.list_rses("(%s|%s)\\%s|%s&%s" % (self.tag1, self.tag2, self.tag2, self.tag2, self.tag1))]
-        assert_equal(sorted(rses), sorted([self.rse1, self.rse2, self.rse3]))
+        rses = sorted([item['rse'] for item in rse_client.list_rses("(%s|%s)\\%s|%s&%s" % (self.tag1, self.tag2, self.tag2, self.tag2, self.tag1))])
+        expected = sorted([self.rse1, self.rse2, self.rse3])
+        assert rses == expected
```

### Comparing `rucio-clients-1.9.6/lib/rucio/tests/test_rucio_server.py` & `rucio-clients-32.0.0rc1/lib/rucio/common/test_rucio_server.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,146 +1,148 @@
-# Copyright European Organization for Nuclear Research (CERN)
+# -*- coding: utf-8 -*-
+# Copyright European Organization for Nuclear Research (CERN) since 2012
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
-# You may not use this file except in compliance with the License.
-# You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
 #
-# Authors:
-# - Joaquin Bogado, <joaquin.bogado@cern.ch>, 2014
-# - Cedric Serfon, <cedric.serfon@cern.ch>, 2015
+#    http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 
-import nose.tools
-import subprocess
+import unittest
 from os import remove
-from rucio.common.utils import generate_uuid as uuid
-
+from os.path import basename
 
-def execute(cmd):
-    """
-    Executes a command in a subprocess. Returns a tuple
-    of (exitcode, out, err), where out is the string output
-    from stdout and err is the string output from stderr when
-    executing the command.
-
-    :param cmd: Command string to execute
-    """
-    process = subprocess.Popen(cmd,
-                               shell=True,
-                               stdin=subprocess.PIPE,
-                               stdout=subprocess.PIPE,
-                               stderr=subprocess.PIPE)
-    out = ''
-    err = ''
-    exitcode = 0
-
-    result = process.communicate()
-    (out, err) = result
-    exitcode = process.returncode
-    return exitcode, out, err
+from rucio.common.utils import generate_uuid as uuid, execute
 
 
 def file_generator(size=2048, namelen=10):
     """ Create a bogus file and returns it's name.
     :param size: size in bytes
     :returns: The name of the generated file.
     """
     fn = '/tmp/rucio_testfile_' + uuid()
     execute('dd if=/dev/urandom of={0} count={1} bs=1'.format(fn, size))
     return fn
 
 
+def get_scope_and_rses():
+    """
+    Check if xrd containers rses for xrootd are available in the testing environment.
+
+    :return: A tuple (scope, rses) for the rucio client where scope is mock/test and rses is a list or (None, [None]) if no suitable rse exists.
+    """
+    cmd = "rucio list-rses --rses 'test_container_xrd=True'"
+    print(cmd)
+    exitcode, out, err = execute(cmd)
+    print(out, err)
+    rses = out.split()
+    if len(rses) == 0:
+        return None, [None]
+    return 'test', rses
+
+
 def delete_rules(did):
     # get the rules for the file
-    print 'Deleting rules'
-    cmd = "rucio list-rules --did {0} | grep {0} | cut -f1 -d\ ".format(did)
-    print cmd
+    print('Deleting rules')
+    cmd = "rucio list-rules {0} | grep {0} | cut -f1 -d\\ ".format(did)
+    print(cmd)
     exitcode, out, err = execute(cmd)
-    print out, err
+    print(out, err)
     rules = out.split()
     # delete the rules for the file
     for rule in rules:
         cmd = "rucio delete-rule {0}".format(rule)
-        print cmd
+        print(cmd)
         exitcode, out, err = execute(cmd)
 
 
-class TestRucioClient():
+class TestRucioServer(unittest.TestCase):
 
-    def setup(self):
+    def setUp(self):
         self.marker = '$ > '
-        self.scope = 'mock'
-        self.rse = 'MOCK-POSIX'
+        self.scope, self.rses = get_scope_and_rses()
+        self.rse = self.rses[0]
         self.generated_dids = []
 
     def tearDown(self):
         for did in self.generated_dids:
             delete_rules(did)
-            self.generated_dids.remove(did)
+        self.generated_dids = []
 
     def test_ping(self):
         """CLIENT (USER): rucio ping"""
         cmd = 'rucio ping'
-        print self.marker + cmd
+        print(self.marker + cmd)
         exitcode, out, err = execute(cmd)
-        print out, err
-        nose.tools.assert_equal(0, exitcode)
+        print(out, err)
+        self.assertEqual(exitcode, 0)
 
     def test_whoami(self):
         """CLIENT (USER): rucio whoami"""
         cmd = 'rucio whoami'
-        print self.marker + cmd
+        print(self.marker + cmd)
         exitcode, out, err = execute(cmd)
-        print out, err
-        nose.tools.assert_equal(0, exitcode)
+        print(out, err)
+        self.assertEqual(exitcode, 0)
 
     def test_upload_download(self):
         """CLIENT(USER): rucio upload files to dataset/download dataset"""
+        if self.rse is None:
+            return
+
         tmp_file1 = file_generator()
         tmp_file2 = file_generator()
         tmp_file3 = file_generator()
         tmp_dsn = 'tests.rucio_client_test_server_' + uuid()
 
         # Adding files to a new dataset
         cmd = 'rucio upload --rse {0} --scope {1} {2} {3} {4} {1}:{5}'.format(self.rse, self.scope, tmp_file1, tmp_file2, tmp_file3, tmp_dsn)
-        print self.marker + cmd
+        print(self.marker + cmd)
         exitcode, out, err = execute(cmd)
-        print out
-        print err
+        print(out)
+        print(err)
         remove(tmp_file1)
         remove(tmp_file2)
         remove(tmp_file3)
-        nose.tools.assert_equal(0, exitcode)
+        self.assertEqual(exitcode, 0)
 
         # List the files
         cmd = 'rucio list-files {0}:{1}'.format(self.scope, tmp_dsn)
-        print self.marker + cmd
+        print(self.marker + cmd)
         exitcode, out, err = execute(cmd)
-        print out
-        print err
-        nose.tools.assert_equal(0, exitcode)
+        print(out)
+        print(err)
+        self.assertEqual(exitcode, 0)
 
         # List the replicas
         cmd = 'rucio list-file-replicas {0}:{1}'.format(self.scope, tmp_dsn)
-        print self.marker + cmd
+        print(self.marker + cmd)
         exitcode, out, err = execute(cmd)
-        print out
-        print err
-        nose.tools.assert_equal(0, exitcode)
+        print(out)
+        print(err)
+        self.assertEqual(exitcode, 0)
 
         # Downloading dataset
         cmd = 'rucio download --dir /tmp/ {0}:{1}'.format(self.scope, tmp_dsn)
-        print self.marker + cmd
+        print(self.marker + cmd)
         exitcode, out, err = execute(cmd)
-        print out
-        print err
+        print(out)
+        print(err)
         # The files should be there
-        cmd = 'ls /tmp/{0}/rucio_testfile_*'.format(self.scope)
         cmd = 'ls /tmp/{0}/rucio_testfile_*'.format(tmp_dsn)
-        print self.marker + cmd
+        print(self.marker + cmd)
         exitcode, out, err = execute(cmd)
-        print err, out
-        nose.tools.assert_equal(0, exitcode)
+        print(err, out)
+        self.assertEqual(exitcode, 0)
+
         # cleaning
-        remove('/tmp/{0}/'.format(tmp_dsn) + tmp_file1[5:])
-        remove('/tmp/{0}/'.format(tmp_dsn) + tmp_file2[5:])
-        remove('/tmp/{0}/'.format(tmp_dsn) + tmp_file3[5:])
-        self.generated_dids + '{0}:{1} {0}:{2} {0}:{3} {0}:{4}'.format(self.scope, tmp_file1, tmp_file2, tmp_file3, tmp_dsn).split(' ')
+        remove('/tmp/{0}/'.format(tmp_dsn) + basename(tmp_file1))
+        remove('/tmp/{0}/'.format(tmp_dsn) + basename(tmp_file2))
+        remove('/tmp/{0}/'.format(tmp_dsn) + basename(tmp_file3))
+        added_dids = ['{0}:{1}'.format(self.scope, did) for did in (basename(tmp_file1), basename(tmp_file2), basename(tmp_file3), tmp_dsn)]
+        self.generated_dids += added_dids
```

### Comparing `rucio-clients-1.9.6/PKG-INFO` & `rucio-clients-32.0.0rc1/PKG-INFO`

 * *Files 24% similar despite different names*

```diff
@@ -1,20 +1,27 @@
-Metadata-Version: 1.1
+Metadata-Version: 2.1
 Name: rucio-clients
-Version: 1.9.6
+Version: 32.0.0rc1
 Summary: Rucio Client Lite Package
-Home-page: http://rucio.cern.ch/
+Home-page: https://rucio.cern.ch/
 Author: Rucio
 Author-email: rucio-dev@cern.ch
 License: Apache License, Version 2.0
-Description: UNKNOWN
-Platform: UNKNOWN
 Classifier: Development Status :: 5 - Production/Stable
 Classifier: License :: OSI Approved :: Apache Software License
 Classifier: Intended Audience :: Information Technology
 Classifier: Intended Audience :: System Administrators
 Classifier: Operating System :: POSIX :: Linux
 Classifier: Natural Language :: English
 Classifier: Programming Language :: Python
-Classifier: Programming Language :: Python :: 2.6
-Classifier: Programming Language :: Python :: 2.7
-Classifier: Environment :: No Input/Output (Daemon)
+Classifier: Programming Language :: Python :: 3
+Classifier: Programming Language :: Python :: 3.9
+Classifier: Programming Language :: Python :: 3.10
+Requires-Python: >=3.9, <4
+Provides-Extra: ssh
+Provides-Extra: kerberos
+Provides-Extra: swift
+Provides-Extra: argcomplete
+Provides-Extra: sftp
+Provides-Extra: dumper
+License-File: LICENSE
+License-File: AUTHORS.rst
```


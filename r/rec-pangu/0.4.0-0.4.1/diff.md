# Comparing `tmp/rec_pangu-0.4.0-py3-none-any.whl.zip` & `tmp/rec_pangu-0.4.1-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,22 +1,22 @@
-Zip file size: 117944 bytes, number of entries: 83
+Zip file size: 123251 bytes, number of entries: 84
 -rw-r--r--  2.0 unx     6148 b- defN 23-Jul-11 08:28 .DS_Store
 -rw-r--r--  2.0 unx     6148 b- defN 23-Jul-11 08:28 rec_pangu/.DS_Store
--rw-r--r--  2.0 unx      199 b- defN 23-Jul-13 07:56 rec_pangu/__init__.py
+-rw-r--r--  2.0 unx      199 b- defN 23-Jul-18 07:48 rec_pangu/__init__.py
 -rw-r--r--  2.0 unx     5837 b- defN 23-Apr-03 08:03 rec_pangu/benchmark_trainer.py
 -rw-r--r--  2.0 unx     9722 b- defN 23-Mar-18 15:33 rec_pangu/gpt_ranktrainer.py
 -rw-r--r--  2.0 unx    14630 b- defN 23-Apr-03 08:03 rec_pangu/model_pipeline.py
 -rw-r--r--  2.0 unx     5448 b- defN 23-Mar-18 15:32 rec_pangu/old_ranktrainer.py
 -rw-r--r--  2.0 unx    20415 b- defN 23-Jul-10 02:50 rec_pangu/trainer.py
--rw-r--r--  2.0 unx      404 b- defN 23-Mar-05 10:13 rec_pangu/dataset/__init__.py
+-rw-r--r--  2.0 unx      434 b- defN 23-Jul-26 01:40 rec_pangu/dataset/__init__.py
 -rw-r--r--  2.0 unx     4954 b- defN 23-Apr-03 08:02 rec_pangu/dataset/base_dataset.py
 -rw-r--r--  2.0 unx     3776 b- defN 23-Apr-03 08:02 rec_pangu/dataset/graph_dataset.py
 -rw-r--r--  2.0 unx     2812 b- defN 23-Apr-03 08:02 rec_pangu/dataset/multi_task_dataset.py
--rw-r--r--  2.0 unx     3968 b- defN 23-Jul-16 04:53 rec_pangu/dataset/process_data.py
--rw-r--r--  2.0 unx     5640 b- defN 23-Jul-16 04:48 rec_pangu/dataset/sequence_dataset.py
+-rw-r--r--  2.0 unx     4908 b- defN 23-Jul-26 01:38 rec_pangu/dataset/process_data.py
+-rw-r--r--  2.0 unx    10311 b- defN 23-Jul-26 03:35 rec_pangu/dataset/sequence_dataset.py
 -rw-r--r--  2.0 unx     6148 b- defN 23-Jul-11 08:28 rec_pangu/models/.DS_Store
 -rw-r--r--  2.0 unx      117 b- defN 22-Jul-28 02:44 rec_pangu/models/__init__.py
 -rw-r--r--  2.0 unx    11105 b- defN 23-Jul-13 07:41 rec_pangu/models/base_model.py
 -rw-r--r--  2.0 unx     8154 b- defN 23-Jul-14 05:26 rec_pangu/models/utils.py
 -rw-r--r--  2.0 unx      949 b- defN 22-Jul-28 02:44 rec_pangu/models/layers/LGConv.py
 -rw-r--r--  2.0 unx      456 b- defN 23-Apr-03 07:29 rec_pangu/models/layers/__init__.py
 -rw-r--r--  2.0 unx     1876 b- defN 23-Mar-20 16:44 rec_pangu/models/layers/activation.py
@@ -49,37 +49,38 @@
 -rw-r--r--  2.0 unx     1928 b- defN 23-Apr-16 10:54 rec_pangu/models/ranking/fm.py
 -rw-r--r--  2.0 unx     1920 b- defN 22-Jul-28 02:44 rec_pangu/models/ranking/lightgcn.py
 -rw-r--r--  2.0 unx     1700 b- defN 23-Apr-16 10:54 rec_pangu/models/ranking/lr.py
 -rw-r--r--  2.0 unx     3831 b- defN 23-Apr-16 10:54 rec_pangu/models/ranking/masknet.py
 -rw-r--r--  2.0 unx     2909 b- defN 23-Apr-16 10:54 rec_pangu/models/ranking/nfm.py
 -rw-r--r--  2.0 unx     2819 b- defN 23-Apr-16 10:54 rec_pangu/models/ranking/wdl.py
 -rw-r--r--  2.0 unx     3196 b- defN 23-Apr-16 10:55 rec_pangu/models/ranking/xdeepfm.py
--rw-r--r--  2.0 unx      547 b- defN 23-Jul-06 08:07 rec_pangu/models/sequence/__init__.py
+-rw-r--r--  2.0 unx      574 b- defN 23-Jul-23 07:39 rec_pangu/models/sequence/__init__.py
 -rw-r--r--  2.0 unx     3682 b- defN 23-Jul-06 08:19 rec_pangu/models/sequence/clrec.py
--rw-r--r--  2.0 unx     9043 b- defN 23-Apr-11 14:28 rec_pangu/models/sequence/cmi.py
--rw-r--r--  2.0 unx     4832 b- defN 23-Apr-11 14:29 rec_pangu/models/sequence/comirec.py
+-rw-r--r--  2.0 unx     9045 b- defN 23-Jul-19 05:47 rec_pangu/models/sequence/cmi.py
+-rw-r--r--  2.0 unx     4832 b- defN 23-Jul-19 02:55 rec_pangu/models/sequence/comirec.py
 -rw-r--r--  2.0 unx     6913 b- defN 23-Jul-06 07:54 rec_pangu/models/sequence/contrarec.py
 -rw-r--r--  2.0 unx     3841 b- defN 23-Apr-11 14:29 rec_pangu/models/sequence/gcsan.py
 -rw-r--r--  2.0 unx     1706 b- defN 23-Jul-05 16:02 rec_pangu/models/sequence/gru4rec.py
--rw-r--r--  2.0 unx     2459 b- defN 23-Apr-11 14:29 rec_pangu/models/sequence/mind.py
+-rw-r--r--  2.0 unx    20561 b- defN 23-Jul-23 08:00 rec_pangu/models/sequence/iocrec.py
+-rw-r--r--  2.0 unx     2459 b- defN 23-Jul-19 02:55 rec_pangu/models/sequence/mind.py
 -rw-r--r--  2.0 unx     3011 b- defN 23-Apr-11 14:29 rec_pangu/models/sequence/narm.py
 -rw-r--r--  2.0 unx     2180 b- defN 23-Apr-11 14:29 rec_pangu/models/sequence/nextitnet.py
 -rw-r--r--  2.0 unx     3635 b- defN 23-Apr-11 14:29 rec_pangu/models/sequence/niser.py
--rw-r--r--  2.0 unx     8090 b- defN 23-Apr-11 14:29 rec_pangu/models/sequence/re4.py
--rw-r--r--  2.0 unx     2814 b- defN 23-Apr-11 14:29 rec_pangu/models/sequence/sasrec.py
+-rw-r--r--  2.0 unx     8173 b- defN 23-Jul-19 02:55 rec_pangu/models/sequence/re4.py
+-rw-r--r--  2.0 unx     2816 b- defN 23-Jul-19 05:47 rec_pangu/models/sequence/sasrec.py
 -rw-r--r--  2.0 unx     4679 b- defN 23-Jul-11 06:06 rec_pangu/models/sequence/sine.py
 -rw-r--r--  2.0 unx     3048 b- defN 23-Apr-11 14:29 rec_pangu/models/sequence/srgnn.py
 -rw-r--r--  2.0 unx     1813 b- defN 23-Apr-11 14:29 rec_pangu/models/sequence/stamp.py
 -rw-r--r--  2.0 unx     1621 b- defN 23-Apr-11 14:29 rec_pangu/models/sequence/yotubednn.py
 -rw-r--r--  2.0 unx      115 b- defN 23-Apr-11 01:35 rec_pangu/serving/__init__.py
 -rw-r--r--  2.0 unx     2211 b- defN 23-Apr-11 02:23 rec_pangu/serving/ranking_server.py
 -rw-r--r--  2.0 unx      301 b- defN 23-Mar-05 10:33 rec_pangu/utils/__init__.py
 -rw-r--r--  2.0 unx     1509 b- defN 23-Apr-03 08:03 rec_pangu/utils/check_version.py
 -rw-r--r--  2.0 unx     8857 b- defN 23-Jul-16 03:32 rec_pangu/utils/evaluate.py
 -rw-r--r--  2.0 unx     1565 b- defN 23-Jul-11 07:09 rec_pangu/utils/gpu_utils.py
 -rw-r--r--  2.0 unx      668 b- defN 23-Apr-03 08:03 rec_pangu/utils/json_utils.py
--rw-r--r--  2.0 unx     1058 b- defN 23-Jul-16 04:53 rec_pangu-0.4.0.dist-info/LICENSE
--rw-r--r--  2.0 unx    17906 b- defN 23-Jul-16 04:53 rec_pangu-0.4.0.dist-info/METADATA
--rw-r--r--  2.0 unx       92 b- defN 23-Jul-16 04:53 rec_pangu-0.4.0.dist-info/WHEEL
--rw-r--r--  2.0 unx       10 b- defN 23-Jul-16 04:53 rec_pangu-0.4.0.dist-info/top_level.txt
-?rw-rw-r--  2.0 unx     7372 b- defN 23-Jul-16 04:53 rec_pangu-0.4.0.dist-info/RECORD
-83 files, 368535 bytes uncompressed, 106204 bytes compressed:  71.2%
+-rw-r--r--  2.0 unx     1058 b- defN 23-Jul-26 03:36 rec_pangu-0.4.1.dist-info/LICENSE
+-rw-r--r--  2.0 unx    17937 b- defN 23-Jul-26 03:36 rec_pangu-0.4.1.dist-info/METADATA
+-rw-r--r--  2.0 unx       92 b- defN 23-Jul-26 03:36 rec_pangu-0.4.1.dist-info/WHEEL
+-rw-r--r--  2.0 unx       10 b- defN 23-Jul-26 03:36 rec_pangu-0.4.1.dist-info/top_level.txt
+?rw-rw-r--  2.0 unx     7466 b- defN 23-Jul-26 03:36 rec_pangu-0.4.1.dist-info/RECORD
+84 files, 394976 bytes uncompressed, 111365 bytes compressed:  71.8%
```

## zipnote {}

```diff
@@ -177,14 +177,17 @@
 
 Filename: rec_pangu/models/sequence/gcsan.py
 Comment: 
 
 Filename: rec_pangu/models/sequence/gru4rec.py
 Comment: 
 
+Filename: rec_pangu/models/sequence/iocrec.py
+Comment: 
+
 Filename: rec_pangu/models/sequence/mind.py
 Comment: 
 
 Filename: rec_pangu/models/sequence/narm.py
 Comment: 
 
 Filename: rec_pangu/models/sequence/nextitnet.py
@@ -228,23 +231,23 @@
 
 Filename: rec_pangu/utils/gpu_utils.py
 Comment: 
 
 Filename: rec_pangu/utils/json_utils.py
 Comment: 
 
-Filename: rec_pangu-0.4.0.dist-info/LICENSE
+Filename: rec_pangu-0.4.1.dist-info/LICENSE
 Comment: 
 
-Filename: rec_pangu-0.4.0.dist-info/METADATA
+Filename: rec_pangu-0.4.1.dist-info/METADATA
 Comment: 
 
-Filename: rec_pangu-0.4.0.dist-info/WHEEL
+Filename: rec_pangu-0.4.1.dist-info/WHEEL
 Comment: 
 
-Filename: rec_pangu-0.4.0.dist-info/top_level.txt
+Filename: rec_pangu-0.4.1.dist-info/top_level.txt
 Comment: 
 
-Filename: rec_pangu-0.4.0.dist-info/RECORD
+Filename: rec_pangu-0.4.1.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## rec_pangu/__init__.py

```diff
@@ -1,9 +1,9 @@
 # -*- ecoding: utf-8 -*-
 # @ModuleName: __init__
 # @Author: wk
 # @Email: 306178200@qq.com
 # @Time: 2022/6/10 8:20 PM
 from .utils import check_version
 
-__version__ = '0.4.0'
+__version__ = '0.4.1'
 check_version(__version__)
```

## rec_pangu/dataset/__init__.py

```diff
@@ -2,11 +2,11 @@
 # @ModuleName: __init__
 # @Copyright: Deep_Wisdom 
 # @Author: wk
 # @Email: wangkai@fuzhi.ai
 # @Time: 2022/1/20 8:21 下午
 
 from .base_dataset import BaseDataset
-from .process_data import get_dataloader, get_single_dataloader
+from .process_data import get_dataloader, get_single_dataloader, get_sequence_dataloader_v2
 from .multi_task_dataset import MultiTaskDataset
 from .graph_dataset import GeneralGraphDataset
-from .sequence_dataset import SequenceDataset,seq_collate
+from .sequence_dataset import SequenceDataset, seq_collate
```

## rec_pangu/dataset/process_data.py

```diff
@@ -1,15 +1,15 @@
 # -*- ecoding: utf-8 -*-
 # @ModuleName: process_data
 # @Author: wk
 # @Email: 306178200@qq.com
 # @Time: 2022/6/10 7:40 PM
 from .base_dataset import BaseDataset
 from .multi_task_dataset import MultiTaskDataset
-from .sequence_dataset import SequenceDataset
+from .sequence_dataset import SequenceDataset, SequenceDatasetV2
 import torch.utils.data as D
 
 
 def get_base_dataloader(train_df, valid_df, test_df, schema, batch_size=512 * 3):
     train_dataset = BaseDataset(schema, train_df)
     enc_dict = train_dataset.get_enc_dict()
     valid_dataset = BaseDataset(schema, valid_df, enc_dict=enc_dict)
@@ -39,17 +39,32 @@
     train_dataset = SequenceDataset(schema, df=train_df, phase='train')
     enc_dict = train_dataset.get_enc_dict()
     valid_dataset = SequenceDataset(schema, df=valid_df, enc_dict=enc_dict, phase='test')
     test_dataset = SequenceDataset(schema, df=test_df, enc_dict=enc_dict, phase='test')
 
     train_loader = D.DataLoader(train_dataset, batch_size=batch_size, shuffle=True,
                                 num_workers=0, pin_memory=True, drop_last=True)
-    valid_loader = D.DataLoader(valid_dataset, batch_size=8, shuffle=False,
+    valid_loader = D.DataLoader(valid_dataset, batch_size=batch_size, shuffle=False,
                                 num_workers=0, pin_memory=True, drop_last=True)
-    test_loader = D.DataLoader(test_dataset, batch_size=8, shuffle=False,
+    test_loader = D.DataLoader(test_dataset, batch_size=batch_size, shuffle=False,
+                               num_workers=0, pin_memory=True, drop_last=True)
+
+    return train_loader, valid_loader, test_loader, enc_dict
+
+def get_sequence_dataloader_v2(df, schema, batch_size=512 * 3):
+    train_dataset = SequenceDatasetV2(schema, df=df, phase='train')
+    enc_dict = train_dataset.get_enc_dict()
+    valid_dataset = SequenceDatasetV2(schema, df=df, enc_dict=enc_dict, phase='valid')
+    test_dataset = SequenceDatasetV2(schema, df=df, enc_dict=enc_dict, phase='test')
+
+    train_loader = D.DataLoader(train_dataset, batch_size=batch_size, shuffle=True,
+                                num_workers=0, pin_memory=True, drop_last=True)
+    valid_loader = D.DataLoader(valid_dataset, batch_size=batch_size, shuffle=False,
+                                num_workers=0, pin_memory=True, drop_last=True)
+    test_loader = D.DataLoader(test_dataset, batch_size=batch_size, shuffle=False,
                                num_workers=0, pin_memory=True, drop_last=True)
 
     return train_loader, valid_loader, test_loader, enc_dict
 
 
 def get_dataloader(train_df, valid_df, test_df, schema, batch_size=512 * 3):
     if schema['task_type'] == 'ranking':
```

## rec_pangu/dataset/sequence_dataset.py

```diff
@@ -14,14 +14,15 @@
         self.df = df
         self.enc_dict = enc_dict
         self.max_length = self.config['max_length']
         self.user_col = self.config['user_col']
         self.item_col = self.config['item_col']
         self.time_col = self.config.get('time_col', None)
         self.cate_cols = self.config.get('cate_cols', [])
+        self.next_seq_length = self.config.get('next_seq_length', 10)
 
         if self.time_col:
             self.df = self.df.sort_values(by=[self.user_col, self.time_col])
 
         if self.enc_dict == None:
             self.get_enc_dict()
         self.enc_data()
@@ -70,18 +71,29 @@
                     setattr(self, f'hist_{col}_list', cate_seq[k - self.max_length: k])
             else:
                 hist_item_list.append(item_list[:k] + [0] * (self.max_length - k))
                 hist_mask_list.append([1.0] * k + [0.0] * (self.max_length - k))
                 for col in self.cate_cols:
                     cate_seq = getattr(self, f'user2{col}')[user_id]
                     setattr(self, f'hist_{col}_list', cate_seq[:k] + [0] * (self.max_length - k))
+
+            next_item_list = item_list[k:k + self.next_seq_length]
+            next_mask_list = [1] * len(next_item_list)
+
+            # 如果不足m个元素，则用0填充，并在mask_list中对应位置设置为0
+            while len(next_item_list) < self.next_seq_length:
+                next_item_list.append(0)
+                next_mask_list.append(0)
+
             data = {
                 'hist_item_list': torch.Tensor(hist_item_list).squeeze(0).long(),
                 'hist_mask_list': torch.Tensor(hist_mask_list).squeeze(0).long(),
-                'target_item': torch.Tensor([item_id]).long()
+                'target_item': torch.Tensor([item_id]).long(),
+                'next_item_list': torch.Tensor(next_item_list).squeeze(0).long(),
+                'next_mask_list': torch.Tensor(next_mask_list).squeeze(0).long()
             }
 
             for col in self.cate_cols:
                 data.update({f'hist_{col}_list': torch.Tensor(getattr(self, f'hist_{col}_list')).squeeze(0).long()})
         else:
             k = int(0.8 * len(item_list))
             if k >= self.max_length:  # 选取seq_len个物品
@@ -113,14 +125,96 @@
         for user in self.user2item:
             item_list = self.user2item[user]
             test_item_index = int(0.8 * len(item_list))
             self.test_gd[str(user)] = item_list[test_item_index:]
         return self.test_gd
 
 
+class SequenceDatasetV2(SequenceDataset):
+    def __init__(self, config, df, enc_dict=None, phase='train'):
+        super().__init__(config, df, enc_dict, phase)
+
+    def get_test_gd(self):
+        self.test_gd = dict()
+        for user in self.user2item:
+            item_list = self.user2item[user]
+            if self.phase == 'valid':
+                test_item_index = len(item_list) - 2
+            else:
+                test_item_index = len(item_list) - 1
+            self.test_gd[str(user)] = [item_list[test_item_index]]
+        return self.test_gd
+
+    def __getitem__(self, index):
+        user_id = self.user_list[index]
+        item_list = self.user2item[user_id]
+        hist_item_list = []
+        hist_mask_list = []
+        if self.phase == 'train':
+
+            k = random.choice(range(2, len(item_list) - 2))  # 从[4,len(item_list))中随机选择一个index
+            item_id = item_list[k]  # 该index对应的item加入item_id_list
+
+            if k >= self.max_length:  # 选取seq_len个物品
+                hist_item_list.append(item_list[k - self.max_length: k])
+                hist_mask_list.append([1.0] * self.max_length)
+                for col in self.cate_cols:
+                    cate_seq = getattr(self, f'user2{col}')[user_id]
+                    setattr(self, f'hist_{col}_list', cate_seq[k - self.max_length: k])
+            else:
+                hist_item_list.append(item_list[:k] + [0] * (self.max_length - k))
+                hist_mask_list.append([1.0] * k + [0.0] * (self.max_length - k))
+                for col in self.cate_cols:
+                    cate_seq = getattr(self, f'user2{col}')[user_id]
+                    setattr(self, f'hist_{col}_list', cate_seq[:k] + [0] * (self.max_length - k))
+            next_item_list = item_list[k:min(k + self.next_seq_length, len(item_list)-2)]
+            next_mask_list = [1] * len(next_item_list)
+
+            # 如果不足m个元素，则用0填充，并在mask_list中对应位置设置为0
+            while len(next_item_list) < self.next_seq_length:
+                next_item_list.append(0)
+                next_mask_list.append(0)
+
+            data = {
+                'hist_item_list': torch.Tensor(hist_item_list).squeeze(0).long(),
+                'hist_mask_list': torch.Tensor(hist_mask_list).squeeze(0).long(),
+                'target_item': torch.Tensor([item_id]).long(),
+                'next_item_list': torch.Tensor(next_item_list).squeeze(0).long(),
+                'next_mask_list': torch.Tensor(next_mask_list).squeeze(0).long()
+            }
+
+            for col in self.cate_cols:
+                data.update({f'hist_{col}_list': torch.Tensor(getattr(self, f'hist_{col}_list')).squeeze(0).long()})
+        else:
+            if self.phase == 'valid':
+                k = len(item_list) - 2
+            else:
+                k = len(item_list) - 1
+            if k >= self.max_length:  # 选取seq_len个物品
+                hist_item_list.append(item_list[k - self.max_length: k])
+                hist_mask_list.append([1.0] * self.max_length)
+                for col in self.cate_cols:
+                    cate_seq = getattr(self, f'user2{col}')[user_id]
+                    setattr(self, f'hist_{col}_list', cate_seq[k - self.max_length: k])
+            else:
+                hist_item_list.append(item_list[:k] + [0] * (self.max_length - k))
+                hist_mask_list.append([1.0] * k + [0.0] * (self.max_length - k))
+                for col in self.cate_cols:
+                    cate_seq = getattr(self, f'user2{col}')[user_id]
+                    setattr(self, f'hist_{col}_list', cate_seq[:k] + [0] * (self.max_length - k))
+            data = {
+                'user': str(user_id),
+                'hist_item_list': torch.Tensor(hist_item_list).squeeze(0).long(),
+                'hist_mask_list': torch.Tensor(hist_mask_list).squeeze(0).long(),
+            }
+            for col in self.cate_cols:
+                data.update({f'hist_{col}_list': torch.Tensor(getattr(self, f'hist_{col}_list')).squeeze(0).long()})
+        return data
+
+
 def seq_collate(batch):
     hist_item = torch.rand(len(batch), batch[0][0].shape[0])
     hist_mask = torch.rand(len(batch), batch[0][0].shape[0])
     item_list = []
     for i in range(len(batch)):
         hist_item[i, :] = batch[i][0]
         hist_mask[i, :] = batch[i][1]
```

## rec_pangu/models/sequence/__init__.py

```diff
@@ -15,7 +15,8 @@
 from .niser import NISER
 from .nextitnet import NextItNet
 from .stamp import STAMP
 from .gru4rec import GRU4Rec
 from .sine import SINE
 from .contrarec import ContraRec
 from .clrec import CLRec
+from .iocrec import IOCRec
```

## rec_pangu/models/sequence/cmi.py

```diff
@@ -29,15 +29,15 @@
         self.W = nn.Linear(self.embedding_dim, self.embedding_dim)
         self.selfatt_W = nn.Linear(self.n_interest, self.n_interest, bias=False)
         self.interest_embedding = nn.Embedding(self.n_interest, self.embedding_dim)
         self.temperature = 0.1
 
         self.gru = nn.GRU(
             input_size=self.embedding_dim,
-            hidden_size=self.hidden_size,
+            hidden_size=self.embedding_dim,
             num_layers=self.num_layers,
             bias=False,
             batch_first=True,
         )
         self.mlp = nn.Sequential(
             nn.Linear(self.embedding_dim, self.embedding_dim, bias=True),
             nn.ReLU()
```

## rec_pangu/models/sequence/re4.py

```diff
@@ -50,15 +50,15 @@
             is_training (bool): a flag variable to set the mode of the model; default is True.
 
         Returns:
             dict: a dictionary with the user embeddings and model loss (if training) as keys and the corresponding 
             tensors as values.
         """
         item_seq = data['hist_item_list']
-        item_mask = data['hist_mask_list']
+        item_mask = 1-data['hist_mask_list']
         dim0, dim1 = item_seq.shape
 
         item_seq_len = torch.sum(item_mask, dim=-1)
         item_seq = torch.reshape(item_seq, (1, dim0 * dim1))
         item_seq_emb = self.item_emb(item_seq)
         item_seq_emb = torch.reshape(item_seq_emb, (dim0, dim1, -1))
 
@@ -67,23 +67,14 @@
         proposals_weight_logits = proposals_weight.masked_fill(item_mask.unsqueeze(1).bool(), -1e9)
         proposals_weight = torch.softmax(proposals_weight_logits, dim=2)
         user_interests = torch.matmul(proposals_weight, torch.matmul(item_seq_emb, self.W2))
 
         if is_training:
             target_item = data['target_item']
             item_e = self.item_emb(target_item)
-            # main loss
-            cos_res = torch.bmm(user_interests, item_e.squeeze(1).unsqueeze(-1))
-            k_index = torch.argmax(cos_res, dim=1)
-
-            best_interest_emb = torch.rand(user_interests.shape[0], user_interests.shape[2]).to(self.device)
-            for k in range(user_interests.shape[0]):
-                best_interest_emb[k, :] = user_interests[k, k_index[k], :]
-
-            loss = self.calculate_loss(best_interest_emb, target_item.squeeze())
 
             # re-attend
             product = torch.matmul(user_interests, torch.transpose(item_seq_emb, 1, 2))
             product = product.masked_fill(item_mask.unsqueeze(1).bool(), -1e9)
             re_att = torch.softmax(product, dim=2)
             att_pred = F.log_softmax(proposals_weight_logits, dim=-1)
             loss_attend = -(re_att * att_pred).sum() / (re_att).sum()
@@ -136,18 +127,29 @@
                 [dim0, self.proposal_num, dim1, -1])
             target_emb = item_seq_emb.unsqueeze(1).repeat(1, self.proposal_num, 1, 1)
             loss_construct = self.recons_mse_loss(recons_item, target_emb)
             loss_construct = loss_construct.masked_fill((positive_weight_idx == 0).unsqueeze(-1), 0.)
             loss_construct = loss_construct.masked_fill(item_mask.unsqueeze(-1).unsqueeze(1).bool(), 0.)
             loss_construct = torch.mean(loss_construct)
 
+            user_interests = F.tanh(self.fc1(user_interests))
+            # main loss
+            cos_res = torch.bmm(user_interests, item_e.squeeze(1).unsqueeze(-1))
+            k_index = torch.argmax(cos_res, dim=1)
+
+            best_interest_emb = torch.rand(user_interests.shape[0], user_interests.shape[2]).to(self.device)
+            for k in range(user_interests.shape[0]):
+                best_interest_emb[k, :] = user_interests[k, k_index[k], :]
+
+            loss = self.calculate_loss(best_interest_emb, target_item.squeeze())
+
             loss = loss + self.att_lambda * loss_attend + self.ct_lambda * loss_contrastive + self.cs_lambda * loss_construct
             output_dict = {
-                'user_emb': user_interests,
                 'loss': loss
             }
         else:
+            user_interests = F.tanh(self.fc1(user_interests))
             output_dict = {
                 'user_emb': user_interests
             }
 
         return output_dict
```

## rec_pangu/models/sequence/sasrec.py

```diff
@@ -22,15 +22,15 @@
         self.attn_dropout_prob = config.get('attn_dropout_prob', 0.1)
         self.hidden_act = config.get('hidden_act', 'gelu')
         self.layer_norm_eps = config.get('layer_norm_eps', 0.001)
 
         self.self_attention = TransformerEncoder(
             n_layers=self.n_layers,
             n_heads=self.n_heads,
-            hidden_size=self.hidden_size,
+            hidden_size=self.embedding_dim,
             inner_size=self.inner_size,
             hidden_dropout_prob=self.hidden_dropout_prob,
             attn_dropout_prob=self.attn_dropout_prob,
             hidden_act=self.hidden_act,
             layer_norm_eps=self.layer_norm_eps
         )
```

## Comparing `rec_pangu-0.4.0.dist-info/LICENSE` & `rec_pangu-0.4.1.dist-info/LICENSE`

 * *Files identical despite different names*

## Comparing `rec_pangu-0.4.0.dist-info/METADATA` & `rec_pangu-0.4.1.dist-info/METADATA`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: rec-pangu
-Version: 0.4.0
+Version: 0.4.1
 Summary: Some Rank/Multi-task model implemented by Pytorch
 Home-page: https://github.com/HaSai666/rec_pangu
 Author: wk
 Author-email: 306178200@qq.com
 Keywords: rank,multi task,deep learning,pytorch,recsys,recommendation
 Platform: all
 Classifier: Intended Audience :: Developers
@@ -46,14 +46,17 @@
 [![Downloads](https://pepy.tech/badge/rec-pangu)](https://pepy.tech/project/rec-pangu)
 [![Downloads](https://pepy.tech/badge/rec-pangu/month)](https://pepy.tech/project/rec-pangu)
 [![Downloads](https://pepy.tech/badge/rec-pangu/week)](https://pepy.tech/project/rec-pangu)
 ## 1.开源定位 
 - 使用pytorch对经典的rank/多任务模型进行实现，并且对外提供统一调用的API接口，极大的降低了使用Rank/多任务模型的时间成本
 - 该项目使用了pytorch来实现我们的各种模型，以便于初学推荐系统的人可以更好的理解算法的核心思想
 - 由于已经有了很多类似的优秀的开源，我们这里对那些十分通用的模块参考了已有的开源，十分感谢这些开源贡献者的贡献 
+
+<img src='pic/overview.png'>
+
 ## 2.安装  
 ```bash
 #最新版
 git clone https://github.com/HaSai666/rec_pangu.git
 cd rec_pangu
 pip install -e . --verbose
```

### html2text {}

```diff
@@ -1,8 +1,8 @@
-Metadata-Version: 2.1 Name: rec-pangu Version: 0.4.0 Summary: Some Rank/Multi-
+Metadata-Version: 2.1 Name: rec-pangu Version: 0.4.1 Summary: Some Rank/Multi-
 task model implemented by Pytorch Home-page: https://github.com/HaSai666/
 rec_pangu Author: wk Author-email: 306178200@qq.com Keywords: rank,multi
 task,deep learning,pytorch,recsys,recommendation Platform: all Classifier:
 Intended Audience :: Developers Classifier: Intended Audience :: Education
 Classifier: Intended Audience :: Science/Research Classifier: Operating System
 :: OS Independent Classifier: Programming Language :: Python :: 3 Classifier:
 Programming Language :: Python :: 3.7 Classifier: Programming Language ::
@@ -34,48 +34,49 @@
 week)](https://pepy.tech/project/rec-pangu) ## 1.å¼æºå®ä½ -
 ä½¿ç¨pytorchå¯¹ç»å¸çrank/
 å¤ä»»å¡æ¨¡åè¿è¡å®ç°ï¼å¹¶ä¸å¯¹å¤æä¾ç»ä¸è°ç¨çAPIæ¥å£ï¼æå¤§çéä½äºä½¿ç¨Rank/
 å¤ä»»å¡æ¨¡åçæ¶é´ææ¬ -
 è¯¥é¡¹ç®ä½¿ç¨äºpytorchæ¥å®ç°æä»¬çåç§æ¨¡åï¼ä»¥ä¾¿äºåå­¦æ¨èç³»ç»çäººå¯ä»¥æ´å¥½ççè§£ç®æ³çæ ¸å¿ææ³
 -
 ç±äºå·²ç»æäºå¾å¤ç±»ä¼¼çä¼ç§çå¼æºï¼æä»¬è¿éå¯¹é£äºååéç¨çæ¨¡ååèäºå·²æçå¼æºï¼ååæè°¢è¿äºå¼æºè´¡ç®èçè´¡ç®
-## 2.å®è£ ```bash #ææ°ç git clone https://github.com/HaSai666/
-rec_pangu.git cd rec_pangu pip install -e . --verbose #ç¨³å®ç pip install
-rec_pangu --upgrade ``` ## 3.Rankæ¨¡å | æ¨¡å | è®ºæ | å¹´ä»½ | |---------
-|------|------| | WDL | [Wide & Deep Learning for Recommender Systems](https://
-arxiv.org/pdf/1606.07792) | 2016 | | DeepFM | [DeepFM: A Factorization-Machine
-based Neural Network for CTR Prediction](https://arxiv.org/pdf/1703.04247) |
-2017 | | NFM | [Neural Factorization Machines for Sparse Predictive Analytics]
-(https://arxiv.org/pdf/1708.05027.pdf) | 2017 | | FiBiNet | [FiBiNET: Combining
-Feature Importance and Bilinear Feature Interaction for Click-Through Rate]
-(https://arxiv.org/pdf/1905.09433.pdf) | 2019 | | AFM | [Attentional
-Factorization Machines](https://arxiv.org/pdf/1708.04617) | 2017 | | AutoInt |
-[AutoInt: Automatic Feature Interaction Learning via Self-Attentive Neural
-Networks](https://arxiv.org/pdf/1810.11921.pdf) | 2018 | | CCPM | [A
-Convolutional Click Prediction Model](http://www.shuwu.name/sw/Liu2015CCPM.pdf)
-| 2015 | | LR | / | 2019 | | FM | / | 2019 | | xDeepFM | [xDeepFM: Combining
-Explicit and Implicit Feature Interactions for Recommender Systems](https://
-arxiv.org/pdf/1803.05170.pdf) | 2018 | | DCN | [Deep & Cross Network for Ad
-Click Predictions](https://arxiv.org/pdf/1708.05123.pdf) | 2019 | | MaskNet |
-[MaskNet: Introducing Feature-Wise Multiplication to CTR Ranking Models by
-Instance-Guided Mask](https://arxiv.org/pdf/2102.07619.pdf) | 2021 | ##
-4.å¤ä»»å¡æ¨¡å | æ¨¡å | è®ºæ | å¹´ä»½ | |-------------|-----------------
+[pic/overview.png] ## 2.å®è£ ```bash #ææ°ç git clone https://github.com/
+HaSai666/rec_pangu.git cd rec_pangu pip install -e . --verbose #ç¨³å®ç pip
+install rec_pangu --upgrade ``` ## 3.Rankæ¨¡å | æ¨¡å | è®ºæ | å¹´ä»½ | |-
+--------|------|------| | WDL | [Wide & Deep Learning for Recommender Systems]
+(https://arxiv.org/pdf/1606.07792) | 2016 | | DeepFM | [DeepFM: A
+Factorization-Machine based Neural Network for CTR Prediction](https://
+arxiv.org/pdf/1703.04247) | 2017 | | NFM | [Neural Factorization Machines for
+Sparse Predictive Analytics](https://arxiv.org/pdf/1708.05027.pdf) | 2017 | |
+FiBiNet | [FiBiNET: Combining Feature Importance and Bilinear Feature
+Interaction for Click-Through Rate](https://arxiv.org/pdf/1905.09433.pdf) |
+2019 | | AFM | [Attentional Factorization Machines](https://arxiv.org/pdf/
+1708.04617) | 2017 | | AutoInt | [AutoInt: Automatic Feature Interaction
+Learning via Self-Attentive Neural Networks](https://arxiv.org/pdf/
+1810.11921.pdf) | 2018 | | CCPM | [A Convolutional Click Prediction Model]
+(http://www.shuwu.name/sw/Liu2015CCPM.pdf) | 2015 | | LR | / | 2019 | | FM | /
+| 2019 | | xDeepFM | [xDeepFM: Combining Explicit and Implicit Feature
+Interactions for Recommender Systems](https://arxiv.org/pdf/1803.05170.pdf) |
+2018 | | DCN | [Deep & Cross Network for Ad Click Predictions](https://
+arxiv.org/pdf/1708.05123.pdf) | 2019 | | MaskNet | [MaskNet: Introducing
+Feature-Wise Multiplication to CTR Ranking Models by Instance-Guided Mask]
+(https://arxiv.org/pdf/2102.07619.pdf) | 2021 | ## 4.å¤ä»»å¡æ¨¡å | æ¨¡å |
+è®ºæ | å¹´ä»½ | |-------------|----------------------------------------------
 -------------------------------------------------------------------------------
----------------------------------------------|------| | MMOE | [Modeling Task
-Relationships in Multi-task Learning with Multi-gate Mixture-of-Experts](https:
-//dl.acm.org/doi/pdf/10.1145/3219819.3220007) | 2018 | | ShareBottom |
-[Modeling Task Relationships in Multi-task Learning with Multi-gate Mixture-of-
-Experts](https://dl.acm.org/doi/pdf/10.1145/3219819.3220007) | 2018 | | ESSM |
-[Entire Space Multi-Task Model: An Effective Approach for Estimating Post-Click
-Conversion Rate](https://arxiv.org/pdf/1804.07931.pdf) | 2018 | | OMOE |
-[Modeling Task Relationships in Multi-task Learning with Multi-gate Mixture-of-
-Experts](https://dl.acm.org/doi/pdf/10.1145/3219819.3220007) | 2018 | | MLMMOE
-| / | / | | AITM | [Modeling the Sequential Dependence among Audience Multi-
-step Conversions with Multi-task Learning in Targeted Display Advertising]
-(https://arxiv.org/pdf/2105.08489.pdf)| 2019 | ## 5.åºåå¬åæ¨¡å
+----------------|------| | MMOE | [Modeling Task Relationships in Multi-task
+Learning with Multi-gate Mixture-of-Experts](https://dl.acm.org/doi/pdf/
+10.1145/3219819.3220007) | 2018 | | ShareBottom | [Modeling Task Relationships
+in Multi-task Learning with Multi-gate Mixture-of-Experts](https://dl.acm.org/
+doi/pdf/10.1145/3219819.3220007) | 2018 | | ESSM | [Entire Space Multi-Task
+Model: An Effective Approach for Estimating Post-Click Conversion Rate](https:/
+/arxiv.org/pdf/1804.07931.pdf) | 2018 | | OMOE | [Modeling Task Relationships
+in Multi-task Learning with Multi-gate Mixture-of-Experts](https://dl.acm.org/
+doi/pdf/10.1145/3219819.3220007) | 2018 | | MLMMOE | / | / | | AITM | [Modeling
+the Sequential Dependence among Audience Multi-step Conversions with Multi-task
+Learning in Targeted Display Advertising](https://arxiv.org/pdf/
+2105.08489.pdf)| 2019 | ## 5.åºåå¬åæ¨¡å
 ç®åæ¯æå¦ä¸ç±»åçåºåå¬åæ¨¡å: - ç»å¸åºåå¬åæ¨¡å -
 åºäºå¾çåºåå¬åæ¨¡å - å¤å´è¶£åºåå¬åæ¨¡å -
 åºäºLLMçåºåå¬åæ¨¡å | æ¨¡å | ç±»å | è®ºæ | å¹´ä»½ | |---------
 --------|---------|------------------------------------------------------------
 -------------------------------------------------------------------------------
 -----------------------------------------------------------------|------| |
 YotubeDNN | ç»å¸åºåå¬å | [Deep Neural Networks for YouTube
```

## Comparing `rec_pangu-0.4.0.dist-info/RECORD` & `rec_pangu-0.4.1.dist-info/RECORD`

 * *Files 4% similar despite different names*

```diff
@@ -1,21 +1,21 @@
 .DS_Store,sha256=0hy_E_6euFbvNj_m5J9LmFXpd3MdTofbcc5rjK6vQUI,6148
 rec_pangu/.DS_Store,sha256=j04wnLcsoeH6cNvpk1rcigZPwx61H1Uqp1PhfSuJcu0,6148
-rec_pangu/__init__.py,sha256=CEEvKPfo_0ZKPGnu2W1tvwRHhsiIvMXNUMTSZw5cypE,199
+rec_pangu/__init__.py,sha256=6U2GAbsvf-7jsaPcyfx7AlxFGTGmsPf1XvDCPhsx6MA,199
 rec_pangu/benchmark_trainer.py,sha256=J5kGL93iq4O7TW1k60oJZksrYRW4OniJa-JoVjmmelk,5837
 rec_pangu/gpt_ranktrainer.py,sha256=cHLZu4OgbpqL9vVMqtR1pvqMeNpLtyO_EdaJHMjWq08,9722
 rec_pangu/model_pipeline.py,sha256=y2_EnjqjU0jY1VpVC6r9sUYePtdQlpqznSskCmxMDn4,14630
 rec_pangu/old_ranktrainer.py,sha256=lMwui-xDAxMvyuxJyp8XKCV3BtQGWl39zbIL_bj6LUM,5448
 rec_pangu/trainer.py,sha256=VTpGls-SaFh5rFsfD-VSI_Hx2AdoOZHl_OeA6aH_Xyw,20415
-rec_pangu/dataset/__init__.py,sha256=o53SpwAQVsmLQp7CUWmIy7kikwH3ZBD-VBrW_5p2ToA,404
+rec_pangu/dataset/__init__.py,sha256=Dv01P5Otf1UGMNJPXPqFdrK5EPd3e2mwRqjYCd43Uu8,434
 rec_pangu/dataset/base_dataset.py,sha256=iImLln0AwCbDWizCuJzahz5eL1etRU7XP0Y2vklLzRs,4954
 rec_pangu/dataset/graph_dataset.py,sha256=Sz_PB9ibzf8X3zBncN8ozXY40k1Q5-ceRAWW0X5LMmk,3776
 rec_pangu/dataset/multi_task_dataset.py,sha256=XnQUTsvwWcYYWq52OtaBfpgfQ0jvLzaowaISNUhMjWg,2812
-rec_pangu/dataset/process_data.py,sha256=WzBtgc2pCmDXGAEQw0Pv2nvTb2s3uXCKn5C4D4qt0zg,3968
-rec_pangu/dataset/sequence_dataset.py,sha256=49RX6alVRWh0IVApkHJhYm2MqGbaH9jQif6TrSFYvJU,5640
+rec_pangu/dataset/process_data.py,sha256=X-myBTFhQsr_9BTUYTxjh4XuBDYmDwLG7PNo2_NRLgw,4908
+rec_pangu/dataset/sequence_dataset.py,sha256=IyemY9fskZ8HXLTaV9uNwLZpmRJDLNdWhMQbfv1HCuM,10311
 rec_pangu/models/.DS_Store,sha256=hLwUnXW0G-Hg5Cs7UvY9nZifdT9p5Tonix-IKykPbTc,6148
 rec_pangu/models/__init__.py,sha256=ZTUYJ8guxLRAXkbK6TxxvRJw8Q153cUDA1XBcJwjqsI,117
 rec_pangu/models/base_model.py,sha256=uKXHZsLfstCAJMkTwncLhZi14gHhy45PWAFarhZ_P3M,11105
 rec_pangu/models/utils.py,sha256=wJoHG-NfvXhxZnv2P_4VFe1j2gr1xAhx4R0cvjDen3w,8154
 rec_pangu/models/layers/LGConv.py,sha256=ZvqwTQ0zkmCK5XdwNZNChytFrCUtNT-AAFNqxp2WtLA,949
 rec_pangu/models/layers/__init__.py,sha256=5nIwNU6Rh0hFRypJV6wAbpQhVmRm3nxlP-c0P9sVEt0,456
 rec_pangu/models/layers/activation.py,sha256=vXcVU5Cvk-Xmoxzhv6A3sk-zjHY5z1Is488SlBhTjPY,1876
@@ -48,36 +48,37 @@
 rec_pangu/models/ranking/fm.py,sha256=oM3SDc4v7oGEXSLae-DXhuQ0JBfcG4CLQpMOtk_rgUI,1928
 rec_pangu/models/ranking/lightgcn.py,sha256=zjv5b7N95UxuZpFtzGTYg2IvoecS0Lkrim1-reE45QA,1920
 rec_pangu/models/ranking/lr.py,sha256=vFLQIjYSkTID25srDlpjhkMxxU0bC04M-s_ANf5i3cw,1700
 rec_pangu/models/ranking/masknet.py,sha256=v_RrG4J4Qz231UIZBAglBwBT5UOUmlkM8F_9QsPCMKc,3831
 rec_pangu/models/ranking/nfm.py,sha256=JH1cND8VWdYq0CCSeDZRWHJUqWlEKpFkLLyZ46OCVmA,2909
 rec_pangu/models/ranking/wdl.py,sha256=BYAYnbmPi3W8-WMIbcdPXlcAJ02ij2Wjyj34pODaBaA,2819
 rec_pangu/models/ranking/xdeepfm.py,sha256=mLoc1zrM2hWeoQLc9p-SFFeHb96o23cRk3IJRpR0Olg,3196
-rec_pangu/models/sequence/__init__.py,sha256=-OYpPynJWbXp4M2JkqO00xTGMdMoqC3hH-Nv5cZh9vQ,547
+rec_pangu/models/sequence/__init__.py,sha256=PK-dmvWFWYGeqW9A1-0yV4D0o7JKyGO1R3lMIbmsRNQ,574
 rec_pangu/models/sequence/clrec.py,sha256=WVmSGxUIUJl0FB3b3EQ6yoHEvR6UXQWoauZOHzgYhsM,3682
-rec_pangu/models/sequence/cmi.py,sha256=-l9okOs8FL4kY_wvUBqbVjBRrWBX8XecCbCV_F5n4M0,9043
+rec_pangu/models/sequence/cmi.py,sha256=5E7k3oQrj0rRiwtMVT4Zf9DWawgON286pZVRau_79lc,9045
 rec_pangu/models/sequence/comirec.py,sha256=ThfdQWVysmmnW94wIRI9vRuyjLDjEa9ZH7EO4zRgaGE,4832
 rec_pangu/models/sequence/contrarec.py,sha256=5W3Zh4FUYq1Q5f9dNgCVrSV0mMAsyvGNsUGyFqtUy20,6913
 rec_pangu/models/sequence/gcsan.py,sha256=f_qEoV8muiUwNbyp0jtIf66lHOSfi0Mi8DQGC6tdAUU,3841
 rec_pangu/models/sequence/gru4rec.py,sha256=kt1pv2FXfvYpYSgs4MlyaNkDEVQgZpKIaU2jT2EuN8s,1706
+rec_pangu/models/sequence/iocrec.py,sha256=4W7GRAV1-VihouRLhlvnSAr3x6yTY8l-OPdyXgQ_u5I,20561
 rec_pangu/models/sequence/mind.py,sha256=95-9NPRtyDRMoM3hVxkfGws-jZP0uGKYhW6t3tkvI-U,2459
 rec_pangu/models/sequence/narm.py,sha256=QhuMhqxxnlm_T0JpxyY_bFdztr2snOwG9OXOPMxmssk,3011
 rec_pangu/models/sequence/nextitnet.py,sha256=rlWRxpg_dm73TxWfhOAJePGkM5TLQ-5Ir3ueJeCjG3M,2180
 rec_pangu/models/sequence/niser.py,sha256=YRo9wA0rN-K8K-NuODpH3L2Slh7WMAYTMDjBZH0xGcM,3635
-rec_pangu/models/sequence/re4.py,sha256=aw0iNyxreDc5CrEjNJoQi1-pclkF3EgiUyNOF4hgDK8,8090
-rec_pangu/models/sequence/sasrec.py,sha256=TiyChBDEqw98QHJivqw9Y1hI7mRWYHlrkfgeWebJLHM,2814
+rec_pangu/models/sequence/re4.py,sha256=r8c93pAVcYRhnidVZ0XnqDWCWZ62KuBF6RbyenjTopY,8173
+rec_pangu/models/sequence/sasrec.py,sha256=LPUpx00VdZ9OXh6Ic9-CLV9ItqLX3Asf2Kn-1U2bKG0,2816
 rec_pangu/models/sequence/sine.py,sha256=JG2Cdtu-gZ6b_oMZdPsKeK70S4o7RVb9qTvhjGRlOdc,4679
 rec_pangu/models/sequence/srgnn.py,sha256=69-a9si8O4CXD_oW5au3XyCGMMoKJ1heKPROX-L0n_M,3048
 rec_pangu/models/sequence/stamp.py,sha256=PTethq9B9yea5Ihw7YvC3e1owIWo8y4bO9j-7BoZmtI,1813
 rec_pangu/models/sequence/yotubednn.py,sha256=TMKwhKp11_vcWp3OH1rIh-9vw7oxjBtgWc7y4_ZMzVk,1621
 rec_pangu/serving/__init__.py,sha256=twv9kol32GZD16l-6Xu5hiaYDHK72UIA1GGJmwcJi2A,115
 rec_pangu/serving/ranking_server.py,sha256=IR3IrhFEVPXnngaMCerunExdoo3XeeAryziEgwjOQQI,2211
 rec_pangu/utils/__init__.py,sha256=dF7uxiBraZf82Do_VlkQqicD4WsNcguSgebsO6dyyag,301
 rec_pangu/utils/check_version.py,sha256=ONSdf0hw-hPTHjJpHldDz5HdlNHo4kqrm2icjDPkUtg,1509
 rec_pangu/utils/evaluate.py,sha256=pJtXGTeIa4QTNUx23_2boFJZ97gXPuOj2wkiEuGuVKw,8857
 rec_pangu/utils/gpu_utils.py,sha256=V5-yE_XaHjGELIElhr5J9MWNTeaJS1HiA37CwF3tvrM,1565
 rec_pangu/utils/json_utils.py,sha256=2NbJgYTDACZXQIX11h5U4l0L5fHwyXNVdVksgWwiccI,668
-rec_pangu-0.4.0.dist-info/LICENSE,sha256=5t15gkJb51GDxbB-lVcG2ZMTYLHbccXypC-X_GK-kPw,1058
-rec_pangu-0.4.0.dist-info/METADATA,sha256=hx4d5G9sfqE2MtMCyWIMrQ9EBTqc9QFMblk2uFhq94w,17906
-rec_pangu-0.4.0.dist-info/WHEEL,sha256=2wepM1nk4DS4eFpYrW1TTqPcoGNfHhhO_i5m4cOimbo,92
-rec_pangu-0.4.0.dist-info/top_level.txt,sha256=VQeGX3ukDrCpoYzPshLjJWRzJ6lzbymbmBqfQQCUOYE,10
-rec_pangu-0.4.0.dist-info/RECORD,,
+rec_pangu-0.4.1.dist-info/LICENSE,sha256=5t15gkJb51GDxbB-lVcG2ZMTYLHbccXypC-X_GK-kPw,1058
+rec_pangu-0.4.1.dist-info/METADATA,sha256=CBfg9xw9LejWadQEC_ogZii-7_cI4qJmWPgF0txw9y0,17937
+rec_pangu-0.4.1.dist-info/WHEEL,sha256=2wepM1nk4DS4eFpYrW1TTqPcoGNfHhhO_i5m4cOimbo,92
+rec_pangu-0.4.1.dist-info/top_level.txt,sha256=VQeGX3ukDrCpoYzPshLjJWRzJ6lzbymbmBqfQQCUOYE,10
+rec_pangu-0.4.1.dist-info/RECORD,,
```

